% This file was created with JabRef 2.10.
% Encoding: UTF-8


@InProceedings{Coban2016,
  Title                    = {T\"urk\cce Şarki S\"ozlerinden M\"uzik T\"ur\"u Siniflandirmasi},
  Author                   = {{\c{C}}oban, {\"{O}}nder and {\"{O}}zyer, G{\"{u}}lşah T{\"{u}}m{\"{u}}kl{\"{u}}},
  Year                     = {2016},
  Month                    = {jun},
  Pages                    = {101--104},
  Publisher                = {Institute of Electrical and Electronics Engineers Inc.},

  Abstract                 = {The amount of music in digital form increases due to the improvement of internet and recording technologies. With this increase, the automatic organization of musics has emerged as a problem needs to be solved. For this reason, Music Information Retrieval (MIR) is commonly studied research area in recent years. In this context, with the developed Music Information Systems solution is sought for some problems such as automatic playlist creation, hit song detection, music genre or mood classification etc. In previous works, meta-data information, melodic or textual content (lyrics) of music used for feature extraction. Also, it is seen that song lyrics not commonly used and number of work in this area is not enough for Turkish. In this paper, Turkish lyrics data set created and used for automatic music genre classification. Experimental results have been conducted on support vector machines (SVM) and the effect of feature model on results has been investigated in music genre classification which considered as a classical text classification problem. The features are extracted from three different models which are Structural and Statistical Text Features (SSTF), Bag of Words (BoW) and NGram. The results shows that lyrics can be effective for Turkish music genre classification.},
  Doi                      = {10.1109/SIU.2016.7495686},
  ISBN                     = {9781509016792}
}

@Article{Sormaz2010,
  Title                    = {Recognition of interacting volumetric features using 2D hints},
  Author                   = {{\v{S}}ormaz, Du{\v{s}}an N. and Tennety, Chandu},
  Year                     = {2010},

  Month                    = {jan},
  Number                   = {2},
  Pages                    = {131--141},
  Volume                   = {30},

  Abstract                 = {Purpose - Recognition of machining features is an essential step in the development of efficient-automated process plans from solid modeling data. This process represents the effective interpretation of the geometric data in a computer-aided design (CAD) model to create semantically rich manufacture-oriented features such as holes, slots, pockets, and others that may be exploited in downstream computer-aided manufacturing/ computer-aided process planning applications. Most successful approaches towards feature recognition have been based on hint-based procedures operating on a 3D B-Rep model. The purpose of this paper is to propose an approach by which features are identified in a solid model that is built mainly using sweep solid modeling operations. Design/methodology/approach - Part geometric model is queried for both 2D and 3D geometric elements. Feature hints are generated by an analysis of sweep operations and their 2D sketches, which are defined prior to building the solid model. These hints are then analyzed and validated by applying a two-phase approach: 2D validation in the sketch geometry; and 3D validation in the final constructive solid geometry tree of the solid model. Valid hints are the basis for the creation of a machining feature model that can be input to a process planning module. In addition, interaction information for machining features is extracted from both 2D hints and their 3D validation. Feature interaction information is obtained by analysis of face/edge neighborhood and their geometric relations in both 2D and 3D spaces. Findings - This approach provides a benefit of performing the majority of geometric analysis in 2D space which is much simpler and computationally more efficient than corresponding analyses in 3D space. Only minimal portion of the analysis is computed on 3D solid models. The approach is implemented in the Java-based prototype system and is demonstrated and tested on several real-world examples. Research limitations/implications - The initial prototype implementation is limited to prismatic parts and linear sweep. Only hole and slot feature can be recognized due to the fact that pocket recognition appears to be trivial. Practical implications - Motivation for this approach is in the fact that sweep operations from 2D sketches are very commonly used in the mechanical design process, so the approach may be applicable in practical applications of CAD. Originality/value - This novel approach provides value to product designers and manufacturing planners since linear (extrusion) and circular (rotation) sweeps are very popular design engineer tools. {\textcopyright} Emerald Group Publishing Limited.},
  Doi                      = {10.1108/01445151011029763},
  ISSN                     = {01445154}
}

@Article{Acher2014,
  Title                    = {Extraction and evolution of architectural variability models in plugin-based systems},
  Author                   = {Acher, Mathieu and Cleve, Anthony and Collet, Philippe and Merle, Philippe and Duchien, Laurence and Lahire, Philippe},
  Year                     = {2014},

  Month                    = {sep},
  Number                   = {4},
  Pages                    = {1367--1394},
  Volume                   = {13},

  Abstract                 = {Variability management is a key issue when building and evolving software-intensive systems, making it possible to extend, configure, customize and adapt such systems to customers' needs and specific deployment contexts. A wide form of variability can be found in extensible software systems, typically built on top of plugin-based architectures that offer a (large) number of configuration options through plugins. In an ideal world, a software architect should be able to generate a system variant on-demand, corresponding to a particular assembly of plugins. To this end, the variation points and constraints between architectural elements should be properly modeled and maintained over time (i.e., for each version of an architecture). A crucial, yet error-prone and time-consuming, task for a software architect is to build an accurate representation of the variability of an architecture, in order to prevent unsafe architectural variants and reach the highest possible level of flexibility. In this article, we propose a reverse engineering process for producing a variability model (i.e., a feature model) of a plugin-based architecture. We develop automated techniques to extract and combine different variability descriptions, including a hierarchical software architecture model, a plugin dependency model and the software architect knowledge. By computing and reasoning about differences between versions of architectural feature models, software architect can control both the variability extraction and evolution processes. The proposed approach has been applied to a representative, large-scale plugin-based system (FraSCAti), considering different versions of its architecture. We report on our experience in this context.},
  Doi                      = {10.1007/s10270-013-0364-2},
  ISSN                     = {16191374},
  Publisher                = {Springer Verlag}
}

@InProceedings{Acher2010,
  Title                    = {Managing variability in workflow with feature model composition operators},
  Author                   = {Acher, Mathieu and Collet, Philippe and Lahire, Philippe and France, Robert},
  Year                     = {2010},
  Pages                    = {17--33},
  Volume                   = {6144 LNCS},

  Abstract                 = {In grid-based scientific applications, building a workflow essentially involves composing parameterized services describing families of services and then configuring the resulting workflow product line. In domains (e.g., medical imaging) in which many different kinds of highly parameterized services exist, there is a strong need to manage variabilities so that scientists can more easily configure and compose services with consistency guarantees. In this paper, we propose an approach in which variable points in services are described with several separate feature models, so that families of workflow can be defined as compositions of feature models. A compositional technique then allows reasoning about the compatibility between connected services to ensure consistency of an entire workflow, while supporting automatic propagation of variability choices when configuring services. {\textcopyright} 2010 Springer-Verlag.},
  Doi                      = {10.1007/978-3-642-14046-4_2},
  ISBN                     = {3642140459},
  ISSN                     = {03029743}
}

@InProceedings{Acher2013,
  Title                    = {FAMILIAR: A domain-specific language for large scale management of feature models},
  Author                   = {Acher, Mathieu and Collet, Philippe and Lahire, Philippe and France, Robert B.},
  Year                     = {2013},
  Month                    = {jun},
  Number                   = {6},
  Pages                    = {657--681},
  Volume                   = {78},

  Abstract                 = {The feature model formalism has become the de facto standard for managing variability in software product lines (SPLs). In practice, developing an SPL can involve modeling a large number of features representing different viewpoints, sub-systems or concerns of the software system. This activity is generally tedious and error-prone. In this article, we present FAMILIAR a Domain-Specific Language (DSL) that is dedicated to the large scale management of feature models and that complements existing tool support. The language provides a powerful support for separating concerns in feature modeling, through the provision of composition and decomposition operators, reasoning facilities and scripting capabilities with modularization mechanisms. We illustrate how an SPL consisting of medical imaging services can be practically managed using reusable FAMILIAR scripts that implement reasoning mechanisms. We also report on various usages and applications of FAMILIAR and its operators, to demonstrate their applicability to different domains and use for different purposes. {\textcopyright} 2012 Elsevier B.V. All rights reserved.},
  Doi                      = {10.1016/j.scico.2012.12.004},
  ISSN                     = {01676423}
}

@InProceedings{Acher2011,
  Title                    = {Modeling variability from requirements to runtime},
  Author                   = {Acher, Mathieu and Collet, Philippe and Lahire, Philippe and Moisan, Sabine and Rigault, Jean Paul},
  Year                     = {2011},
  Pages                    = {77--86},

  Abstract                 = {In software product line (SPL) engineering, a software configuration can be obtained through a valid selection of features represented in a feature model (FM). With a strong separation between requirements and reusable components and a deep impact of high level choices on technical parts, determining and configuring an well-adapted software configuration is a long, cumbersome and error-prone activity. This paper presents a modeling process in which variability sources are separated in different FMs and inter-related by propositional constraints while consistency checking and propagation of variability choices are automated. We show how the variability requirements can be expressed and then refined at design time so that the set of valid software configurations to be considered at runtime may be highly reduced. Software tools support the approach and some experimentations on a video surveillance SPL are also reported. {\textcopyright} 2011 IEEE.},
  Doi                      = {10.1109/ICECCS.2011.15},
  ISBN                     = {9780769543819}
}

@InProceedings{Ahlqvist2011,
  Title                    = {On the (limited) difference between feature and geometric semantic similarity models},
  Author                   = {Ahlqvist, Ola},
  Year                     = {2011},
  Pages                    = {124--132},
  Volume                   = {6631 LNCS},

  Abstract                 = {Semantic similarity assessment is central to many geographic information analysis tasks. A reader of the geographic information science literature on semantic similarity assessment processes could easily get the impression that two of the most common approaches, the feature model and the geometric model, are incompatible and radically different. Through a review of literature I seek to elaborate on and clarify that these two approaches are in fact compatible, and I finish with a brief discussion of the handling of uncertain and missing values in these representations. {\textcopyright} 2011 Springer-Verlag.},
  Doi                      = {10.1007/978-3-642-20630-6_8},
  ISBN                     = {9783642206290},
  ISSN                     = {03029743}
}

@InProceedings{Al-Bashayreh2012,
  Title                    = {Feature model to design application framework for context-aware mobile patient monitoring systems},
  Author                   = {Al-Bashayreh, Mahmood Ghaleb and Hashim, Nor Laily and Khorma, Ola Taiseer},
  Year                     = {2012},
  Pages                    = {72--77},

  Abstract                 = {The objective of this paper is to present a feature model as a main deliverable of a domain analysis for context-aware Mobile Patient Monitoring Systems (MPMS). This model is a part of ongoing work to design an application framework to develop context-aware MPMS. These systems will enable elderly populations and patients with chronic diseases to undertake monitoring of themselves during their daily life. Unfortunately, developing these systems is very complex. An application framework, as an ideal reuse technique, is one of the most suitable solutions to simplify the development of such systems and overcome their development complexity. The scope of this paper is limited to construction process for context-aware MPMS feature model. The expected benefits of the resulted model are twofold. First, it enhances the understanding of the domain of context-aware MPMS. Second, it supports designing frameworks that satisfy the main characteristics of application frameworks, which are framework extensibility and reusability. {\textcopyright} 2012 IEEE.},
  Doi                      = {10.1109/IECBES.2012.6498052},
  ISBN                     = {9781467316668}
}

@Article{Ali2016,
  Title                    = {A Machine Learning Approach to Meter Placement for Power Quality Estimation in Smart Grid},
  Author                   = {Ali, Sardar and Wu, Kui and Weston, Kyle and Marinakis, Dimitri},
  Year                     = {2016},

  Month                    = {may},
  Number                   = {3},
  Pages                    = {1552--1561},
  Volume                   = {7},

  Abstract                 = {Due to the high-measuring cost, the monitoring of power quality (PQ) is nontrivial. This paper is aimed at reducing the cost of PQ monitoring in power network. Using a real-world PQ dataset, this paper adopts a learn-from-data approach to obtain a device latent feature model, which captures the device behavior as a PQ transition function. With the latent feature model, the power network could be modeled, in analogy, as a data-driven network, which presents the opportunity to use the well-investigated network monitoring and data estimation algorithms to solve the network quality monitoring problem in power grid. Based on this network model, algorithms are proposed to intelligently place measurement devices on suitable power links to reduce the uncertainty of PQ estimation on unmonitored power links. The meter placement algorithms use entropy-based measurements and Bayesian network models to identify the most suitable power links for PQ meter placement. Evaluation results on various simulated networks including IEEE distribution test feeder system show that the meter placement solution is efficient, and has the potential to significantly reduce the uncertainty of PQ values on unmonitored power links.},
  Doi                      = {10.1109/TSG.2015.2442837},
  ISSN                     = {19493053},
  Publisher                = {Institute of Electrical and Electronics Engineers Inc.}
}

@Article{Al-MsieDeen2014,
  Title                    = {Automatic documentation of [Mined] feature implementations from source code elements and use-case diagrams with the REVPLINE approach},
  Author                   = {Al-Msie'Deen, R. and Huchard, M. and Seriai, A. D. and Urtado, C. and Vauttier, S.},
  Year                     = {2014},

  Month                    = {dec},
  Number                   = {10},
  Pages                    = {1413--1438},
  Volume                   = {24},

  Abstract                 = {Companies often develop a set of software variants that share some features and differ in others to meet specific requirements. To exploit the existing software variants as a Software Product Line (SPL), a Feature Model of this SPL must be built as a first step. To do so, it is necessary to define and document the optional and mandatory features that compose the variants. In our previous work, we mined a set of feature implementations as identified sets of source code elements. In this paper, we propose a complementary approach, which aims to document the mined feature implementations by giving them names and descriptions, based on the source code elements that form feature implementations and the use-case diagrams that specify software variants. The novelty of our approach is its use of commonality and variability across software variants, at feature implementation and use-case levels, to run Information Retrieval methods in an efficient way. Experiments on several real case studies (Mobile media and ArgoUML-SPL) validate our approach and show promising results.},
  Doi                      = {10.1142/S0218194014400142},
  ISSN                     = {02181940},
  Publisher                = {World Scientific Publishing Co. Pte Ltd}
}

@InProceedings{Al-MsieDeen2013,
  Title                    = {Mining features from the object-oriented source code of software variants by combining lexical and structural similarity},
  Author                   = {Al-Msie'Deen, R. and Seriai, A. D. and Huchard, M. and Urtado, C. and Vauttier, S.},
  Year                     = {2013},
  Pages                    = {586--593},
  Publisher                = {IEEE Computer Society},

  Abstract                 = {Migrating software product variants which are deemed similar into a product line is a challenging task with main impact in software reengineering. To exploit existing software variants to build a software product line (SPL), the first step is to mine the feature model of this SPL which involves extracting common and optional features. Thus, we propose, in this paper, a new approach to mine features from the object-oriented source code of software variants by using lexical and structural similarity. To validate our approach, we applied it on ArgoUML, Health Watcher and Mobile Media software. The results of this evaluation showed that most of the features were identified1. {\textcopyright} 2013 IEEE.},
  Doi                      = {10.1109/IRI.2013.6642522},
  ISBN                     = {9781479910502}
}

@InProceedings{AL-Msiedeen2013,
  Title                    = {Mining features from the object-oriented source code of a collection of software variants using formal concept analysis and latent semantic indexing},
  Author                   = {AL-Msie'deen, R. and Seriai, A. D. and Huchard, M. and Urtado, C. and Vauttier, S. and Salman, H. Eyal},
  Year                     = {2013},
  Number                   = {January},
  Pages                    = {244--249},
  Publisher                = {Knowledge Systems Institute Graduate School},

  Abstract                 = {Companies often develop a set of software variants that share some features and differ in other ones to meet specific requirements. To exploit existing software variants and build a software product line (SPL), a feature model of this SPL must be built as a first step. To do so, it is necessary to mine optional and mandatory features from the source code of the software variants. Thus, we propose, in this paper, a new approach to mine features from the object-oriented source code of a set of software variants based on Formal Concept Analysis and Latent Semantic Indexing. To validate our approach, we applied it on ArgoUML and Mobile Media case studies. The results of this evaluation validate the relevance and the performance of our proposal as most of the features were correctly identified.},
  ISSN                     = {23259086}
}

@Article{Araar2016,
  Title                    = {Software features extraction from object-oriented source code using an overlapping clustering approach},
  Author                   = {Araar, Imad Eddine and Seridi, Hassina},
  Year                     = {2016},

  Month                    = {jun},
  Number                   = {2},
  Pages                    = {245--255},
  Volume                   = {40},

  Abstract                 = {For many decades, numerous organizations have launched software reuse initiatives to improve their productivity. Software product lines (SPL) addressed this problem by organizing software development around a set of features that are shared by a set of products. In order to exploit existing software products for building a new SPL, features composing each of the used products must be specified in the first place. In this paper we analyze the effectiveness of overlapping clustering based technique to mine functional features from object-oriented (OO) source code of existing systems. The evaluation of the proposed approach using two different Java open-source applications, i.e. "Mobile media" and "Drawing Shapes", has revealed encouraging results.},
  ISSN                     = {03505596},
  Publisher                = {Slovene Society Informatika}
}

@InProceedings{Aranega2012,
  Title                    = {Using feature model to build model transformation chains},
  Author                   = {Aranega, Vincent and Etien, Anne and Mosser, Sebastien},
  Year                     = {2012},
  Pages                    = {562--578},
  Volume                   = {7590 LNCS},

  Abstract                 = {Model transformations are intrinsically related to model-driven engineering. According to the increasing size of standardised meta-model, large transformations need to be developed to cover them. Several approaches promote separation of concerns in this context, that is, the definition of small transformations in order to master the overall complexity. Unfortunately, the decomposition of transformations into smaller ones raises new issues: organising the increasing number of transformations and ensuring their composition (i.e. the chaining). In this paper, we propose to use feature models to classify model transformations dedicated to a given business domain. Based on this feature models, automated techniques are used to support the designer, according to two axis: (i)the definition of a valid set of model transformations and (ii) the generation of an executable chain of model transformation that accurately implement designer's intention. This approach is validated on Gaspard2, a tool dedicated to the design of embedded system. {\textcopyright} 2012 Springer-Verlag.},
  Doi                      = {10.1007/978-3-642-33666-9_36},
  ISBN                     = {9783642336652},
  ISSN                     = {03029743}
}

@InProceedings{Arcaini2016,
  Title                    = {Automatic Detection and Removal of Conformance Faults in Feature Models},
  Author                   = {Arcaini, Paolo and Gargantini, Angelo and Vavassori, Paolo},
  Year                     = {2016},
  Month                    = {jul},
  Pages                    = {102--112},
  Publisher                = {Institute of Electrical and Electronics Engineers Inc.},

  Abstract                 = {Building a feature model for an existing SPL can improve the automatic analysis of the SPL and reduce the effort in maintenance. However, developing a feature model can be error prone, and checking that it correctly identifies each actual product of the SPL may be unfeasible due to the huge number of possible configurations. We apply mutation analysis and propose a method to detect and remove conformance faults by selecting special configurations that distinguish a feature model from its mutants. We propose a technique that, by iterating this process, is able to repair a faulty model. We devise several variations of a simple hill climbing algorithm for automatic fault removal and we compare them by a series of experiments on three different sets of feature models. We find that our technique is able to improve the conformance of around 90{\%} of the models and find the correct model in around 40{\%} of the cases.},
  Doi                      = {10.1109/ICST.2016.10},
  ISBN                     = {9781509018260}
}

@InProceedings{Ardagna2016,
  Title                    = {A certification technique for cloud security adaptation},
  Author                   = {Ardagna, Claudio A. and Asal, Rasool and Damiani, Ernesto and {El Ioini}, Nabil and Pahl, Claus and Dimitrakos, Theo},
  Year                     = {2016},
  Month                    = {aug},
  Pages                    = {324--331},
  Publisher                = {Institute of Electrical and Electronics Engineers Inc.},

  Abstract                 = {Unpredictability of cloud computing due to segregation of visibility and control between applications, data owners, and cloud providers increases tenants' uncertainty when using cloud services. Adaptation techniques become fundamental to provide a reliable cloud-based infrastructure with definite behavior, which preserves a stable quality of service for tenants. Existing adaptation techniques mostly focus on performance properties and are based on unverifiable evidence, which is collected in an untrusted way. In this paper, we propose a security-oriented adaptation technique for the cloud, based on evidence collected by means of a reliable certification process. Our approach adapts the cloud to maintain stable security properties over time, by continuously verifying certificate validity. It uses the output of verification activities to index a feature model, where equivalent configurations are used as the basis for adaptation. We also provide an analysis of the approach on British Telecommunications (BT) premises.},
  Doi                      = {10.1109/SCC.2016.49},
  ISBN                     = {9781509026289}
}

@Article{Asadi2014,
  Title                    = {Toward automated feature model configuration with optimizing non-functional requirements},
  Author                   = {Asadi, Mohsen and Soltani, Samaneh and Gasevic, Dragan and Hatala, Marek and Bagheri, Ebrahim},
  Year                     = {2014},
  Number                   = {9},
  Pages                    = {1144--1165},
  Volume                   = {56},

  Abstract                 = {Context A software product line is a family of software systems that share some common features but also have significant variabilities. A feature model is a variability modeling artifact, which represents differences among software products with respect to the variability relationships among their features. Having a feature model along with a reference model developed in the domain engineering lifecycle, a concrete product of the family is derived by binding the variation points in the feature model (called configuration process) and by instantiating the reference model. Objective In this work we address the feature model configuration problem and propose a framework to automatically select suitable features that satisfy both the functional and non-functional preferences and constraints of stakeholders. Additionally, interdependencies between various non-functional properties are taken into account in the framework. Method The proposed framework combines Analytical Hierarchy Process (AHP) and Fuzzy Cognitive Maps (FCM) to compute the non-functional properties weights based on stakeholders' preferences and interdependencies between non-functional properties. Afterwards, Hierarchical Task Network (HTN) planning is applied to find the optimal feature model configuration. Result Our approach improves state-of-art of feature model configuration by considering positive or negative impacts of the features on non-functional properties, the stakeholders' preferences, and non-functional interdependencies. The approach presented in this paper extends earlier work presented in [1] from several distinct perspectives including mechanisms handling interdependencies between non-functional properties, proposing a novel tooling architecture, and offering visualization and interaction techniques for representing functional and non-functional aspects of feature models. Conclusion our experiments show the scalability of our configuration approach when considering both functional and non-functional requirements of stakeholders. {\textcopyright} 2014 Elsevier B.V. All rights reserved.},
  Doi                      = {10.1016/j.infsof.2014.03.005},
  ISSN                     = {09505849},
  Publisher                = {Elsevier}
}

@Article{Becan2016,
  Title                    = {Breathing ontological knowledge into feature model synthesis: an empirical study},
  Author                   = {B{\'{e}}can, Guillaume and Acher, Mathieu and Baudry, Benoit and Nasr, Sana Ben},
  Year                     = {2016},

  Month                    = {aug},
  Number                   = {4},
  Pages                    = {1794--1841},
  Volume                   = {21},

  Abstract                 = {Feature Models (FMs) are a popular formalism for modeling and reasoning about the configurations of a software product line. As the manual construction of an FM is time-consuming and error-prone, management operations have been developed for reverse engineering, merging, slicing, or refactoring FMs from a set of configurations/dependencies. Yet the synthesis of meaningless ontological relations in the FM – as defined by its feature hierarchy and feature groups – may arise and cause severe difficulties when reading, maintaining or exploiting it. Numerous synthesis techniques and tools have been proposed, but only a few consider both configuration and ontological semantics of an FM. There are also few empirical studies investigating ontological aspects when synthesizing FMs. In this article, we define a generic, ontologic-aware synthesis procedure that computes the likely siblings or parent candidates for a given feature. We develop six heuristics for clustering and weighting the logical, syntactical and semantical relationships between feature names. We then perform an empirical evaluation on hundreds of FMs, coming from the SPLOT repository and Wikipedia. We provide evidence that a fully automated synthesis (i.e., without any user intervention) is likely to produce FMs far from the ground truths. As the role of the user is crucial, we empirically analyze the strengths and weaknesses of heuristics for computing ranking lists and different kinds of clusters. We show that a hybrid approach mixing logical and ontological techniques outperforms state-of-the-art solutions. We believe our approach, environment, and empirical results support researchers and practitioners working on reverse engineering and management of FMs.},
  Doi                      = {10.1007/s10664-014-9357-1},
  ISSN                     = {15737616},
  Publisher                = {Springer New York LLC}
}

@Article{Burdek2016,
  Title                    = {Reasoning about product-line evolution using complex feature model differences},
  Author                   = {B{\"{u}}rdek, Johannes and Kehrer, Timo and Lochau, Malte and Reuling, Dennis and Kelter, Udo and Sch{\"{u}}rr, Andy},
  Year                     = {2016},

  Month                    = {dec},
  Number                   = {4},
  Pages                    = {687--733},
  Volume                   = {23},

  Abstract                 = {Features define common and variable parts of the members of a (software) product line. Feature models are used to specify the set of all valid feature combinations. Feature models not only enjoy an intuitive tree-like graphical syntax, but also a precise formal semantics, which can be denoted as propositional formulae over Boolean feature variables. A product line usually constitutes a long-term investment and, therefore, has to undergo continuous evolution to meet ever-changing requirements. First of all, product-line evolution leads to changes of the feature model due to its central role in the product-line paradigm. As a result, product-line engineers are often faced with the problems that (1) feature models are changed in an ad-hoc manner without proper documentation, and (2) the semantic impact of feature diagram changes is unclear. In this article, we propose a comprehensive approach to tackle both challenges. For (1), our approach compares the old and new version of the diagram representation of a feature model and specifies the changes using complex edit operations on feature diagrams. In this way, feature model changes are automatically detected and formally documented. For (2), we propose an approach for reasoning about the semantic impact of diagram changes. We present a set of edit operations on feature diagrams, where complex operations are primarily derived from evolution scenarios observed in a real-world case study, i.e., a product line from the automation engineering domain. We evaluated our approach to demonstrate its applicability with respect to the case study, as well as its scalability concerning experimental data sets.},
  Doi                      = {10.1007/s10515-015-0185-3},
  ISSN                     = {15737535},
  Publisher                = {Springer New York LLC}
}

@InProceedings{Babur2015,
  Title                    = {A survey of open source multiphysics frameworks in engineering},
  Author                   = {Babur, {\"{O}}nder and Smilauer, Vit and Verhoeff, Tom and {Van Den Brand}, Mark},
  Year                     = {2015},
  Number                   = {1},
  Pages                    = {1088--1097},
  Publisher                = {Elsevier},
  Volume                   = {51},

  Abstract                 = {This paper presents a systematic survey of open source multiphysics frameworks in the engineering domains. These domains share many commonalities despite the diverse application areas. A thorough search for the available frameworks with both academic and industrial origins has revealed numerous candidates. Considering key characteristics such as project size, maturity and visibility, we selected Elmer, OpenFOAM and Salome for a detailed analysis. All the public documentation for these tools has been manually collected and inspected. Based on the analysis, we built a feature model for multiphysics in engineering, which captures the commonalities and variability in the domain. We in turn validated the resulting model via two other tools; Kratos by manual inspection, and OOFEM by means of expert validation by domain experts.},
  Doi                      = {10.1016/j.procs.2015.05.273},
  ISSN                     = {18770509}
}

@InProceedings{Bagheri2010,
  Title                    = {Stratified analytic hierarchy process: Prioritization and selection of software features},
  Author                   = {Bagheri, Ebrahim and Asadi, Mohsen and Gasevic, Dragan and Soltani, Samaneh},
  Year                     = {2010},
  Pages                    = {300--315},
  Volume                   = {6287 LNCS},

  Abstract                 = {Product line engineering allows for the rapid development of variants of a domain specific application by using a common set of reusable assets often known as core assets. Variability modeling is a critical issue in product line engineering, where the use of feature modeling is one of most commonly used formalisms. To support an effective and automated derivation of concrete products for a product family, staged configuration has been proposed in the research literature. In this paper, we propose the integration of well-known requirements engineering principles into stage configuration. Being inspired by the well-established Preview requirements engineering framework, we initially propose an extension of feature models with capabilities for capturing business oriented requirements. This representation enables a more effective capturing of stakeholders' preferences over the business requirements and objectives (e.g.,. implementation costs or security) in the form of fuzzy linguistic variables (e.g., high, medium, and low). On top of this extension, we propose a novel method, the Stratified Analytic Hierarchy process, which first helps to rank and select the most relevant high level business objectives for the target stakeholders (e.g., security over implementation costs), and then helps to rank and select the most relevant features from the feature model to be used as the starting point in the staged configuration process. Besides a complete formalization of the process, we define the place of our proposal in existing software product line lifecycles as well as demonstrate the use of our proposal on the widely-used e-Shop case study. Finally, we report on the results of our user study, which indicates a high appreciation of the proposed method by the participating industrial software developers. The tool support for S-AHP is also introduced. {\textcopyright} 2010 Springer-Verlag Berlin Heidelberg.},
  Doi                      = {10.1007/978-3-642-15579-6_21},
  ISBN                     = {3642155782},
  ISSN                     = {03029743}
}

@InProceedings{Bagheri2010a,
  Title                    = {Configuring software product line feature models based on stakeholders' soft and hard requirements},
  Author                   = {Bagheri, Ebrahim and {Di Noia}, Tommaso and Ragone, Azzurra and Gasevic, Dragan},
  Year                     = {2010},
  Pages                    = {16--31},
  Volume                   = {6287 LNCS},

  Abstract                 = {Feature modeling is a technique for capturing commonality and variability. Feature models symbolize a representation of the possible application configuration space, and can be customized based on specific domain requirements and stakeholder goals. Most feature model configuration processes neglect the need to have a holistic approach towards the integration and satisfaction of the stakeholder's soft and hard constraints, and the application-domain integrity constraints. In this paper, we will show how the structure and constraints of a feature model can be modeled uniformly through Propositional Logic extended with concrete domains, called . Furthermore, we formalize the representation of soft constraints in fuzzy and explain how semi-automated feature model configuration is performed. The model configuration derivation process that we propose respects the soundness and completeness properties. {\textcopyright} 2010 Springer-Verlag Berlin Heidelberg.},
  Doi                      = {10.1007/978-3-642-15579-6_2},
  ISBN                     = {3642155782},
  ISSN                     = {03029743}
}

@Article{Bagheri2011,
  Title                    = {Assessing the maintainability of software product line feature models using structural metrics},
  Author                   = {Bagheri, Ebrahim and Gasevic, Dragan},
  Year                     = {2011},

  Month                    = {sep},
  Number                   = {3},
  Pages                    = {579--612},
  Volume                   = {19},

  Abstract                 = {A software product line is a unified representation of a set of conceptually similar software systems that share many common features and satisfy the requirements of a particular domain. Within the context of software product lines, feature models are tree-like structures that are widely used for modeling and representing the inherent commonality and variability of software product lines. Given the fact that many different software systems can be spawned from a single software product line, it can be anticipated that a low-quality design can ripple through to many spawned software systems. Therefore, the need for early indicators of external quality attributes is recognized in order to avoid the implications of defective and low-quality design during the late stages of production. In this paper, we propose a set of structural metrics for software product line feature models and theoretically validate them using valid measurement-theoretic principles. Further, we investigate through controlled experimentation whether these structural metrics can be good predictors (early indicators) of the three main subcharacteristics of maintainability: analyzability, changeability, and understandability. More specifically, a four-step analysis is conducted: (1) investigating whether feature model structural metrics are correlated with feature model maintainability through the employment of classical statistical correlation techniques; (2) understanding how well each of the structural metrics can serve as discriminatory references for maintainability; (3) identifying the sufficient set of structural metrics for evaluating each of the subcharacteristics of maintainability; and (4) evaluating how well different prediction models based on the proposed structural metrics can perform in indicating the maintainability of a feature model. Results obtained from the controlled experiment support the idea that useful prediction models can be built for the purpose of evaluating feature model maintainability using early structural metrics. Some of the structural metrics show significant correlation with the subjective perception of the subjects about the maintainability of the feature models. {\textcopyright} 2010 Springer Science+Business Media, LLC.},
  Doi                      = {10.1007/s11219-010-9127-2},
  ISSN                     = {09639314}
}

@InProceedings{Barata2013,
  Title                    = {Bag-of-features classification model for the diagnose of melanoma in dermoscopy images using color and texture descriptors},
  Author                   = {Barata, Catarina and Marques, Jorge S. and Mendon{\c{c}}a, Teresa},
  Year                     = {2013},
  Pages                    = {547--555},
  Volume                   = {7950 LNCS},

  Abstract                 = {Melanoma detection using medical oriented approaches has been a trend in skin cancer research. This paper uses a Bag-of-Feature model for the detection of melanomas in dermoscopy images and aims at identifying the role of different local texture and color descriptors. This is a medical oriented approach and the reported results are promising (Sensitivity = 93{\%}, Specificity=85{\%}), showing the ability of this method to describe medical dermoscopic features. Moreover, the results show that color descriptors outperform texture ones. {\textcopyright} 2013 Springer-Verlag.},
  Doi                      = {10.1007/978-3-642-39094-4_62},
  ISBN                     = {9783642390937},
  ISSN                     = {03029743}
}

@InProceedings{Bezerra2016,
  Title                    = {Analyzing the feature models maintainability over their evolution process: An exploratory study},
  Author                   = {Bezerra, Carla I M and Monteiro, Jos{\'{e}} Maria and Andrade, Rossana M C and Rocha, Lincoln S.},
  Year                     = {2016},
  Month                    = {jan},
  Pages                    = {17--24},
  Publisher                = {Association for Computing Machinery},

  Abstract                 = {The feature model is one of the most important artifact of a Software Product Line (SPL). It is built in the early stages of SPL development and describes the main features and relationships. The feature model evolves according to the evolution of the SPL. Thus, it is important to build maintainable feature models. In this scenario, measures have been proven useful in the maintainability evaluation of the feature models. This paper presents an exploratory study on the impact of feature models maintainability over the SPL evolution process. In order to support this analysis, we built a dataset containing a compiled set of 21 maintainability structural measures extracted from 16 feature models and respective versions. Although not conclusive, our findings indicate that the feature models maintainability tends to decrease as it evolves. We also identified the most common changes performed in a feature model during its evolution process.},
  Doi                      = {10.1145/2866614.2866617},
  ISBN                     = {9781450340199}
}

@Article{Boskovi2010,
  Title                    = {Automated staged configuration with semantic web technologies},
  Author                   = {Bo{\v{s}}kovi, Marko and Bagheri, Ebrahim and Ga{\v{S}}evi, Dragan and Mohabbati, Bardia and Kaviani, Nima and Hatala, Marek},
  Year                     = {2010},

  Month                    = {jun},
  Number                   = {4},
  Pages                    = {459--484},
  Volume                   = {20},

  Abstract                 = {Since the introduction in the early nineties, feature models receive a great deal of attention in industry and academia. Industrial success stories in applying feature models for modeling software product lines, and using them for configuring software-intensive systems motivate academia to discover ways to integrate different feature dependencies into the feature model, and automate verified feature configuration. In this paper we demonstrate how ontologies and Semantic Web technologies facilitate seamless integration of required external services and deployment platform capabilities into the feature model. Furthermore, we also contribute with an algorithm for automating staged configuration using Semantic Web reasoners to discover unfeasible features of the feature model. {\textcopyright} 2010 World Scientific Publishing Company.},
  Doi                      = {10.1142/S0218194010004827},
  ISSN                     = {02181940}
}

@Misc{Bose2016,
  Title                    = {Spiking Neural Networks for Crop Yield Estimation Based on Spatiotemporal Analysis of Image Time Series},

  Author                   = {Bose, Pritam and Kasabov, Nikola K. and Bruzzone, Lorenzo and Hartono, Reggio N.},
  Month                    = {jul},
  Year                     = {2016},

  Abstract                 = {This paper presents spiking neural networks (SNNs) for remote sensing spatiotemporal analysis of image time series, which make use of the highly parallel and low-power-consuming neuromorphic hardware platforms possible. This paper illustrates this concept with the introduction of the first SNN computational model for crop yield estimation from normalized difference vegetation index image time series. It presents the development and testing of a methodological framework which utilizes the spatial accumulation of time series of Moderate Resolution Imaging Spectroradiometer 250-m resolution data and historical crop yield data to train an SNN to make timely prediction of crop yield. The research work also includes an analysis on the optimum number of features needed to optimize the results from our experimental data set. The proposed approach was applied to estimate the winter wheat (Triticum aestivum L.) yield in Shandong province, one of the main winter-wheat-growing regions of China. Our method was able to predict the yield around six weeks before harvest with a very high accuracy. Our methodology provided an average accuracy of 95.64{\%}, with an average error of prediction of 0.236 t/ha and correlation coefficient of 0.801 based on a nine-feature model.},
  Doi                      = {10.1109/TGRS.2016.2586602},
  ISSN                     = {01962892},
  Publisher                = {Institute of Electrical and Electronics Engineers Inc.}
}

@InProceedings{Boucher2012,
  Title                    = {Towards more reliable configurators: A re-engineering perspective},
  Author                   = {Boucher, Quentin and Abbasi, Ebrahim Khalil and Hubaux, Arnaud and Perrouin, Gilles and Acher, Mathieu and Heymans, Patrick},
  Year                     = {2012},
  Pages                    = {29--32},

  Abstract                 = {Delivering configurable solutions, that is products tailored to the requirements of a particular customer, is a priority of most B2B and B2C markets. These markets now heavily rely on interactive configurators that help customers build complete and correct products. Reliability is thus a critical requirement for configurators. Yet, our experience in industry reveals that many configurators are developed in an ad hoc manner, raising correctness and maintenance issues. In this paper, we present a vision to re-engineering more reliable configurators and the challenges it poses. The first challenge is to reverse engineer from an existing configurator the variability information, including complex rules, and to consolidate it in a variability model, namely a feature model. The second challenge is to forward engineer a new configurator that uses the feature model to generate a customized graphical user interface and the underlying reasoning engine. {\textcopyright} 2012 IEEE.},
  Doi                      = {10.1109/PLEASE.2012.6229766},
  ISBN                     = {9781467317511}
}

@InProceedings{Buchmann2013,
  Title                    = {MOD2-SCM: A model-driven product line for software configuration management systems},
  Author                   = {Buchmann, Thomas and Dotor, Alexander and Westfechtel, Bernhard},
  Year                     = {2013},
  Month                    = {mar},
  Number                   = {3},
  Pages                    = {630--650},
  Volume                   = {55},

  Abstract                 = {Context: Software Configuration Management (SCM) is the discipline of controlling the evolution of large and complex software systems. Over the years many different SCM systems sharing similar concepts have been implemented from scratch. Since these concepts usually are hard-wired into the respective program code, reuse is hardly possible. Objective: Our objective is to create a model-driven product line for SCM systems. By explicitly describing the different concepts using models, reuse can be performed on the modeling level. Since models are executable, the need for manual programming is eliminated. Furthermore, by providing a library of loosely coupled modules, we intend to support flexible composition of SCM systems. Method: We developed a method and a tool set for model-driven software product line engineering which we applied to the SCM domain. For domain analysis, we applied the FORM method, resulting in a layered feature model for SCM systems. Furthermore, we developed an executable object-oriented domain model which was annotated with features from the feature model. A specific SCM system is configured by selecting features from the feature model and elements of the domain model realizing these features. Results: Due to the orthogonality of both feature model and domain model, a very large number of SCM systems may be configured. We tested our approach by creating instances of the product line which mimic wide-spread systems such as CVS, GIT, Mercurial, and Subversion. Conclusion: The experiences gained from this project demonstrate the feasibility of our approach to model-driven software product line engineering. Furthermore, our work advances the state of the art in the domain of SCM systems since it support the modular composition of SCM systems at the model rather than the code level. {\textcopyright} 2012 Elsevier B.V. All rights reserved.},
  Doi                      = {10.1016/j.infsof.2012.07.010},
  ISSN                     = {09505849}
}

@Article{Cameron2016,
  Title                    = {MAPS: A Quantitative Radiomics Approach for Prostate Cancer Detection},
  Author                   = {Cameron, Andrew and Khalvati, Farzad and Haider, Masoom A. and Wong, Alexander},
  Year                     = {2016},

  Month                    = {jun},
  Number                   = {6},
  Pages                    = {1145--1156},
  Volume                   = {63},

  Abstract                 = {This paper presents a quantitative radiomics feature model for performing prostate cancer detection using multiparametric MRI (mpMRI). It incorporates a novel tumor candidate identification algorithm to efficiently and thoroughly identify the regions of concern and constructs a comprehensive radiomics feature model to detect tumorous regions. In contrast to conventional automated classification schemes, this radiomics-based feature model aims to ground its decisions in a way that can be interpreted and understood by the diagnostician. This is done by grouping features into high-level feature categories which are already used by radiologists to diagnose prostate cancer: Morphology, Asymmetry, Physiology, and Size (MAPS), using biomarkers inspired by the PI-RADS guidelines for performing structured reporting on prostate MRI. Clinical mpMRI data were collected from 13 men with histology-confirmed prostate cancer and labeled by an experienced radiologist. These annotated data were used to train classifiers using the proposed radiomics-driven feature model in order to evaluate the classification performance. The preliminary experimental results indicated that the proposed model outperformed each of its constituent feature groups as well as a comparable conventional mpMRI feature model. A further validation of the proposed algorithm will be conducted using a larger dataset as future work.},
  Doi                      = {10.1109/TBME.2015.2485779},
  ISSN                     = {15582531},
  Publisher                = {IEEE Computer Society}
}

@InProceedings{Cameron2014,
  Title                    = {Multiparametric MRI prostate cancer analysis via a hybrid morphological-textural model},
  Author                   = {Cameron, Andrew and Modhafar, Amen and Khalvati, Farzad and Lui, Dorothy and Shafiee, Mohammad J. and Wong, Alexander and Haider, Masoom},
  Year                     = {2014},
  Month                    = {nov},
  Pages                    = {3357--3360},
  Publisher                = {Institute of Electrical and Electronics Engineers Inc.},

  Abstract                 = {Multiparametric MRI has shown considerable promise as a diagnostic tool for prostate cancer grading. Diffusion-weighted MRI (DWI) has shown particularly strong potential for improving the delineation between cancerous and healthy tissue in the prostate gland. Current automated diagnostic methods using multiparametric MRI, however, tend to either use low-level features, which are difficult to interpret by radiologists and clinicians, or use highly subjective heuristic methods. We propose a novel strategy comprising a tumor candidate identification scheme and a hybrid textural-morphological feature model for delineating between cancerous and non-cancerous tumor candidates in the prostate gland via multiparametric MRI. Experimental results using clinical multiparametric MRI datasets show that the proposed strategy has strong potential as a diagnostic tool to aid radiologists and clinicians identify and detect prostate cancer more efficiently and effectively.},
  Doi                      = {10.1109/EMBC.2014.6944342},
  ISBN                     = {9781424479290}
}

@Article{Cao2012,
  Title                    = {An approach to automated conversion from design feature model to analysis feature model},
  Author                   = {Cao, Weijuan and Chen, Xiaoshen and Gao, Shuming},
  Year                     = {2012},

  Month                    = {aug},
  Number                   = {8},
  Pages                    = {1090--1098},
  Volume                   = {24},

  Abstract                 = {An approach to automatically converting a design feature model to an analysis feature model for downstream finite element analysis is proposed. In the approach, the design feature model is first decomposed into a set of remnants of additive features, each of which represents part of an additive feature's volume that remain in the final volume of the design model. The remnant of each additive feature is then decomposed into swept bodies and non-swept bodies. After that, the candidate analysis regions of each swept body are effectively determined based on its contour information. These detected candidate regions may be wrongly recognized, so, together with potentially missing ones, they are thus detected and corrected by a synthesis process. Finally, the analysis features and their relative interfaces are generated, which ultimately gives the corresponding analysis feature model to the input design feature model. Experimental results are also shown to demonstrate the proposed method's effectiveness.},
  ISSN                     = {10039775}
}

@InProceedings{Cao2011,
  Title                    = {An approach to automated conversion from design feature model to analysis feature model},
  Author                   = {Cao, Weijuan and Chen, Xiaoshen and Gao, Shuming},
  Year                     = {2011},
  Number                   = {PARTS A AND B},
  Pages                    = {655--665},
  Volume                   = {5},

  Abstract                 = {An approach to automatically converting a design feature model to an analysis feature model for downstream finite element analysis is proposed. The analysis feature model is a mixed- dimensional model with shell features representing thin regions and solid features representing thick regions. In the approach, the design feature model is first decomposed into a set of remnants of additive features, each of which represents part of an additive feature's volume that remain in the final volume of the design model. The remnant of each additive feature is then de- composed into swept bodies and non-swept bodies. After that, the thin regions of each swept body are effectively recognized based on its sketch information. These detected thin regions can be wrongly recognized, which, together with potentially missing ones are thus detected and corrected by a synthesization process. Finally, the analysis features and their relative interfaces are generated, which ultimately gives the corresponding analysis feature model to the input design feature model. Experimental results are also shown to demonstrate the proposed method's effectiveness. Copyright {\textcopyright} 2011 by ASME.},
  Doi                      = {10.1115/DETC2011-47555},
  ISBN                     = {9780791854822}
}

@Article{Chan2013,
  Title                    = {Multi-feature based boresight self-calibration of a terrestrial mobile mapping system},
  Author                   = {Chan, Ting On and Lichti, Derek D. and Glennie, Craig L.},
  Year                     = {2013},
  Pages                    = {112--124},
  Volume                   = {82},

  Abstract                 = {This paper presents a multi-feature based system calibration method for estimating the boresight angles of a land-based mobile mapping system (MMS) comprised of multiple two dimensional (2D) scanners. The method invokes a least-squares adjustment (LSA) to simultaneously estimate several sets of boresight angles for multiple laser scanners incorporated in an MMS as well as the parameters associated with one or more types of geometric features. This is achieved by constraining the groups of feature point clouds captured by multiple runs to fit their corresponding geometric models in such a way that the weighted sum of squares of adjustment residuals is minimized. The method is particularly suitable for in situ calibration because the geometric features involved are commonly occurring structures (e.g. building fa{\c{c}}ades, bridge surfaces, highway signs and hanging power cables) that are usually captured during the actual survey. In addition to using a planar feature model for calibration, a novel and rigorous three-dimensional (3D) catenary curve model is proposed for geometric modelling of hanging cables to augment the calibration. The proposed calibrations were examined with several different combinations of groups of planar and catenary features and the resulting analysis suggests that the in situ calibrations are effective when compared to the manufacturer's dedicated calibration, with the overall point cloud accuracies for plane fitting being 5.5. cm and 5.4. cm for the vertical and horizontal directions, respectively. It has been successfully demonstrated that the proposed method can be used in a scene having no building fa{\c{c}}ades but only some long hanging cables and horizontal ground surfaces. This is particularly useful for rural areas or inter-city/provincial highways where building fa{\c{c}}ades cannot commonly be captured. Parameter correlations in the calibrations were also addressed. It has also been shown that using catenary features in addition to planar features for the calibration can help de-correlate some parameters and improve the overall accuracy. The in situ nature and the high flexibility of integrating different features of the calibration make the proposed method straightforward for most end-users. {\textcopyright} 2013 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS).},
  Doi                      = {10.1016/j.isprsjprs.2013.04.005},
  ISSN                     = {09242716}
}

@InProceedings{Chen2010,
  Title                    = {Face age estimation using model selection},
  Author                   = {Chen, Cuixian and Chang, Yaw and Ricanek, Karl and Wang, Yishi},
  Year                     = {2010},
  Pages                    = {93--99},

  Abstract                 = {Face age estimation is a difficult problem due to the dynamics of facial aging and its complex interactions owing to genetics and behavior factors. In this work we develop a robust age estimation system tuned by model selection that outperforms all prior systems on the FG-NET face database. We study various model selection methods systematically to determine the best selection methods among Least Angle Regression (LAR), Principle Component Analysis (PCA), and Locality Preserving Projections (LPP) for age estimation. Our performance analysis on PAL and FGNET databases suggest that age estimation with LAR or LPP outperforms the full feature model. Furthermore, this work develops a novel operator named "graph age preserving" (GAP) to build a neighborhood graph for LPP for age estimation. {\textcopyright} 2010 IEEE.},
  Doi                      = {10.1109/CVPRW.2010.5543820},
  ISBN                     = {9781424470297}
}

@Article{Chen2011,
  Title                    = {Descriptive method for collaborative computing tasks over multiple virtual machines},
  Author                   = {Chen, Xiao Jun and Zhang, Jing and Li, Jun Huai},
  Year                     = {2011},

  Month                    = {dec},
  Number                   = {12},
  Pages                    = {2767--2775},
  Volume                   = {33},

  Abstract                 = {A descriptive methodology and its aided construction tool based on feature model for collaborative computing tasks over multiple virtual machines (CCTMVM) are studied to simple the description of parallel computing in virtualized platform and improve the development efficiency of applications. The tasks decomposition policies are discussed from the principle and method of tasks decomposition, tasks particle judgment and the heuristic rules, and then the appraisal criterion of parallel relationships among tasks is determined to describe the semantic actions of feature units based on their timing relationships. The validity of the descriptive method is determined to judge the effectiveness of the methodology presented from the coefficients of cohesion coupling and the granularities of feature units. A prototype of aided construction tools for CCTMVM is designed, and then a problem-solving application in computer aided engineering (CAE) is taken for an example to analyze its task decomposition. The experimental results show that the proposed idea and methodology have a certain feasibility for constructing a parallel computing-oriented CCTMVM.},
  Doi                      = {10.3969/j.issn.1001-506X.2011.12.36},
  ISSN                     = {1001506X}
}

@InProceedings{Chen2010a,
  Title                    = {Polygon overlay analysis algorithm based on monotone chain and STR tree in the simple feature mode},
  Author                   = {Chen, Zhanlong and Ma, Lina and Wu, Liang},
  Year                     = {2010},
  Pages                    = {2905--2909},

  Abstract                 = {An improved overlay analysis algorithm based on monotone chain and STR (Sort-Tile-Recursive) tree index is introduced. The algorithm can save the time for vertex listing and intersection point computation, also the memory space. Making full use of the function of overlay analysis for simple features, as many as possible nodes of the polygon can be filled in the STR tree index structure. The algorithm reduces the access times when querying the polygons in the spatial database. The algorithm splits the edges in the polygon by the monotone chain algorithm to compute the intersect point firstly. Secondly, the concept of plane graph is used in this algorithm. The algorithm organizes the result polygons by computing the topology location between the plane graph components of the two polygons. It has been reduced the computing intersect point time and emphasizes on the solution of the problem of the entry point or exit-point successive and the alternative searching of the intersected polygon. Performance tests show the algorithm is more efficient and quicker than the algorithm based on the topological model and the other algorithms. Chen Zhanlong, male, China University of Geosciences, assist professor, research: Geospatial Database, Distributed computing, Grid computing, P2P computing, the study of geographic information system and Software developing. Ma Lina, female, China University of Geosciences, postgraduate, research: Grid computing, the study of geographic information system and Software developing. Liang Wu, male, China University of Geosciences, assist professor, research: Geospatial Database, Distributed computing, Grid computing, P2P computing, the study of geographic information system and Software developing. {\textcopyright} 2010 IEEE.},
  Doi                      = {10.1109/iCECE.2010.1420},
  ISBN                     = {9780769540313}
}

@InProceedings{Chi2014,
  Title                    = {Person verification based on skeleton biometrics by RGB-D camera},
  Author                   = {Chi, Wenzheng and Wang, Jiaole and Meng, Max Q H},
  Year                     = {2014},
  Month                    = {apr},
  Pages                    = {671--676},
  Publisher                = {Institute of Electrical and Electronics Engineers Inc.},

  Abstract                 = {The state-of-the-art person identification and verification technologies require apparent human biological features as inputs explicitly. In many special scenarios, however, it is impossible to obtain the apparent human biological features since such features usually require intrusive human interaction directly. In this paper, we have proposed a hierarchical skeleton feature model (HSFM) for person verification by using implicit biometric skeletal data estimated from the emerging RGB-D camera. To validate the viability of the proposed feature model, experiments have been carried out to retrieve the estimated skeletal data from 11 volunteers by using the RGB-D camera. 2fold cross-validations with SVM (Support Vector Machine) and KNN (K-Nearest Neighbors) methods were adopted and carried out 200 times to train and test the obtained data for regarding each participant as the authorized person. Furthermore, the parameter accuracy, sensitivity, specificity and time cost of the proposed human verification method were analyzed in the experiments. The intensive analyses on our data set have shown the feasibility to use the estimated human skeletal data from RGB-D camera for human verification.},
  Doi                      = {10.1109/ROBIO.2014.7090408},
  ISBN                     = {9781479973965}
}

@InProceedings{Contreras2010,
  Title                    = {Facial feature model for emotion recognition using fuzzy reasoning},
  Author                   = {Contreras, Renan and Starostenko, Oleg and Alarcon-Aquino, Vicente and Flores-Pulido, Leticia},
  Year                     = {2010},
  Pages                    = {11--21},
  Volume                   = {6256 LNCS},

  Abstract                 = {In this paper we present a fuzzy reasoning system that can measure and recognize the intensity of basic or non-prototypical facial expressions. The system inputs are the encoded facial deformations described either in terms of Ekma{\'{n}}s Action Units (AUs) or Facial Animation Parameters (FAPs) of MPEG-4 standard. The proposed fuzzy system uses a knowledge base implemented on knowledge acquisition and ontology editor Prot{\'{e}}g{\'{e}}. It allows the modeling of facial features obtained from geometric parameters coded by AUs - FAPs and also the definition of rules required for classification of measured expressions. This paper also presents the designed framework for fuzzyfication of input variables for fuzzy classifier based on statistical analysis of emotions expressed in video records of standard Cohn-Kanade's and Panti{\'{c}}s MMI face databases. The proposed system has been tested in order to evaluate its capability for detection, classifying, and interpretation of facial expressions. {\textcopyright} 2010 Springer-Verlag Berlin Heidelberg.},
  Doi                      = {10.1007/978-3-642-15992-3_2},
  ISBN                     = {3642159915},
  ISSN                     = {03029743}
}

@Article{Costa2015,
  Title                    = {A Scientific Software Product Line for the Bioinformatics domain},
  Author                   = {Costa, Gabriella Castro B and Braga, Regina and David, Jos{\'{e}} Maria N and Campos, Fernanda},
  Year                     = {2015},

  Month                    = {aug},
  Pages                    = {239--264},
  Volume                   = {56},

  Abstract                 = {Context: Most specialized users (scientists) that use bioinformatics applications do not have suitable training on software development. Software Product Line (SPL) employs the concept of reuse considering that it is defined as a set of systems that are developed from a common set of base artifacts. In some contexts, such as in bioinformatics applications, it is advantageous to develop a collection of related software products, using SPL approach. If software products are similar enough, there is the possibility of predicting their commonalities, differences and then reuse these common features to support the development of new applications in the bioinformatics area. Objectives: This paper presents the PL-Science approach which considers the context of SPL and ontology in order to assist scientists to define a scientific experiment, and to specify a workflow that encompasses bioinformatics applications of a given experiment. This paper also focuses on the use of ontologies to enable the use of Software Product Line in biological domains. Method: In the context of this paper, Scientific Software Product Line (SSPL) differs from the Software Product Line due to the fact that SSPL uses an abstract scientific workflow model. This workflow is defined according to a scientific domain and using this abstract workflow model the products (scientific applications/algorithms) are instantiated. Results: Through the use of ontology as a knowledge representation model, we can provide domain restrictions as well as add semantic aspects in order to facilitate the selection and organization of bioinformatics workflows in a Scientific Software Product Line. The use of ontologies enables not only the expression of formal restrictions but also the inferences on these restrictions, considering that a scientific domain needs a formal specification. Conclusions: This paper presents the development of the PL-Science approach, encompassing a methodology and an infrastructure, and also presents an approach evaluation. This evaluation presents case studies in bioinformatics, which were conducted in two renowned research institutions in Brazil.},
  Doi                      = {10.1016/j.jbi.2015.05.014},
  ISSN                     = {15320464},
  Publisher                = {Academic Press Inc.}
}

@InProceedings{Davril2013,
  Title                    = {Feature model extraction from large collections of informal product descriptions},
  Author                   = {Davril, Jean Marc and Delfosse, Edouard and Hariri, Negar and Acher, Mathieu and Cleland-Huang, Jane and Heymans, Patrick},
  Year                     = {2013},
  Pages                    = {290--300},

  Abstract                 = {Feature Models (FMs) are used extensively in software product line engineering to help generate and validate individual product configurations and to provide support for domain analysis. As FM construction can be tedious and time-consuming, researchers have previously developed techniques for extracting FMs from sets of formally specified individual configurations, or from software requirements specifications for families of existing products. However, such artifacts are often not available. In this paper we present a novel, automated approach for constructing FMs from publicly available product descriptions found in online product repositories and marketing websites such as SoftPedia and CNET. While each individual product description provides only a partial view of features in the domain, a large set of descriptions can provide fairly comprehensive coverage. Our approach utilizes hundreds of partial product descriptions to construct an FM and is described and evaluated against antivirus product descriptions mined from SoftPedia.},
  Doi                      = {10.1145/2491411.2491455},
  ISBN                     = {9781450322379}
}

@InProceedings{Dumitru2011,
  Title                    = {On-demand feature recommendations derived from mining public product descriptions},
  Author                   = {Dumitru, Horatiu and Gibiec, Marek and Hariri, Negar and Cleland-Huang, Jane and Mobasher, Bamshad and Castro-Herrera, Carlos and Mirakhorli, Mehdi},
  Year                     = {2011},
  Pages                    = {181--190},

  Abstract                 = {We present a recommender system that models and recommends product features for a given domain. Our approach mines product descriptions from publicly available online specifications, utilizes text mining and a novel incremental diffusive clustering algorithm to discover domain-specific features, generates a probabilistic feature model that represents commonalities, variants, and cross-category features, and then uses association rule mining and the k-Nearest-Neighbor machine learning strategy to generate product specific feature recommendations. Our recommender system supports the relatively labor-intensive task of domain analysis, potentially increasing opportunities for re-use, reducing time-to-market, and delivering more competitive software products. The approach is empirically validated against 20 different product categories using thousands of product descriptions mined from a repository of free software applications. {\textcopyright} 2011 ACM.},
  Doi                      = {10.1145/1985793.1985819},
  ISBN                     = {9781450304450},
  ISSN                     = {02705257}
}

@Article{Dunlap2013,
  Title                    = {A feature model of coupling technologies for Earth System Models},
  Author                   = {Dunlap, Rocky and Rugaber, Spencer and Mark, Leo},
  Year                     = {2013},

  Month                    = {apr},
  Pages                    = {13--20},
  Volume                   = {53},

  Abstract                 = {Couplers that link together two or more numerical simulations are well-known abstractions in the Earth System Modeling (ESM) community. In the past decade, reusable software assets have emerged to facilitate scientists in implementing couplers. While there is a large amount of overlap in the features supported by software coupling technologies, their implementations differ significantly in terms of both functional and non-functional properties. Using a domain analysis method called feature analysis, we explore the spectrum of features supported by coupling technologies used to build today's production ESMs. {\textcopyright} 2011 Elsevier Ltd.},
  Doi                      = {10.1016/j.cageo.2011.10.002},
  ISSN                     = {00983004}
}

@Article{Felice2010,
  Title                    = {Integrating formal methods with domain analysis},
  Author                   = {Felice, Laura and Leonardo, Mar{\'{i}}a Carmen and Mauco, Mana Virginia and Montejano, Germ{\'{a}}n and Riesco, Daniel and Debnath, Narayan},
  Year                     = {2010},
  Number                   = {1-2 SUPPL. 2},
  Volume                   = {10},

  Abstract                 = {The use of formal methods should help to achieve a high degree of confidence that a system will conform to its specification, enhancing in consequence software quality and reliability. However, a general acceptance of formal methods among software engineers is still some way off because formal methods are usually only accessible to specialists and they do not have developed in depth strategies for the first stages of development. This is also valid to Domain Analysis, because its first stage is to capture the knowledge of a particular domain, making necessary to have a model comprehensible by software engineers and domain experts. In order to address this problem and take advantage of formal methods, we suggest integrating the phase reusable Domain Analysis into the RAISE Method, combining Domain Analysis notions with a formal language in the early steps of software development process. In this paper, we present a set of heuristics to fruitfully use knowledge represented in a Domain Analysis model to derive a formal specification in the RAISE Specification Language. {\textcopyright} 2010 - IOS Press and the authors. All rights reserved.},
  Doi                      = {10.3233/JCM-2010-0275},
  ISSN                     = {14727978}
}

@Article{Fernandes2011,
  Title                    = {An approach for feature modeling of context-aware software product line},
  Author                   = {Fernandes, Paula and Werner, Cl{\'{a}}udia and Teixeira, Eld{\^{a}}nae},
  Year                     = {2011},
  Number                   = {5},
  Pages                    = {807--829},
  Volume                   = {17},

  Abstract                 = {Feature modeling is an approach to represent commonalities and variabilities among products of a product line. Context-aware applications use context information to provide relevant services and information for their users. One of the challenges to build a context-aware product line is to develop mechanisms to incorporate context information and adaptation knowledge in a feature model. This paper presents UbiFEX, an approach to support feature analysis for context-aware software product lines, which incorporates a modeling notation and a mechanism to verify the consistency of product configuration regarding context variations. Moreover, an experimental study was performed as a preliminary evaluation, and a prototype was developed to enable the application of the proposed approach. {\textcopyright} J.UCS.},
  ISSN                     = {0958695X}
}

@Article{Galindo2016,
  Title                    = {Testing variability-intensive systems using automated analysis: an application to Android},
  Author                   = {Galindo, Jos{\'{e}} A. and Turner, Hamilton and Benavides, David and White, Jules},
  Year                     = {2016},

  Month                    = {jun},
  Number                   = {2},
  Pages                    = {365--405},
  Volume                   = {24},

  Abstract                 = {Software product lines are used to develop a set of software products that, while being different, share a common set of features. Feature models are used as a compact representation of all the products (e.g., possible configurations) of the product line. The number of products that a feature model encodes may grow exponentially with the number of features. This increases the cost of testing the products within a product line. Some proposals deal with this problem by reducing the testing space using different techniques. However, a daunting challenge is to explore how the cost and value of test cases can be modeled and optimized in order to have lower-cost testing processes. In this paper, we present TESting vAriAbiLity Intensive Systems (TESALIA), an approach that uses automated analysis of feature models to optimize the testing of variability-intensive systems. We model test value and cost as feature attributes, and then we use a constraint satisfaction solver to prune, prioritize and package product line tests complementing prior work in the software product line testing literature. A prototype implementation of TESALIA is used for validation in an Android example showing the benefits of maximizing the mobile market share (the value function) while meeting a budgetary constraint.},
  Doi                      = {10.1007/s11219-014-9258-y},
  ISSN                     = {15731367},
  Publisher                = {Springer New York LLC}
}

@InProceedings{Gao2010,
  Title                    = {Affine Stable Characteristic based sample expansion for object detection},
  Author                   = {Gao, Ke and Zhang, Yongdong and Zhang, Wei and Lin, Shouxun},
  Year                     = {2010},
  Pages                    = {422--429},

  Abstract                 = {Generating better object model from automatic expanded samples is an effective approach to improve the performance of object detection. However, most existing methods either don't work well with limited relevance images in corpus, or result in redundant features and the decrease of detection speed. In this paper, we propose a novel method called Affine Stable Characteristic to generate an object feature model using only one object sample. By integrating affine simulation with stable characteristic mining, a compact and informative object model is generated with high robustness to viewpoint and scale transformations. For characteristic mining, two new notions, Global Stability and Local Stability, are introduced to calculate the robustness of each object feature from complementary hierarchies. And they are combined to generate the final object feature model. Experiments show that our novel method is capable of detecting objects in various geometric and photometric transformations, while only acquiring one sample image. In a compiled dataset composed of many famous test sets, the detection accuracy can be improved 35.8{\%} compared with traditional methods at rapid on-line speed. The proposed approach can also be well generalized to other content analysis tasks. {\textcopyright} 2010 ACM.},
  Doi                      = {10.1145/1816041.1816103},
  ISBN                     = {9781450301176}
}

@InProceedings{Gao2012,
  Title                    = {Link prediction via latent factor blockmodel},
  Author                   = {Gao, Sheng and Denoyer, Ludovic and Gallinari, Patrick},
  Year                     = {2012},
  Pages                    = {507--508},

  Abstract                 = {In this paper we address the problem of link prediction in networked data, which appears in many applications such as social network analysis or recommender systems. Previous studies either consider latent feature based models but disregarding local structure in the network, or focus exclusively on capturing local structure of objects based on latent blockmodels without coupling with latent characteristics of objects. To combine the benefits of previous work, we propose a novel model that can incorporate the effects of latent features of objects and local structure in the network simultaneously. To achieve this, we model the relation graph as a function of both latent feature factors and latent cluster memberships of objects to collectively discover globally predictive intrinsic properties of objects and capture latent block structure in the network to improve prediction performance. Extensive experiments on several real world datasets suggest that our proposed model outperforms the other state of the art approaches for link prediction. Copyright is held by the author/owner(s).},
  Doi                      = {10.1145/2187980.2188100},
  ISBN                     = {9781450312301}
}

@InProceedings{Gao2012a,
  Title                    = {What makes a good detector? Structured priors for learning from few examples},
  Author                   = {Gao, Tianshi and Stark, Michael and Koller, Daphne},
  Year                     = {2012},
  Number                   = {PART 5},
  Pages                    = {354--367},
  Volume                   = {7576 LNCS},

  Abstract                 = {Transfer learning can counter the heavy-tailed nature of the distribution of training examples over object classes. Here, we study transfer learning for object class detection. Starting from the intuition that "what makes a good detector" should manifest itself in the form of repeatable statistics over existing "good" detectors, we design a low-level feature model that can be used as a prior for learning new object class models from scarce training data. Our priors are structured, capturing dependencies both on the level of individual features and spatially neighboring pairs of features. We confirm experimentally the connection between the information captured by our priors and "good" detectors as well as the connection to transfer learning from sources of different quality. We give an in-depth analysis of our priors on a subset of the challenging PASCAL VOC 2007 data set and demonstrate improved average performance over all 20 classes, achieved without manual intervention. {\textcopyright} 2012 Springer-Verlag.},
  Doi                      = {10.1007/978-3-642-33715-4_26},
  ISBN                     = {9783642337147},
  ISSN                     = {03029743}
}

@InProceedings{Garcia-Galan2013,
  Title                    = {Migrating to the Cloud: A software product line based analysis},
  Author                   = {Garc{\'{i}}a-Gal{\'{a}}n, Jes{\'{u}}s and Rana, Omer and Trinidad, Pablo and Ruiz-Cort{\'{e}}s, Antonio},
  Year                     = {2013},
  Pages                    = {416--426},

  Abstract                 = {Identifying which part of a local system should be migrated to a public Cloud environment is often a difficult and error prone process. With the significant (and increasing) number of commercial Cloud providers, choosing a provider whose capability best meets requirements is also often difficult. Most Cloud service providers offer large amounts of configurable resources, which can be combined in a number of different ways. In the case of small and medium companies, finding a suitable configuration with the minimum cost is often an essential requirement to migrate, or even to initiate the decision process for migration. We interpret this need as a problem associated with variability management and analysis. Variability techniques and models deal with large configuration spaces, and have been proposed previously to support configuration processes in industrial cases. Furthermore, this is a mature field which has a large catalog of analysis operations to extract valuable information in an automated way. Some of these operations can be used and tailored for Cloud environments. We focus in this work on Amazon Cloud services, primarily due to the large number of possible configurations available by this service provider and its popularity. Our approach can also be adapted to other providers offering similar capabilities.},
  ISBN                     = {9789898565525}
}

@Article{Garcia-Galan2016,
  Title                    = {Automated configuration support for infrastructure migration to the cloud},
  Author                   = {Garc{\'{i}}a-Gal{\'{a}}n, Jes{\'{u}}s and Trinidad, Pablo and Rana, Omer F. and Ruiz-Cort{\'{e}}s, Antonio},
  Year                     = {2016},

  Month                    = {feb},
  Pages                    = {200--212},
  Volume                   = {55},

  Abstract                 = {With an increasing number of cloud computing offerings in the market, migrating an existing computational infrastructure to the cloud requires comparison of different offers in order to find the most suitable configuration. Cloud providers offer many configuration options, such as location, purchasing mode, redundancy, and extra storage. Often, the information about such options is not well organised. This leads to large and unstructured configuration spaces, and turns the comparison into a tedious, error-prone search problem for the customers. In this work we focus on supporting customer decision making for selecting the most suitable cloud configuration - in terms of infrastructural requirements and cost. We achieve this by means of variability modelling and analysis techniques. Firstly, we structure the configuration space of an IaaS using feature models, usually employed for the modelling of variability-intensive systems, and present the case study of the Amazon EC2. Secondly, we assist the configuration search process. Feature models enable the use of different analysis operations that, among others, automate the search of optimal configurations. Results of our analysis show how our approach, with a negligible analysis time, outperforms commercial approaches in terms of expressiveness and accuracy.},
  Doi                      = {10.1016/j.future.2015.03.006},
  ISSN                     = {0167739X},
  Publisher                = {Elsevier}
}

@InProceedings{Gey2014,
  Title                    = {Feature models at run time feature middleware for multi-tenant SaaS applications},
  Author                   = {Gey, Fatih and {Van Landuyt}, Dimitri and Walraven, Stefan and Joosen, Wouter},
  Year                     = {2014},
  Pages                    = {21--30},
  Publisher                = {CEUR-WS},
  Volume                   = {1270},

  Abstract                 = {Software product line engineering (SPLE) techniques revolve around a central variability model which in many cases is a feature model that documents the logical capabilities of the system as features and the variability relationships between them. In more traditional SPLE, this feature model is a result of domain analysis and requirement elicitation, while more recently this approach has been extended to represent also design-time variability, for example to document different ways to realize the same functionality. In many approaches, the feature model has run-time relevance as well. For example, in earlier work, we have used SPLE techniques to develop customizable multi-tenant SaaS applications, i.e. SaaS applications of which a single run-time instance is offered to many customer organizations (tenants), often with widely different requirements. In such systems, tenant customization is accomplished entirely at run time. In this paper, we present and explore the idea of promoting the feature model as a run-time artifact in the context of customizable multi-tenant SaaS applications, and we discuss the potential benefits in terms of the deployment, operation, maintenance, and evolution of these systems. In addition, we discuss the requirements this will impose on the development methods, the variability modeling languages, and the middleware.},
  ISSN                     = {16130073}
}

@Article{Gheyi2011,
  Title                    = {Automatically checking feature model refactorings},
  Author                   = {Gheyi, Rohit and Massoni, Tiago and Borba, Paulo},
  Year                     = {2011},
  Number                   = {5},
  Pages                    = {684--711},
  Volume                   = {17},

  Abstract                 = {A feature model (FM) defines the valid combinations of features, whose combinations correspond to a program in a Software Product Line (SPL). FMs may evolve, for instance, during refactoring activities. Developers may use a catalog of refactorings as support. However, the catalog is incomplete in principle. Additionally, it is non-trivial to propose correct refactorings. To our knowledge, no previous analysis technique for FMs is used for checking properties of general FM refactorings (a transformation that can be applied to a number of FMs) containing a representative number of features. We propose an efficient encoding of FMs in the Alloy formal specification language. Based on this encoding, we show how the Alloy Analyzer tool, which performs analysis on Alloy models, can be used to automatically check whether encoded general and specific FM refactorings are correct. Our approach can analyze general transformations automatically to a significant scale in a few seconds. In order to evaluate the analysis performance of our encoding, we evaluated in automatically generated FMs ranging from 500 to 2,000 features. Furthermore, we analyze the soundness of general transformations. {\textcopyright} J.UCS.},
  ISSN                     = {0958695X}
}

@Article{Guo2011,
  Title                    = {A genetic algorithm for optimized feature selection with resource constraints in software product lines},
  Author                   = {Guo, Jianmei and White, Jules and Wang, Guangxin and Li, Jian and Wang, Yinglin},
  Year                     = {2011},

  Month                    = {dec},
  Number                   = {12},
  Pages                    = {2208--2221},
  Volume                   = {84},

  Abstract                 = {Software product line (SPL) engineering is a software engineering approach to building configurable software systems. SPLs commonly use a feature model to capture and document the commonalities and variabilities of the underlying software system. A key challenge when using a feature model to derive a new SPL configuration is determining how to find an optimized feature selection that minimizes or maximizes an objective function, such as total cost, subject to resource constraints. To help address the challenges of optimizing feature selection in the face of resource constraints, this paper presents an approach that uses G enetic A lgorithms for optimized FE ature S election (GAFES) in SPLs. Our empirical results show that GAFES can produce solutions with 86-97{\%} of the optimality of other automated feature selection algorithms and in 45-99{\%} less time than existing exact and heuristic feature selection techniques. {\textcopyright} 2011 Elsevier Inc. All rights reserved.},
  Doi                      = {10.1016/j.jss.2011.06.026},
  ISSN                     = {01641212}
}

@InProceedings{Han2013,
  Title                    = {A texture feature analysis for diagnosis of pulmonary nodules using LIDC-IDRI database},
  Author                   = {Han, Fangfang and Zhang, Guopeng and Wang, Huafeng and Song, Bowen and Lu, Hongbing and Zhao, Dazhe and Zhao, Hong and Liang, Zhengrong},
  Year                     = {2013},
  Pages                    = {14--18},
  Publisher                = {IEEE Computer Society},

  Abstract                 = {This paper evaluated the performance of two-dimensional (2D) and 3D texture features from CT images on pulmonary nodules diagnosis using the large database LIDC-IDRI. Total of 905 nodules (422 malignant and 483 benign) with certain expert observer ratings of malignancy were extracted from the database based on the radiologists' painting boundaries. Feature analysis on the extracted nodules was not only based on the popular texture analysis method, e.g., the 2D Haralick texture feature model, we also explored a 3D Haralick feature model with variable directions in space. The relationships of more neighbour voxels on more directions were included for texture feature analysis. The well-established Support Vector Machine (SVM) classifier was used for the malignancy classification based on the 2D and 3D Haralick texture features. Half of the benign and malignant nodules were extracted randomly for training, and the left half nodules for testing. This operation was implemented for 100 iterations. Then the 100 classification results were shown based on the area under the curve (AUC) of the Receiver Operating Characteristics (ROC). The distinguishing results on the nodule malignancy based on the 3D Haralick texture features (Az = 0.9441) is noticeably more consistent with the expert observer ratings than that on the 2D features (Az = 0.9372). {\textcopyright} 2013 IEEE.},
  Doi                      = {10.1109/ICMIPE.2013.6864494},
  ISBN                     = {9781467360128}
}

@Article{Han2012,
  Title                    = {Multilinear supervised neighborhood embedding of a local descriptor tensor for scene/object recognition},
  Author                   = {Han, Xian Hua and Chen, Yen Wei and Ruan, Xiang},
  Year                     = {2012},

  Month                    = {mar},
  Number                   = {3},
  Pages                    = {1314--1326},
  Volume                   = {21},

  Abstract                 = {In this paper, we propose to represent an image as a local descriptor tensor and use a multilinear supervised neighborhood embedding (MSNE) for discriminant feature extraction, which is able to be used for subject or scene recognition. The contributions of this paper include: 1) a novel feature extraction approach denoted as the histogram of orientation weighted with a normalized gradient (NHOG) for local region representation, which is robust to large illumination variation in an image; 2) an image representation framework denoted as the local descriptor tensor, which can effectively combine a moderate amount of local features together for image representation and be more efficient than the popular existing bag-of-feature model; and 3) an MSNE analysis algorithm, which can directly deal with the local descriptor tensor for extracting discriminant and compact features and, at the same time, preserve neighborhood structure in tensor-feature space for subject/scene recognition. We demonstrate the performance advantages of our proposed approach over existing techniques on different types of benchmark database such as a scene data set (i.e., OT8), face data sets (i.e., YALE and PIE), and view-based object data sets (COIL-100 and ETH-80). {\textcopyright} 2011 IEEE.},
  Doi                      = {10.1109/TIP.2011.2168417},
  ISSN                     = {10577149}
}

@InProceedings{Henard2013,
  Title                    = {Towards automated testing and fixing of re-engineered Feature Models},
  Author                   = {Henard, Christopher and Papadakis, Mike and Perrouin, Gilles and Klein, Jacques and {Le Traon}, Yves},
  Year                     = {2013},
  Pages                    = {1245--1248},

  Abstract                 = {Mass customization of software products requires their efficient tailoring performed through combination of features. Such features and the constraints linking them can be represented by Feature Models (FMs), allowing formal analysis, derivation of specific variants and interactive configuration. Since they are seldom present in existing systems, techniques to re-engineer FMs have been proposed. There are nevertheless error-prone and require human intervention. This paper introduces an automated search-based process to test and fix FMs so that they adequately represent actual products. Preliminary evaluation on the Linux kernel FM exhibit erroneous FM constraints and significant reduction of the inconsistencies. {\textcopyright} 2013 IEEE.},
  Doi                      = {10.1109/ICSE.2013.6606689},
  ISBN                     = {9781467330763},
  ISSN                     = {02705257}
}

@Article{Hidaka2016,
  Title                    = {Feature-based classification of bidirectional transformation approaches},
  Author                   = {Hidaka, Soichiro and Tisi, Massimo and Cabot, Jordi and Hu, Zhenjiang},
  Year                     = {2016},

  Month                    = {jul},
  Number                   = {3},
  Pages                    = {907--928},
  Volume                   = {15},

  Abstract                 = {Bidirectional model transformation is a key technology in model-driven engineering (MDE), when two models that can change over time have to be kept constantly consistent with each other. While several model transformation tools include at least a partial support to bidirectionality, it is not clear how these bidirectional capabilities relate to each other and to similar classical problems in computer science, from the view update problem in databases to bidirectional graph transformations. This paper tries to clarify and visualize the space of design choices for bidirectional transformations from an MDE point of view, in the form of a feature model. The selected list of existing approaches are characterized by mapping them to the feature model. Then, the feature model is used to highlight some unexplored research lines in bidirectional transformations.},
  Doi                      = {10.1007/s10270-014-0450-0},
  ISSN                     = {16191374},
  Publisher                = {Springer Verlag}
}

@Article{Ho2013,
  Title                    = {A multispecies polyadenylation site model},
  Author                   = {Ho, Eric S. and Gunderson, Samuel I. and Duffy, Siobain},
  Year                     = {2013},

  Month                    = {jan},
  Volume                   = {14},

  Abstract                 = {Background: Polyadenylation is present in all three domains of life, making it the most conserved post-transcriptional process compared with splicing and 5'-capping. Even though most mammalian poly(A) sites contain a highly conserved hexanucleotide in the upstream region and a far less conserved U/GU-rich sequence in the downstream region, there are many exceptions. Furthermore, poly(A) sites in other species, such as plants and invertebrates, exhibit high deviation from this genomic structure, making the construction of a general poly(A) site recognition model challenging. We surveyed nine poly(A) site prediction methods published between 1999 and 2011. All methods exploit the skewed nucleotide profile across the poly(A) sites, and the highly conserved poly(A) signal as the primary features for recognition. These methods typically use a large number of features, which increases the dimensionality of the models to crippling degrees, and typically are not validated against many kinds of genomes. Results: We propose a poly(A) site model that employs minimal features to capture the essence of poly(A) sites, and yet, produces better prediction accuracy across diverse species. Our model consists of three dior-trinucleotide profiles identified through principle component analysis, and the predicted nucleosome occupancy flanking the poly(A) sites. We validated our model using two machine learning methods: logistic regression and linear discriminant analysis. Results show that models achieve 85-92{\%} sensitivity and 85-96{\%} specificity in seven animals and plants. When we applied one model from one species to predict poly(A) sites from other species, the sensitivity scores correlate with phylogenetic distances. Conclusions: A four-feature model geared towards small motifs was sufficient to accurately learn and predict poly (A) sites across eukaryotes.},
  Doi                      = {10.1186/1471-2105-14-S2-S9},
  ISSN                     = {14712105},
  Publisher                = {BioMed Central Ltd.}
}

@InProceedings{Hong2011,
  Title                    = {A novel segmentation method of high resolution remote sensing image based on multi-feature object-oriented Markov random fields model},
  Author                   = {Hong, Liang and Yang, Kun},
  Year                     = {2011},
  Pages                    = {8019--8024},

  Abstract                 = {A novel methodology base on multi-feature object-oriented MRF(MFOMRF) is proposed in order to obtain precise segmentation of high resolution satellite image. Conventional pixel-by-pixel MRF model methods only consider spatial correlation and texture of each pixel fixed square neighborhood,which are not satisfactory as the high resolution satellite contains complex spatial and texture information. the segmentation method of high resolution remote sensing image based on pixel-by-pixel MRF model usually suffer from salt and pepper noise. Based on the analysis of problems existing in pixelby-pixel MRF model methods of high-resolution remote sensed images, an multi-feature object-oriented MRF-based segmentation algorithm is proposed. The proposed method is made up of two blocks: (1) Mean-Shift algorithm is employed to obtain the over-segmentation results and the primary processing units are generated, based on which the object adjacent graph (OAG) can be constructed.(2) the generation of objects by overly segmented, the spectral, textural, and shape feature are extracted for each node in the OAG, all of these features are constructed in a feature vector, based on which the feature model is defined on the OAG, and the neighbor system, potential cliques and energy functions of OAG are exploited in the labeling model. The proposed segmentation method is evaluated on high resolution remote sensed image data set-GeoEye, And the experimental results verified that MFOMRF has the capability to obtain better segmentation results, especially for textural and shape richer images. {\textcopyright} 2011 IEEE.},
  Doi                      = {10.1109/RSETE.2011.5964014},
  ISBN                     = {9781424491711}
}

@InProceedings{Hu2010,
  Title                    = {Locating facial features by robust active shape model},
  Author                   = {Hu, Jiani and Li, Yu and Deng, Weihong and Guo, Jun and Xu, Weiran},
  Year                     = {2010},
  Pages                    = {196--200},

  Abstract                 = {Active shape model statistically represents a shape by a set of well-defined landmark points and can model object variations using principal component analysis. However, the shape generated by standard active shape model is unsmooth when the test sample has a large variation compared with the training images. In this paper, we introduce a robust active shape model for facial feature location. First, a color information and 2-dimension based local feature model is presented to characterize salient facial features, such as the eyes and the mouth. Then, a regularized principal component analysis based shape model is proposed to construct a smooth global shape. We evaluate our approach on a challenging dataset containing 2,000 well-labeled facial images with a large range of variations in pose, lighting and expression. Experimental results demonstrate the efficiency and effectiveness of the proposed approach. {\textcopyright}2010 IEEE.},
  Doi                      = {10.1109/ICNIDC.2010.5657840},
  ISBN                     = {9781424468546}
}

@Article{Hu2016,
  Title                    = {Extensions and evolution analysis method for software feature models},
  Author                   = {Hu, Jie and Wang, Qing},
  Year                     = {2016},

  Month                    = {may},
  Number                   = {5},
  Pages                    = {1212--1229},
  Volume                   = {27},

  Abstract                 = {Feature model is an essential concept and artifact in feature oriented software development (FOSD). It depicts commonality and variability (C{\&}V) of products in terms of features. With increasingly frequent software evolution, keeping the feature model in consistent with the evolution is very important. Most of the related researches usually analyze the C{\&}V on the requirement level, and modeling the analyzed C{\&}V by the feature model. However, since the feature changes may cause the ripple effect during the modeling process, some new commonalities and variability may be derived. The current researches are still not able to resolve this problem, which leads to some potential overlooking commonalities and inefficiency in reuse. This paper proposes an approach to extend the feature model and analyze the software evolution based on the feature model. The extensions of feature dependency and evolution meta-operators can support the ripple effect analysis of the feature changes, as well as the exploration of the potential commonalities. The new approach also develops some refactoring strategies and a semi-automated tool to support commonality extraction and feature refactoring. In addition, rules and strategies are designed to resolve typical configuration conflicts. Finally, the paper employs a case study to validate the applicability and effectiveness of the presented method.},
  Doi                      = {10.13328/j.cnki.jos.004829},
  ISSN                     = {10009825},
  Publisher                = {Chinese Academy of Sciences}
}

@Article{Hunter2011,
  Title                    = {Least-squares estimation of imaging parameters for an ultrasonic array using known geometric image features},
  Author                   = {Hunter, Alan J. and Drinkwater, Bruce W. and Wilcox, Paul D.},
  Year                     = {2011},

  Month                    = {feb},
  Number                   = {2},
  Pages                    = {414--426},
  Volume                   = {58},

  Abstract                 = {Ultrasonic array images are adversely affected by errors in the assumed or measured imaging parameters. For non-destructive testing and evaluation, this can result in reduced defect detection and characterization performance. In this paper, an autofocus algorithm is presented for estimating and correcting imaging parameter errors using the collected echo data and a priori knowledge of the image geometry. Focusing is achieved by isolating a known geometric feature in the collected data and then performing a weighted leastsquares minimization of the errors between the data and a feature model, with respect to the unknown parameters. The autofocus algorithm is described for the estimation of element positions in a flexible array coupled to a specimen with an unknown surface profile. Experimental results are shown using a prototype flexible array and it is demonstrated that (for an isolated feature and a well-prescribed feature model) the algorithm is capable of generating autofocused images that are comparable in quality to benchmark images generated using accurately known imaging parameters. {\textcopyright} 2006 IEEE.},
  Doi                      = {10.1109/TUFFC.2011.1819},
  ISSN                     = {08853010}
}

@InProceedings{Inrak2010,
  Title                    = {Applying latent semantic analysis to classify emotions in Thai text},
  Author                   = {Inrak, Piyatida and Sinthupinyo, Sukree},
  Year                     = {2010},
  Volume                   = {6},

  Abstract                 = {With a rapid growth of the internet communication, many types of text are produced. They can convey the meanings that can contribute to text categorization. Emotion classification also becomes more interesting, but emotion classification in Thai text is still not able to be correctly classified. Thus, this paper proposes a novel approach that takes advantage of bi-words occurrence to classify emotion hidden in a short sentence. In this paper, we classify Thai text into six basic universal emotions including anger, disgust, fear, happiness, sadness, and surprise based on latent semantic analysis approach. We compared the results between two models which construct features from the sentences and applied to three classification methods, i.e. Naive Bayes, SVM, and Decision Tree. The first feature model uses only single word occurrence in the classification. The second model uses single word combined with bi-words occurrence in the classification. The results show that the second model can yield higher accuracy than the first model based on the Na{\"{i}}ve Bayes classification method. {\textcopyright} 2010 IEEE.},
  Doi                      = {10.1109/ICCET.2010.5486137},
  ISBN                     = {9781424463503}
}

@InProceedings{Javed2015,
  Title                    = {OR-PCA with dynamic feature selection for robust background subtraction},
  Author                   = {Javed, Sajid and Sobral, Andrews and Bouwmans, Thierry and Jung, Soon Ki},
  Year                     = {2015},
  Month                    = {apr},
  Pages                    = {86--91},
  Publisher                = {Association for Computing Machinery},

  Abstract                 = {Background modeling and foreground object detection is the first step in visual surveillance system. The task becomes more difficult when the background scene contains significant variations, such as water surface, waving trees and sudden illumination conditions, etc. Recently, subspace learning model such as Robust Principal Component Analysis (RPCA) provides a very nice framework for separating the moving objects from the stationary scenes. However, due to its batch optimization process, high dimensional data should be processed. As a result, huge computational complexity and memory problems occur in traditional RPCA based approaches. In contrast, Online Robust PCA (OR-PCA) has the ability to process such large dimensional data via stochastic manners. OR-PCA processes one frame per time instance and updates the subspace basis accordingly when a new frame arrives. However, due to the lack of features, the sparse component of OR-PCA is not always robust to handle various background modeling challenges. As a consequence, the system shows a very weak performance, which is not desirable for real applications. To handle these challenges, this paper presents a multi-feature based OR-PCA scheme. A multi-feature model is able to build a robust low-rank background model of the scene. In addition, a very nice feature selection process is designed to dynamically select a useful set of features frame by frame, according to the weighted sum of total features. Experimental results on challenging datasets such as Wallower, I2R and BMC 2012 show that the proposed scheme outperforms the state of the art approaches for the background subtraction task.},
  Doi                      = {10.1145/2695664.2695863},
  ISBN                     = {9781450331968}
}

@InProceedings{Jia2012,
  Title                    = {A variable granularity user classification algorithm based on multi-dimensional features of users},
  Author                   = {Jia, Dawen and Zeng, Cheng and Peng, Zhiyong and Cheng, Peng and Yang, Zhimin},
  Year                     = {2012},
  Pages                    = {43--48},

  Abstract                 = {Classifying Web users based on multi-dimensional features is one of the foundations of realizing personalized Web applications. It could be used for user classification model, users' multi-dimensional data analysis, potential user group discovery and personalized recommendation and so forth. In this paper, a variable granularity user classification algorithm based on Web users' multidimensional features is proposed. Given a user feature model, the algorithm will mine all common feature categories and find the relationships between them. A series of experiments are conducted to analyze the performances of this algorithm with different condition. The experimental results indicate that this algorithm has good performance and can be deployed in Web applications with massive Web users. {\textcopyright} 2012 IEEE.},
  Doi                      = {10.1109/WISA.2012.45}
}

@Misc{Jiang2016,
  Title                    = {A feature based method for trajectory dataset segmentation and profiling},

  Author                   = {Jiang, Wei and Zhu, Jie and Xu, Jiajie and Li, Zhixu and Zhao, Pengpeng and Zhao, Lei},
  Month                    = {sep},
  Year                     = {2016},

  Abstract                 = {The pervasiveness of location-acquisition and mobile computing techniques has generated massive spatial trajectory data, which has brought great challenges to the management and analysis of such a big data. In this paper, we focus on the sub-trajectory dataset profiling problem, and aim to extract the representative sub-trajectories from the raw trajectory as a subset, called profile, which can best describe the whole dataset. This problem is very challenging subject to finding the most representative sub-trajectories set by trading off the size and quality of the profile. To tackle this problem, we model the features of the trajectory dataset from the aspects of density, speed and the direction flow. Meanwhile we present our two-step method to select the representative trajectories based on the feature model. First, a novel trajectory segmentation algorithm is applied on a raw trajectory to identify the representative segments concerning their feature representativeness and automatically estimate the number of segments and the segment borders. Then, a sub-trajectory profiling method is performed to yield the most representative sub-trajectories in the dataset, based on a local heuristic evolution strategy. We evaluate our method based on extensive experiments by using two real-world trajectory datasets generated by over 12,000 taxicabs in Beijing and Shanghai. The results demonstrate the efficiency and effectiveness of our methods in different applications.},
  Doi                      = {10.1007/s11280-016-0396-y},
  ISSN                     = {1386145X},
  Pages                    = {1--18},
  Publisher                = {Springer New York LLC}
}

@InProceedings{Johansen2011,
  Title                    = {Properties of realistic feature models make combinatorial testing of product lines feasible},
  Author                   = {Johansen, Martin Fagereng and Haugen, {\O}ystein and Fleurey, Franck},
  Year                     = {2011},
  Pages                    = {638--652},
  Volume                   = {6981 LNCS},

  Abstract                 = {Feature models and associated feature diagrams allow modeling and visualizing the constraints leading to the valid products of a product line. In terms of their expressiveness, feature diagrams are equivalent to propositional formulas which makes them theoretically expensive to process and analyze. For example, satisfying propositional formulas, which translates into finding a valid product for a given feature model, is an NP-hard problem, which has no fast, optimal solution. This theoretical complexity could prevent the use of powerful analysis techniques to assist in the development and testing of product lines. However, we have found that satisfying realistic feature models is quick. Thus, we show that combinatorial interaction testing of product lines is feasible in practice. Based on this, we investigate covering array generation time and results for realistic feature models and find where the algorithms can be improved. {\textcopyright} 2011 Springer-Verlag.},
  Doi                      = {10.1007/978-3-642-24485-8_47},
  ISBN                     = {9783642244841},
  ISSN                     = {03029743}
}

@InProceedings{Kamoun2016,
  Title                    = {Multiple software product lines for service oriented architecture},
  Author                   = {Kamoun, Akram and Kacem, Mohamed Hadj and Kacem, Ahmed Hadj},
  Year                     = {2016},
  Month                    = {aug},
  Pages                    = {56--61},
  Publisher                = {Institute of Electrical and Electronics Engineers Inc.},

  Abstract                 = {Combining the Service Oriented Architecture (SOA) and Software Product Line (SPL) paradigms is an emerging research area that has gained a considerable interest in recent years. We observe that the approaches proposed in the literature address mostly the variability modeling of Service Providers (SPs) (e.g., developing and composing SPs). However, handling the variability of Service Consumers (SCs) and how to interrelate the variability of SCs and SPs have not been studied. In this paper, our objective is to carry out an in-depth and rigorous study that addresses these issues. We propose a new model-based, top-down, formal and end-to-end SOA approach based on the Multiple SPLs (MSPL) paradigm. The main idea is to develop an MSPL composed of two dependent SPLs for SP and SC in order to generate customized, valid and consistent SPs and SCs. We propose that the variability of each SPL is managed by a Feature Model (FM). In order to ensure the consistency between these two SPLs and in particular between their FMs, we define the automated analysis update operator based on formal propositional logical techniques. We developed a tool that implements all the required steps of our approach and we demonstrate its efficiency in a practical case study.},
  Doi                      = {10.1109/WETICE.2016.21},
  ISBN                     = {9781509016631}
}

@Article{Kang2014,
  Title                    = {Bayesian common spatial patterns for multi-subject EEG classification},
  Author                   = {Kang, Hyohyeong and Choi, Seungjin},
  Year                     = {2014},
  Pages                    = {39--50},
  Volume                   = {57},

  Abstract                 = {Multi-subject electroencephalography (EEG) classification involves algorithm development for automatically categorizing brain waves measured from multiple subjects who undergo the same mental task. Common spatial patterns (CSP) or its probabilistic counterpart, PCSP, is a popular discriminative feature extraction method for EEG classification. Models in CSP or PCSP are trained on a subject-by-subject basis so that inter-subject information is neglected. In the case of multi-subject EEG classification, however, it is desirable to capture inter-subject relatedness in learning a model. In this paper we present a nonparametric Bayesian model for a multi-subject extension of PCSP where subject relatedness is captured by assuming that spatial patterns across subjects share a latent subspace. Spatial patterns and the shared latent subspace are jointly learned by variational inference. We use an infinite latent feature model to automatically infer the dimension of the shared latent subspace, placing Indian Buffet process (IBP) priors on our model. Numerical experiments on BCI competition III IVa and IV 2a dataset demonstrate the high performance of our method, compared to PCSP and existing Bayesian multi-task CSP models. {\textcopyright} 2014 Elsevier Ltd.},
  Doi                      = {10.1016/j.neunet.2014.05.012},
  ISSN                     = {18792782},
  Publisher                = {Elsevier Ltd}
}

@Article{Karatas2016,
  Title                    = {Attribute-based variability in feature models},
  Author                   = {Karataş, Ahmet Serkan and Oğuzt{\"{u}}z{\"{u}}n, Halit},
  Year                     = {2016},

  Month                    = {jun},
  Number                   = {2},
  Pages                    = {185--208},
  Volume                   = {21},

  Abstract                 = {Extended feature models enable the expression of complex cross-tree constraints involving feature attributes. The inclusion of attributes in cross-tree relations not only enriches the constraints, but also engenders an extended type of variability that involves attributes. In this article, we elaborate on the effects of this new variability type on feature models. We start by analyzing the nature of the variability involving attributes and extend the definitions of the configuration and the product to suit the emerging requirements. Next, we propose classifications for the features, configurations, and products to identify and formalize the ramifications that arise due to the new type of variability. Then, we provide a semantic foundation grounded on constraint satisfaction for our proposal. We introduce an ordering relation between configurations and show that the set of all the configurations represented by a feature model forms a semilattice. This is followed by a demonstration of how the feature model analyses will be affected using illustrative examples selected from existing and novel analysis operations. Finally, we summarize our experiences, gained from a commercial research and development project that employs an extended feature model.},
  Doi                      = {10.1007/s00766-014-0216-9},
  ISSN                     = {1432010X},
  Publisher                = {Springer-Verlag London Ltd}
}

@InProceedings{Karatas2013,
  Title                    = {From extended feature models to constraint logic programming},
  Author                   = {Karataş, Ahmet Serkan and Oǧuzt{\"{u}}z{\"{u}}n, Halit and Doǧru, Ali},
  Year                     = {2013},
  Month                    = {dec},
  Number                   = {12},
  Pages                    = {2295--2312},
  Volume                   = {78},

  Abstract                 = {Since feature models for realistic product families may be quite complicated, the automated analysis of feature models is desirable. Although several approaches reported in the literature address this issue, complex cross-tree relationships involving attributes in extended feature models have not been handled. In this article, we introduce a mapping from extended feature models to constraint logic programming over finite domains. This mapping is used to translate into constraint logic programs; basic, cardinality-based and extended feature models, which can include complex cross-tree relationships involving attributes. This translation enables the use of off-the-shelf constraint solvers for the automated analysis of extended feature models involving such complex relationships. We also present the performance results of some well-known analysis operations on an example translated model. {\textcopyright} 2012 Elsevier B.V. All rights reserved.},
  Doi                      = {10.1016/j.scico.2012.06.004},
  ISSN                     = {01676423}
}

@InProceedings{Karatas2010,
  Title                    = {Mapping extended feature models to constraint logic programming over finite domains},
  Author                   = {Karataş, Ahmet Serkan and Oǧuzt{\"{u}}z{\"{u}}n, Halit and Doǧru, Ali},
  Year                     = {2010},
  Pages                    = {286--299},
  Volume                   = {6287 LNCS},

  Abstract                 = {As feature models for realistic product families may be quite complicated, automated analysis of feature models is desirable. Although several approaches reported in the literature addressed this issue, complex feature-attribute and attribute-attribute relationships in extended feature models were not handled effectively. In this article, we introduce a mapping from extended feature models to constraint logic programming over finite domains. This mapping is used to translate basic, cardinality-based, and extended feature models, which may include complex feature-feature, feature-attribute and attribute-attribute cross-tree relationships, into constraint logic programs. It thus enables use of off-the-shelf constraint solvers for the automated analysis of extended feature models involving such complex relationships. We also briefly discuss the ramifications of including feature-attribute relationships in operations of analysis. We believe that this proposal will be effective for further leveraging of constraint logic programming for automated analysis of feature models. {\textcopyright} 2010 Springer-Verlag Berlin Heidelberg.},
  Doi                      = {10.1007/978-3-642-15579-6_20},
  ISBN                     = {3642155782},
  ISSN                     = {03029743}
}

@InProceedings{Karol2010,
  Title                    = {Using feature models for creating families of documents},
  Author                   = {Karol, Sven and Heinzerling, Martin and Heidenreich, Florian and A{\ss}mann, Uwe},
  Year                     = {2010},
  Pages                    = {259--262},

  Abstract                 = {Variants in a family of office documents are usually created by ad-hoc copy and paste actions from a set of base documents. As a result, the set of variants is decoupled from the original documents and is difficult to manage. In this paper we present a novel approach that uses concepts from Feature Oriented Domain Analysis (FODA) to specify document families to generate variants. As a proof of concept, we implemented the Document Feature Mapper tool, which is based on our previous experience in Software Product Line Engineering (SPLE) with FODA. In our tool, variant spaces are precisely specified using feature models and mappings relating features to slices in the document family. Given a selection of features satisfying the feature model's constraints, a variant can be derived. To show the applicability of our approach and tool, we conducted two case studies with documents in the Open Document Format (ODF). Copyright 2010 ACM.},
  Doi                      = {10.1145/1860559.1860618},
  ISBN                     = {9781450302319}
}

@InProceedings{Keller2012,
  Title                    = {Breast cancer risk prediction via area and volumetric estimates of breast density},
  Author                   = {Keller, Brad M. and Conant, Emily F. and Oh, Huen and Kontos, Despina},
  Year                     = {2012},
  Pages                    = {236--243},
  Volume                   = {7361 LNCS},

  Abstract                 = {We performed a study to assess the potential value of absolute and relative measures of area and volumetric breast density in predicting breast cancer risk. A case-control study was performed. The raw mediolateral-oblique (MLO) view digital mammography (DM) images of 106 women with unilateral breast cancer and 318 age-matched controls were retrospectively analyzed. The unaffected breast of the cancer cases was used as a surrogate of higher cancer risk. For each image, area and volumetric breast density measures were estimated using fully-automated software. The performance of the density metrics to distinguish between cancer cases and controls was assessed using linear discriminant and ROC curve analysis. Absolute measures of dense tissue content had stronger discriminatory capacity (AUCs=0.65-0.67) than percent density (AUCs=0.57). Shape-location features also showed modest discriminatory power (AUC=0.56-0.65). A combined area-volumetric model was able to outperform (AUC=0.70) any single-feature model. Absolute measures of fibroglandular tissue content were seen to be more discriminative than percent density estimates, indicating that total fibroglandular tissue content may be more reflective of cancer risk than relative measures of density. Our results suggest that area and volumetric breast density measures could be complementary in breast cancer risk assessment. {\textcopyright} 2012 Springer-Verlag Berlin Heidelberg.},
  Doi                      = {10.1007/978-3-642-31271-7_31},
  ISBN                     = {9783642312700},
  ISSN                     = {03029743}
}

@InProceedings{Kim2011,
  Title                    = {Spatial and spatiotemporal analysis of soccer},
  Author                   = {Kim, Ho Chul and Kwon, Oje and Li, Ki Joune},
  Year                     = {2011},
  Pages                    = {385--388},

  Abstract                 = {Soccer matches consist of spatial and spatiotemporal objects including a ball, players, and a field. Players and ball are also moving objects. This means that spatial and spatiotemporal analysis on ball and players could be useful in studying soccer tactics. From this view point, we extend the application areas of spatial information science to soccer tactic analysis. First we define a feature model to specify the basic units of analysis. Second, we study the morphological properties of each feature type. Third, a set of operations are defined for each feature type. Fourth, we perform an experiment with a real data set of the 2006 World Cup final match to validate the usefulness of our framework. Even though we focus on soccer match, we expect that this framework can be applied to similar sports such as handball and basket ball. {\textcopyright} 2011 Authors.},
  Doi                      = {10.1145/2093973.2094029},
  ISBN                     = {9781450310314}
}

@InProceedings{Kim2016,
  Title                    = {A formal modeling and analysis framework for software product line of preemptive real-time systems},
  Author                   = {Kim, Jin Hyun and Legay, Axel and Traonouez, Louis Marie and Acher, Mathieu and Kaist, Sungwon Kang},
  Year                     = {2016},
  Month                    = {apr},
  Pages                    = {1562--1565},
  Publisher                = {Association for Computing Machinery},

  Abstract                 = {This paper presents a formal analysis framework to analyze a family of platform products w.r.t. real-time properties. First, we propose an extension of the widely-used feature model, called Property Feature Model (PFM), that distinguishes features and properties explicitly Second, we present formal behavioral models of components of a realtime scheduling unit such that all real-time scheduling units implied by a PFM are automatically composed to be analyzed against the properties given by the PFM. We apply our approach to the verification of the schedulability of a family of scheduling units using the symbolic and statistical model checkers of Uppaal.},
  Doi                      = {10.1145/2851613.2851977},
  ISBN                     = {9781450337397}
}

@Article{Kim2015,
  Title                    = {A quantitative and knowledge-based approach to choosing security architectural tactics},
  Author                   = {Kim, Suntae},
  Year                     = {2015},

  Month                    = {jan},
  Number                   = {1-2},
  Pages                    = {45--53},
  Volume                   = {18},

  Abstract                 = {Biographical notes: Suntae Kim is an Assistant Professor of the Department of Software Engineering at Chonbuk National University. His research focuses on software architecture, design patterns, requirements engineering and mining software repository. Abstract: This paper presents a quantitative approach to choosing security architectural tactics using architectural tactic knowledge base. An architectural tactic is an architectural design building block pertaining to a software quality. The tactic knowledge base is a tactic repository composing of architectural tactic specifications defined in role based metamodelling language (RBML) and their relationships expressed in a feature model. In this paper, a cost of an architectural tactic is estimated by using the use case points method, and a level of tactic contribution for non-functional requirements (NFRs) is predicted by the analytic hierarchy process (AHP) and sensitivity analysis. Then, the proposed approach suggests the best possible fit which is likely to satisfy NFRs.We applied the approach to choosing security architectural tactics for building software architecture of an online trading system.},
  Doi                      = {10.1504/IJAHUC.2015.067780},
  ISSN                     = {17438233},
  Publisher                = {Inderscience Enterprises Ltd.}
}

@Article{Lee2014,
  Title                    = {Domain-oriented variability modeling for reuse of simulation models},
  Author                   = {Lee, Hyesun and Yang, Jin Seok and Kang, Kyo Chul and Pyun, Jai Jeong},
  Year                     = {2014},
  Number                   = {4},
  Pages                    = {438--459},
  Volume                   = {90},

  Abstract                 = {Reusability is an important quality attribute for defense modeling and simulation (MS) due to the ever-changing combat simulations and new requirements. There has been research conducted worldwide for reusing simulation models. The methods proposed in these studies (including One Semi-Automated Forces (OneSAF)) support reuse of simulation components in the development of new models. As the reuse units in the existing methods are at the simulation component level, when existing components do not satisfy new simulation requirements, new components have to be developed and maintained separately from the existing ones. However, simulation components in the same domain tend to have common parts; behavior models for tactical missions and battlefield functions in the same domain are derived from the same tactical doctrine/manual, and thus they tend to have a common structure. There is a need for a new method to maximize reusability by providing "fine-grained" reuse, i.e. composing simulation components from reusable fine-grained modules (i.e. behaviors/functions). We address the problem by applying the product line engineering concept to the development of simulation components. Commonalities and variabilities (CVs) of domain-specific simulation requirements and CVs of tactical behaviors and battlefield functions are identified in domain-oriented variability modeling. Then, the CVs are used to design and implement domain-specific simulation component assets with domain-specific tactical behaviors and battlefield functions while embedding the identified variabilities. These domain-specific component assets are instantiated based on selections of variabilities and then integrated to develop a simulation model. Feasibility of the method was demonstrated in an infantry squad combat domain of the Republic of Korea armed forces. {\textcopyright} 2014 The Society for Modeling and Simulation International.},
  Doi                      = {10.1177/0037549714525679},
  ISSN                     = {17413133},
  Publisher                = {SAGE Publications Ltd}
}

@Article{Lei2014,
  Title                    = {Geometric invariant features in the Radon transform domain for near-duplicate image detection},
  Author                   = {Lei, Yanqiang and Zheng, Ligang and Huang, Jiwu},
  Year                     = {2014},
  Number                   = {11},
  Pages                    = {3630--3640},
  Volume                   = {47},

  Abstract                 = {Radon transform has been widely used in content-based image representation due to its excellent geometric properties. In this paper, we propose a family of geometric invariant features based on Radon transform for near-duplicate image detection. According to the theoretical analysis between geometric operations (translation, scaling, and rotation) and Radon transform, we present a geometric invariant feature model. Based on the feature model, we developed four kinds of geometric invariant features. In addition, a uniform sampling technique is introduced to combine different features. The comprehensive performance of the combined feature is better than that of a single one. Extensive experiments show that the proposed features are robust, not only to rotation and scaling, but also to other operations, such as compression, noise contamination, blurring, illumination modification, cropping, etc., and achieve strong competitive performance compared with the state-of-the-art image features. {\textcopyright} 2014 Elsevier Ltd.},
  Doi                      = {10.1016/j.patcog.2014.05.009},
  ISSN                     = {00313203},
  Publisher                = {Elsevier Ltd}
}

@InProceedings{Leitner2010,
  Title                    = {Managing ERP configuration variants: An experience report},
  Author                   = {Leitner, Andrea and Kreiner, Christian},
  Year                     = {2010},

  Abstract                 = {The concepts of Software Product Line Engineering (SPLE) have been adapted and applied to enterprise IT systems, in particular the ERP systems of a production company. Based on a 2-layer feature model for the domain of the company's business processes, individual, albeit similar division's ERP system configurations can be derived by feature selection forming a variant description model. It is indicated that regular release upgrades can also benefit from the SPLE approach. The customization capabilities of the ERP platform are captured in another model; building up this model is automated according to information extracted online. As well, customizing an ERP system - based on the models mentioned - is performed online with the help of a connector developed in this project. Quantitative analysis and lessons learned during the project conclude this experience report. {\textcopyright} 2010 ACM.},
  Doi                      = {10.1145/1964138.1964140},
  ISBN                     = {9781450305426}
}

@InProceedings{Li2012,
  Title                    = {MbFM: A matrix-based tool for modeling and configuring feature models},
  Author                   = {Li, Long and Zhao, Haiyan and Zhang, Wei},
  Year                     = {2012},
  Pages                    = {325--326},

  Abstract                 = {Feature-oriented analysis and modeling is widely accepted in software reuse, which consists of two major phases that should be taken seriously. The first is to construct a feature model, and the second is to configure products based on the feature model attained in the first. This paper presents a matrix-based approach to constructing and configuring feature models, whose main advantage is its scalability compared to traditional graphic-based feature models, and the supporting tool is presented to demonstrate its feasibility. {\textcopyright} 2012 IEEE.},
  Doi                      = {10.1109/RE.2012.6345827},
  ISBN                     = {9781467327855}
}

@Article{Li2013,
  Title                    = {An automatic-propagation strategy and selective-undo mechanism for feature model configuration},
  Author                   = {Li, Long and Zhao, Hai Yan and Zhang, Wei},
  Year                     = {2013},

  Month                    = {jan},
  Number                   = {1},
  Pages                    = {132--142},
  Volume                   = {36},

  Abstract                 = {Feature-oriented domain analysis methods have been adopted by most important software reuse methods, which regard features as the basic elements in the problem space, and employ features and the relationships between features (called feature model) to structure the problem space. By customization and extension, feature models support the reuse of domain requirements in some extent. In feature-oriented methods, it is a subject worthy of in-depth study on how to use intrinsic relationships in a feature model to realize automatic-propagation and selective-undo of feature model configuration. This paper presents an automatic-propagation algorithm and one selective-undo mechanism for configuring the matrix-based feature model, by taking both the simple and composite constraints into consideration, and investigating the details of the algorithms implementation.},
  Doi                      = {10.3724/SP.J.1016.2013.00132},
  ISSN                     = {02544164}
}

@Article{Lin2013,
  Title                    = {Orientation-coherence-feature-based method to detect small target in drift-scanning image},
  Author                   = {Lin, Jian Lin and Ping, Xi Jian and Ma, De Bao},
  Year                     = {2013},

  Month                    = {jun},
  Number                   = {6},
  Pages                    = {875--882},
  Volume                   = {39},

  Abstract                 = {In order to detect the small target in the drift-scanning image, this paper proposes a method based on orientation coherence features. According to the general description about small target, a feature model is established to represent the region of small target firstly. By the analysis of Gabor filter, the primary and secondary filter groups with four orientation channels are then constructed to extract the features of objects. And a consistency description indicator is used for measuring the feature quantitatively. Finally, the small target detection operator and the corresponding parameter selection principle are given by the feature description model. Experiments on an actual image have shown that the method proposed in this paper has good performance on target detection and high-frequency interference suppression. And the model proposed is testified to be reasonable.},
  Doi                      = {10.3724/SP.J.1004.2013.00875},
  ISSN                     = {02544156}
}

@Article{Ling2010,
  Title                    = {An Analysis of HMM-based prediction of articulatory movements},
  Author                   = {Ling, Zhen Hua and Richmond, Korin and Yamagishi, Junichi},
  Year                     = {2010},

  Month                    = {oct},
  Number                   = {10},
  Pages                    = {834--846},
  Volume                   = {52},

  Abstract                 = {This paper presents an investigation into predicting the movement of a speaker's mouth from text input using hidden Markov models (HMM). A corpus of human articulatory movements, recorded by electromagnetic articulography (EMA), is used to train HMMs. To predict articulatory movements for input text, a suitable model sequence is selected and a maximum-likelihood parameter generation (MLPG) algorithm is used to generate output articulatory trajectories. Unified acoustic-articulatory HMMs are introduced to integrate acoustic features when an acoustic signal is also provided with the input text. Several aspects of this method are analyzed in this paper, including the effectiveness of context-dependent modeling, the role of supplementary acoustic input, and the appropriateness of certain model structures for the unified acoustic-articulatory models. When text is the sole input, we find that fully context-dependent models significantly outperform monophone and quinphone models, achieving an average root mean square (RMS) error of 1.945 mm and an average correlation coefficient of 0.600. When both text and acoustic features are given as input to the system, the difference between the performance of quinphone models and fully context-dependent models is no longer significant. The best performance overall is achieved using unified acoustic-articulatory quinphone HMMs with separate clustering of acoustic and articulatory model parameters, a synchronous-state sequence, and a dependent-feature model structure, with an RMS error of 0.900 mm and a correlation coefficient of 0.855 on average. Finally, we also apply the same quinphone HMMs to the acoustic-articulatory, or inversion, mapping problem, where only acoustic input is available. An average root mean square (RMS) error of 1.076 mm and an average correlation coefficient of 0.812 are achieved. Taken together, our results demonstrate how text and acoustic inputs both contribute to the prediction of articulatory movements in the method used. {\textcopyright} 2010 Elsevier B.V.},
  Doi                      = {10.1016/j.specom.2010.06.006},
  ISSN                     = {01676393}
}

@Article{Liu2014,
  Title                    = {Adaptive spatial partition learning for image classification},
  Author                   = {Liu, Bingyuan and Liu, Jing and Lu, Hanqing},
  Year                     = {2014},

  Month                    = {oct},
  Pages                    = {282--290},
  Volume                   = {142},

  Abstract                 = {Spatial Pyramid Matching is a successful extension of bag-of-feature model to embed spatial information of local features, in which the image is divided into a sequence of increasingly finer girds, and the grids are taken as uniform spatial partitions in ad-hoc manner without any theoretical motivation. Obviously, the uniform spatial partition cannot adapt to different spatial distribution across image categories. To this end, we propose a data-driven approach to adaptively learn the discriminative spatial partitions corresponding to each class, and explore them for image classification. First, a set of over-complete spatial partitions covering kinds of spatial distribution of local features are created in a flexible manner, and we concatenate the feature representations of each partitioned region. Then we adopt a discriminative learning formulation with the group sparse constraint to find a sparse mapping from the feature representation to the label space. To further enhance the robustness of the model, we compress the feature representation by removing the dimensions corresponding to those unimportant partitioned regions, and explore the compressed representation to generate a multi-region matching kernel prepared to train a one-versus-others SVM classifier. The experiments on three object datasets (i.e. Caltech-101, Caltech-256, Pascal VOC 2007), and one scene dataset (i.e. 15-Scenes) demonstrate the effectiveness of our proposed method. {\textcopyright} 2014 Elsevier B.V.},
  Doi                      = {10.1016/j.neucom.2014.03.057},
  ISSN                     = {18728286},
  Publisher                = {Elsevier}
}

@Article{Liu2015,
  Title                    = {Rapid diagnosis of tomato N-P-K nutrition level based on hyperspectral technology},
  Author                   = {Liu, Hongyu and Mao, Hanping and Zhu, Wenjing and Zhang, Xiaodong and Gao, Hongyan},
  Year                     = {2015},

  Month                    = {jan},
  Pages                    = {212--220},
  Volume                   = {31},

  Abstract                 = {Because of the short growth cycle, large yield and high fertilizer requirements of facility crop, which were the characteristics causing high cost and high complement in cultivation medium, nitrogen (N), phosphorus (P) and Potassium (K) deficiencies frequent occurrence in the growth of facility crops. Accurate monitoring and diagnosis of nutrient content in facility crops during the growth process was very important. In order to diagnose tomato nitrogen, phosphorus and potassium nutrition level more accurately, rapidly and stably, in the aspect of the spectral diagnostics technology, the changes of reflectance on characteristic wavelengths were taking into consideration to assess the nutritional status of crops. Sensitive bands were selected by using genetic algorithms. Then, the quantitative models of tomato nitrogen, phosphorus, potassium were established via stepwise regression, principal component regression and partial least squares method respectively based on reflectance spectra. The results verified that the stepwise regression models outperformed the principal component and partial smallest squares regression models of nitrogen and phosphorus, while principal component regression get the best models of phosphorus. The correlation coefficient R of the best models were nitrogen (0.9026)〉phosphorus (0.8819)〉potassium (0.8561). The root mean square error (RMSE) were nitrogen (0.3191) 〈phosphorus (0.4978)〈potassium (0.5128). Imaging technology can analyze the change of texture and other characteristics that were caused by plant nutrient deficiency. Texture features were extracted from images under-sensitive wavelength by using principal component analysis. The nutrients models of the tomato leaf nitrogen, phosphorus and potassium based on image features were established by stepwise regression, principal component regression and partial least squares method respectively. The results verified that the principal component regression models outperformed others models of nitrogen and phosphorus, while partial least squares method get the best models of phosphorus. The correlation coefficient R of the best models were nitrogen (0.9271)〉potassium (0.8991) 〉phosphorus (0.8673). The root mean square error (RMSE) were nitrogen (0.3413)〈phosphorus (0.3994)〈potassium (0.5628). For overcoming the inadequacies of models build with single feature sauce, diagnosed models of multi-information fusion was established for tomato nutrients stress via artificial neural network modeling. Feature layer fusion was combining with the internal components and external morphological caused by crop nutrients stress. The correlation coefficient R of nitrogen, phosphorus and potassium were 0.9651, 0.9216 and 0.9353. The root mean square error (RMSE) were 0.19, 0.33 and 0.29. Results fully showed that the spectra reflection technology and image technology after feature layer integration models were better than spectra reflection or single image technology. Artificial neural network models of nitrogen, phosphorus and potassium improve the correlation coefficient R accurately were 6.25{\%}, 3.97{\%}, 7.92{\%} than the single spectral model and 3.80{\%}, 5.43{\%}, 3.26{\%} than the single image model. Furthermore, the detection root mean square error was reducing. The results showed that the multi-information fusion models achieve a substantial increase in model accuracy and have better diagnostic accuracy in achieving high accuracy compared with a single feature model, thus the rapid and high sensitivity detection of nutritional stress of the tomato leaves could be realized, which provides basis to methods about crop nutrients for the development of fast and accurate diagnostic instrument with important academic value and application prospect.},
  Doi                      = {10.3969/j.issn.1002-6819.2015.z1.025},
  ISSN                     = {10026819},
  Publisher                = {Chinese Society of Agricultural Engineering}
}

@Article{Liu2015a,
  Title                    = {A novel CACD/CAD/CAE integrated design framework for fiber-reinforced plastic parts},
  Author                   = {Liu, Jikai and Ma, Yongsheng and Fu, Junyu and Duke, Kajsa},
  Year                     = {2015},

  Month                    = {apr},
  Pages                    = {13--29},
  Volume                   = {87},

  Abstract                 = {This work presents a novel CACD/CAD/CAE integrated framework for design, modeling, and optimization of fiber-reinforced plastic parts, which can greatly enhance the current design practice by realizing partial automation and multi-stage optimization. To support this framework, a new heterogeneous feature model (HFM) has been developed to model the fiber-reinforced objects and to be transferred between engineering modules. To be specific, the CACD (computer-aided conceptual design) module employs the level-set structure and material optimization to produce the initial design with thickness control, and also the initial HFM; the CAD (computer-aided design) module allows manual editing on the HFM to reflect various design intents; then, the injection molding CAE (computer-aided engineering) simulates the manufacturing process, and the response surface method (RSM) is applied to optimize the process parameters of gate location, injection flow rate, mold temperature and melt temperature, to approach the manufactured fiber orientation distribution close to the optimized result produced by the CACD module; besides, the structural analysis CAE module generates the mechanical performance result to support the CACD module, as well as to validate the final design. By applying this framework, the final structural design including the fiber orientation distribution, will perform better in mechanical properties, and consume less matrix and fiber materials; besides, the design maturity can be approached in shorter time. To prove the effectiveness, a plastic gripper design will be comprehensively studied.},
  Doi                      = {10.1016/j.advengsoft.2015.04.013},
  ISSN                     = {18735339},
  Publisher                = {Elsevier Ltd}
}

@InProceedings{Liu2014a,
  Title                    = {Combined goal and feature model reasoning with the User Requirements Notation and jUCMNav},
  Author                   = {Liu, Yanji and Su, Yukun and Yin, Xinshang and Mussbacher, Gunter},
  Year                     = {2014},
  Month                    = {sep},
  Pages                    = {321--322},
  Publisher                = {Institute of Electrical and Electronics Engineers Inc.},

  Abstract                 = {The User Requirements Notation (URN) is an international requirements engineering standard published by the International Telecommunication Union. URN supports goal-oriented and scenario-based modeling and analysis. jUCMNav is an open-source, Eclipse-based modeling tool for URN. This tool demonstration focuses on recent extensions to jUCMNav that have incorporated feature models into a URN-based modeling and reasoning framework. Feature modeling is a well-establishing technique for capturing commonalities and variabilities of Software Product Lines. Combined with URN, it is possible to reason about the impact of feature configurations on stakeholder goals and system qualities, thus helping to identify the most appropriate features for a stakeholder. Furthermore, coordinated feature and goal model reasoning is fundamental to Concern-Driven Development, where concerns are defined with a three-part variation, customization, and usage interface. As the variation interface is described with feature and goal models, it is now possible with jUCMNav to define and reason about a concern's variation interface, which is a prerequisite for composing multiple concerns based on their three-part interfaces.},
  Doi                      = {10.1109/RE.2014.6912277},
  ISBN                     = {9781479930333}
}

@Article{Liu2010,
  Title                    = {A semantic feature model in concurrent engineering},
  Author                   = {Liu, Yong Jin and Lai, Kam Lung and Dai, Gang and Yuen, Matthew Ming Fai},
  Year                     = {2010},

  Month                    = {jul},
  Number                   = {3},
  Pages                    = {659--665},
  Volume                   = {7},

  Abstract                 = {Concurrent engineering (CE) is a methodology applied to product lifecycle development so that high quality, well designed products can be provided at lower prices and in less time. Many research works have been proposed for efficiently modeling of different domains in CE. However, an integration of these works with consistent data flow is absent and still in great demand in industry. In this paper, we present a generic integration framework with a semantic feature model for knowledge representation and reasoning across domains in CE. An implementation of the proposed semantic feature model is presented to demonstrate its advantage in knowledge representation by feature transformation across domains in CE. {\textcopyright} 2010 IEEE.},
  Doi                      = {10.1109/TASE.2009.2039996},
  ISSN                     = {15455955}
}

@InProceedings{Lopez-Herrejon2012,
  Title                    = {Reverse engineering feature models with evolutionary algorithms: An exploratory study},
  Author                   = {Lopez-Herrejon, Roberto Erick and Galindo, Jos{\'{e}} A. and Benavides, David and Segura, Sergio and Egyed, Alexander},
  Year                     = {2012},
  Pages                    = {168--182},
  Volume                   = {7515 LNCS},

  Abstract                 = {Successful software evolves, more and more commonly, from a single system to a set of system variants tailored to meet the similiar and yet different functionality required by the distinct clients and users. Software Product Line Engineering (SPLE) is a software development paradigm that has proven effective for coping with this scenario. At the core of SPLE is variability modeling which employs Feature Models (FMs) as the de facto standard to represent the combinations of features that distinguish the systems variants. Reverse engineering FMs consist in constructing a feature model from a set of products descriptions. This research area is becoming increasingly active within the SPLE community, where the problem has been addressed with different perspectives and approaches ranging from analysis of configuration scripts, use of propositional logic or natural language techniques, to ad hoc algorithms. In this paper, we explore the feasibility of using Evolutionary Algorithms (EAs) to synthesize FMs from the feature sets that describe the system variants. We analyzed 59 representative case studies of different characteristics and complexity. Our exploratory study found that FMs that denote proper supersets of the desired feature sets can be obtained with a small number of generations. However, reducing the differences between these two sets with an effective and scalable fitness function remains an open question. We believe that this work is a first step towards leveraging the extensive wealth of Search-Based Software Engineering techniques to address this and other variability management challenges. {\textcopyright} 2012 Springer-Verlag.},
  Doi                      = {10.1007/978-3-642-33119-0_13},
  ISBN                     = {9783642331183},
  ISSN                     = {03029743}
}

@Article{Luo2011,
  Title                    = {A of feature reuse method at requirement level based on aspect encapsulation},
  Author                   = {Luo, Shutong and Pei, Zhili and Zhang, Changhai and Jin, Ying},
  Year                     = {2011},

  Month                    = {sep},
  Number                   = {9},
  Pages                    = {1714--1721},
  Volume                   = {48},

  Abstract                 = {Identification of reusable software assets is the basis of software reusable exercise. Feature model can organize software requirements effectively in a certain domain by defining features and their relationship, which provides strong support for domain requirements reuse. Aspect-oriented system design emphasizes reducing entangles among requirements or codes produced during software development and achieving high modularity by encapsulating crosscutting concerns into aspects, which benefits maintenance and reuse. A method of aspect encapsulation of features from feature model at requirement level is proposed for the purpose of feature reuse, and it can identify the module reused from legacy systems in one domain. At first, through analyzing requirements documents of multi-legacy systems, system concerns are elicited and domain concern hierarchical structure is established. Next, a set of domain features are identified, and aspect encapsulation is done on similar features, and the feature layer model is set up. Finally a new system is developed with the assistance and reuse of feature layer model and encapsulated aspects. A case study is done by applying our method to design a new Web system from two legacy Web systems. It has been indicated that our approach is helpful for reusing multi-legacy systems in one domain.},
  ISSN                     = {10001239}
}

@InProceedings{Mørup2011,
  Title                    = {Infinite multiple membership relational modeling for complex networks},
  Author                   = {M{\o}rup, Morten and Schmidt, Mikkel N. and Hansen, Lars Kai},
  Year                     = {2011},

  Abstract                 = {Learning latent structure in complex networks has become an important problem fueled by many types of networked data originating from practically all fields of science. In this paper, we propose a new non-parametric Bayesian multiple-membership latent feature model for networks. Contrary to existing multiple-membership models that scale quadratically in the number of vertices the proposed model scales linearly in the number of links admitting multiple-membership analysis in large scale networks. We demonstrate a connection between the single membership relational model and multiple membership models and show on "real" size benchmark network data that accounting for multiple memberships improves the learning of latent structure as measured by link prediction while explicitly accounting for multiple membership result in a more compact representation of the latent structure of networks. {\textcopyright} 2011 IEEE.},
  Doi                      = {10.1109/MLSP.2011.6064546},
  ISBN                     = {9781457716232}
}

@Article{Ma2014,
  Title                    = {Building modeling tools based on metamodeling and product line technologies},
  Author                   = {Ma, Zhiyi and He, Xiao},
  Year                     = {2014},
  Number                   = {2},
  Pages                    = {219--226},
  Volume                   = {23},

  Abstract                 = {With the evolution of existing modeling languages and the emergence of more and more new modcling languages, it is necessary to rapidly build the corresponding software modeling tools with good quality. However, modeling tools for larger modeling languages are usually diversity in function and complexity in implementation technology. Taking building modeling tools as a domain, this paper presents an approach to building software modcling tools based on metamodeling and product line technologies. The paper provides the concept system of the approach and a feature model from diverse functions of modcling tools in order to specify the commonality and variability of the tools by deeply making the domain analysis, discusses the design and implementation of a general tool framework that provides the conveniences for reusing components and generating code for components, and specifies the mapping between the feature model and the components for modeling tools.},
  ISSN                     = {10224653},
  Publisher                = {Chinese Institute of Electronics}
}

@Misc{Maazoun2016,
  Title                    = {Change impact analysis for software product lines},

  Author                   = {Ma{\^{a}}zoun, Jihen and Bouassida, Nadia and Ben-Abdallah, Han{\^{e}}ne},
  Month                    = {oct},
  Year                     = {2016},

  Abstract                 = {A software product line (SPL) represents a family of products in a given application domain. Each SPL is constructed to provide for the derivation of new products by covering a wide range of features in its domain. Nevertheless, over time, some domain features may become obsolete with the apparition of new features while others may become refined. Accordingly, the SPL must be maintained to account for the domain evolution. Such evolution requires a means for managing the impact of changes on the SPL models, including the feature model and design. This paper presents an automated method that analyzes feature model evolution, traces their impact on the SPL design, and offers a set of recommendations to ensure the consistency of both models. The proposed method defines a set of new metrics adapted to SPL evolution to identify the effort needed to maintain the SPL models consistently and with a quality as good as the original models. The method and its tool are illustrated through an example of an SPL in the Text Editing domain. In addition, they are experimentally evaluated in terms of both the quality of the maintained SPL models and the precision of the impact change management.},
  Doi                      = {10.1016/j.jksuci.2016.01.005},
  ISSN                     = {22131248},
  Number                   = {4},
  Pages                    = {364--380},
  Publisher                = {King Saud bin Abdulaziz University},
  Volume                   = {28}
}

@InProceedings{Maazoun2013,
  Title                    = {Feature model extraction from product source codes based on the semantic aspect},
  Author                   = {Maazoun, Jihen and Bouassida, Nadia and Ben-Abdallah, Han{\^{e}}ne and Seriai, Abdelhak Djamel},
  Year                     = {2013},
  Pages                    = {154--161},

  Abstract                 = {Software Product Lines can be constructed through either a top-down or bottom-up process. A top-down process begins by a domain analysis where variabilities are specified then it derives the product line. It is especially interesting for the creation of new product lines. However, in practice, SPL are often set up after several similar product variants have been in use. This practice prompted the search for bottom-up processes that start from an analysis of existing product variants to identify the product line. The proposed bottom-up processes rely on two hypotheses: The product variants use the same vocabulary to name elements in their source code, and the product variants have very similar/identical structures. However, while the names represent the application domain of the products, when different developers were involved in the development of the product variants, the naming assumption becomes too restrictive. Furthermore, the variants' code structures are often different when developed separately and even when one variant is derived from another through several modifications. To loosen these two hypotheses, this paper proposes a bottom-up approach that integrates the semantic aspect of the product variants when extracting the SPL feature model. In addition, a second contribution of our approach is its capability to identify automatically the constraints among the identified features. Copyright {\textcopyright} 2013 SCITEPRESS.},
  ISBN                     = {9789898565686}
}

@InProceedings{Malin2012,
  Title                    = {Beta process based adaptive learning for immunosignature microarray feature identification},
  Author                   = {Malin, Anna and Kovvali, Narayan and Papandreou-Suppappola, Antonia and Zhang, Jun Jason and Johnston, Stephen and Stafford, Phillip},
  Year                     = {2012},
  Pages                    = {1651--1655},

  Abstract                 = {We propose a latent feature model for immunosignature random peptide microarray data using beta process factor analysis to identify relationships between patients and infectious agents. The method uses Bayesian nonparametric adaptive learning techniques that allow for further classification if additional patient data is received, and new relationships between patients and disease states are obtained. In addition to feature discovery, this methodology can also detect biothreat agents on the fly. Using experimental immunosignature microarray data, we demonstrate the identification and classification of underlying relationships between patients with different disease states. {\textcopyright} 2012 IEEE.},
  Doi                      = {10.1109/ACSSC.2012.6489312},
  ISBN                     = {9781467350518},
  ISSN                     = {10586393}
}

@InProceedings{Maoz2011,
  Title                    = {Semantically configurable consistency analysis for class and object diagrams},
  Author                   = {Maoz, Shahar and Ringert, Jan Oliver and Rumpe, Bernhard},
  Year                     = {2011},
  Pages                    = {153--167},
  Volume                   = {6981 LNCS},

  Abstract                 = {Checking consistency between an object diagram (OD) and a class diagram (CD) is an important analysis problem. However, several variations in the semantics of CDs and ODs, as used in different contexts and for different purposes, create a challenge for analysis tools. To address this challenge in this paper we investigate semantically configurable model analysis. We formalize the variability in the language's semantics using a feature model: each configuration that the model permits induces a different semantics. Moreover, we develop a parametrized analysis that can be instantiated to comply with every legal configuration of the feature model. Thus, the analysis is semantically configured and its results change according to the semantics induced by the selected feature configuration. The ideas are implemented using a parametrized transformation to Alloy. The work can be viewed as a case study example for a formal and automated approach to handling semantic variability in modeling languages. {\textcopyright} 2011 Springer-Verlag.},
  Doi                      = {10.1007/978-3-642-24485-8_12},
  ISBN                     = {9783642244841},
  ISSN                     = {03029743}
}

@Article{Mazo2012,
  Title                    = {Constraints: The heart of domain and application engineering in the product lines engineering strategy},
  Author                   = {Mazo, Ra{\'{u}}l and Salinesi, Camille and Diaz, Daniel and Djebbi, Olfa and Lora-Michiels, Alberto},
  Year                     = {2012},

  Month                    = {apr},
  Number                   = {2},
  Pages                    = {33--68},
  Volume                   = {3},

  Abstract                 = {Drawing from an analogy between features based Product Line (PL) models and Constraint Programming (CP), this paper explores the use of CP in the Domain Engineering and Application Engineering activities that are put in motion in a Product Line Engineering strategy. Specifying a PL as a constraint program instead of a feature model carries out two important qualities of CP: expressiveness and direct automation. On the one hand, variables in CP can take values over boolean, integer, real or even complex domains and not only boolean values as in most PL languages such as the Feature-Oriented Domain Analysis (FODA). Specifying boolean, arithmetic, symbolic and reified constraint, provides a power of expression that spans beyond that provided by the boolean dependencies in FODA models. On the other hand, PL models expressed as constraint programs can directly be executed and analyzed by off-the-shelf solvers. This paper explores the issues of (a) how to specify a PL model using CP, including in the presence of multi-model representation, (b) how to verify PL specifications, (c) how to specify configuration requirements, and (d) how to support the product configuration activity. Tests performed on a benchmark of 50 PL models show that the approach is efficient and scales up easily to very large and complex PL specifications. Copyright {\textcopyright} 2012, IGI Global.},
  Doi                      = {10.4018/jismd.2012040102},
  ISSN                     = {19478186}
}

@InProceedings{Mazo2011,
  Title                    = {Transforming attribute and clone-enabled feature models into constraint programs over finite domains},
  Author                   = {Mazo, Ra{\'{u}}l and Salinesi, Camille and Diaz, Daniel and Lora-Michiels, Alberto},
  Year                     = {2011},
  Pages                    = {188--199},

  Abstract                 = {Product line models are important artefacts in product line engineering. One of the most popular languages to model the variability of a product line is the feature notation. Since the initial proposal of feature models in 1990, the notation has evolved in different aspects. One of the most important improvements allows specify the number of instances that a feature can have in a particular product. This improvement implies an important increase on the number of variables needed to represent a feature model. Another improvement consists in allowing features to have attributes, which can take values on a different domain than the boolean one. These two extensions have increased the complexity of feature models and therefore have made more difficult the manually or even automated reasoning on feature models. To the best of our knowledge, very few works exist in literature to address this problem. In this paper we show that reasoning on extended feature models is easy and scalable by using constraint programming over integer domains. The aim of the paper is double (a) to show the rules for transforming extended feature models into constraint programs, and (b) to demonstrate, by means of 11 reasoning operations over feature models, the usefulness and benefits of our approach. We evaluated our approach by transforming 60 feature models of sizes up to 2000 features and by comparing it with 2 other approaches available in the literature. The evaluation showed that our approach is correct, useful and scalable to industry size models.},
  ISBN                     = {9789898425577}
}

@InProceedings{Mefteh2015,
  Title                    = {Implementation and evaluation of an approach for extracting feature models from documented UML use case diagrams},
  Author                   = {Mefteh, Mariem and Bouassida, Nadia and Ben-Abdallah, Han{\^{e}}ne},
  Year                     = {2015},
  Month                    = {apr},
  Pages                    = {1602--1609},
  Publisher                = {Association for Computing Machinery},

  Abstract                 = {Software product lines (SPL) aim at facing the increasing costs of software products by reusing core assets of existing products in a given domain. They are often described using feature models which, as we proposed in a previous work, can be built from possibly incomplete, documented UML use case diagrams assets using the Formal Concept Analysis method, semantic model and trigger model. In order to evaluate this approach, we present in this paper the UC2FM-tool which automates all its steps. In addition, we report on a comparison of the values of quality metrics of feature models produced by our approach with those of existing feature models built by experts for five different domains.},
  Doi                      = {10.1145/2695664.2695907},
  ISBN                     = {9781450331968}
}

@InProceedings{Mefteh2014,
  Title                    = {Feature model extraction from documented UML use case diagrams},
  Author                   = {Mefteh, Mariem and Bouassida, Nadia and Ben-Abdallah, Han{\^{e}}ne},
  Year                     = {2014},
  Number                   = {2},
  Pages                    = {107--116},
  Publisher                = {Ada-Europe},
  Volume                   = {35},

  Abstract                 = {The development of a feature model for a software product line requires a thorough domain analysis which needs a high expertise. Indeed, analysts must not only understand the requirements for one particular application, but they must also identify variable ways in which the requirements can be combined in all applications in the domain. Such an expertise being often hard to acquire, analysts need approaches that provide assistance based on any existing artefacts produced during the development of applications in the domain of the product line. In this paper, we present a fully automated approach that assists domain analysts in specifying the feature model of a software product line. Our approach exploits the use case diagrams of existing applications along with their textual documentation. Besides using the natural language documentation of the requirements, it has the merit of overcoming the possible incompleteness of such documentation.},
  ISSN                     = {13816551}
}

@Article{Mendonca2010,
  Title                    = {Decision-making coordination and efficient reasoning techniques for feature-based configuration},
  Author                   = {Mendonca, Marcilio and Cowan, Donald},
  Year                     = {2010},

  Month                    = {may},
  Number                   = {5},
  Pages                    = {311--332},
  Volume                   = {75},

  Abstract                 = {Software Product Lines is a contemporary approach to software development that exploits the similarities and differences within a family of systems in a particular domain of interest in order to provide a common infrastructure for deriving members of this family in a timely fashion, with high-quality standards, and at lower costs. In Software Product Lines, feature-based product configuration is the process of selecting the desired features for a given software product from a repository of features called a feature model. This process is usually carried out collaboratively by people with distinct skills and interests called stakeholders. Collaboration benefits stakeholders by allowing them to directly intervene in the configuration process. However, collaboration also raises an important side effect, i.e., the need of stakeholders to cope with decision conflicts. Conflicts arise when decisions that are locally consistent cannot be applied globally because they violate one or more constraints in the feature model. Unfortunately, current product configuration systems are typically single-user-based in the sense that they do not provide means to coordinate concurrent decision-making on the feature model. As a consequence, configuration is carried out by a single person that is in charge of representing the interests of all stakeholders and managing decision conflicts on their own. This results in an error-prone and time-consuming process that requires past decisions to be revisited continuously either to correct misinterpreted stakeholder requirements or to handle decision conflicts. Yet another challenging issue related to configuration problems is the typically high computational cost of configuration algorithms. In fact, these algorithms frequently fall into the category of NP-hard and thus can become intractable in practice. In this paper, our goal is two-fold. First, we revisit our work on Collaborative Product Configuration (CPC) in which we proposed an approach to describe and validate collaborative configuration scenarios. We discuss how collaborative configuration can be described in terms of a workflow-like plan that safely guides stakeholders during the configuration process. Second, we propose a preliminary set of reasoning algorithms tailored to the feature modelling domain that can be used to provide automated support for product configuration. In addition, we compare empirically the performance of the proposed algorithms to that of a general-purpose solution. We hope that the insights provided in this paper will encourage other researchers to develop new algorithms in the near future. {\textcopyright} 2009 Elsevier B.V. All rights reserved.},
  Doi                      = {10.1016/j.scico.2009.12.004},
  ISSN                     = {01676423}
}

@InProceedings{Menkyna2012,
  Title                    = {Aspect-oriented change realization based on multi-paradigm design with feature modeling},
  Author                   = {Menkyna, Radoslav and Vrani{\'{c}}, Valentino},
  Year                     = {2012},
  Pages                    = {40--53},
  Volume                   = {7054 LNCS},

  Abstract                 = {It has been shown earlier that aspect-oriented change realization based on a two-level change type framework can be employed to deal with changes so they can be realized in a modular, pluggable, and reusable way. In this paper, this idea is extended towards enabling direct change manipulation using multi-paradigm design with feature modeling. For this, generally applicable change types are considered to be (small-scale) paradigms and expressed by feature models. Feature models of the Method Substitution and Performing Action After Event change types are presented as examples. In this form, generally applicable change types enter an adapted process of the transformational analysis to determine their application by their instantiation over an application domain feature model. The application of the transformational analysis in identifying the details of change interaction is presented. {\textcopyright} 2012 Springer-Verlag.},
  Doi                      = {10.1007/978-3-642-28038-2_4},
  ISBN                     = {9783642280375},
  ISSN                     = {03029743}
}

@Article{Muenzing2012,
  Title                    = {Supervised quality assessment of medical image registration: Application to intra-patient CT lung registration},
  Author                   = {Muenzing, Sascha E A and van Ginneken, Bram and Murphy, Keelin and Pluim, Josien P W},
  Year                     = {2012},
  Number                   = {8},
  Pages                    = {1521--1531},
  Volume                   = {16},

  Abstract                 = {A novel method for automatic quality assessment of medical image registration is presented. The method is based on supervised learning of local alignment patterns, which are captured by statistical image features at distinctive landmark points. A two-stage classifier cascade, employing an optimal multi-feature model, classifies local alignments into three quality categories: correct, poor or wrong alignment. We establish a reference registration error set as basis for training and testing of the method. It consists of image registrations obtained from different non-rigid registration algorithms and manually established point correspondences of automatically determined landmarks. We employ a set of different classifiers and evaluate the performance of the proposed image features based on the classification performance of corresponding single-feature classifiers. Feature selection is conducted to find an optimal subset of image features and the resulting multi-feature model is validated against the set of single-feature classifiers. We consider the setup generic, however, its application is demonstrated on 51 CT follow-up scan pairs of the lung. On this data, the proposed method performs with an overall classification accuracy of 90{\%}. {\textcopyright} 2012 Elsevier B.V.},
  Doi                      = {10.1016/j.media.2012.06.010},
  ISSN                     = {13618415}
}

@Article{Mussbacher2012,
  Title                    = {AoURN-based modeling and analysis of software product lines},
  Author                   = {Mussbacher, Gunter and Ara{\'{u}}jo, Jo{\~{a}}o and Moreira, Ana and Amyot, Daniel},
  Year                     = {2012},

  Month                    = {sep},
  Number                   = {3-4},
  Pages                    = {645--687},
  Volume                   = {20},

  Abstract                 = {Software Product Line Engineering concerns itself with domain engineering and application engineering. During domain engineering, the whole product family is modeled with a preferred flavor of feature models and additional models as required (e. g., domain models or scenario-based models). During application engineering, the focus shifts toward a single family member and the configuration of the member's features. Recently, aspectual concepts have been employed to better encapsulate individual features of a Software Product Line (SPL), but the existing body of SPL work does not include a unified reasoning framework that integrates aspect-oriented feature description artifacts with the capability to reason about stakeholders' goals while taking feature interactions into consideration. Goal-oriented SPL approaches have been proposed, but do not provide analysis capabilities that help modelers meet the needs of the numerous stakeholders involved in an SPL while at the same time considering feature interactions. We present an aspect-oriented SPL approach for the requirements phase that allows modelers (a) to capture features, goals, and scenarios in a unified framework and (b) to reason about stakeholders' needs and perform trade-off analyses while considering undesirable interactions that are not obvious from the feature model. The approach is based on the Aspect-oriented User Requirements Notation (AoURN) and helps identify, prioritize, and choose products based on analysis results provided by AoURN editor and analysis tools. We apply the AoURN-based SPL framework to the Via Verde SPL to demonstrate the feasibility of this approach through the selection of a Via Verde product configuration that satisfies stakeholders' needs and results in a high-level, scenario-based specification that is free from undesirable feature interactions. {\textcopyright} 2011 Springer Science+Business Media, LLC.},
  Doi                      = {10.1007/s11219-011-9153-8},
  ISSN                     = {09639314}
}

@InProceedings{Niu2011,
  Title                    = {A feature-based CAD-CAE integrated approach of machine tool and its implementation},
  Author                   = {Niu, Wen Tie and Wang, Peng Fei and Shen, Yu and Gao, Wei Guo and Wang, Li Na},
  Year                     = {2011},
  Pages                    = {54--58},
  Volume                   = {201-203},

  Abstract                 = {An analysis feature-based CAD-CAE integrated approach was proposed to solve the problems of rapidly CAE modeling for static and dynamic analysis process of machine tool. Firstly, analysis features were defined in CAD system and analysis feature library was constructed for machine tool and its structural components. Secondly, analysis feature model was constructed by attaching analysis feature to CAD model interactively. Finally, ANSYS parametric design language (APDL) file was generated automatically by mapping analysis features to APDL codes, which realized the integration of CAD system and ANSYS system. Based on application programming interface (API) of SolidWorks, a parametric CAD-CAE tool oriented to static and dynamic analysis of machine tool was developed, which realized parametric modeling and automatic analysis of machine tool and improved design efficiency and quality of machine tool. {\textcopyright} (2011) Trans Tech Publications.},
  Doi                      = {10.4028/www.scientific.net/AMR.201-203.54},
  ISBN                     = {9783037850398},
  ISSN                     = {10226680}
}

@Article{Noorian2014,
  Title                    = {Addressing non-functional properties in feature models: A goal-oriented approach},
  Author                   = {Noorian, Mehdi and Asadi, Mohsen and Bagheri, Ebrahim and Du, Weichang},
  Year                     = {2014},

  Month                    = {dec},
  Number                   = {10},
  Pages                    = {1439--1487},
  Volume                   = {24},

  Abstract                 = {Software Product Line (SPL) engineering is a systematic reuse-based software development approach which is founded on the idea of building software products using a set of core assets rather than developing individual software systems from scratch. Feature models are among the widely used artefacts for SPL development that mostly capture functional and operational variability of a system. Researchers have argued that connecting intentional variability models such as goal models with feature variability models in a target domain can enrich feature models with valuable quality and non-functional information. Interrelating goal models and feature models has already been proposed in the literature for capturing non-functional properties in software product lines; however, this manual integration process is cumbersome and tedious. In this paper, we propose a (semi) automated approach that systematically integrates feature models and goal models through standard ontologies. Our proposed approach connects feature model and goal model elements through measuring the semantic similarity of their annotated ontological concepts. Our work not only provides the means to systematically interrelate feature models and goal models but also allows domain engineers to identify and model the role and significance of non-functional properties in the domain represented by the feature model.},
  Doi                      = {10.1142/S0218194014400154},
  ISSN                     = {02181940},
  Publisher                = {World Scientific Publishing Co. Pte Ltd}
}

@InProceedings{Noorian2011,
  Title                    = {Feature model debugging based on description logic reasoning},
  Author                   = {Noorian, Mahdi and Ensan, Alireza and Bagheri, Ebrahim and Boley, Harold and Biletskiy, Yevgen},
  Year                     = {2011},
  Pages                    = {158--164},
  Publisher                = {Knowledge Systems Institute Graduate School},

  Abstract                 = {Software product line engineering refers to the concept of sharing commonalities and variabilities of a set of software products in a target domain of interest. Feature models are one of the prominent representation formalisms for software product lines. Given the fact that feature models cover all possible applications and products of a target domain, it is possible that the artifacts are not necessarily and always consistent. Therefore, identifying and resolving inconsistencies in feature models is a significant task; especially, due to the fact that a large number of possible products and complex interactions between the software product line features need to be checked. To address these challenges, in this paper, we propose a framework with an automated tool to find and fix the inconsistencies of feature models based on Description Logic (DL) reasoning. The basic idea of our approach is to first transform and represent a feature model using Description Logics. The second step is to identify the possible inconsistencies of the feature model using DL reasoning and then recommend appropriate solutions to a domain analyst for resolving existing inconsistencies.},
  ISBN                     = {1891706306}
}

@InProceedings{Nummenmaa2014,
  Title                    = {On the Use of LTSs to Analyze Software Product Line Products Composed of Features},
  Author                   = {Nummenmaa, Jyrki and Nummenmaa, Timo and Zhang, Zheying},
  Year                     = {2014},
  Pages                    = {531--541},
  Publisher                = {Springer Verlag},
  Volume                   = {214},

  Abstract                 = {In product line engineering, it is common to define the products as sets of features, where each feature has a related set of requirements. Typically, there is a common set of features/requirements, and some variable features/requirements for building different products. In an earlier proposal to use labeled transition systems (LTSs) to model and check the products, the products were composed using the feature-oriented approach and LTS models were analyzed using a related LTS analyzer tool. However, no further details or analysis about the models and possible conflicts were given. We investigate in more detail the types of conflicts that may arise and discuss the integration strategies for building an integrated LTS for the product composed of features. {\textcopyright} Springer-Verlag Berlin Heidelberg 2014.},
  Doi                      = {10.1007/978-3-642-37832-4_48},
  ISSN                     = {21945357}
}

@InProceedings{Oster2010,
  Title                    = {Automated incremental pairwise testing of software product lines},
  Author                   = {Oster, Sebastian and Markert, Florian and Ritter, Philipp},
  Year                     = {2010},
  Pages                    = {196--210},
  Volume                   = {6287 LNCS},

  Abstract                 = {Testing Software Product Lines is very challenging due to a high degree of variability leading to an enormous number of possible products. The vast majority of today's testing approaches for Software Product Lines validate products individually using different kinds of reuse techniques for testing. Due to the enormous number of possible products, individual product testing becomes more and more unfeasible. Combinatorial testing offers one possibility to test a subset of all possible products. In this contribution we provide a detailed description of a methodology to apply combinatorial testing to a feature model of a Software Product Line. We combine graph transformation, combinatorial testing, and forward checking for that purpose. Additionally, our approach considers predefined sets of products. {\textcopyright} 2010 Springer-Verlag Berlin Heidelberg.},
  Doi                      = {10.1007/978-3-642-15579-6_14},
  ISBN                     = {3642155782},
  ISSN                     = {03029743}
}

@Article{Paskevicius2012,
  Title                    = {Automatic extraction of features and generation of feature models from java programs},
  Author                   = {Pa{\v{s}}kevi{\v{c}}ius, Paulius and Dama{\v{s}}evi{\v{c}}ius, Robertas and Kar{\v{c}}iauskas, Eimutis and Marcinkevi{\v{c}}ius, Romas},
  Year                     = {2012},
  Number                   = {4},
  Pages                    = {376--384},
  Volume                   = {41},

  Abstract                 = {Feature modelling is a key technique for identifying common and variable features in software (software component families). The result of feature modelling is a feature model: a concise specification of product features and their relationships. Feature models have been proven to be useful for software variability modelling and management. However, there is a wide gap between feature models and program source code. Here we focus on reverse engineering of source code to feature models. We present a framework for the automated derivation of feature models from the existing software artefacts (components, libraries, etc.), which includes a formal description of a feature model, a program-feature relation meta-model, and a method for feature model generation based on feature dependency extraction and clustering. Feature models are generated in Feature Description Language (FDL) and as Prolog rules.},
  ISSN                     = {1392124X}
}

@Article{Pan2010,
  Title                    = {Shape recognition and clustering algorithm based on a feature model},
  Author                   = {Pan, Hongfei and Liang, Dong and Chen, Junning and Tang, Jun and Wang, Nian},
  Year                     = {2010},

  Month                    = {dec},
  Number                   = {12},
  Volume                   = {50},

  Abstract                 = {Shape recognition and clustering are the key point in artificial intelligence and pattern recognition. Singular value decomposition (SVD) of Laplacian matrix and the random walk model were analyzed to construct the mathematical model of the shape structural feature. Based on this model and the public data sets, shape clustering was completed using the state-vector. Experimental results illustrate that the performance of the proposed approach is better than that of other compared algorithms.},
  ISSN                     = {10000054}
}

@InProceedings{Papakonstantinou2012,
  Title                    = {Using fault propagation analyses for early elimination of unreliable design alternatives of complex cyber-physical systems},
  Author                   = {Papakonstantinou, Nikolaos and Sierla, Seppo and Tumer, Irem Y. and Jensen, David C.},
  Year                     = {2012},
  Number                   = {PARTS A AND B},
  Pages                    = {1183--1191},
  Volume                   = {2},

  Abstract                 = {The Functional Failure Identification and Propagation (FFIP) framework has been proposed in prior work to study the reliability of early phase designs of complex systems. For the specified functionality, a model of mechanical, electrical and software components has been defined to support simulation and discovery of fault propagation paths. The advantage of this approach has been the possibility to identify unreliable designs before high cost design commitments have been made. However, a weakness is that the results are specific to the component model that is created for the purpose of running the FFIP simulations; it is unclear how the results would change if different modeling choices would have been made. Further, the usefulness of the method in design has been limited to evaluating reliability rather than actively finding more robust design alternatives. In order to address these weaknesses, the FFIP component model needs to incorporate a capability to describe design alternatives. The feature modeling syntax and semantics, which has been successfully used by software engineers to describe customer variations in product lines, is applied here to specify alternative mechanical, electrical and software features of a cyber-physical system. In the concept phase, all plausible design alternatives are described with a feature model. FFIP analyses can be performed for each valid configuration of this model, and all alternatives that are found unreliable are removed. The result is a restricted feature model, comprising significantly fewer design alternatives, that is delivered as source information for the detailed design phase. A toolchain for performing these analyses is presented, integrating open source feature modeling and configuration tools to the FFIP environment. The methodology is illustrated with a case study from boiling water nuclear reactor design. Copyright {\textcopyright} 2012 by ASME.},
  Doi                      = {10.1115/DETC2012-70241},
  ISBN                     = {9780791845011}
}

@InProceedings{Paskevicius2012a,
  Title                    = {Change impact analysis of feature models},
  Author                   = {Paskevicius, Paulius and Damasevicius, Robertas and {\v{S}}tuikys, Vytautas},
  Year                     = {2012},
  Pages                    = {108--122},
  Volume                   = {319 CCIS},

  Abstract                 = {Changeability is a fundamental property of software systems. Every software system must evolve at all levels of abstraction (models, architecture, source code, documentation, etc.) to meet changing user and context requirements. To assess the extent of a change, change impact analysis must be performed. In this paper, we propose a taxonomy of change aspects in feature modelling domain, and analyse changeability of feature models, a high level representation of system's external user-visible characteristics. We propose the change impact model based on a feature dependency matrix to assess validity of feature change, to follow feature change propagation and to estimate changeability of a feature model using a Jaccard distance measure. The model is implemented using Prolog logic rules. A case study is presented. {\textcopyright} 2012 Springer-Verlag.},
  Doi                      = {10.1007/978-3-642-33308-8_10},
  ISBN                     = {9783642333071},
  ISSN                     = {18650929}
}

@Misc{Peng2016,
  Title                    = {Research on application classification method in cloud computing environment},

  Author                   = {Peng, Junjie and Chen, Jinbao and Zhi, Xiaofei and Qiu, Meikang and Xie, Xiaolan},
  Month                    = {feb},
  Year                     = {2016},

  Abstract                 = {Energy consumption is a very important issue that has attracted the attention of many cloud providers as it takes a large quotient of the operation cost for cloud data center. To decrease the energy consumption in cloud data center, one possible solution is to process different types of applications with different strategies. To reach this goal, it is important to know the type of application before it be dealt with. In this paper, we present an application type classification method by monitoring the usage of the resources of application. Through analysis, we find that only part of the parameters are much related to different types of applications. Using these parameters we put forward a feature model that can effectively classify the types of different applications. Extensive experiments show that the model put forward can effectively and accurately classify CPU intensive application, I/O intensive application and network intensive application. It can be used as the basis of efficient utilization of the cloud resources.},
  Doi                      = {10.1007/s11227-016-1663-5},
  ISSN                     = {15730484},
  Pages                    = {1--20},
  Publisher                = {Springer New York LLC}
}

@InProceedings{Pereira2013,
  Title                    = {Software variability management: An exploratory study with two feature modeling tools},
  Author                   = {Pereira, Juliana Alves and Souza, Carlos and Figueiredo, Eduardo and Abilio, Ramon and Vale, Gustavo and Costa, Heitor Augustus Xavier},
  Year                     = {2013},
  Pages                    = {20--29},
  Publisher                = {IEEE Computer Society},

  Abstract                 = {Software Product Line (SPL) is becoming widely adopted in industry due to its capability of minimizing costs and improving quality of software systems through systematic reuse of software artifacts. An SPL is a set of software systems sharing a common, managed set of features that satisfies the specific needs of a particular market segment. A feature represents an increment in functionality relevant to some stakeholders. There are several tools to support variability management by modeling features in SPL. However, it is hard for a developer to choose the most appropriate feature modeling tool due to the several options available. This paper presents the results of an exploratory study aiming to support SPL engineers choosing the feature modeling tool that best fits their needs. This exploratory study compares and analyzes two feature modeling tools, namely FeatureIDE and SPLOT, based on data from 56 participants that used the analyzed tools. In this study, we performed a four-dimension qualitative analysis with respect to common functionalities provided by feature modeling tools: (i) Feature Model Editor, (ii) Automated Analysis of Feature Models, (iii) Product Configuration, and (iv) Tool Notation. The main issues we observed in SPLOT are related to its interface. FeatureIDE, on the other hand, revealed some constraints when creating feature models. {\textcopyright} 2013 IEEE.},
  Doi                      = {10.1109/SBCARS.2013.13},
  ISBN                     = {9780769551562}
}

@Article{Pleuss2012,
  Title                    = {Visualization of variability and configuration options},
  Author                   = {Pleuss, Andreas and Botterweck, Goetz},
  Year                     = {2012},
  Number                   = {5},
  Pages                    = {497--510},
  Volume                   = {14},

  Abstract                 = {When designing, constructing, and maintaining diverse and variable software systems, a key challenge is the complexity of systems. A potential approach to tackle this challenge are techniques from variability management and product line engineering to handle the diversity and variability. A key asset in variability management is a variability model, which explicitly specifies the commonalities and variability of a system and the constraints between variants. However, handling variability and configurations remains a challenge due to the complexity on a cognitive level as human engineers reach their limits in identifying, understanding, and using all relevant details. In this paper we address this issue by providing concepts for interactive visual tool support for the configuration of systems with the help of feature models. We discuss relevant principles from the area of information visualization and their application to the domain of feature model configuration. We discuss techniques for interactive configuration support based on a reasoning engine, which, e.g., ensures the validity of configurations. We illustrate our findings by a concrete tool solution called S2T2 Configurator. {\textcopyright} 2012 Springer-Verlag.},
  Doi                      = {10.1007/s10009-012-0252-z},
  ISSN                     = {14332779}
}

@Article{Pleuss2012a,
  Title                    = {Model-driven support for product line evolution on feature level},
  Author                   = {Pleuss, Andreas and Botterweck, Goetz and Dhungana, Deepak and Polzer, Andreas and Kowalewski, Stefan},
  Year                     = {2012},

  Month                    = {oct},
  Number                   = {10},
  Pages                    = {2261--2274},
  Volume                   = {85},

  Abstract                 = {Software Product Lines (SPL) are an engineering technique to efficiently derive a set of similar products from a set of shared assets. In particular in conjunction with model-driven engineering, SPL engineering promises high productivity benefits. There is however, a lack of support for systematic management of SPL evolution, which is an important success factor as a product line often represents a long term investment. In this article, we present a model-driven approach for managing SPL evolution on feature level. To reduce complexity we use model fragments to cluster related elements. The relationships between these fragments are specified using feature model concepts itself leading to a specific kind of feature model called EvoFM. A configuration of EvoFM represents an evolution step and can be transformed to a concrete instance of the product line (i.e., a feature model for the corresponding point in time). Similarly, automatic transformations allow the derivation of an EvoFM from a given set of feature models. This enables retrospective analysis of historic evolution and serves as a starting point for introduction of EvoFM, e.g., to plan future evolution steps. {\textcopyright} 2011 Elsevier Inc. All rights reserved.},
  Doi                      = {10.1016/j.jss.2011.08.008},
  ISSN                     = {01641212}
}

@InProceedings{Pleuss2012b,
  Title                    = {User interface engineering for software product lines - The dilemma between automation and usability},
  Author                   = {Pleuss, Andreas and Hauptmann, Benedikt and Dhungana, Deepak and Botterweck, Goetz},
  Year                     = {2012},
  Pages                    = {25--34},

  Abstract                 = {Software Product Lines (SPL) are systematic approach to develop families of similar software products by explicating their commonalities and variability, e.g., in a feature model. Using techniques from model-driven development, it is then possible to automatically derive a concrete product from a given configuration (i.e., selection of features). However, this is problematic for interactive applications with complex user interfaces (UIs) as automatically derived UIs often provide limited usability. Thus, in practice, the UI is mostly created manually for each product, which results in major drawbacks concerning efficiency and maintenance, e.g., when applying changes that affect the whole product family. This paper investigates these problems based on real-world examples and analyses the development of product families from a UI perspective. To address the underlying challenges, we propose the use of abstract UI models, as used in HCI, to bridge the gap between automated, traceable product derivation and customized, high quality user interfaces. We demonstrate the feasibility of the approach by a concrete example implementation for the suggested model-driven development process. Copyright 2012 ACM.},
  Doi                      = {10.1145/2305484.2305491},
  ISBN                     = {9781450311687}
}

@InProceedings{Pohl2011,
  Title                    = {A performance comparison of contemporary algorithmic approaches for automated analysis operations on feature models},
  Author                   = {Pohl, Richard and Lauenroth, Kim and Pohl, Klaus},
  Year                     = {2011},
  Pages                    = {313--322},

  Abstract                 = {The formalization of variability models (e.g. feature models) is a prerequisite for the automated analysis of these models. The efficient execution of the analysis operations depends on the selection of well-suited solver implementations. Regarding feature models, on the one hand, the formalization with Boolean expressions enables the use of SAT or BDD solvers. On the other hand, feature models can be transformed into a Constraint-Satisfaction Problem (CSP) in order to use CSP solvers for validation. This paper presents a performance comparison regarding nine contemporary high-performance solvers, three for each base problem structure (BDD, CSP, and SAT). Four operations on 90 feature models are run on each solver. The results will in turn clear the way for new improvements regarding the automatic verification of software product lines, since the efficient execution of analysis operations is essential to such automatic verification approaches. {\textcopyright} 2011 IEEE.},
  Doi                      = {10.1109/ASE.2011.6100068},
  ISBN                     = {9781457716393}
}

@InProceedings{Pohl2013,
  Title                    = {Measuring the structural complexity of feature models},
  Author                   = {Pohl, Richard and Stricker, Vanessa and Pohl, Klaus},
  Year                     = {2013},
  Pages                    = {454--464},

  Abstract                 = {The automated analysis of feature models (FM) is based on SAT, BDD, and CSP - known NP-complete problems. Therefore, the analysis could have an exponential worst-case execution time. However, for many practical relevant analysis cases, state-of-the-art (SOTA) analysis tools quite successfully master the problem of exponential worst-case execution time based on heuristics. So far, however, very little is known about the structure of FMs that cause the cases in which the execution time (hardness) for analyzing a given FM increases unpredictably for SOTA analysis tools. In this paper, we propose to use width measures from graph theory to characterize the structural complexity of FMs as a basis for an estimation of the hardness of analysis operations on FMs with SOTA analysis tools. We present an experiment that we use to analyze the reasonability of graph width measures as metric for the structural complexity of FMs and the hardness of FM analysis. Such a complexity metric can be used as a basis for a unified method to systematically improve SOTA analysis tools. {\textcopyright} 2013 IEEE.},
  Doi                      = {10.1109/ASE.2013.6693103},
  ISBN                     = {9781479902156}
}

@InProceedings{Qi2012,
  Title                    = {Multi-feature model analysis-based target identification for remote sensing image},
  Author                   = {Qi, Lin and Yang, Jinfeng and Wen, Yuchun and Ma, Pengfei and Xu, Chenghua and Hao, Hui},
  Year                     = {2012},
  Pages                    = {771--774},
  Volume                   = {1},

  Abstract                 = {Recently, target identification has become a key topic due to the richness of nature resources contained in high-resolution remote sensing image. Many algorithms have been proposed in this aspect. In order to reduce algorithmic complexity and shorten computing cost, a multi-feature analysis model is proposed to identify targets in remote sensing images. The originality of the method includes two aspects. (1) City planning diagram, which is a vector file in the term of polygons of indispensable attributes, is used for image partition. (2) Multiple features are modeled, and then an identification rule is developed based on the model and minimum distance classification. Experimental results show that the proposed method is reliable in performing target identification. {\textcopyright} 2012 IEEE.},
  Doi                      = {10.1109/ICoSP.2012.6491601},
  ISBN                     = {9781467321945}
}

@InProceedings{Quinton2014,
  Title                    = {Consistency checking for the evolution of cardinality-based feature models},
  Author                   = {Quinton, Cl{\'{e}}ment and Pleuss, Andreas and {Le Berre}, Daniel and Duchien, Laurence and Botterweck, Goetz},
  Year                     = {2014},
  Month                    = {sep},
  Pages                    = {122--131},
  Publisher                = {Association for Computing Machinery},
  Volume                   = {1},

  Abstract                 = {Feature-models (fms) are a widely used approach to specify the commonalities and variability in variable systems and software product lines. Various works have addressed edits to fms for fm evolution and tool support to ensure consistency of fms. An important extension to fms are feature cardinalities and related constraints, as extensively used e.g., when modeling variability of cloud computing environments. Since cardinality-based fms pose additional complexity, additional support for evolution and consistency checking with respect to feature cardinalities would be desirable, but has not been addressed yet. In this paper, we discuss common cardinality-based fm edits and resulting inconsistencies based on experiences with fms in cloud domain. We introduce tool-support for automated inconsistency detection and explanation based on an off-the-shelf solver. We demonstrate the feasibility of the approach by an empirical evaluation showing the performance of the tool. Copyright 2014 ACM.},
  Doi                      = {10.1145/2648511.2648524},
  ISBN                     = {9781450327404}
}

@InProceedings{Razzaq2012,
  Title                    = {Automated separation of crosscutting concerns: Earlier automated identification and modularization of cross-cutting features at analysis phase},
  Author                   = {Razzaq, Abdul and Abbasi, Rabeeh},
  Year                     = {2012},
  Pages                    = {471--478},

  Abstract                 = {Early aspect mining captures the concerns that can propagate to other artifacts in later stage. However, current approaches and tools required a self made input by following specific grammatical patterns to expose to the approach what the concern is. Moreover, requirements are mostly communicated between the stakeholders in form of features. However, the early aspect mining from the feature introduced the labor intensive task of creating feature model that is unable to support cross-cutting relations. There seems to be a tradeoff between the requirement abstraction and automaticity for aspect discovery at early analysis phase. In this paper, we present an enhanced form of aspect-oriented feature analysis (AOFA), which discovers meaningful concerns and feature interactions, then associates them to feature modules without disbursing automaticity. It takes publically available unstructured features as input then creates a knowledge base of domain by natural language processing and finally models each feature's dependencies by utilizing this domain knowledge and variability patterns. We evaluate our approach against early aspect miner tool and statistical method and found our approach to be optimal. {\textcopyright} 2012 IEEE.},
  Doi                      = {10.1109/INMIC.2012.6511500},
  ISBN                     = {9781467322508}
}

@InProceedings{Ribeiro2012,
  Title                    = {Emergo: A tool for improving maintainability of preprocessor-based product lines},
  Author                   = {Ribeiro, M{\'{a}}rcio and Tol{\^{e}}do, T{\'{a}}rsis and Winther, Johnni and Brabrand, Claus and Borba, Paulo},
  Year                     = {2012},
  Pages                    = {23--26},

  Abstract                 = {When maintaining a feature in preprocessor-based Software Product Lines (SPLs), developers are susceptible to introduce problems into other features. This is possible because features eventually share elements (like variables and methods) with the maintained one. This scenario might be even worse when hiding features by using techniques like Virtual Separation of Concerns (VSoC), since developers cannot see the feature dependencies and, consequently, they become unaware of them. Emergent Interfaces was proposed to minimize this problem by capturing feature dependencies and then providing information about other features that can be impacted during a maintenance task. In this paper, we present Emergo, a tool capable of computing emergent interfaces between the feature we are maintaining and the others. Emergo relies on feature-sensitive dataflow analyses in the sense it takes features and the SPL feature model into consideration when computing the interfaces. {\textcopyright} 2012 ACM.},
  Doi                      = {10.1145/2162110.2162128},
  ISBN                     = {9781450312226}
}

@Article{Ries2016,
  Title                    = {Towards automatic bounding box annotations from weakly labeled images},
  Author                   = {Ries, Christian X. and Richter, Fabian and Lienhart, Rainer},
  Year                     = {2016},

  Month                    = {jun},
  Number                   = {11},
  Pages                    = {6091--6118},
  Volume                   = {75},

  Abstract                 = {In this work we discuss the problem of automatically determining bounding box annotations for objects in images whereas we only assume weak labeling in the form of global image labels. We therefore are only given a set of positive images all containing at least one instance of a desired object and a negative set of images which represent background. Our goal is then to determine the locations of the object instances within the positive images by bounding boxes. We also describe and analyze a method for automatic bounding box annotation which consists of two major steps. First, we apply a statistical model for determining visual features which are likely to be indicative for the respective object class. Based on these feature models we infer preliminary estimations for bounding boxes. Second, we use a CCCP training algorithm for latent structured SVM in order to improve the initial estimations by using them as initializations for latent variables modeling the optimal bounding box positions. We evaluate our approach on three publicly available datasets.},
  Doi                      = {10.1007/s11042-014-2434-z},
  ISSN                     = {15737721},
  Publisher                = {Springer New York LLC}
}

@InProceedings{Rose2012,
  Title                    = {A feature model for model-to-text transformation languages},
  Author                   = {Rose, Louis M. and Matragkas, Nicholas and Kolovos, Dimitrios S. and Paige, Richard F.},
  Year                     = {2012},
  Pages                    = {57--63},

  Abstract                 = {Model-to-text (M2T) transformation is an important model management operation, as it is used to implement code and documentation generation; model serialisation (enabling model interchange); and model visualisation and exploration. Despite the creation of the MOF Model-To-Text Transformation Language (MOFM2T) in 2008, many very different M2T languages exist today. Because there is little interoperability between M2T languages and rewriting an existing M2T transformation in a new language is costly, developers face a difficult choice when selecting a M2T language. In this paper, we use domain analysis to identify a preliminary feature model for M2T languages. We demonstrate the appropriateness of the feature model by describing two different M2T languages, and discuss potential applications for a tool-supported and model-driven approach to describing the features of M2T languages. {\textcopyright} 2012 IEEE.},
  Doi                      = {10.1109/MISE.2012.6226015},
  ISBN                     = {9781467317573}
}

@InProceedings{Rosenmuller2011,
  Title                    = {Multi-dimensional variability modeling},
  Author                   = {Rosenm{\"{u}}ller, Marko and Siegmund, Norbert and Th{\"{u}}m, Thomas and Saake, Gunter},
  Year                     = {2011},
  Pages                    = {11--20},

  Abstract                 = {The variability of a software product line (SPL) is often described with a feature model. To avoid highly complex models, stakeholders usually try to separate different variability dimensions, such as domain variability and implementation variability. This results in distinct variability models, which are easier to handle than one large model. On the other hand, it is sometimes required to analyze the variability dimensions of an SPL in combination using a single model only. To combine separate modeling and integrated analysis of variability, we present Velvet, a language for multi-dimensional variability modeling. Velvet allows stakeholders to model each variability dimension of an SPL separately and to compose the separated dimensions on demand. This improves reuse of feature models and supports independent modeling variability dimensions. Furthermore, Velvet integrates feature modeling and configuration in a single language. The combination of both concepts creates further reuse opportunities and allows stakeholders to independently configure variability dimensions. Copyright 2011 ACM.},
  Doi                      = {10.1145/1944892.1944894},
  ISBN                     = {9781450305709}
}

@InProceedings{Ryssel2012,
  Title                    = {Reasoning of feature models from derived features},
  Author                   = {Ryssel, Uwe and Ploennigs, Joern and Kabitzsch, Klaus},
  Year                     = {2012},
  Pages                    = {21--30},

  Abstract                 = {When using product lines, whose variability models are based on derived features, e.g., Simulink variant objects, the dependencies among the features are only described implicitly. This makes it difficult to verify the mapping of the features to the solution space and to create a comprehensive overview of the feature dependencies like in a feature model. In this paper, an OWL-based approach is presented, which permits the automatic verification of the feature mapping and an automatic feature model synthesis for derived features using OWL reasoning and formal concept analysis. Copyright 2012 ACM.},
  Doi                      = {10.1145/2371401.2371405},
  ISBN                     = {9781450311298}
}

@Misc{Sanchez2015,
  Title                    = {Variability testing in the wild: the Drupal case study},

  Author                   = {S{\'{a}}nchez, Ana B. and Segura, Sergio and Parejo, Jos{\'{e}} A. and Ruiz-Cort{\'{e}}s, Antonio},
  Month                    = {apr},
  Year                     = {2015},

  Abstract                 = {Variability testing techniques search for effective and manageable test suites that lead to the rapid detection of faults in systems with high variability. Evaluating the effectiveness of these techniques in realistic settings is a must, but challenging due to the lack of variability-intensive systems with available code, automated tests and fault reports. In this article, we propose using the Drupal framework as a case study to evaluate variability testing techniques. First, we represent the framework variability using a feature model. Then, we report on extensive non-functional data extracted from the Drupal Git repository and the Drupal issue tracking system. Among other results, we identified 3392 faults in single features and 160 faults triggered by the interaction of up to four features in Drupal v7.23. We also found positive correlations relating the number of bugs in Drupal features to their size, cyclomatic complexity, number of changes and fault history. To show the feasibility of our work, we evaluated the effectiveness of non-functional data for test case prioritization in Drupal. Results show that non-functional attributes are effective at accelerating the detection of faults, outperforming related prioritization criteria as test case similarity.},
  Doi                      = {10.1007/s10270-015-0459-z},
  ISSN                     = {16191374},
  Publisher                = {Springer Verlag}
}

@Article{Sakthivel2011,
  Title                    = {Enhancing clarity of face recognition to augment the accuracy rate through weighted attribute sets},
  Author                   = {Sakthivel, S. and Rajaram, M.},
  Year                     = {2011},
  Number                   = {4},
  Pages                    = {537--545},
  Volume                   = {48},

  Abstract                 = {Recognizing a face based on its attributes is an easy task for a human to perform as it is a cognitive process. In recent years, Face Recognition is achieved with different kinds of facial features which were used separately or in a combined manner. Currently, Feature fusion methods and parallel methods are the facial features used and performed by integrating multiple feature sets at different levels. However, this integration and the combinational methods do not guarantee better result. Hence to achieve better results, the feature fusion model with multiple weighted facial attribute set is selected. For this feature model, face images from predefined data set has been taken from Olivetti Research Laboratory (ORL) and applied on different methods like Principal Component Analysis (PCA) based Eigen feature extraction technique, Discrete Cosine Transformation (DCT) based feature extraction technique, Histogram Based Feature Extraction technique and Simple Intensity based features. The extracted feature set obtained from these methods were compared and tested for accuracy. In this work we have developed a model which will use the above set of feature extraction techniques with different levels of weights to attain better accuracy. The results show that the selection of optimum weight for a particular feature will lead to improvement in recognition rate. {\textcopyright} EuroJournals Publishing, Inc. 2011.},
  ISSN                     = {1450202X},
  Publisher                = {EuroJournals, Inc.}
}

@InProceedings{Sayyad2012,
  Title                    = {Software feature model recommendations using data mining},
  Author                   = {Sayyad, Abdel Salam and Ammar, Hany and Menzies, Tim},
  Year                     = {2012},
  Pages                    = {47--51},

  Abstract                 = {Feature Models are popular tools for describing software product lines. Analysis of feature models has traditionally focused on consistency checking (yielding a yes/no answer) and product selection assistance, interactive or offline. In this paper, we describe a novel approach to identify the most critical decisions in product selection/configuration by taking advantage of a large pool of randomly generated, generally inconsistent, product variants. Range Ranking, a data mining technique, is utilized to single out the most critical design choices, reducing the job of the human designer to making less consequential decisions. A large feature model is used as a case study; we show preliminary results of the new approach to illustrate its usefulness for practical product derivation. {\textcopyright} 2012 IEEE.},
  Doi                      = {10.1109/RSSE.2012.6233409},
  ISBN                     = {9781467317597}
}

@InProceedings{Schrock2015,
  Title                    = {Systematic reuse of interdisciplinary components supported by engineering relations},
  Author                   = {Schr{\"{o}}ck, S. and Zimmer, F. and Fay, A. and J{\"{a}}ger, T.},
  Year                     = {2015},
  Month                    = {may},
  Number                   = {3},
  Pages                    = {1545--1552},
  Publisher                = {IFAC Secretariat},
  Volume                   = {48},

  Abstract                 = {This paper presents a methodology for systematic interdisciplinary reuse within the engineering of automated plants. This methodology is based on reusable components that are applied during the engineering. In the process industry, these reusable components can be aligned to the basic operations which are defined in the block diagram by the process engineers. To ensure the interdisciplinary acceptance of this methodology, reuse approaches of different involved disciplines, esp. the process engineering and the automation engineering discipline, have been analyzed and combined. This results in a methodology that supports both, the engineering of reusable components and the engineering of the automated plant using these components. As reusable components have to be adaptable to different customer requirements, the possibility of including variability within the reusable components is provided, to allow an adaption to the plant under construction. The methodology is exemplified by a desalination plant, focusing on the process and automation discipline. In order to assure the consistency both within the reusable components and within the solution, an approach for checking the consistency by the use of engineering relations is included.},
  Doi                      = {10.1016/j.ifacol.2015.06.306},
  ISSN                     = {14746670}
}

@InProceedings{Schroter2016,
  Title                    = {Feature-model interfaces: The highway to compositional analyses of highly-configurable systems},
  Author                   = {Schr{\"{o}}ter, Reimar and Krieter, Sebastian and Th{\"{u}}m, Thomas and Benduhn, Fabian and Saake, Gunter},
  Year                     = {2016},
  Month                    = {may},
  Pages                    = {667--678},
  Publisher                = {IEEE Computer Society},

  Abstract                 = {Today's software systems are often customizable by means of load-time or compile-time configuration options. These options are typically not independent and their dependencies can be specified by means of feature models. As many industrial systems contain thousands of options, the maintenance and utilization of feature models is a challenge for all stakeholders. In the last two decades, numerous approaches have been presented to support stakeholders in analyzing feature models. Such analyses are commonly reduced to satis fiability problems, which suffer from the growing number of options. While first attempts have been made to decompose feature models into smaller parts, they still require to compose all parts for analysis. We propose the concept of a feature-model interface that only consists of a subset of features, typically selected by experts, and hides all other features and dependencies. Based on a formalization of feature-model interfaces, we prove compositionality properties. We evaluate feature-model interfaces using a three-month history of an industrial feature model from the automotive domain with 18,616 features. Our results indicate performance benefits especially under evolution as often only parts of the feature model need to be analyzed again.},
  Doi                      = {10.1145/2884781.2884823},
  ISBN                     = {9781450339001},
  ISSN                     = {02705257}
}

@InProceedings{Schroeter2012,
  Title                    = {Multi-perspectives on feature models},
  Author                   = {Schroeter, Julia and Lochau, Malte and Winkelmann, Tim},
  Year                     = {2012},
  Pages                    = {252--268},
  Volume                   = {7590 LNCS},

  Abstract                 = {Domain feature models concisely express commonality and variability among variants of a software product line. For supporting separation of concerns, e.g., due to legal restrictions, technical considerations and business requirements, multi-view approaches restrict the configuration choices on feature models for different stakeholders. However, recent approaches lack a formalization for precise, yet flexible specifications of views that ensure every derivable configuration perspective to obey feature model semantics. Here, we introduce a novel approach for preconfiguring feature models to create multi-perspectives. Such customized perspectives result from composition of various concern-relevant views. A structured view model is used to organize features in view groups, wherein a feature may be contained in multiple views. We provide formalizations for view composition and guaranteed consistency of perspectives w.r.t. feature model semantics. Thereupon, an efficient algorithm to verify consistency for entire multi-perspectives is provided. We present an implementation and evaluate our concepts by means of various experiments. {\textcopyright} 2012 Springer-Verlag.},
  Doi                      = {10.1007/978-3-642-33666-9_17},
  ISBN                     = {9783642336652},
  ISSN                     = {03029743}
}

@Article{Segura2011,
  Title                    = {Functional testing of feature model analysis tools: A test suite},
  Author                   = {Segura, S. and Benavides, D. and Ruiz-Cort{\'{e}}s, A.},
  Year                     = {2011},

  Month                    = {feb},
  Number                   = {1},
  Pages                    = {70--82},
  Volume                   = {5},

  Abstract                 = {A feature model is a compact representation of all the products of a software product line. Automated analysis of feature models is rapidly gaining importance: new operations of analysis have been proposed, new tools have been developed to support those operations and different logical paradigms and algorithms have been proposed to perform them. Implementing operations is a complex task that easily leads to errors in analysis solutions. In this context, the lack of specific testing mechanisms is becoming a major obstacle hindering the development of tools and affecting their quality and reliability. In this article, the authors present FaMa test suite, a set of implementation- independent test cases to validate the functionality of feature model analysis tools. This is an efficient and handy mechanism to assist in the development of tools, detecting faults and improving their quality. In order to show the effectiveness of their proposal, the authors evaluated the suite using mutation testing as well as real faults and tools. Their results are promising and directly applicable in the testing of analysis solutions. The authors intend this work to be a first step towards the development of a widely accepted test suite to support functional testing in the community of automated analysis of feature models. {\textcopyright} 2011 The Institution of Engineering and Technology.},
  Doi                      = {10.1049/iet-sen.2009.0096},
  ISSN                     = {17518806}
}

@InProceedings{Segura2012,
  Title                    = {BeTTy: Benchmarking and Testing on the automated analysis of feature models},
  Author                   = {Segura, Sergio and Galindo, Jos{\'{e}} A. and Benavides, David and Parejo, Jos{\'{e}} A. and Ruiz-Cort{\'{e}}s, Antonio},
  Year                     = {2012},
  Pages                    = {63--71},

  Abstract                 = {The automated analysis of feature models is a ourishing research topic that has called the attention of both researchers and practitioners during the last two decades. During this time, the number of tools and techniques enabling the analysis of feature models has increased and also their complexity. In this scenario, the lack of specific testing mechanisms to assess the correctness and good performance of analysis tools is becoming a major obstacle hindering the development of tools and affecting their quality and reliability. In this paper, we present BeTTy, a framework for BEnchmarking and TesTing on the analY sis of feature models. Among other features, BeTTy enables the automated detection of faults in feature model analysis tools. Also, it supports the generation of motivating test data to evaluate the performance of analysis tools in both average and pessimistic cases. Part of the functionality of the framework is provided through a web-based interface facilitating the random generation of both classic and attributed feature models. Copyright {\textcopyright} 2012.},
  Doi                      = {10.1145/2110147.2110155},
  ISBN                     = {9781450310581}
}

@Article{Segura2011a,
  Title                    = {Automated metamorphic testing on the analyses of feature models},
  Author                   = {Segura, Sergio and Hierons, Robert M. and Benavides, David and Ruiz-Cort{\'{e}}s, Antonio},
  Year                     = {2011},

  Month                    = {mar},
  Number                   = {3},
  Pages                    = {245--258},
  Volume                   = {53},

  Abstract                 = {Context: A feature model (FM) represents the valid combinations of features in a domain. The automated extraction of information from FMs is a complex task that involves numerous analysis operations, techniques and tools. Current testing methods in this context are manual and rely on the ability of the tester to decide whether the output of an analysis is correct. However, this is acknowledged to be time-consuming, error-prone and in most cases infeasible due to the combinatorial complexity of the analyses, this is known as the oracle problem. Objective: In this paper, we propose using metamorphic testing to automate the generation of test data for feature model analysis tools overcoming the oracle problem. An automated test data generator is presented and evaluated to show the feasibility of our approach. Method: We present a set of relations (so-called metamorphic relations) between input FMs and the set of products they represent. Based on these relations and given a FM and its known set of products, a set of neighbouring FMs together with their corresponding set of products are automatically generated and used for testing multiple analyses. Complex FMs representing millions of products can be efficiently created by applying this process iteratively. Results: Our evaluation results using mutation testing and real faults reveal that most faults can be automatically detected within a few seconds. Two defects were found in FaMa and another two in SPLOT, two real tools for the automated analysis of feature models. Also, we show how our generator outperforms a related manual suite for the automated analysis of feature models and how this suite can be used to guide the automated generation of test cases obtaining important gains in efficiency. Conclusion: Our results show that the application of metamorphic testing in the domain of automated analysis of feature models is efficient and effective in detecting most faults in a few seconds without the need for a human oracle. {\textcopyright} 2010 Elsevier B.V. All rights reserved.},
  Doi                      = {10.1016/j.infsof.2010.11.002},
  ISSN                     = {09505849}
}

@InProceedings{Segura2010,
  Title                    = {Automated test data generation on the analyses of feature models: A metamorphic testing approach},
  Author                   = {Segura, Sergio and Hierons, Robert M. and Benavides, David and Ruiz-Cort{\'{e}}s, Antonio},
  Year                     = {2010},
  Pages                    = {35--44},

  Abstract                 = {A Feature Model (FM) is a compact representation of all the products of a software product line. The automated extraction of information from FMs is a thriving research topic involving a number of analysis operations, algorithms, paradigms and tools. Implementing these operations is far from trivial and easily leads to errors and defects in analysis solutions. Current testing methods in this context mainly rely on the ability of the tester to decide whether the output of an analysis is correct. However, this is acknowledged to be time-consuming, error-prone and in most cases infeasible due to the combinatorial complexity of the analyses. In this paper, we present a set of relations (so-called metamorphic relations) between input FMs and their set of products and a test data generator relying on them. Given an FM and its known set of products, a set of neighbour FMs together with their corresponding set of products are automatically generated and used for testing different analyses. Complex FMs representing millions of products can be efficiently created applying this process iteratively. The evaluation of our approach using mutation testing as well as real faults and tools reveals that most faults can be automatically detected within a few seconds. {\textcopyright} 2010 IEEE.},
  Doi                      = {10.1109/ICST.2010.20},
  ISBN                     = {9780769539904}
}

@Article{Segura2014,
  Title                    = {Automated generation of computationally hard feature models using evolutionary algorithms},
  Author                   = {Segura, Sergio and Parejo, Jos{\'{e}} A. and Hierons, Robert M. and Benavides, David and Ruiz-Cort{\'{e}}s, Antonio},
  Year                     = {2014},

  Month                    = {jun},
  Number                   = {8},
  Pages                    = {3975--3992},
  Volume                   = {41},

  Abstract                 = {A feature model is a compact representation of the products of a software product line. The automated extraction of information from feature models is a thriving topic involving numerous analysis operations, techniques and tools. Performance evaluations in this domain mainly rely on the use of random feature models. However, these only provide a rough idea of the behaviour of the tools with average problems and are not sufficient to reveal their real strengths and weaknesses. In this article, we propose to model the problem of finding computationally hard feature models as an optimization problem and we solve it using a novel evolutionary algorithm for optimized feature models (ETHOM). Given a tool and an analysis operation, ETHOM generates input models of a predefined size maximizing aspects such as the execution time or the memory consumption of the tool when performing the operation over the model. This allows users and developers to know the performance of tools in pessimistic cases providing a better idea of their real power and revealing performance bugs. Experiments using ETHOM on a number of analyses and tools have successfully identified models producing much longer executions times and higher memory consumption than those obtained with random models of identical or even larger size. {\textcopyright} 2013 Elsevier Ltd. All rights reserved.},
  Doi                      = {10.1016/j.eswa.2013.12.028},
  ISSN                     = {09574174}
}

@InProceedings{Segura2014a,
  Title                    = {Automated variability analysis and testing of an e-commerce site. An experience report},
  Author                   = {Segura, Sergio and S{\'{a}}nchez, Ana B. and Ruiz-Cort{\'{e}}s, Antonio},
  Year                     = {2014},
  Pages                    = {139--149},
  Publisher                = {Association for Computing Machinery, Inc},

  Abstract                 = {In this paper, we report on our experience on the development of La Hilandera, an e-commerce site selling haberdashery products and craft supplies in Europe. The store has a huge input space where customers can place almost three millions of different orders which made testing an extremely difficult task. To address the challenge, we explored the applicability of some of the practices for variability management in software product lines. First, we used a feature model to represent the store input space which provided us with a variability view easy to understand, share and discuss with all the stakeholders. Second, we used techniques for the automated analysis of feature models for the detection and repair of inconsistent and missing configuration settings. Finally, we used test selection and prioritization techniques for the generation of a manageable and effective set of test cases. Our findings, summarized in a set of lessons learnt, suggest that variability techniques could successfully address many of the challenges found when developing e-commerce sites.},
  Doi                      = {10.1145/2642937.2642939},
  ISBN                     = {9781450330138}
}

@InProceedings{Shao2014,
  Title                    = {A static semantic model for trusted forensics using OCL},
  Author                   = {Shao, Zehui and Ding, Qiufeng and Jin, Xianli and Sun, Guozi},
  Year                     = {2014},
  Pages                    = {259--268},
  Publisher                = {Springer Verlag},
  Volume                   = {276 LNEE},

  Abstract                 = {According to the features of various properties of digital data, a static semantic model of features for trusted digital data using OCL (Object Constraint Language) is proposed. These features obtained from the forensic domain of digital data are hierarchically decomposed and merged based on FODA (Feature Oriented Domain Analysis) modeling process. Then a feature tree is built with semantic logical relation in order to get the overall semantic description of features in the forensic domain of digital data, meanwhile, formally describing the features of various attributes of digital data by OCL which has a rigorous mathematical semantics and is easy to understand. The features of digital data are classified with the concept of set in OCL, and the relevance and dependence among various features are described with the operations of set in OCL. Finally, a feature model is built in digital data of Windows system with the use of OCL operations. {\textcopyright} 2014 Springer-Verlag.},
  Doi                      = {10.1007/978-3-642-40861-8_39},
  ISBN                     = {9783642408601},
  ISSN                     = {18761119}
}

@InProceedings{Shi2010,
  Title                    = {A preliminary experimental study on optimal feature selection for product derivation using knapsack approximation},
  Author                   = {Shi, Runyu and Guo, Jianmei and Wang, Yinglin},
  Year                     = {2010},
  Pages                    = {665--669},
  Volume                   = {1},

  Abstract                 = {Software product lines (SPLs) technology produce software by integrating reusable software components based on customer requirements. Current researchers pay great attention to feature modeling technology that can represent SPLs' production requirements and functionalities. A key challenge is selecting valid and optimal feature combinations from the feature model to satisfy various requirements of customers and vendors, including various value and cost constraints. This paper experimentally studies a knapsack approximation algorithm of feature selection for automated product derivation in SPLs. Our approach generates an approximation solution by a modified Filtered Cartesian Flattening algorithm and obtains the optimal solution with a greed search. We performed experiments on randomly generated feature models with different characteristics. Experiments show that our approach can select highly optimal feature combinations effectively. {\textcopyright}2010 IEEE.},
  Doi                      = {10.1109/PIC.2010.5687874},
  ISBN                     = {9781424467860}
}

@InProceedings{Soltani2012,
  Title                    = {Automated planning for feature model configuration based on functional and non-functional requirements},
  Author                   = {Soltani, Samaneh and Asadi, Mohsen and Ga{\v{s}}evi{\'{c}}, Dragan and Hatala, Marek and Bagheri, Ebrahim},
  Year                     = {2012},
  Pages                    = {56--65},
  Volume                   = {1},

  Abstract                 = {Feature modeling is one of the main techniques used in Software Product Line Engineering to manage the variability within the products of a family. Concrete products of the family can be generated through a configuration process. The configuration process selects and/or removes features from the feature model according to the stakeholders' requirements. Selecting the right set of features for one product from amongst all of the available features in the feature model is a complex task because: 1) the multiplicity of stakeholders' functional requirements; 2) the positive or negative impact of features on non-functional properties; and 3) the stakeholders' preferences w.r.t. the desirable non-functional properties of the final product. Many configurations techniques have already been proposed to facilitate automated product derivation. However, most of the current proposals are not designed to consider stakeholders' preferences and constraints especially with regard to non-functional properties. We address the software product line configuration problem and propose a framework, which employs an artificial intelligence planning technique to automatically select suitable features that satisfy both the stakeholders' functional and non-functional preferences and constraints. We also provide tooling support to facilitate the use of our framework. Our experiments show that despite the complexity involved with the simultaneous consideration of both functional and non-functional properties our configuration technique is scalable. Copyright {\textcopyright} 2012 ACM.},
  Doi                      = {10.1145/2362536.2362548},
  ISBN                     = {9781450310956}
}

@InProceedings{Soltani2011,
  Title                    = {Automated planning for feature model configuration based on stakeholders' business concerns},
  Author                   = {Soltani, Samaneh and Asadi, Mohsen and Hatala, Marek and Ga{\v{s}}evi{\'{c}}, Dragan and Bagheri, Ebrahim},
  Year                     = {2011},
  Pages                    = {536--539},

  Abstract                 = {In Software Product Line Engineering, concrete products of a family can be generated through a configuration process over a feature model. The configuration process selects features from the feature model according to the stakeholders' requirements. Selecting the right set of features for one product from all the available features in the feature model is a cumbersome task because 1) the stakeholders may have diverse business concerns and limited resources that they can spend on a product and 2) features may have negative and positive contributions on different business concern. Many configurations techniques have been proposed to facilitate software developers' tasks through automated product derivation. However, most of the current proposals for automatic configuration are not devised to cope with business oriented requirements and stakeholders' resource limitations. We propose a framework, which employs an artificial intelligence planning technique to automatically select suitable features that satisfy the stakeholders' business concerns and resource limitations. We also provide tooling support to facilitate the use of our framework. {\textcopyright} 2011 IEEE.},
  Doi                      = {10.1109/ASE.2011.6100118},
  ISBN                     = {9781457716393}
}

@Article{Song2015,
  Title                    = {Component optimization design for remanufacturing based on life matching},
  Author                   = {Song, Shouxu and Feng, Yan and Ke, Qingdi and Liu, Ming},
  Year                     = {2015},

  Month                    = {may},
  Number                   = {10},
  Pages                    = {1323--1329},
  Volume                   = {26},

  Abstract                 = {Remanufacturing blanks were retired products and the component status was uncertain, the components could not achieve the best use. For the problem, this paper presented components optimization design for remanufacturing based on life matching. Through the analyses of the existence of “short board” effect in the product failure, the mean and multiple matching were established, and also the corresponding matching means. The design information model, performance feature model and their interaction and feedback model were presented. Finally, crank-bush life matching and the improvement of the initial design due to remanufacturing were carried out to verify the effectiveness and feasibility of the method.},
  Doi                      = {10.3969/j.issn.1004-132X.2015.10.009},
  ISSN                     = {1004132X},
  Publisher                = {China Mechanical Engineering Magazine Office}
}

@InProceedings{Sriman2016,
  Title                    = {Explicit foreground and background modeling in the classification of text blocks in scene images},
  Author                   = {Sriman, Bowornrat and Schomaker, Lambert},
  Year                     = {2016},
  Month                    = {jun},
  Pages                    = {755--759},
  Publisher                = {Institute of Electrical and Electronics Engineers Inc.},

  Abstract                 = {Achieving high accuracy for classifying foreground and background is an interesting challenge in the field of scene image analysis because of the wide range of illumination, complex background, and scale changes. Classifying foreground and background using bag-of-feature model gives a good result. However, the performance of the classifier depends on designed features. Therefore, this paper presents an alternative classification method based on three categories of object-attributes features namely object description, color distribution and gradient strength. Each feature is computed to a classifier model. The robustness of the method has been tested on the ICDAR2015 dataset. The experimental results show that the performance of the proposed method performs competitively against the results of existing methods in term of precision and recall.},
  Doi                      = {10.1109/ACPR.2015.7486604},
  ISBN                     = {9781479961009}
}

@Article{Sun2016,
  Title                    = {A novel efficient SVM-based fault diagnosis method for multi-split air conditioning system's refrigerant charge fault amount},
  Author                   = {Sun, Kaizheng and Li, Guannan and Chen, Huanxin and Liu, Jiangyan and Li, Jiong and Hu, Wenju},
  Year                     = {2016},

  Month                    = {sep},
  Pages                    = {989--998},
  Volume                   = {108},

  Abstract                 = {For the multi-split variable refrigerant flow (VRF) system, the key of efficient operation is to achieve the appropriate refrigerant charge amount (RCA). However, it is difficult to achieve because of the complexity of VRF systems. To overcome the difficulty, this paper presents a hybrid RCA fault diagnosis model combined support vector machine (SVM) with wavelet de-noising (WD) and improved max-relevance and min-redundancy (mRMR) algorithm. WD is responsible for improving the quality of collected VRF experimental data. In addition, mRMR is firstly used to rank all the variables in descending order in terms of their importance for identify RCA faults. After top-ranked variable is determined, correlation analysis of features is implemented for further feature selection removing the redundant variables in linkage to the variable at the top. Finally, a subset of seven features are selected to develop the SVM model. Results indicate that fault diagnosis accuracy of the seven-feature SVM model decreases only 2.14{\%} compared with the initial eighteen-feature model. The proposed wavelet de-noising-max-relevance and min-redundancy-support vector machine (WD-mRMR-SVM) model shows good fault diagnosis performance for RCA faults.},
  Doi                      = {10.1016/j.applthermaleng.2016.07.109},
  ISSN                     = {13594311},
  Publisher                = {Elsevier Ltd}
}

@InProceedings{Sun2010,
  Title                    = {A 3D feature model for image matching},
  Author                   = {Sun, Zachary and Bliss, Nadya and Ni, Karl},
  Year                     = {2010},
  Pages                    = {2194--2197},

  Abstract                 = {The proposed algorithm identifies whether or not a test photo belongs to a set of co-located training images based on its spatial proximity to the training set. We leverage concepts from Lowe's SIFT and Snavely's Photo Tourism algorithms, and match an image by its 2D features to the 3D features representing the training set. To reduce complexity and increase efficiency, the proposed algorithm implements a compact representation of the image set by merging collections similar features. Test images are then matched with the derived structure. Finally, a decision statistic is determined based on the percentage of features that match. Receiver operating characteristics, computational analysis, and distributions are included in the performance analysis. {\textcopyright}2010 IEEE.},
  Doi                      = {10.1109/ICASSP.2010.5495705},
  ISBN                     = {9781424442966},
  ISSN                     = {15206149}
}

@Article{Tanhaei2016,
  Title                    = {Automating feature model refactoring: A Model transformation approach},
  Author                   = {Tanhaei, Mohammad and Habibi, Jafar and Mirian-Hosseinabadi, Seyed Hassan},
  Year                     = {2016},

  Month                    = {dec},
  Pages                    = {138--157},
  Volume                   = {80},

  Abstract                 = {Context: Feature model is an appropriate and indispensable tool for modeling similarities and differences among products of the Software Product Line (SPL). It not only exposes the validity of the products' configurations in an SPL but also changes in the course of time to support new requirements of the SPL. Modifications made on the feature model in the course of time raise a number of issues. Useless enlargements of the feature model, the existence of dead features, and violated constraints in the feature model are some of the key problems that make its maintenance difficult. Objective: The initial approach to dealing with the above-mentioned problems and improving maintainability of the feature model is refactoring. Refactoring modifies software artifacts in a way that their externally visible behavior does not change. Method: We introduce a method for defining refactoring rules and executing them on the feature model. We use the ATL model transformation language to define the refactoring rules. Moreover, we provide an Alloy model to check the feature model and the safety of the refactorings that are performed on it. Results: In this research, we propose a safe framework for refactoring a feature model. This framework enables users to perform automatic and semi-automatic refactoring on the feature model. Conclusions: Automated tool support for refactoring is a key issue for adopting approaches such as utilizing feature models and integrating them into the software development process of companies. In this work, we define some of the important refactoring rules on the feature model and provide tools that enable users to add new rules using the ATL M2M language. Our framework assesses the correctness of the refactorings using the Alloy language.},
  Doi                      = {10.1016/j.infsof.2016.08.011},
  ISSN                     = {09505849},
  Publisher                = {Elsevier}
}

@InProceedings{Tawhid2011,
  Title                    = {Product model derivation by model transformation in software product lines},
  Author                   = {Tawhid, Rasha and Petriu, Dorina C.},
  Year                     = {2011},
  Pages                    = {72--79},

  Abstract                 = {Product derivation is an essential part of the Software Product Line (SPL) development process. The paperproposes a model transformation for deriving automatically a UML model of a specific product from the UML model of a product line. This work is a part of a larger project aiming to integrate performance analysis in the SPL model-driven development. The SPL source model is expressed in UML extended with two separate profiles: a "product line" profile from literature for specifying the commonality and variability between products, and the MARTE profile recently standardized by OMG for performance annotations. The automatic derivation of a concrete product model based on a given feature configuration is enabled through the mapping between features from the feature model and their realizations in the design model. The paper proposes an efficient mapping technique that aims to minimize the amount of explicit feature annotations in the UML design model of SPL. Implicit feature mapping is inferred during product derivation from the relationships between annotated and non-annotated model elements as defined in the UML metamodel and well formedness rules. The transformation is realized in the Atlas Transformation Language (ATL) and illustrated with an ecommerce case study that models structural and behavioural SPL views. {\textcopyright} 2011 IEEE.},
  Doi                      = {10.1109/ISORCW.2011.18},
  ISBN                     = {9780769543772}
}

@InProceedings{Tawhid2011a,
  Title                    = {Automatic derivation of a product performance model from a software product line model},
  Author                   = {Tawhid, Rasha and Petriu, Dorina C.},
  Year                     = {2011},
  Pages                    = {80--89},

  Abstract                 = {We propose to integrate performance analysis in the early phases of the model-driven development process for Software Product Lines (SPL). We start with a multi-view UML model of the core family assets representing the commonality and variability between different products, which we call the SPL model. We add another perspective to the SPL model, annotating it with generic performance specifications expressed in the standard UML profile MARTE, recently adopted by OMG. The runtime performance of a product is affected by factors contained in the UML model of the product (derived from the SPL model), but also by external factors depending on the implementation and execution environments. The external factors not contained in the SPL model need to be eventually represented in the performance model. In order to do so, we propose to represent the variability space of different possible implementation and execution environments through a so called "performance completion (PC) feature model". These PC features are mapped to MARTE performance-related stereotypes and attributes attached to the SPL model elements. A first model transformation realized in the Atlas Transformation Language (ATL) derives the UML model of a specific product with concrete MARTE annotations from the SPL model. A second transformation generates a Layered Queueing Network (LQN) performance model for the given product by applying an existing transformation named PUMA, developed in previous work. The proposed technique is illustrated with an e-commerce case study. A LQN model is derived for a product and the impact of different levels of secure communication channels on its performance is analyzed by using the LQN model. {\textcopyright} 2011 IEEE.},
  Doi                      = {10.1109/SPLC.2011.27},
  ISBN                     = {9780769544878}
}

@InProceedings{Teixeira2013,
  Title                    = {Safe composition of configuration knowledge-based software product lines},
  Author                   = {Teixeira, Leopoldo and Borba, Paulo and Gheyi, Rohit},
  Year                     = {2013},
  Month                    = {apr},
  Number                   = {4},
  Pages                    = {1038--1053},
  Volume                   = {86},

  Abstract                 = {Mistakes made when implementing or specifying the models of a Software Product Line (SPL) can result in ill-formed products - the safe composition problem. Such problem can hinder productivity and it might be hard to detect, since SPLs can have thousands of products. In this article, we propose a language independent approach for verifying safe composition of SPLs with dedicated Configuration Knowledge models. We translate feature model and Configuration Knowledge into propositional logic and use the Alloy Analyzer to perform the verification. To provide evidence for the generality of our approach, we instantiate this approach in different compositional settings. We deal with different kinds of assets such as use case scenarios and Eclipse RCP components. We analyze both the code and the requirements for a larger scale SPL, finding problems that affect thousands of products in minutes. Moreover, our evaluation suggests that the analysis time grows linearly with respect to the number of products in the analyzed SPLs. {\textcopyright} 2012 Elsevier Inc.},
  Doi                      = {10.1016/j.jss.2012.11.006},
  ISSN                     = {01641212}
}

@InProceedings{Teixeira2011,
  Title                    = {Safe composition of configuration knowledge-based software product lines},
  Author                   = {Teixeira, Leopoldo and Borba, Paulo and Gheyi, Rohit},
  Year                     = {2011},
  Pages                    = {263--272},

  Abstract                 = {Feature models and configuration knowledge drive product generation in a Software Product Line (SPL). Mistakes when specifying these models or in the implementation might result in ill-formed products - the safe composition problem. This work proposes an automated approach for verifying safe composition for SPLs with explicit configuration knowledge models. We translate feature models and configuration knowledge into propositional logic and use SAT Solvers to perform the verification. We evaluate our approach using seven releases of the MobileMedia SPL, which generate up to 272 products in the 7th release. We report safe composition problems related to non-conformity with the feature model, bad specification of the configuration knowledge, and implementation not envisioning the full SPL scope, that affect over 40{\%} of the products in the 7th release. {\textcopyright} 2011 IEEE.},
  Doi                      = {10.1109/SBES.2011.15},
  ISBN                     = {9780769546032}
}

@InProceedings{Tekinerdogan2011,
  Title                    = {Modeling and reasoning about design alternatives of software as a service architectures},
  Author                   = {Tekinerdogan, Bedir and {\"{O}}zt{\"{u}}rk, Karahan and Doǧru, Ali},
  Year                     = {2011},
  Pages                    = {312--319},

  Abstract                 = {In general, a common reference architecture can be derived for Software as a Service (SaaS). However, while designing particular applications one may derive various application design alternatives from the same reference SaaS architecture specification. To meet the required functional and nonfunctional requirements of different enterprise applications it is important to model the possible design so that a feasible alternative can be defined. In this paper, we propose a systematic approach and corresponding tool support for guiding the design of SaaS application architectures. The approach defines a SaaS reference architecture, a family feature model and a set of reference design rules. Based on the business requirements an application feature model is defined using the family feature model. Selected features are related to design decisions and a SaaS application architecture design is derived. {\textcopyright} 2011 IEEE.},
  Doi                      = {10.1109/WICSA.2011.49},
  ISBN                     = {9780769543512}
}

@InProceedings{TerBeek2015,
  Title                    = {Applying the product lines paradigm to the quantitative analysis of collective adaptive systems},
  Author                   = {{Ter Beek}, Maurice H. and Fantechi, Alessandro and Gnesi, Stefania},
  Year                     = {2015},
  Month                    = {jul},
  Pages                    = {321--326},
  Publisher                = {Association for Computing Machinery},

  Abstract                 = {Engineering a Collective Adaptive System (CAS) requires the support of a framework for quantitative modeling and analysis of the system. In order to jointly address variability and quantitative analysis, we apply the Product Lines paradigm, considered at the level of system engineering, to a case study of the European project QUANTICOL, by first defining a reference feature model and then adding feature attributes and global quantitative constraints, in the form of a Clafer attributed feature model. ClaferMOOVisualizer is subsequently used for quantitative analyses and multiobjective optimization of the resulting attributed feature model.},
  Doi                      = {10.1145/2791060.2791100},
  ISBN                     = {9781450336130}
}

@InProceedings{Thum2011,
  Title                    = {Abstract features in feature modeling},
  Author                   = {Th{\"{u}}m, Thomas and K{\"{a}}stner, Christian and Erdweg, Sebastian and Siegmund, Norbert},
  Year                     = {2011},
  Pages                    = {191--200},

  Abstract                 = {A software product line is a set of program variants, typically generated from a common code base. Feature models describe variability in product lines by documenting features and their valid combinations. In product-line engineering, we need to reason about variability and program variants for many different tasks. For example, given a feature model, we might want to determine the number of all valid feature combinations or compute specific feature combinations for testing. However, we found that contemporary reasoning approaches can only reason about feature combinations, not about program variants, because they do not take abstract features into account. Abstract features are features used to structure a feature model that, however, do not have any impact at implementation level. Using existing feature-model reasoning mechanisms for program variants leads to incorrect results. Hence, although abstract features represent domain decisions that do not affect the generation of a program variant. We raise awareness of the problem of abstract features for different kinds of analyses on feature models. We argue that, in order to reason about program variants, abstract features should be made explicit in feature models. We present a technique based on propositional formulas that enables to reason about program variants rather than feature combinations. In practice, our technique can save effort that is caused by considering the same program variant multiple times, for example, in product-line testing. {\textcopyright} 2011 IEEE.},
  Doi                      = {10.1109/SPLC.2011.53},
  ISBN                     = {9780769544878}
}

@InProceedings{Tizzei2012,
  Title                    = {An aspect-based feature model for architecting component product lines},
  Author                   = {Tizzei, Leonardo P. and Rubira, Cec{\'{i}}lia M F and Lee, Jaejoon},
  Year                     = {2012},
  Pages                    = {85--92},

  Abstract                 = {Feature modeling is widely used for software product line analysis to capture commonality and variability of a product line. As product line variations are mainly captured in a feature model, the mapping between features and architectural components is essential to enable the derivation of product architectures from the feature model. However, current SPL architecture design approaches that map features to architectural components do not model crosscutting concerns explicitly either at a feature model or at product line architecture design. We propose a feature-oriented solution with aspects for product line architecture design aiming at improving product line architecture evolvability by adopting aspect-oriented techniques, which provide a promising support for modeling crosscutting concerns. Our approach includes guidelines for developing and refining SPL requirements into component-based product line architecture with aspects. We evaluated our approach through a preliminary evaluation which has shown promising results. {\textcopyright} 2012 IEEE.},
  Doi                      = {10.1109/SEAA.2012.64},
  ISBN                     = {9780769547909}
}

@InProceedings{Tran2014,
  Title                    = {An approach for decision support on the uncertainty in feature model evolution},
  Author                   = {Tran, Le Minh Sang and Massacci, Fabio},
  Year                     = {2014},
  Month                    = {sep},
  Pages                    = {93--102},
  Publisher                = {Institute of Electrical and Electronics Engineers Inc.},

  Abstract                 = {Software systems could be seen as a hierarchy of features which are evolving due to the dynamic of the working environments. The companies who build software thus need to make an appropriate strategy, which takes into consideration of such dynamic, to select features to be implemented. In this work, we propose an approach to facilitate such selection by providing a means to capture the uncertainty of evolution in feature models. We also provide two analyses to support the decision makers. The approach is exemplified in the Smart Grid scenario.},
  Doi                      = {10.1109/RE.2014.6912251},
  ISBN                     = {9781479930333}
}

@InProceedings{Varela2011,
  Title                    = {Aspect-oriented analysis for software product lines requirements engineering},
  Author                   = {Varela, Patr{\'{i}}cia and Ara{\'{u}}jo, Jo{\~{a}}o and Brito, Isabel and Moreira, Ana},
  Year                     = {2011},
  Pages                    = {667--674},

  Abstract                 = {Requirements analysis and modeling for Software Product Lines demands the use of feature models, but also requires additional models to help identifying, describing, and specifying features. Traditional approaches usually perform this manually and, in general, the identification and modularization of crosscutting features is ignored, or not handled systematically. This hinders requirements change. We propose an aspect-oriented approach for SPL enriched to automatically derive feature models where crosscutting features are identified and modularized using aspect-oriented concepts and techniques. This is achieved by adapting and extending the AORA (Aspect-Oriented Requirements Analysis) approach. AORA provides templates to specify and organize requirements based on concerns and responsibilities. A set of heuristics is defined to help identifying features and their dependencies in a product line. A tool was developed to automatically generate the feature model from AORA templates. {\textcopyright} 2011 ACM.},
  Doi                      = {10.1145/1982185.1982333},
  ISBN                     = {9781450301138}
}

@Article{Varela-Vaca2013,
  Title                    = {Towards the automatic and optimal selection of risk treatments for business processes using a constraint programming approach},
  Author                   = {Varela-Vaca, Angel Jesus and Gasca, Rafael M.},
  Year                     = {2013},

  Month                    = {nov},
  Number                   = {11},
  Pages                    = {1948--1973},
  Volume                   = {55},

  Abstract                 = {Context The use of Business Process Management Systems (BPMS) has emerged in the IT arena for the automation of business processes. In the majority of cases, the issue of security is overlooked by default in these systems, and hence the potential cost and consequences of the materialization of threats could produce catastrophic loss for organizations. Therefore, the early selection of security controls that mitigate risks is a real and important necessity. Nevertheless, there exists an enormous range of IT security controls and their configuration is a human, manual, time-consuming and error-prone task. Furthermore, configurations are carried out separately from the organization perspective and involve many security stakeholders. This separation makes difficult to ensure the effectiveness of the configuration with regard to organizational requirements. Objective In this paper, we strive to provide security stakeholders with automated tools for the optimal selection of IT security configurations in accordance with a range of business process scenarios and organizational multi-criteria. Method An approach based on feature model analysis and constraint programming techniques is presented, which enable the automated analysis and selection of optimal security configurations. Results A catalogue of feature models is determined by analyzing typical IT security controls for BPMSs for the enforcement of the standard goals of security: integrity, confidentiality, availability, authorization, and authentication. These feature models have been implemented through constraint programs, and Constraint Programming techniques based on optimized and non-optimized searches are used to automate the selection and generation of configurations. In order to compare the results of the determination of configuration a comparative analysis is given. Conclusion In this paper, we present innovative tools based on feature models, Constraint Programming and multi-objective techniques that enable the agile, adaptable and automatic selection and generation of security configurations in accordance with the needs of the organization. {\textcopyright} 2013 Elsevier B.V. All rights reserved.},
  Doi                      = {10.1016/j.infsof.2013.05.007},
  ISSN                     = {09505849}
}

@Article{Venckauskas2016,
  Title                    = {Model-driven approach for body area network application development},
  Author                   = {Ven{\v{c}}kauskas, Algimantas and {\v{S}}tuikys, Vytautas and Jusas, Nerijus and Burbaitė, Renata},
  Year                     = {2016},

  Month                    = {may},
  Number                   = {5},
  Volume                   = {16},

  Abstract                 = {This paper introduces the sensor-networked IoT model as a prototype to support the design of Body Area Network (BAN) applications for healthcare. Using the model, we analyze the synergistic effect of the functional requirements (data collection from the human body and transferring it to the top level) and non-functional requirements (trade-offs between energy-security-environmental factors, treated as Quality-of-Service (QoS)). We use feature models to represent the requirements at the earliest stage for the analysis and describe a model-driven methodology to design the possible BAN applications. Firstly, we specify the requirements as the problem domain (PD) variability model for the BAN applications. Next, we introduce the generative technology (meta-programming as the solution domain (SD)) and the mapping procedure to map the PD feature-based variability model onto the SD feature model. Finally, we create an executable meta-specification that represents the BAN functionality to describe the variability of the problem domain though transformations. The meta-specification (along with the meta-language processor) is a software generator for multiple BAN-oriented applications. We validate the methodology with experiments and a case study to generate a family of programs for the BAN sensor controllers. This enables to obtain the adequate measure of QoS efficiently through the interactive adjustment of the meta-parameter values and re-generation process for the concrete BAN application.},
  Doi                      = {10.3390/s16050670},
  ISSN                     = {14248220},
  Publisher                = {MDPI AG}
}

@Article{Vijaya2016,
  Title                    = {A model driven framework for portable cloud services},
  Author                   = {Vijaya, Aparna and Neelanarayanan, V.},
  Year                     = {2016},

  Month                    = {apr},
  Number                   = {2},
  Pages                    = {708--716},
  Volume                   = {6},

  Abstract                 = {Cloud Computing is an evolving technology as it offers significant benefits like pay only for what you use, scale the resources according to the needs and less in-house staff and resources. These benefits have resulted in tremendous increase in the number of applications and services hosted in the cloud which inturn has resulted in increase in the number of cloud providers in the market. Cloud service providers have a lot of heterogeneity in the resources they use. They have their own servers, different cloud infrastructures, API's and methods to access the cloud resources. Despite its benefits; lack of standards among service providers has caused a high level of vendor lock-in when a software developer tries to change its cloud provider. In this paper we give an overview on the ongoing and current trends in the area of cloud service portability and we also propose a new cloud portability platform. Our new platform is based on establishing feature models which offers the desired cloud portability. Our solution DSkyL uses feature models and domain model analysis to support development, customization and deployment of application components across multiple clouds. The main goal of our approach is to reduce the effort and time needed for porting applications across different clouds. This paper aims to give an overview on DSkyL.},
  Doi                      = {10.11591/ijece.v6i1.8270},
  ISSN                     = {20888708},
  Publisher                = {Institute of Advanced Engineering and Science}
}

@Article{Wang2014,
  Title                    = {Fast simultaneous localization and mapping based on iterative extended Kalman filter proposal distribution and linear optimization resampling},
  Author                   = {Wang, Hong Jian and Wang, Jing and Liu, Zhen Ye},
  Year                     = {2014},
  Number                   = {2},
  Pages                    = {318--324},
  Volume                   = {36},

  Abstract                 = {The location estimated accuracy of Autonomous Underwater Vehicle (AUV) and landmarks decrease because of the degeneracy and impoverishment of samples in standard Fast Simultaneous Localization And Mapping (FastSLAM) algorithm. A improved FastSLAM algorithm based on Iterative Extended Kalman Filter (IEKF) proposal distribution and linear optimization resampling is presented in order to solve this issue. The latest observation is integrated with IEKF in order to decrease the sample degeneracy while the new samples are produced by the linear combination of copied samples and some abandoned ones in order to reduce the sample impoverishment. The kinematic model of AUV, feature model and the measurement models of sensors are all established. And then features are extracted with Hough transform to build the global map. The experiment of the improved FastSLAM algorithm with trial data shows that it can avoid the degeneracy and impoverishment of samples effectively and enhance the location estimation accuracy of AUV and landmarks. Moreover, the consistency analysis showed that the method possesses the consistency of long term.},
  Doi                      = {10.3724/SP.J.1146.2012.01373},
  ISSN                     = {10095896},
  Publisher                = {Science Press}
}

@InProceedings{Wang2013,
  Title                    = {Developing a holistic modeling approach for search-based system architecting},
  Author                   = {Wang, Renzhong and Dagli, Cihan H.},
  Year                     = {2013},
  Pages                    = {206--215},
  Volume                   = {16},

  Abstract                 = {This paper proposes a holistic modeling approach that combines the capabilities of Object Process Methodology (OPM), Colored Petri Net (CPN), and feature model. The resultant holistic model not only can capture the structural, behavioral, and dynamic aspects of a system, allowing simulation and strong analysis methods to be applied, it can also specify the architectural design space. This modeling approach is developed to facilitate the implementation of search-based system architecting where search algorithms are used to explore design trade space for good architecture alternatives. Such architecting approach integrates certain model construction, alternative generation, simulation, and assessment processes into a coherent and automated framework. Both the proposed holistic modeling approach and the search-based architecting framework are generic. They are targeted at systems that can be specified by conceptual models using object-oriented or process-oriented paradigms. The broad applicability of the proposed approach is demonstrated with the configuration of reconfigurable manufacturing systems (RMSs) under multiobjective optimization as an example. The test results showed that the proposed modeling approach could cover a huge number of architecture alternatives and supported the assessment of several performance measures. A set of quality results was obtained after running the optimization algorithm following the proposed search-based architecting framework. {\textcopyright} 2013 The authors. Published by Elsevier B.V.},
  Doi                      = {10.1016/j.procs.2013.01.022},
  ISSN                     = {18770509}
}

@InProceedings{Wang2012,
  Title                    = {Computational System Architecture Development Using a Holistic Modeling Approach},
  Author                   = {Wang, Renzhong and Dagli, Cihan H.},
  Year                     = {2012},
  Pages                    = {13--20},
  Publisher                = {Elsevier},
  Volume                   = {12},

  Abstract                 = {This paper presents an innovative system architecture development framework that allows the search of optimum architecture solutions within large design space by automating certain model construction, alternative generation, simulation, and assessment tasks. Such framework is facilitated by a holistic modeling approach that combines the capabilities of Object Process Methodology (OPM), Colored Petri Net (CPN) and feature model. The resultant holistic model not only can capture the structural, behavior, and dynamic aspects of a system, allowing strong analysis methods to be applied, but also can specify the architectural design space allowing generation of architecture alternatives that cover it. The proposed framework and suggested implementation is generic targeted at systems that can be specified by logic models using object-oriented paradigm. A partial implementation of the proposed approaches is presented with the design of reconfigurable manufacturing systems (RMSs) as an example, which is formulated as a multi-objective optimization problem with the Genetic Algorithm (GA, particularly, NSGA-2) as the search algorithm. The RMS is a multi-part flow line structure with identical machines in each production stage. {\textcopyright} 2012 Published by Elsevier B.V.},
  Doi                      = {10.1016/j.procs.2012.09.023},
  ISSN                     = {18770509}
}

@Misc{Wang2016,
  Title                    = {A systematic test case selection methodology for product lines: results and insights from an industrial case study},

  Author                   = {Wang, Shuai and Ali, Shaukat and Gotlieb, Arnaud and Liaaen, Marius},
  Month                    = {aug},
  Year                     = {2016},

  Abstract                 = {In the context of product lines, test case selection aims at obtaining a set of relevant test cases for a product from the entire set of test cases available for a product line. While working on a research-based innovation project on automated testing of product lines of Video Conferencing Systems (VCSs) developed by Cisco, we felt the need to devise a cost-effective way of selecting relevant test cases for a product. To fulfill such need, we propose a systematic and automated test selection methodology using: 1) Feature Model for Testing (FM{\_}T) to capture commonalities and variabilities of a product line; 2) Component Family Model for Testing (CFM{\_}T) to model the structure of test case repository; 3) A tool to automatically build restrictions from CFM{\_}T to FM{\_}T and traces from CFM{\_}T to the actual test cases. Using our methodology, a test engineer is only required to select relevant features through FM{\_}T at a higher level of abstraction for a product and the corresponding test cases will be obtained automatically. We evaluate our methodology by applying it to a VCS product line called Saturn with seven commercial products and the results show that our methodology can significantly reduce cost measured as test selection time and at the same time achieves higher effectiveness (feature coverage, feature pairwise coverage and fault detection) as compared with the current manual process. Moreover, we conduct a questionnaire-based study to solicit the views of test engineers who are involved in developing FM{\_}T and CFM{\_}T. The results show that test engineers are positive about adapting our methodology in their current practice. Finally, we present a set of lessons learnt while applying product line engineering at Cisco for test case selection.},
  Doi                      = {10.1007/s10664-014-9345-5},
  ISSN                     = {15737616},
  Number                   = {4},
  Pages                    = {1586--1622},
  Publisher                = {Springer New York LLC},
  Volume                   = {21}
}

@Misc{Wang2015,
  Title                    = {Automated product line test case selection: industrial case study and controlled experiment},

  Author                   = {Wang, Shuai and Ali, Shaukat and Gotlieb, Arnaud and Liaaen, Marius},
  Month                    = {apr},
  Year                     = {2015},

  Abstract                 = {Automated test case selection for a new product in a product line is challenging due to several reasons. First, the variability within the product line needs to be captured in a systematic way; second, the reusable test cases from the repository are required to be identified for testing a new product. The objective of such automated process is to reduce the overall effort for selection (e.g., selection time), while achieving an acceptable level of the coverage of testing functionalities. In this paper, we propose a systematic and automated methodology using a feature model for testing (FM{\_}T) to capture commonalities and variabilities of a product line and a component family model for testing (CFM{\_}T) to capture the overall structure of test cases in the repository. With our methodology, a test engineer does not need to manually go through the repository to select a relevant set of test cases for a new product. Instead, a test engineer only needs to select a set of relevant features using FM{\_}T at a higher level of abstraction for a product and a set of relevant test cases will be selected automatically. We evaluated our methodology via three different ways: (1) We applied our methodology to a product line of video conferencing systems called Saturn developed by Cisco, and the results show that our methodology can reduce the selection effort significantly; (2) we conducted a questionnaire-based study to solicit the views of test engineers who were involved in developing FM{\_}T and CFM{\_}T. The results show that test engineers are positive about adapting our methodology and models (FM{\_}T and CFM{\_}T) in their current practice; (3) we conducted a controlled experiment with 20 graduate students to assess the performance (i.e., cost, effectiveness and efficiency) of our automated methodology as compared to the manual approach. The results showed that our methodology is cost-effective as compared to the manual approach, and at the same time, its efficiency is not affected by the increased complexity of products.},
  Doi                      = {10.1007/s10270-015-0462-4},
  ISSN                     = {16191374},
  Publisher                = {Springer Verlag}
}

@InProceedings{Wang2013a,
  Title                    = {Automated test case selection using feature model: An industrial case study},
  Author                   = {Wang, Shuai and Gotlieb, Arnaud and Ali, Shaukat and Liaaen, Marius},
  Year                     = {2013},
  Pages                    = {237--253},
  Volume                   = {8107 LNCS},

  Abstract                 = {Automated test case selection for a new product in a product line is challenging due to several reasons. First, the variability within the product line needs to be captured in a systematic way; second, the reusable test cases from the repository are required to be identified for testing a new product. The objective of such automated process is to reduce the overall effort for selection (e.g., selection time), while achieving an acceptable level of the coverage of testing functionalities. In this paper, we propose a systematic and automated methodology using a Feature Model for Testing (FM-T) to capture commonalities and variabilities of a product line and a Component Family Model for Testing (CFM-T) to capture the overall structure of test cases in the repository. With our methodology, a test engineer does not need to manually go through the repository to select a relevant set of test cases for a new product. Instead, a test engineer only needs to select a set of relevant features using FM-T at a higher level of abstraction for a product and a set of relevant test cases will be selected automatically. We applied our methodology to a product line of video conferencing systems called Saturn developed by Cisco and the results show that our methodology can reduce the selection effort significantly. Moreover, we conducted a questionnaire-based study to solicit the views of test engineers who were involved in developing FM-T and CFM-T. The results show that test engineers are positive about adapting our methodology and models (FM-T and CFM-T) in their current practice. {\textcopyright} 2013 Springer-Verlag.},
  Doi                      = {10.1007/978-3-642-41533-3_15},
  ISBN                     = {9783642415326},
  ISSN                     = {03029743}
}

@InProceedings{Wang2010,
  Title                    = {Research on method of virtual modeling based on reconstruction of virtual assembly feature},
  Author                   = {Wang, Tao and Yan, Qing Dong and Li, Hong Cai},
  Year                     = {2010},
  Pages                    = {1295--1300},

  Abstract                 = {According to the concept and characteristics of virtual assembly feature, reconstruction method of virtual assembly feature model is proposed. Using the secondary development tools provided by ProEngineer, namely, ToolKit, getting assembly information is implemented. Based on analysis about three kinds mapping methods between the geometric elements and triangulated patches, an complete information model of virtual assembly features is established that regard model information database as underlying data to support, and finally, virtual assembly modeling method of a certain type of hydraulic torque converter in the transmission system is taken as an example to be verified. {\textcopyright}2010 IEEE.},
  Doi                      = {10.1109/ICALIP.2010.5685085},
  ISBN                     = {9781424458653}
}

@Article{Wang2016a,
  Title                    = {Study on the changing ruler of plasma in laser welding and the quick testing method of blowhole defects - Integral analysis method for signals detection},
  Author                   = {Wang, Xuyou and Sun, Qian and Wang, Wei and Li, Xiaoyu and Huang, Ruisheng},
  Year                     = {2016},

  Month                    = {mar},
  Number                   = {3},
  Pages                    = {45--48},
  Volume                   = {37},

  Abstract                 = {According to the study on linear track and global distribution of plasma size signals in laser welding, it is found that the global signals distribution feature of plasma has a close relationship with the real laser welding conditions. As aresults anew method based on integral analysis for plasma size signals and its transform feature model has been established in this paper. The results show that this method can not only effectly remove the random noise signals and the negative influence of the signals fluctuation during data treating process, but also has special features on its repeatability and legibility. Therefore, it is a quick and accurate way in testing the change of plasma morphology in laser welding.},
  ISSN                     = {0253360X},
  Publisher                = {Harbin Research Institute of Welding}
}

@InProceedings{Wang2010a,
  Title                    = {Multi-class target recognition based on adaptive feature selection},
  Author                   = {Wang, Yuehuan and Yao, Wei and Song, Yunfeng and Sang, Nong and Zhang, Tianxu},
  Year                     = {2010},
  Volume                   = {7696},

  Abstract                 = {In this paper, a new approach of multi-class target recognition is proposed for remote sensing image analysis. A multi-class feature model is built, which is based on sharing features among classes. In order to make the recognition process efficient, we adopted the idea of adaptive feature selection. In each layer of the integrated feature model, the most salient and stable feature are selected first, and then the less ones. Experiments demonstrated the approach proposed is efficient in computation and is adaptive to scene variation. {\textcopyright} 2010 SPIE.},
  Doi                      = {10.1117/12.850649},
  ISBN                     = {9780819481603},
  ISSN                     = {0277786X}
}

@Article{Wang2016b,
  Title                    = {Social content based latent influence propagation model},
  Author                   = {Wang, Zhen Jun and Wang, Shu Hui and Zhang, Wei Gang and Huang, Qing Ming},
  Year                     = {2016},

  Month                    = {aug},
  Number                   = {8},
  Pages                    = {1528--1540},
  Volume                   = {39},

  Abstract                 = {With the proliferation of diversified social network services, understanding how the influence is propagated could help us apprehend the network evolution mechanism and the social impact of different kinds of information better. Most previous works have focused on the analysis of the influence propagation on the static network structure and the discovery of the subset of the most influential users. They fail to identify the user susceptibility delivered by user generated content. In this paper, we propose the InfoIBP (Influence propagation on Indian Buffet Process) model, a general framework for the latent influence propagation on social content with dynamic network structure, which based on the Indian buffet process. The influential users could be taken as the latent features in the social network and be found by different sampling algorithms based on numerical approximation. For the dynamic evolutional property of the network, hidden Markov model was adopted to describe the influence propagation in different time steps. A series of experiments for link prediction, preference prediction and running time evaluation are conducted on the DBLP and Digg datasets. The results show that the InfoIBP is more accurate and more efficient for modeling the latent influence propagation and discovering the influential users. It also can describe the dynamic evolutional property more comprehensively and achieve relatively accurate predictions for the future observations.},
  Doi                      = {10.11897/SP.J.1016.2016.01528},
  ISSN                     = {02544164},
  Publisher                = {Science Press}
}

@Article{Wang2013b,
  Title                    = {Breast tumor detection algorithm based on feature selection ELM},
  Author                   = {Wang, Zhi Qiong and Kang, Yan and Yu, Ge and Zhao, Ying Jie},
  Year                     = {2013},

  Month                    = {jun},
  Number                   = {6},
  Pages                    = {792--796},
  Volume                   = {34},

  Abstract                 = {Breast tumor detection is an effective way for preventing breast cancer, and the classification algorithm of extreme learning machine(ELM) that based on X-Ray image feature model of breast had been used in computer aided detection of breast tumor. Due to the low learning efficiency and detection accuracy of ELM caused by the dependence between features, a breast tumor detction algorithm was proposed in this paper based on features selection ELM. The methods of impact value selection, sequential forward selection and genetic algorithm were used to improve the performance of ELM. The 490 X-Ray images used in the experiment came from Tumor Hospital of Liaoning Province, and the results showed that the precision of breast tumor detection could be improved with the proposed method especially for genetic selection algorithm.},
  ISSN                     = {10053026}
}

@Article{Wen2012,
  Title                    = {Image-based orchard insect automated identification and classification method},
  Author                   = {Wen, Chenglu and Guyer, Daniel},
  Year                     = {2012},

  Month                    = {nov},
  Pages                    = {110--115},
  Volume                   = {89},

  Abstract                 = {Insect identification and classification is time-consuming work requiring expert knowledge for integrated pest management in orchards. An image-based automated insect identification and classification method is described in the paper. The complete method includes three models. An invariant local feature model was built for insect identification and classification using affine invariant local features; a global feature model was built for insect identification and classification using 54 global features; and a hierarchical combination model was proposed based on local feature and global feature models to combine advantages of the two models and increase performance. The three models were applied and tested for insect classification on eight insect species from pest colonies and orchards. The hierarchical combination model yielded better performance over global and local models. Moreover, to study the pose change of insects on traps and the hypothesis that an optimal time to acquire and image after landing exists, advanced analysis on time-dependent pose change of insects on traps is included in this study. The experimental results on field insect image classification with field-based images for training achieved the classification rate of 86.6{\%} when testing with the combination model. This demonstrates the image-based insect identification and classification method could be a potential way for automated insect classification in integrated pest management. {\textcopyright} 2012 Elsevier B.V.},
  Doi                      = {10.1016/j.compag.2012.08.008},
  ISSN                     = {01681699}
}

@Article{Wen2016,
  Title                    = {Reliability modeling and analysis for process system of sheet metal assembly},
  Author                   = {Wen, Zejun and Liu, Jijun and Zhao, Yanming and Hu, Zhongju and Liu, Zhan and Chen, Lifeng},
  Year                     = {2016},

  Month                    = {feb},
  Number                   = {3},
  Pages                    = {376--382},
  Volume                   = {27},

  Abstract                 = {A product quality oriented reliability modeling method was developed containing pin/hole (slot) tolerance for process system of sheet metal assembly. Firstly, considering locating pin tolerance, part hole (slot) tolerance and pin wear, a statistics feature model of assembly deviation was presented. Then the locating pin wear model was deduced and analyzed, according to analyze the relationship between assembly qualities and process system reliability, and considering the impacts of locating failures rate and process wear on the process system reliability, the structure reliability model and assembly quality oriented reliability model of process system were built, then the process system reliability modeling method was formed. An automotive body side panel assembly was given as an example, the assembly process system reliability was analyzed based on the modeling method. The results show that locating pin wear, fixture layout and tolerance of pin/hole (slot) are important factors that affect process system reliability of automotive body side assembly. The method provides a new way of process system reliability analyses for product assembly.},
  Doi                      = {10.3969/j.issn.1004-132X. 2016.03.017},
  ISSN                     = {1004132X},
  Publisher                = {China Mechanical Engineering Magazine Office}
}

@Article{White2010,
  Title                    = {Automated diagnosis of feature model configurations},
  Author                   = {White, J. and Benavides, D. and Schmidt, D. C. and Trinidad, P. and Dougherty, B. and Ruiz-Cortes, A.},
  Year                     = {2010},

  Month                    = {jul},
  Number                   = {7},
  Pages                    = {1094--1107},
  Volume                   = {83},

  Abstract                 = {Software product-lines (SPLs) are software platforms that can be readily reconfigured for different project requirements. A key part of an SPL is a model that captures the rules for reconfiguring the software. SPLs commonly use feature models to capture SPL configuration rules. Each SPL configuration is represented as a selection of features from the feature model. Invalid SPL configurations can be created due to feature conflicts introduced via staged or parallel configuration or changes to the constraints in a feature model. When invalid configurations are created, a method is needed to automate the diagnosis of the errors and repair the feature selections. This paper provides two contributions to research on automated configuration of SPLs. First, it shows how configurations and feature models can be transformed into constraint satisfaction problems to automatically diagnose errors and repair invalid feature selections. Second, it presents empirical results from diagnosing configuration errors in feature models ranging in size from 100 to 5,000 features. The results of our experiments show that our CSP-based diagnostic technique can scale up to models with thousands of features. {\textcopyright} 2010 Elsevier Inc. All rights reserved.},
  Doi                      = {10.1016/j.jss.2010.02.017},
  ISSN                     = {01641212}
}

@Article{White2014,
  Title                    = {Evolving feature model configurations in software product lines},
  Author                   = {White, Jules and Galindo, Jose A. and Saxena, Tripti and Dougherty, Brian and Benavides, David and Schmidt, Douglas C.},
  Year                     = {2014},

  Month                    = {jan},
  Number                   = {1},
  Pages                    = {119--136},
  Volume                   = {87},

  Abstract                 = {The increasing complexity and cost of software-intensive systems has led developers to seek ways of reusing software components across development projects. One approach to increasing software reusability is to develop a software product-line (SPL), which is a software architecture that can be reconfigured and reused across projects. Rather than developing software from scratch for a new project, a new configuration of the SPL is produced. It is hard, however, to find a configuration of an SPL that meets an arbitrary requirement set and does not violate any configuration constraints in the SPL. Existing research has focused on techniques that produce a configuration of an SPL in a single step. Budgetary constraints or other restrictions, however, may require multi-step configuration processes. For example, an aircraft manufacturer may want to produce a series of configurations of a plane over a span of years without exceeding a yearly budget to add features. This paper provides three contributions to the study of multi-step configuration for SPLs. First, we present a formal model of multi-step SPL configuration and map this model to constraint satisfaction problems (CSPs). Second, we show how solutions to these SPL configuration problems can be automatically derived with a constraint solver by mapping them to CSPs. Moreover, we show how feature model changes can be mapped to our approach in a multi-step scenario by using feature model drift. Third, we present empirical results demonstrating that our CSP-based reasoning technique can scale to SPL models with hundreds of features and multiple configuration steps. {\textcopyright} 2013 Elsevier Inc.},
  Doi                      = {10.1016/j.jss.2013.10.010},
  ISSN                     = {01641212}
}

@InProceedings{Wulf-Hadash2013,
  Title                    = {Cross product line analysis},
  Author                   = {Wulf-Hadash, Ora and Reinhartz-Berger, Iris},
  Year                     = {2013},

  Abstract                 = {Due to increase in market competition and merger and acquisition of companies, different software product lines (SPLs) may exist under the same roof. These SPLs may be developed applying different domain analysis processes, but are likely not disjoint. Cross product line analysis aims to examine the common and variable aspects of different SPLs for improving maintenance and future development of related SPLs. Currently different SPL artifacts, or more accurately feature models, are compared, matched, and merged for supporting scalability, increasing modularity and reuse, synchronizing feature model versions, and modeling multiple SPLs for software supply chains. However, in all these cases the focus is on creating valid merged models from the input feature models. Furthermore, the terminology used in all the input feature models is assumed to be the same, namely similar features are named the same. As a result these methods cannot be simply applied to feature models that represent different SPLs. In this work we offer adapting similarity metrics and text clustering techniques in order to enable cross product line analysis. This way analysis of feature models that use different terminologies in the same domain can be done in order to improve the management of the involved SPLs. Preliminary results reveal that the suggested method helps systematically analyze the commonality and variability between related SPLs, potentially suggesting improvements to existing SPLs and to the maintenance of sets of SPLs. {\textcopyright} 2013 ACM.},
  Doi                      = {10.1145/2430502.2430531},
  ISBN                     = {9781450315418}
}

@Article{Xiong2012,
  Title                    = {Research on the feature model of the formation and evolution of social networks},
  Author                   = {Xiong, Xi and Cao, Wei and Zhou, Xin and Hu, Yong},
  Year                     = {2012},

  Month                    = {jul},
  Pages                    = {140--144},
  Volume                   = {44},

  Abstract                 = {Besides the typical characteristics of scale-free network, the indispensable features of social networking services and microblog can not be described by existing models completely and accurately. Based on comparison and analysis of network data, a hybrid model was put forward to describe the features of formation and evolution of generalized social network. Average-field equations were constructed, and its solution showed that the degree distribution follows the translated and stretched power-law. The simulation results showed that the model can describe the overall characteristics of social network. Finally, several models, including this model, were compared, and reasons were given for the differences of features among the models.},
  ISSN                     = {10093087}
}

@InProceedings{Xue2010,
  Title                    = {Understanding feature evolution in a family of product variants},
  Author                   = {Xue, Yinxing and Xing, Zhenchang and Jarzabek, Stan},
  Year                     = {2010},
  Pages                    = {109--118},

  Abstract                 = {Existing software product variants, developed by ad hoc reuse such as copy-paste-modify, are often a starting point for building Software Product Line (SPL). Understanding of how features evolved in product variants is a prerequisite to transition from ad hoc to systematic SPL reuse. We propose a method that assists analysts in detecting changes to product features during evolution. We first entail that features and their inter-dependencies for each product variant are documented as product feature model. We then apply model differencing algorithm to identify evolutionary changes that occurred to features of different product variants. We evaluate the effectiveness of our approach on a family of medium-size financial systems. We also investigate the scalability of our approach with synthetic data. The evaluation demonstrates that our approach yields good results and scales to large systems. Our approach enables the subsequent variability analysis and consolidation of product variants in the task of reengineering product variants into SPL. {\textcopyright} 2010 IEEE.},
  Doi                      = {10.1109/WCRE.2010.20},
  ISBN                     = {9780769541235},
  ISSN                     = {10951350}
}

@Article{Yan2010,
  Title                    = {BDD-based approach to the verification of feature models},
  Author                   = {Yan, Hua and Zhang, Wei and Zhao, Hai Yan and Mei, Hong},
  Year                     = {2010},

  Month                    = {jan},
  Number                   = {1},
  Pages                    = {84--97},
  Volume                   = {21},

  Abstract                 = {The feature model is a reusable requirements model generated from the domain analysis. The reuse of feature models is usually achieved by a customizing-based approach. One important issue in feature models' customization is the verification problem, caused by the fact that there are usually constraints among features, and that a valid customizing result must satisfy all these constraints. Because of the NP-hard nature of this problem, it is usually difficult to verify feature models in an efficient way. This paper presents a BDD (binary decision diagram)-based approach to verifying feature models by only traversing once to the nodes in BDDs, an approach that makes an efficient use of the BDD data structures based on the unique characteristics of feature models' verification. It should be pointed out that this approach does not attempt to resolve the NP-hard difficulty of the verification problem in a general sense, but just tries to improve the scalability and efficiency of methods for feature models' verification based on the utilization of this problem's uniqueness. Experimental results show that this BDD-based approach is more efficient and can verify more complex feature models than the previous method. {\textcopyright} by Institute of Software, the Chinese Academy of Sciences. All rights reserved.},
  Doi                      = {10.3724/SP.J.1001.2010.03525},
  ISSN                     = {10009825}
}

@InProceedings{Yang2016,
  Title                    = {A feature-oriented modeling approach for embedded product line engineering},
  Author                   = {Yang, Guanzhong and Zhang, Yaru},
  Year                     = {2016},
  Month                    = {jan},
  Pages                    = {1607--1612},
  Publisher                = {Institute of Electrical and Electronics Engineers Inc.},

  Abstract                 = {Feature model is an important model of capturing domain requirements. The traditional feature modeling methods extract the characteristics from the extension of the system. However, the methods fail to describe feature in details especially for embedded software product line engineering. As for these deficiencies, this paper combines the characteristics of the embedded products, with the domain ontology as the basis of feature modeling, dividing the feature modeling into building domain ontology and the feature analysis. First, it will identify the concept of mutual recognition through acquiring, describing and denoting the related domain knowledge. Then, it will divide the feature into three parts which are concept, attribute and the relation, and describe the feature from connotation and extension. It defines and normalizes feature in a clear way. Finally, the effectiveness of the proposed modeling method will be verified through a ventilator embedded product line.},
  Doi                      = {10.1109/FSKD.2015.7382185},
  ISBN                     = {9781467376822}
}

@InProceedings{Yi2012,
  Title                    = {Mining binary constraints in the construction of feature models},
  Author                   = {Yi, Li and Zhang, Wei and Zhao, Haiyan and Jin, Zhi and Mei, Hong},
  Year                     = {2012},
  Pages                    = {141--150},

  Abstract                 = {Feature models provide an effective way to organize and reuse requirements in a specific domain. A feature model consists of a feature tree and cross-tree constraints. Identifying features and then building a feature tree takes a lot of effort, and many semi-automated approaches have been proposed to help the situation. However, finding cross-tree constraints is often more challenging which still lacks the help of automation. In this paper, we propose an approach to mining cross-tree binary constraints in the construction of feature models. Binary constraints are the most basic kind of cross-tree constraints that involve exactly two features and can be further classified into two sub-types, i.e. requires and excludes. Given these two sub-types, a pair of any two features in a feature model falls into one of the following classes: no constraints between them, a requires between them, or an excludes between them. Therefore we perform a 3-class classification on feature pairs to mine binary constraints from features. We incorporate a support vector machine as the classifier and utilize a genetic algorithm to optimize it. We conduct a series of experiments on two feature models constructed by third parties, to evaluate the effectiveness of our approach under different conditions that might occur in practical use. Results show that we can mine binary constraints at a high recall (near 100{\%} in most cases), which is important because finding a missing constraint is very costly in real, often large, feature models. {\textcopyright} 2012 IEEE.},
  Doi                      = {10.1109/RE.2012.6345798},
  ISBN                     = {9781467327855}
}

@Article{Yi2013,
  Title                    = {Research on the merging of feature models},
  Author                   = {Yi, Li and Zhao, Hai Yan and Zhang, Wei and Jin, Zhi and Mei, Hong},
  Year                     = {2013},

  Month                    = {jan},
  Number                   = {1},
  Pages                    = {1--9},
  Volume                   = {36},

  Abstract                 = {Feature models provide an effective way to organize and reuse software requirements in a specific domain. Constructing a feature model needs a systematic analysis of as many applications as possible in a domain, to identify commonality, variability, and dependencies among requirements. With the increasing complexity of domains, the scale of feature models can be extremely large, and the construction of large feature models is an overwhelming task for human that computer-aided automation is needed. A feasible way is to merge existing feature models into a large one, and human developers only need to do some refactoring work. In this paper, we survey six methods of merging feature models. We propose a conceptual framework first, and then analyze and compare the six methods. Finally, we identify three problems in existing research, and propose possible ideas to handle these problems.},
  Doi                      = {10.3724/SP.J.1016.2013.00001},
  ISSN                     = {02544164}
}

@InProceedings{Ying2011,
  Title                    = {Domain service acquisition and domain modeling based on feature model},
  Author                   = {Ying, Guo and Xiao-Yan, Zhang and Jun, Wang and Meihong, Yang},
  Year                     = {2011},
  Pages                    = {26--33},

  Abstract                 = {Domain engineering is the core technology of systematic software reuse and the precedent period of software requirements engineering. FODA method and other FODA-based methods of domain analysis have made great progress. In these methods, feature model and feature-oriented modeling methodology are widely accepted. However, they have three limitations: 1) Feature model cannot be applied separately, it must work together with other models, such as entity-relationship model and function model to present the whole domain. 2) Feature model is a conceptual tree model, which needs standardized description mechanism to support better conversion to domain design, domain implementation and application engineering. 3) With the service-oriented method permeating into current software development process, traditional domain model encounters challenges in providing more effective reuse to service-oriented software requirements activities. To solve these problems, we proposed to combine service-oriented method and domain engineering together, introduced a new concept of Domain Service, and gave its acquisition process and OWL-S based ontology definition. Domain Service is acquired from the classic feature model and could encapsulate features and their relations to model domain commonalities and differences. OWL-S based specification of Domain Service enhances the application of domain model. After Domain Service acquisition and specification, we can get a domain model composed of a set of Domain Services with different granularity. The model has two layers: Domain Service Layer and Ontology Layer, which can be mapped to the domain model produced from the FODA method. Finally, we applied our method in the domain of Public Travel Information Systems. {\textcopyright} 2011 IEEE.},
  Doi                      = {10.1109/CSE.2011.20},
  ISBN                     = {9780769544779}
}

@Article{Yu2013,
  Title                    = {Feature integration analysis of bag-of-features model for image retrieval},
  Author                   = {Yu, Jing and Qin, Zengchang and Wan, Tao and Zhang, Xi},
  Year                     = {2013},

  Month                    = {nov},
  Pages                    = {355--364},
  Volume                   = {120},

  Abstract                 = {One of the biggest challenges in content based image retrieval is to solve the problem of "semantic gaps" between low-level features and high-level semantic concepts. In this paper, we aim to investigate various combinations of mid-level features to build an effective image retrieval system based on the bag-of-features (BoF) model. Specifically, we study two ways of integrating the SIFT and LBP descriptors, HOG and LBP descriptors, respectively. Based on the qualitative and quantitative evaluations on two benchmark datasets, we show that the integrations of these features yield complementary and substantial improvement on image retrieval even with noisy background and ambiguous objects. Two integration models are proposed: the patch-based integration and image-based integration. By using a weighted K-means clustering algorithm, the image-based SIFT-LBP integration achieves the best performance on the given benchmark problems comparing to the existing algorithms. {\textcopyright} 2013 Elsevier B.V.},
  Doi                      = {10.1016/j.neucom.2012.08.061},
  ISSN                     = {09252312}
}

@Article{Yu2016,
  Title                    = {Semantic database design and semantic map construction of robots based on the cloud},
  Author                   = {Yu, Jinshan and Wu, Hao and Tian, Guohui and Xue, Yinghua and Zhao, Guixiang},
  Year                     = {2016},

  Month                    = {jul},
  Number                   = {4},
  Pages                    = {410--419},
  Volume                   = {38},

  Abstract                 = {In intelligent service task, it is difficult for indoor mobile robots to obtain semantic information of complex environment. A semantic map based on semantic acquisition structure of environment is constructed by designing cloud semantic database. The robot can not only get the geometric description of environment, but also obtain the semantic map which contains objects relationship based on rich semantic database of complex environment. It solves the low reliability of adding semantic information, the error of updating map and the lack of scalability in the process of constructing the semantic map. It begins by presenting a semantic database construction project. Then semantic sub-databases are obtained by classifying the semantic database based on SVM (support vector machine) algorithm. On the base of semantic sub-databases, the feature model database is formed by extracting key feature points based on network text classification. By combining the semantic sub-database with the semantic classification list, the objects can be identified. Secondly, the implementation of cloud semantic map for the intelligent service task is discussed. Based on the multi-scale image segmentation and the analysis of disparity map, annotation database and belonging database are designed to describe the belonging relationship between objects. Finally, the semantic map is constructed and the classification efficiency of semantic database is analyzed in simulation experiments to verify the validity of the method.},
  Doi                      = {10.13973/j.cnki.robot.2016.0410},
  ISSN                     = {10020446},
  Publisher                = {Chinese Academy of Sciences}
}

@InProceedings{Yu2011,
  Title                    = {Naxi-English bilingual word alignment based on language feature model of Naxi},
  Author                   = {Yu, Zhengtao and Zhang, Tao and Guo, Jianyi and Mao, Cunli and Li, Jian},
  Year                     = {2011},
  Pages                    = {100--105},

  Abstract                 = {To achieve the corpus of Naxi - English bilingual words alignment, aim at syntactic characteristics of Naxi language. A Naxi-English bilingual words alignment method is proposed. This method uses the log-linear model, and introduces feature functions based on the characteristic of the Naxi language, which are English - Naxi interval switching function and Naxi - English bilingual words position transformation function. With the artificial labeling of Naxi - English words alignment corpus, the parameters of the model are trained by using the minimum error. The Naxi-English bilingual words are alignment automatically by this model. Experiments with IBM Model 3 as a benchmark, and gradually add constraints on the characteristics of the Naxi language with the basis of IBM Model 3. The final experiment results show that the Naxi - English bilingual word alignment accuracy can be improved significantly with the feature functions which are base on characteristic of Naxi. {\textcopyright} 2011 IEEE.},
  Doi                      = {10.1109/NLPKE.2011.6138176},
  ISBN                     = {9781612847283}
}

@InProceedings{Zhan2016,
  Title                    = {Features of web subject-related image and its retrieval significance},
  Author                   = {Zhan, Daning and Zou, Yongli},
  Year                     = {2016},
  Month                    = {feb},
  Pages                    = {1151--1154},
  Publisher                = {Institute of Electrical and Electronics Engineers Inc.},

  Abstract                 = {Keyword information retrieval is the mainstream way of information retrieval at present. Users need to scan a large amount of text information in the search results to find the information they want, but the process causes inefficiency and poor user experience. In fact, images in the web pages can provide a direct-viewing and fast retrieval way. Through the research on the features of web subject-related image and achieve its automatic identification and extraction, we can display it in the thumbnail together with the page title and summary in the results pages. It can help users filter and browse information in a more convenient way. This paper establishes a web image attribute system from both HTML attributes and external attributes. Then through corresponding automatic extracting algorithms and data analysis it succeeds to gain 16 feature rules of web subject-related image, and finishes building a web subject-related image feature model. The model proves to obtain more than 99{\%} of the extraction rate and filtering rate while applying to the sample data, demonstrating its value in web information retrieval.},
  Doi                      = {10.1109/APSIPA.2015.7415452},
  ISBN                     = {9789881476807}
}

@InProceedings{Zhang2012,
  Title                    = {A Role-based feature model componentization framework and related algorithms},
  Author                   = {Zhang, Jun and Liu, Shufen},
  Year                     = {2012},
  Pages                    = {366--371},

  Abstract                 = {As a kind of requirement model, feature model represents functions of a family of products in a uniform form. To solve the chaos and entanglement problem in the process of feature model description and configuration, this paper designs a feature model componentization framework. With the help of the concept of Role and Reference Role, the framework proposes a process to analyze and decompose complex requirement specification to simple and coherent roles, and then implements Feature -Role-Component algorithms which map all features generated by requirement elicitation and analysis to different model components. In the framework and algorithms, role and reference role plays as an intermediary who decouple the feature and component, which makes convenient feature variants selection and composition and enhances the componentization level of the system. By decomposing the system functions in the early phase of software lifecycle, the framework improves the flexibility and adaptability of software artifact, which makes a stable foundation for higher quality product. {\textcopyright} 2012 IEEE.},
  Doi                      = {10.1109/CSCWD.2012.6221844},
  ISBN                     = {9781467312127}
}

@Article{Zhang2011,
  Title                    = {A feature model componentization method based on Role},
  Author                   = {Zhang, Jun and Liu, Shu Fen and Yao, Zhi Lin},
  Year                     = {2011},

  Month                    = {feb},
  Number                   = {2},
  Pages                    = {304--308},
  Volume                   = {39},

  Abstract                 = {To solve feature model's chaos and entanglement problems in certain domain and decouple the feature model and requirement model, we design a feature model componentization method. The method introduces the concept of Role, and implements an algorithm based on it called Feature-Role-Component Algorithm which maps domain features generated by requirement elicitation and analysis to different model components. The Role plays the role of intermediary, and decouples the feature and component, which enables convenient selection and composition between feature variants and enhances the componentization level of the system.},
  ISSN                     = {03722112}
}

@InProceedings{Zhang2010,
  Title                    = {Semantic query and reasoning for design meta-intent information},
  Author                   = {Zhang, Yingzhong and Luo, Xiaofang},
  Year                     = {2010},
  Pages                    = {672--676},
  Volume                   = {9},

  Abstract                 = {The design meta-intent information is a kind of information about design rationale for design intent objects such as features, parameters and constraints. Based on the design meta-intent representation model this paper explores an ontology-based semantic query and reasoning method for design meta-intent information. A Jena-based semantic query process framework is presented. The user's queries for features, parameters and constraints about design rationale are converted into design meta-intent semantic queries which can be input by text and interactively selecting from visual feature model. A better query results can be achieved for different query terms by semantic reasoning in the process of design modification, design reuse or collaborative design. {\textcopyright} 2010 IEEE.},
  Doi                      = {10.1109/ICCSIT.2010.5564546},
  ISBN                     = {9781424455386}
}

@InProceedings{Zhang2012a,
  Title                    = {A kernel canonical correlation analysis based idle-state detection method for SSVEP-based brain-computer interfaces},
  Author                   = {Zhang, Zimu and Deng, Zhidong},
  Year                     = {2012},
  Pages                    = {634--640},
  Volume                   = {341-342},

  Abstract                 = {In this paper, we propose a kernel canonical correlation analysis (KCCA) based idle-state detection method for asynchronous steady-state visual evoked potential (SSVEP)-based brain-computer interface (BCI) systems. KCCA method can offer a flexible nonlinear solution to adequately extract nonlinear features of multi-electrode electroencephalogram signals. Based on this method, an ensemble KCCA coefficients feature model is proposed by weighting effectively multi-harmonic information and afterwards a threshold classification strategy for idle-state detection is presented. The weights of the model and optimal threshold are trained by the presented parameters learning scheme. Using our method, offline analysis was performed on 10 subjects with 8 fixed common electrodes. The results showed that the idle state could be detected with 95.9{\%} average accuracy when SSVEP could be determined with 93.8{\%} average accuracy. Further, the analysis verified the effectiveness and significant superiority of our method over other widely used ones. {\textcopyright} (2012) Trans Tech Publications, Switzerland.},
  Doi                      = {10.4028/www.scientific.net/AMR.341-342.634},
  ISBN                     = {9783037852521},
  ISSN                     = {10226680}
}

@Article{Zhao2016,
  Title                    = {Anti-Forensics of Environmental-Signature-Based Audio Splicing Detection and Its Countermeasure via Rich-Features Classification},
  Author                   = {Zhao, Hong and Chen, Yifan and Wang, Rui and Malik, Hafiz},
  Year                     = {2016},

  Month                    = {jul},
  Number                   = {7},
  Pages                    = {1603--1617},
  Volume                   = {11},

  Abstract                 = {Numerous methods for detecting audio splicing have been proposed. Environmental-signature-based methods are considered to be the most effective forgery detection methods. The performance of existing audio forensic analysis methods is generally measured in the absence of any anti-forensic attack. Effectiveness of these methods in the presence of anti-forensic attacks is therefore unknown. In this paper, we propose an effective anti-forensic attack for environmental-signature-based splicing detection method and countermeasures to detect the presence of the anti-forensic attack. For anti-forensic attack, dereverberation-based processing is proposed. Three dereverberation methods are considered to tamper with the acoustic environment signature. Experimental results indicate that the proposed dereverberation-based anti-forensic attack significantly degrades the performance of the selected splicing detection method. The proposed countermeasures exploit artifacts introduced by the anti-forensic processing. To detect the presence of potential anti-forensic processing, a machine learning-based framework is proposed. In particular, the proposed anti-forensic detection method uses a rich-feature model consisting of Fourier coefficients, spectral properties, high-order statistics of musical noise residuals, and modulation spectral coefficients to capture traces of dereverberation attacks. The performance of the proposed framework is evaluated on both synthetic data and real-world speech recordings. The experimental results show that the proposed rich-feature model can detect the presence of anti-forensic processing with an average accuracy of 95{\%}.},
  Doi                      = {10.1109/TIFS.2016.2543205},
  ISSN                     = {15566013},
  Publisher                = {Institute of Electrical and Electronics Engineers Inc.}
}

@Article{Zhao2011,
  Title                    = {Accurate landmarking of three-dimensional facial data in the presence of facial expressions and occlusions using a three-dimensional statistical facial feature model},
  Author                   = {Zhao, Xi and Dellandr{\'{e}}a, Emmanuel and Chen, Liming and Kakadiaris, Ioannis A.},
  Year                     = {2011},

  Month                    = {oct},
  Number                   = {5},
  Pages                    = {1417--1428},
  Volume                   = {41},

  Abstract                 = {Three-dimensional face landmarking aims at automatically localizing facial landmarks and has a wide range of applications (e.g., face recognition, face tracking, and facial expression analysis). Existing methods assume neutral facial expressions and unoccluded faces. In this paper, we propose a general learning-based framework for reliable landmark localization on 3-D facial data under challenging conditions (i.e., facial expressions and occlusions). Our approach relies on a statistical model, called 3-D statistical facial feature model, which learns both the global variations in configurational relationships between landmarks and the local variations of texture and geometry around each landmark. Based on this model, we further propose an occlusion classifier and a fitting algorithm. Results from experiments on three publicly available 3-D face databases (FRGC, BU-3-DFE, and Bosphorus) demonstrate the effectiveness of our approach, in terms of landmarking accuracy and robustness, in the presence of expressions and occlusions. {\textcopyright} 2011 IEEE.},
  Doi                      = {10.1109/TSMCB.2011.2148711},
  ISSN                     = {10834419}
}

@Article{Zhao2013,
  Title                    = {A unified probabilistic framework for automatic 3D facial expression analysis based on a Bayesian belief inference and statistical feature models},
  Author                   = {Zhao, Xi and Dellandr{\'{e}}a, Emmanuel and Zou, Jianhua and Chen, Liming},
  Year                     = {2013},
  Number                   = {3},
  Pages                    = {231--245},
  Volume                   = {31},

  Abstract                 = {Textured 3D face models capture precise facial surfaces along with the associated textures, making it possible for an accurate description of facial activities. In this paper, we present a unified probabilistic framework based on a novel Bayesian Belief Network (BBN) for 3D facial expression and Action Unit (AU) recognition. The proposed BBN performs Bayesian inference based on Statistical Feature Models (SFM) and Gibbs-Boltzmann distribution and feature a hybrid approach in fusing both geometric and appearance features along with morphological ones. When combined with our previously developed morphable partial face model (SFAM), the proposed BBN has the capacity of conducting fully automatic facial expression analysis. We conducted extensive experiments on the two public databases, namely the BU-3DFE dataset and the Bosphorus dataset. When using manually labeled landmarks, the proposed framework achieved an average recognition rate of 94.2{\%} and 85.6{\%} for the 7 and 16 AU on face data from the Bosphorus dataset respectively, and 89.2{\%} for the six universal expressions on the BU-3DFE dataset. Using the landmarks automatically located by SFAM, the proposed BBN still achieved an average recognition rate of 84.9{\%} for the six prototypical facial expressions. These experimental results demonstrate the effectiveness of the proposed approach and its robustness in landmark localization errors. {\textcopyright} 2013 Elsevier Ltd. All rights reserved.},
  Doi                      = {10.1016/j.imavis.2012.10.001},
  ISSN                     = {02628856},
  Publisher                = {Elsevier Ltd}
}

@InProceedings{Zhou2016,
  Title                    = {Feature model augmentation with sentiment analysis for product line planning},
  Author                   = {Zhou, F. and Jiao, R. J.},
  Year                     = {2016},
  Month                    = {jan},
  Pages                    = {1689--1693},
  Publisher                = {IEEE Computer Society},

  Abstract                 = {A feature model is able to identify commonality and variability within a product line, helping stakeholders configure product variants and seize opportunities for reuse. However, no direct customer preference information is incorporated in the feature model when it comes to the question-how many product variants are needed in order to satisfy individual customer needs. This paper proposes to mine customer preference information for individual product features by sentiment analysis of online product reviews. The features commented by the users of a product are used to augment a simple feature model predefined with customer opinionated preference information. In such a way, the customer preference information is considered as one attribute of the features in the model, helping designers make informed decisions when trading off between commonality and variability of a product line. Finally, we present a Kindle Fire tablet case study to demonstrate the proposed method.},
  Doi                      = {10.1109/IEEM.2015.7385935},
  ISBN                     = {9781467380669},
  ISSN                     = {2157362X}
}

@Article{Zhu2015,
  Title                    = {A Two-Stage Geometric Method for Pruning Unreliable Links in Protein-Protein Networks},
  Author                   = {Zhu, Lin and Deng, Su Ping and Huang, De Shuang},
  Year                     = {2015},

  Month                    = {jul},
  Number                   = {5},
  Pages                    = {528--534},
  Volume                   = {14},

  Abstract                 = {Protein-protein interactions (PPIs) play essential roles for determining the outcomes of most of the cellular functions of the cell. Although the experimentally detected high-throughput PPI data promise new opportunities for the study of many biological mechanisms including cellular metabolism and protein functions, experimentally detected PPIs have high levels of false positive rate. Therefore, it is of high practical value to develop novel computational tools for pruning low-confidence PPIs. In this paper, we propose a new geometric approach called Leave-One-Out Logistic Metric Embedding (LOO-LME) for assessing the reliability of interactions. Unlike previous approaches which mainly seek to preserve the noisy topological information of the PPI networks in the embedding space, LOO-LME first transforms the learning task into an equivalent discriminant form, then directly deals with the uncertainty in PPI networks using a leave-one-out-style approach. The experimental results show that LOO-LME substantially outperforms previous methods on PPI assessment problems. LOO-LME could thus facilitate further graph-based studies of PPIs and may help infer their hidden underlying biological knowledge.},
  Doi                      = {10.1109/TNB.2015.2420754},
  ISSN                     = {15361241},
  Publisher                = {Institute of Electrical and Electronics Engineers Inc.}
}

@Article{,
  Title                    = {string},

  Abstract                 = { ( {\{}feature model{\}} AND ( {\{}reasoning{\}} OR {\{}analysis{\}} OR {\{}automated{\}} OR "analyses" ) ) AND ( LIMIT-TO ( PUBYEAR , 2016 ) OR LIMIT-TO ( PUBYEAR , 2015 ) OR LIMIT-TO ( PUBYEAR , 2014 ) OR LIMIT-TO ( PUBYEAR , 2013 ) OR LIMIT-TO ( PUBYEAR , 2012 ) OR LIMIT-TO ( PUBYEAR , 2011 ) OR LIMIT-TO ( PUBYEAR , 2010 ) ) AND ( LIMIT-TO ( SUBJAREA , "COMP" ) OR LIMIT-TO ( SUBJAREA , "ENGI" ) ) }
}

@Article{,
  Title                    = {string},

  Abstract                 = { ( {\{}feature model{\}} AND ( {\{}reasoning{\}} OR {\{}analysis{\}} OR {\{}automated{\}} OR "analyses" ) ) AND ( LIMIT-TO ( PUBYEAR , 2016 ) OR LIMIT-TO ( PUBYEAR , 2015 ) OR LIMIT-TO ( PUBYEAR , 2014 ) OR LIMIT-TO ( PUBYEAR , 2013 ) OR LIMIT-TO ( PUBYEAR , 2012 ) OR LIMIT-TO ( PUBYEAR , 2011 ) OR LIMIT-TO ( PUBYEAR , 2010 ) ) AND ( LIMIT-TO ( SUBJAREA , "COMP" ) OR LIMIT-TO ( SUBJAREA , "ENGI" ) ) }
}

