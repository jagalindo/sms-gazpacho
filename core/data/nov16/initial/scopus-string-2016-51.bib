@article{Ali2016,
abstract = {Due to the high-measuring cost, the monitoring of power quality (PQ) is nontrivial. This paper is aimed at reducing the cost of PQ monitoring in power network. Using a real-world PQ dataset, this paper adopts a learn-from-data approach to obtain a device latent feature model, which captures the device behavior as a PQ transition function. With the latent feature model, the power network could be modeled, in analogy, as a data-driven network, which presents the opportunity to use the well-investigated network monitoring and data estimation algorithms to solve the network quality monitoring problem in power grid. Based on this network model, algorithms are proposed to intelligently place measurement devices on suitable power links to reduce the uncertainty of PQ estimation on unmonitored power links. The meter placement algorithms use entropy-based measurements and Bayesian network models to identify the most suitable power links for PQ meter placement. Evaluation results on various simulated networks including IEEE distribution test feeder system show that the meter placement solution is efficient, and has the potential to significantly reduce the uncertainty of PQ values on unmonitored power links.},
author = {Ali, Sardar and Wu, Kui and Weston, Kyle and Marinakis, Dimitri},
doi = {10.1109/TSG.2015.2442837},
issn = {19493053},
month = {may},
number = {3},
pages = {1552--1561},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{A Machine Learning Approach to Meter Placement for Power Quality Estimation in Smart Grid}},
volume = {7},
year = {2016}
}
@article{Araar2016,
abstract = {For many decades, numerous organizations have launched software reuse initiatives to improve their productivity. Software product lines (SPL) addressed this problem by organizing software development around a set of features that are shared by a set of products. In order to exploit existing software products for building a new SPL, features composing each of the used products must be specified in the first place. In this paper we analyze the effectiveness of overlapping clustering based technique to mine functional features from object-oriented (OO) source code of existing systems. The evaluation of the proposed approach using two different Java open-source applications, i.e. "Mobile media" and "Drawing Shapes", has revealed encouraging results.},
author = {Araar, Imad Eddine and Seridi, Hassina},
issn = {03505596},
month = {jun},
number = {2},
pages = {245--255},
publisher = {Slovene Society Informatika},
title = {{Software features extraction from object-oriented source code using an overlapping clustering approach}},
volume = {40},
year = {2016}
}
@inproceedings{Arcaini2016,
abstract = {Building a feature model for an existing SPL can improve the automatic analysis of the SPL and reduce the effort in maintenance. However, developing a feature model can be error prone, and checking that it correctly identifies each actual product of the SPL may be unfeasible due to the huge number of possible configurations. We apply mutation analysis and propose a method to detect and remove conformance faults by selecting special configurations that distinguish a feature model from its mutants. We propose a technique that, by iterating this process, is able to repair a faulty model. We devise several variations of a simple hill climbing algorithm for automatic fault removal and we compare them by a series of experiments on three different sets of feature models. We find that our technique is able to improve the conformance of around 90{\%} of the models and find the correct model in around 40{\%} of the cases.},
author = {Arcaini, Paolo and Gargantini, Angelo and Vavassori, Paolo},
doi = {10.1109/ICST.2016.10},
isbn = {9781509018260},
month = {jul},
pages = {102--112},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Automatic Detection and Removal of Conformance Faults in Feature Models}},
year = {2016}
}
@inproceedings{Ardagna2016,
abstract = {Unpredictability of cloud computing due to segregation of visibility and control between applications, data owners, and cloud providers increases tenants' uncertainty when using cloud services. Adaptation techniques become fundamental to provide a reliable cloud-based infrastructure with definite behavior, which preserves a stable quality of service for tenants. Existing adaptation techniques mostly focus on performance properties and are based on unverifiable evidence, which is collected in an untrusted way. In this paper, we propose a security-oriented adaptation technique for the cloud, based on evidence collected by means of a reliable certification process. Our approach adapts the cloud to maintain stable security properties over time, by continuously verifying certificate validity. It uses the output of verification activities to index a feature model, where equivalent configurations are used as the basis for adaptation. We also provide an analysis of the approach on British Telecommunications (BT) premises.},
author = {Ardagna, Claudio A. and Asal, Rasool and Damiani, Ernesto and {El Ioini}, Nabil and Pahl, Claus and Dimitrakos, Theo},
doi = {10.1109/SCC.2016.49},
isbn = {9781509026289},
month = {aug},
pages = {324--331},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{A certification technique for cloud security adaptation}},
year = {2016}
}
@inproceedings{Bashari2016,
abstract = {The growing number of online resources, including data and services, has motivated both researchers and practitioners to provide methods and tools for non-expert end-users to create desirable applications by putting these resources together leading to the so called mashups. In this paper, we focus on a class of mashups referred to as service mashups. A service mashup is built from existing services such that the developed service mashup offers added-value through new functionalities. We propose an approach which adopts concepts from software product line engineering and automated AI planning to support the automated composition of service mashups. One of the advantages of our work is that it allows non-experts to build and optimize desired mashups with little knowledge of service composition. We report on the results of the experimentation that we have performed which support the practicality and scalability of our proposed work.},
author = {Bashari, Mahdi and Bagheri, Ebrahim and Du, Weichang},
doi = {10.1007/978-3-319-35122-3_2},
isbn = {9783319351216},
issn = {16113349},
pages = {20--38},
publisher = {Springer Verlag},
title = {{Automated composition of service mashups through software product line engineering}},
volume = {9679},
year = {2016}
}
@article{Becan2016,
abstract = {Feature Models (FMs) are a popular formalism for modeling and reasoning about the configurations of a software product line. As the manual construction of an FM is time-consuming and error-prone, management operations have been developed for reverse engineering, merging, slicing, or refactoring FMs from a set of configurations/dependencies. Yet the synthesis of meaningless ontological relations in the FM – as defined by its feature hierarchy and feature groups – may arise and cause severe difficulties when reading, maintaining or exploiting it. Numerous synthesis techniques and tools have been proposed, but only a few consider both configuration and ontological semantics of an FM. There are also few empirical studies investigating ontological aspects when synthesizing FMs. In this article, we define a generic, ontologic-aware synthesis procedure that computes the likely siblings or parent candidates for a given feature. We develop six heuristics for clustering and weighting the logical, syntactical and semantical relationships between feature names. We then perform an empirical evaluation on hundreds of FMs, coming from the SPLOT repository and Wikipedia. We provide evidence that a fully automated synthesis (i.e., without any user intervention) is likely to produce FMs far from the ground truths. As the role of the user is crucial, we empirically analyze the strengths and weaknesses of heuristics for computing ranking lists and different kinds of clusters. We show that a hybrid approach mixing logical and ontological techniques outperforms state-of-the-art solutions. We believe our approach, environment, and empirical results support researchers and practitioners working on reverse engineering and management of FMs.},
author = {B{\'{e}}can, Guillaume and Acher, Mathieu and Baudry, Benoit and Nasr, Sana Ben},
doi = {10.1007/s10664-014-9357-1},
issn = {15737616},
month = {aug},
number = {4},
pages = {1794--1841},
publisher = {Springer New York LLC},
title = {{Breathing ontological knowledge into feature model synthesis: an empirical study}},
volume = {21},
year = {2016}
}
@inproceedings{Bezerra2016,
abstract = {The feature model is one of the most important artifact of a Software Product Line (SPL). It is built in the early stages of SPL development and describes the main features and relationships. The feature model evolves according to the evolution of the SPL. Thus, it is important to build maintainable feature models. In this scenario, measures have been proven useful in the maintainability evaluation of the feature models. This paper presents an exploratory study on the impact of feature models maintainability over the SPL evolution process. In order to support this analysis, we built a dataset containing a compiled set of 21 maintainability structural measures extracted from 16 feature models and respective versions. Although not conclusive, our findings indicate that the feature models maintainability tends to decrease as it evolves. We also identified the most common changes performed in a feature model during its evolution process.},
author = {Bezerra, Carla I M and Monteiro, Jos{\'{e}} Maria and Andrade, Rossana M C and Rocha, Lincoln S.},
doi = {10.1145/2866614.2866617},
isbn = {9781450340199},
month = {jan},
pages = {17--24},
publisher = {Association for Computing Machinery},
title = {{Analyzing the feature models maintainability over their evolution process: An exploratory study}},
year = {2016}
}
@misc{Bose2016,
abstract = {This paper presents spiking neural networks (SNNs) for remote sensing spatiotemporal analysis of image time series, which make use of the highly parallel and low-power-consuming neuromorphic hardware platforms possible. This paper illustrates this concept with the introduction of the first SNN computational model for crop yield estimation from normalized difference vegetation index image time series. It presents the development and testing of a methodological framework which utilizes the spatial accumulation of time series of Moderate Resolution Imaging Spectroradiometer 250-m resolution data and historical crop yield data to train an SNN to make timely prediction of crop yield. The research work also includes an analysis on the optimum number of features needed to optimize the results from our experimental data set. The proposed approach was applied to estimate the winter wheat (Triticum aestivum L.) yield in Shandong province, one of the main winter-wheat-growing regions of China. Our method was able to predict the yield around six weeks before harvest with a very high accuracy. Our methodology provided an average accuracy of 95.64{\%}, with an average error of prediction of 0.236 t/ha and correlation coefficient of 0.801 based on a nine-feature model.},
author = {Bose, Pritam and Kasabov, Nikola K. and Bruzzone, Lorenzo and Hartono, Reggio N.},
doi = {10.1109/TGRS.2016.2586602},
issn = {01962892},
month = {jul},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Spiking Neural Networks for Crop Yield Estimation Based on Spatiotemporal Analysis of Image Time Series}},
year = {2016}
}
@article{Burdek2016,
abstract = {Features define common and variable parts of the members of a (software) product line. Feature models are used to specify the set of all valid feature combinations. Feature models not only enjoy an intuitive tree-like graphical syntax, but also a precise formal semantics, which can be denoted as propositional formulae over Boolean feature variables. A product line usually constitutes a long-term investment and, therefore, has to undergo continuous evolution to meet ever-changing requirements. First of all, product-line evolution leads to changes of the feature model due to its central role in the product-line paradigm. As a result, product-line engineers are often faced with the problems that (1) feature models are changed in an ad-hoc manner without proper documentation, and (2) the semantic impact of feature diagram changes is unclear. In this article, we propose a comprehensive approach to tackle both challenges. For (1), our approach compares the old and new version of the diagram representation of a feature model and specifies the changes using complex edit operations on feature diagrams. In this way, feature model changes are automatically detected and formally documented. For (2), we propose an approach for reasoning about the semantic impact of diagram changes. We present a set of edit operations on feature diagrams, where complex operations are primarily derived from evolution scenarios observed in a real-world case study, i.e., a product line from the automation engineering domain. We evaluated our approach to demonstrate its applicability with respect to the case study, as well as its scalability concerning experimental data sets.},
author = {B{\"{u}}rdek, Johannes and Kehrer, Timo and Lochau, Malte and Reuling, Dennis and Kelter, Udo and Sch{\"{u}}rr, Andy},
doi = {10.1007/s10515-015-0185-3},
issn = {15737535},
month = {dec},
number = {4},
pages = {687--733},
publisher = {Springer New York LLC},
title = {{Reasoning about product-line evolution using complex feature model differences}},
volume = {23},
year = {2016}
}
@article{Cameron2016,
abstract = {This paper presents a quantitative radiomics feature model for performing prostate cancer detection using multiparametric MRI (mpMRI). It incorporates a novel tumor candidate identification algorithm to efficiently and thoroughly identify the regions of concern and constructs a comprehensive radiomics feature model to detect tumorous regions. In contrast to conventional automated classification schemes, this radiomics-based feature model aims to ground its decisions in a way that can be interpreted and understood by the diagnostician. This is done by grouping features into high-level feature categories which are already used by radiologists to diagnose prostate cancer: Morphology, Asymmetry, Physiology, and Size (MAPS), using biomarkers inspired by the PI-RADS guidelines for performing structured reporting on prostate MRI. Clinical mpMRI data were collected from 13 men with histology-confirmed prostate cancer and labeled by an experienced radiologist. These annotated data were used to train classifiers using the proposed radiomics-driven feature model in order to evaluate the classification performance. The preliminary experimental results indicated that the proposed model outperformed each of its constituent feature groups as well as a comparable conventional mpMRI feature model. A further validation of the proposed algorithm will be conducted using a larger dataset as future work.},
author = {Cameron, Andrew and Khalvati, Farzad and Haider, Masoom A. and Wong, Alexander},
doi = {10.1109/TBME.2015.2485779},
issn = {15582531},
month = {jun},
number = {6},
pages = {1145--1156},
publisher = {IEEE Computer Society},
title = {{MAPS: A Quantitative Radiomics Approach for Prostate Cancer Detection}},
volume = {63},
year = {2016}
}
@inproceedings{Carbonnel2016,
abstract = {Software Product Line Engineering (SPLE) is a software engineering domain in which families of similar softwares (called products) are built reusing common artifacts. This requires to analyze commonalities and variabilities, for example to detect which parts are common to several products and which parts differ from one product to another. Such software characteristics that may be present or not in a product are called features. Several approaches in the literature exist to organize features and product configurations in terms of features. In this paper we review those approaches and show that concept lattices are a relevant structure to organize features and product configurations. We also address scaling issues related to formal context computation in the domain of SPLE.},
author = {Carbonnel, Jessie and Bertet, Karell and Huchard, Marianne and Nebut, Cl{\'{e}}mentine},
issn = {16130073},
pages = {109--122},
publisher = {CEUR-WS},
title = {{FCA for software product lines representation: Mixing configuration and feature relationships in a unique canonical representation}},
volume = {1624},
year = {2016}
}
@article{Christaline2016,
abstract = {Feature based image Steganalysis demands the best feature model for accurate steganalysis. The extracted feature model includes the components in DCT features of JPEG image. Existing research in this field show extraction of different types of image features that show slightly improved classification accuracies. Though few recent methods of image steganalysis involve extracting all possible features of the image, they suffer dimensionality problem. The dataset used in our research include raw images from the BOSS database. The original dimension of the feature set extracted has 8726 features from 2000 images. While a larger feature set is expected to have all important information about the steganographic changes, it affects the classifier accuracy due to redundancy. To overcome the curse of dimensionality, we intend to introduce an unsupervised optimization technique before classification. The individual classifiers implemented are SVM and MLP and the fusion techniques implemented to combine these classifiers are Bayes, Dempster Schafer and Decision Template schemes. The performances of classifiers are analyzed for optimization based on Euclidean distance measure and Mahalanobis distance measure. Comparing individual classifiers, it has been found that SVM classifier outperforms MLP classifier for both Euclidean distance measure and Mahalanobis distance measure. Among the fusion schemes, the accuracy of Bayes fusion scheme proves to be best compared to Decision template and Dempster Schafer schemes. Also, the best possible classification accuracy has been obtained for Euclidean distance based optimization followed by Bayes fusion classifier scheme. The classification accuracies obtained in our research are better compared to existing methods.},
author = {Christaline, J. Anita and Ramesh, R. and Vaishali, D.},
doi = {10.14257/ijmue.2016.11.1.37},
issn = {19750080},
number = {1},
pages = {385--396},
publisher = {Science and Engineering Research Support Society},
title = {{Optimized JPEG steganalysis}},
volume = {11},
year = {2016}
}
@inproceedings{Coban2016,
abstract = {The amount of music in digital form increases due to the improvement of internet and recording technologies. With this increase, the automatic organization of musics has emerged as a problem needs to be solved. For this reason, Music Information Retrieval (MIR) is commonly studied research area in recent years. In this context, with the developed Music Information Systems solution is sought for some problems such as automatic playlist creation, hit song detection, music genre or mood classification etc. In previous works, meta-data information, melodic or textual content (lyrics) of music used for feature extraction. Also, it is seen that song lyrics not commonly used and number of work in this area is not enough for Turkish. In this paper, Turkish lyrics data set created and used for automatic music genre classification. Experimental results have been conducted on support vector machines (SVM) and the effect of feature model on results has been investigated in music genre classification which considered as a classical text classification problem. The features are extracted from three different models which are Structural and Statistical Text Features (SSTF), Bag of Words (BoW) and NGram. The results shows that lyrics can be effective for Turkish music genre classification.},
author = {{\c{C}}oban, {\"{O}}nder and {\"{O}}zyer, G{\"{u}}lşah T{\"{u}}m{\"{u}}kl{\"{u}}},
doi = {10.1109/SIU.2016.7495686},
isbn = {9781509016792},
month = {jun},
pages = {101--104},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{T{\"{u}}rk{\c{c}}e Şarki S{\"{o}}zlerinden M{\"{u}}zik T{\"{u}}r{\"{u}} Siniflandirmasi}},
year = {2016}
}
@inproceedings{Constantino2016,
abstract = {In the last decades, software product lines (SPL) have proven to be an efficient software development technique in industries due its capability to increase quality and productivity and decrease cost and time-tomarket through extensive reuse of software artifacts. To achieve these benefits, tool support is fundamental to guide industries during the SPL development life-cycle. However, many different SPL tools are available nowadays and the adoption of the appropriate tool is a big challenge in industries. In order to support engineers choosing a tool that best fits their needs, this paper presents the results of a controlled empirical study to assess two Eclipse-based tools, namely FeatureIDE and pure::variants. This empirical study involved 84 students who used and evaluated both tools. The main weakness we observe in both tools are the lack adequate mechanisms for managing the variability, such as for product configuration. As a strength, we observe the automated analysis and the feature model editor.},
author = {Constantino, Kattiana and Pereira, Juliana Alves and Padilha, Juliana and Vasconcelos, Priscilla and Figueiredo, Eduardo},
isbn = {9789897581892},
pages = {164--171},
publisher = {SciTePress},
title = {{An empirical study of two software product line tools}},
year = {2016}
}
@inproceedings{Estuar2016,
abstract = {Most, if not all, mining sites in the Philippines are not equipped with expensive or modern monitoring tools to check for quality of soil, water and air elements which are relevant to ensure safety and wellness of miners. This study focused on the development of low cost mobile electronic sensors to monitor quality of water from rivers near mining sites. Low cost electronic sensors connected to a smart phone were developed to capture dissolved oxygen (DO2), pH, Turbidity, Temperature, and Salinity. The data for mercury (Hg) and arsenic (As) were obtained through AAS analyses to form baseline data for the model. Data was collected for over a period of one year, with site visits once every two months. A conditional inference tree (ctree) using recursive binary partitioning was used to generate the prediction model using 70-30 split on the training and test data set. The multi-feature model returns Good, Not Good or Unknown based on the scores of each element. The results showed a possible three feature model with significant results for site, salinity and pH balance.},
author = {Estuar, Maria Regina Justina E and Espiritu, Emilyn Q. and Enriquez, Erwin and Oppus, Carlos and Coronel, Andrei D. and Guico, Maria Leonora and Monje, Jose Claro},
doi = {10.1109/TENCON.2015.7373128},
isbn = {9781479986415},
issn = {21593450},
month = {jan},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Towards building a predictive model for remote river quality monitoring for mining sites}},
year = {2016}
}
@article{Galindo2016,
abstract = {Software product lines are used to develop a set of software products that, while being different, share a common set of features. Feature models are used as a compact representation of all the products (e.g., possible configurations) of the product line. The number of products that a feature model encodes may grow exponentially with the number of features. This increases the cost of testing the products within a product line. Some proposals deal with this problem by reducing the testing space using different techniques. However, a daunting challenge is to explore how the cost and value of test cases can be modeled and optimized in order to have lower-cost testing processes. In this paper, we present TESting vAriAbiLity Intensive Systems (TESALIA), an approach that uses automated analysis of feature models to optimize the testing of variability-intensive systems. We model test value and cost as feature attributes, and then we use a constraint satisfaction solver to prune, prioritize and package product line tests complementing prior work in the software product line testing literature. A prototype implementation of TESALIA is used for validation in an Android example showing the benefits of maximizing the mobile market share (the value function) while meeting a budgetary constraint.},
author = {Galindo, Jos{\'{e}} A. and Turner, Hamilton and Benavides, David and White, Jules},
doi = {10.1007/s11219-014-9258-y},
issn = {15731367},
month = {jun},
number = {2},
pages = {365--405},
publisher = {Springer New York LLC},
title = {{Testing variability-intensive systems using automated analysis: an application to Android}},
volume = {24},
year = {2016}
}
@article{Garcia-Galan2016,
abstract = {With an increasing number of cloud computing offerings in the market, migrating an existing computational infrastructure to the cloud requires comparison of different offers in order to find the most suitable configuration. Cloud providers offer many configuration options, such as location, purchasing mode, redundancy, and extra storage. Often, the information about such options is not well organised. This leads to large and unstructured configuration spaces, and turns the comparison into a tedious, error-prone search problem for the customers. In this work we focus on supporting customer decision making for selecting the most suitable cloud configuration - in terms of infrastructural requirements and cost. We achieve this by means of variability modelling and analysis techniques. Firstly, we structure the configuration space of an IaaS using feature models, usually employed for the modelling of variability-intensive systems, and present the case study of the Amazon EC2. Secondly, we assist the configuration search process. Feature models enable the use of different analysis operations that, among others, automate the search of optimal configurations. Results of our analysis show how our approach, with a negligible analysis time, outperforms commercial approaches in terms of expressiveness and accuracy.},
author = {Garc{\'{i}}a-Gal{\'{a}}n, Jes{\'{u}}s and Trinidad, Pablo and Rana, Omer F. and Ruiz-Cort{\'{e}}s, Antonio},
doi = {10.1016/j.future.2015.03.006},
issn = {0167739X},
month = {feb},
pages = {200--212},
publisher = {Elsevier},
title = {{Automated configuration support for infrastructure migration to the cloud}},
volume = {55},
year = {2016}
}
@inproceedings{Han2016,
abstract = {In the current mobile software environment, the device fragmentation phenomenon causes a serious problem to the mobile software ecosystem stakeholders. Since mobile manufacturers make various differentiated hardware components for product differentiation around strategically selected open platforms, a huge number of devices are produced each year. Since the application developers have to verify manually whether the developed application is compatible with specific devices, a tremendous burden is put on the application developers. To solve this problem, we propose a feature-oriented mobile software development framework and implement as part of it an automated tool for compatibility verification. To evaluate our framework, we conduct a case study with 10 devices and 21 features from the real world. The result of the case study indicates that a significant effort reduction can be achieved by using our framework.},
author = {Han, Younghun and Go, Gyeongmin and Kang, Sungwon and Lee, Heuijin},
doi = {10.1007/978-3-319-38904-2_20},
isbn = {9783319389035},
issn = {18678211},
pages = {189--199},
publisher = {Springer Verlag},
title = {{A feature-oriented mobile software development framework to resolve the device fragmentation phenomenon for application developers in the mobile software ecosystem}},
volume = {167},
year = {2016}
}
@article{Hidaka2016,
abstract = {Bidirectional model transformation is a key technology in model-driven engineering (MDE), when two models that can change over time have to be kept constantly consistent with each other. While several model transformation tools include at least a partial support to bidirectionality, it is not clear how these bidirectional capabilities relate to each other and to similar classical problems in computer science, from the view update problem in databases to bidirectional graph transformations. This paper tries to clarify and visualize the space of design choices for bidirectional transformations from an MDE point of view, in the form of a feature model. The selected list of existing approaches are characterized by mapping them to the feature model. Then, the feature model is used to highlight some unexplored research lines in bidirectional transformations.},
author = {Hidaka, Soichiro and Tisi, Massimo and Cabot, Jordi and Hu, Zhenjiang},
doi = {10.1007/s10270-014-0450-0},
issn = {16191374},
month = {jul},
number = {3},
pages = {907--928},
publisher = {Springer Verlag},
title = {{Feature-based classification of bidirectional transformation approaches}},
volume = {15},
year = {2016}
}
@article{Hu2016,
abstract = {Feature model is an essential concept and artifact in feature oriented software development (FOSD). It depicts commonality and variability (C{\&}V) of products in terms of features. With increasingly frequent software evolution, keeping the feature model in consistent with the evolution is very important. Most of the related researches usually analyze the C{\&}V on the requirement level, and modeling the analyzed C{\&}V by the feature model. However, since the feature changes may cause the ripple effect during the modeling process, some new commonalities and variability may be derived. The current researches are still not able to resolve this problem, which leads to some potential overlooking commonalities and inefficiency in reuse. This paper proposes an approach to extend the feature model and analyze the software evolution based on the feature model. The extensions of feature dependency and evolution meta-operators can support the ripple effect analysis of the feature changes, as well as the exploration of the potential commonalities. The new approach also develops some refactoring strategies and a semi-automated tool to support commonality extraction and feature refactoring. In addition, rules and strategies are designed to resolve typical configuration conflicts. Finally, the paper employs a case study to validate the applicability and effectiveness of the presented method.},
author = {Hu, Jie and Wang, Qing},
doi = {10.13328/j.cnki.jos.004829},
issn = {10009825},
month = {may},
number = {5},
pages = {1212--1229},
publisher = {Chinese Academy of Sciences},
title = {{Extensions and evolution analysis method for software feature models}},
volume = {27},
year = {2016}
}
@misc{Jiang2016,
abstract = {The pervasiveness of location-acquisition and mobile computing techniques has generated massive spatial trajectory data, which has brought great challenges to the management and analysis of such a big data. In this paper, we focus on the sub-trajectory dataset profiling problem, and aim to extract the representative sub-trajectories from the raw trajectory as a subset, called profile, which can best describe the whole dataset. This problem is very challenging subject to finding the most representative sub-trajectories set by trading off the size and quality of the profile. To tackle this problem, we model the features of the trajectory dataset from the aspects of density, speed and the direction flow. Meanwhile we present our two-step method to select the representative trajectories based on the feature model. First, a novel trajectory segmentation algorithm is applied on a raw trajectory to identify the representative segments concerning their feature representativeness and automatically estimate the number of segments and the segment borders. Then, a sub-trajectory profiling method is performed to yield the most representative sub-trajectories in the dataset, based on a local heuristic evolution strategy. We evaluate our method based on extensive experiments by using two real-world trajectory datasets generated by over 12,000 taxicabs in Beijing and Shanghai. The results demonstrate the efficiency and effectiveness of our methods in different applications.},
author = {Jiang, Wei and Zhu, Jie and Xu, Jiajie and Li, Zhixu and Zhao, Pengpeng and Zhao, Lei},
doi = {10.1007/s11280-016-0396-y},
issn = {1386145X},
month = {sep},
pages = {1--18},
publisher = {Springer New York LLC},
title = {{A feature based method for trajectory dataset segmentation and profiling}},
year = {2016}
}
@inproceedings{Kamoun2016,
abstract = {Combining the Service Oriented Architecture (SOA) and Software Product Line (SPL) paradigms is an emerging research area that has gained a considerable interest in recent years. We observe that the approaches proposed in the literature address mostly the variability modeling of Service Providers (SPs) (e.g., developing and composing SPs). However, handling the variability of Service Consumers (SCs) and how to interrelate the variability of SCs and SPs have not been studied. In this paper, our objective is to carry out an in-depth and rigorous study that addresses these issues. We propose a new model-based, top-down, formal and end-to-end SOA approach based on the Multiple SPLs (MSPL) paradigm. The main idea is to develop an MSPL composed of two dependent SPLs for SP and SC in order to generate customized, valid and consistent SPs and SCs. We propose that the variability of each SPL is managed by a Feature Model (FM). In order to ensure the consistency between these two SPLs and in particular between their FMs, we define the automated analysis update operator based on formal propositional logical techniques. We developed a tool that implements all the required steps of our approach and we demonstrate its efficiency in a practical case study.},
author = {Kamoun, Akram and Kacem, Mohamed Hadj and Kacem, Ahmed Hadj},
doi = {10.1109/WETICE.2016.21},
isbn = {9781509016631},
month = {aug},
pages = {56--61},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Multiple software product lines for service oriented architecture}},
year = {2016}
}
@article{Karatas2016,
abstract = {Extended feature models enable the expression of complex cross-tree constraints involving feature attributes. The inclusion of attributes in cross-tree relations not only enriches the constraints, but also engenders an extended type of variability that involves attributes. In this article, we elaborate on the effects of this new variability type on feature models. We start by analyzing the nature of the variability involving attributes and extend the definitions of the configuration and the product to suit the emerging requirements. Next, we propose classifications for the features, configurations, and products to identify and formalize the ramifications that arise due to the new type of variability. Then, we provide a semantic foundation grounded on constraint satisfaction for our proposal. We introduce an ordering relation between configurations and show that the set of all the configurations represented by a feature model forms a semilattice. This is followed by a demonstration of how the feature model analyses will be affected using illustrative examples selected from existing and novel analysis operations. Finally, we summarize our experiences, gained from a commercial research and development project that employs an extended feature model.},
author = {Karataş, Ahmet Serkan and Oğuzt{\"{u}}z{\"{u}}n, Halit},
doi = {10.1007/s00766-014-0216-9},
issn = {1432010X},
month = {jun},
number = {2},
pages = {185--208},
publisher = {Springer-Verlag London Ltd},
title = {{Attribute-based variability in feature models}},
volume = {21},
year = {2016}
}
@inproceedings{Kim2016,
abstract = {This paper presents a formal analysis framework to analyze a family of platform products w.r.t. real-time properties. First, we propose an extension of the widely-used feature model, called Property Feature Model (PFM), that distinguishes features and properties explicitly Second, we present formal behavioral models of components of a realtime scheduling unit such that all real-time scheduling units implied by a PFM are automatically composed to be analyzed against the properties given by the PFM. We apply our approach to the verification of the schedulability of a family of scheduling units using the symbolic and statistical model checkers of Uppaal.},
author = {Kim, Jin Hyun and Legay, Axel and Traonouez, Louis Marie and Acher, Mathieu and Kaist, Sungwon Kang},
doi = {10.1145/2851613.2851977},
isbn = {9781450337397},
month = {apr},
pages = {1562--1565},
publisher = {Association for Computing Machinery},
title = {{A formal modeling and analysis framework for software product line of preemptive real-time systems}},
year = {2016}
}
@article{Li2016,
abstract = {It is an issue to be solved urgently transforming Digital Museum simulation resource description from natural language into visual simulation in order to reuse and share the resource expeditiously. During the analysis for feature model technique and Internet of Data (IOD), a method for feature model visualized configuration of digital museum based on IOD is proposed. The approach achieves the visualized configuration for simulation resource from domain feature model to application feature model which includes definition of feature model, description with feature data semantic tag and association semantic tag, visualized configuration with feature editor, feature association editor and model merge engine. With an application case of Digital Museum, feature model visualized configuration on Web3D is demonstrated. Experimental result shows the effectiveness of the method.},
author = {Li, Caixia and Song, Yuan and Feng, Zhaoyang and Wang, Yi and Li, Zhi},
issn = {1004731X},
month = {jan},
number = {1},
pages = {83--90},
publisher = {Acta Simulata Systematica Sinica},
title = {{Feature model visualized configuration method of Digital Museum}},
volume = {28},
year = {2016}
}
@inproceedings{Li2016a,
abstract = {Grayscale and thermal data can complement to each other to improve tracking performance in some challenging scenarios. In this paper, we propose a real-time online grayscale-thermal tracking method via Laplacian sparse representation in Bayesian filtering framework. Specifically, a generative multimodal feature model is induced by the Laplacian sparse representation, which makes the best use of similarities among local patches to refine their sparse codes, so that different source data can be seamlessly fused for object tracking. In particular, the multimodal feature model encodes both the spatial local information and occlusion handling to improve its robustness. With such feature representation, the confidence of each candidate is computed by the sparse feature similarity with the object template. Given the motion model, object tracking is then carried out in Bayesian filtering framework by maximizing the observation likelihood, i.e., finding the candidate with highest confidence. In addition, to achieve real-time demand in related visual information processing systems, we adopt the reverse representation and the parallel computation to improve tracking efficiency. Extensive experiments on both public and collected grayscale-thermal video sequences demonstrate accuracy and efficiency of the proposed method against other state-of-the-art sparse representation based trackers.},
author = {Li, Chenglong and Hu, Shiyi and Gao, Sihan and Tang, Jin},
doi = {10.1007/978-3-319-27674-8_6},
isbn = {9783319276731},
issn = {16113349},
pages = {54--65},
publisher = {Springer Verlag},
title = {{Real-time grayscale-thermal tracking via laplacian sparse representation}},
volume = {9517},
year = {2016}
}
@misc{Maazoun2016,
abstract = {A software product line (SPL) represents a family of products in a given application domain. Each SPL is constructed to provide for the derivation of new products by covering a wide range of features in its domain. Nevertheless, over time, some domain features may become obsolete with the apparition of new features while others may become refined. Accordingly, the SPL must be maintained to account for the domain evolution. Such evolution requires a means for managing the impact of changes on the SPL models, including the feature model and design. This paper presents an automated method that analyzes feature model evolution, traces their impact on the SPL design, and offers a set of recommendations to ensure the consistency of both models. The proposed method defines a set of new metrics adapted to SPL evolution to identify the effort needed to maintain the SPL models consistently and with a quality as good as the original models. The method and its tool are illustrated through an example of an SPL in the Text Editing domain. In addition, they are experimentally evaluated in terms of both the quality of the maintained SPL models and the precision of the impact change management.},
author = {Ma{\^{a}}zoun, Jihen and Bouassida, Nadia and Ben-Abdallah, Han{\^{e}}ne},
doi = {10.1016/j.jksuci.2016.01.005},
issn = {22131248},
month = {oct},
number = {4},
pages = {364--380},
publisher = {King Saud bin Abdulaziz University},
title = {{Change impact analysis for software product lines}},
volume = {28},
year = {2016}
}
@misc{Peng2016,
abstract = {Energy consumption is a very important issue that has attracted the attention of many cloud providers as it takes a large quotient of the operation cost for cloud data center. To decrease the energy consumption in cloud data center, one possible solution is to process different types of applications with different strategies. To reach this goal, it is important to know the type of application before it be dealt with. In this paper, we present an application type classification method by monitoring the usage of the resources of application. Through analysis, we find that only part of the parameters are much related to different types of applications. Using these parameters we put forward a feature model that can effectively classify the types of different applications. Extensive experiments show that the model put forward can effectively and accurately classify CPU intensive application, I/O intensive application and network intensive application. It can be used as the basis of efficient utilization of the cloud resources.},
author = {Peng, Junjie and Chen, Jinbao and Zhi, Xiaofei and Qiu, Meikang and Xie, Xiaolan},
doi = {10.1007/s11227-016-1663-5},
issn = {15730484},
month = {feb},
pages = {1--20},
publisher = {Springer New York LLC},
title = {{Research on application classification method in cloud computing environment}},
year = {2016}
}
@inproceedings{Peng2016a,
abstract = {Due to the increasing complexity of software products as well as the restriction of the development budget and time, requirements prioritization, i.e., selecting more crucial requirements to be designed and developed firstly, has become increasingly important in the software development lifetime. Considering the fact that a feature in a feature model can be viewed as a set of closely related requirements, feature prioritization will contribute to requirements prioritization to a large extent. Therefore, how to measure the priority of features within a feature model becomes an important issue in requirements analysis. In this paper, a software feature prioritization approach is proposed, which utilizes the dependencies between features to build a feature probability network and measures feature prioritization through the nodes centrality in the network. Experiments conducted on real world feature models show that the proposed approach can accurately prioritize features in feature models.},
author = {Peng, Zhenlian and Wang, Jian and He, Keqing and Li, Hongtao},
doi = {10.1007/978-3-319-35122-3_8},
isbn = {9783319351216},
issn = {16113349},
pages = {106--121},
publisher = {Springer Verlag},
title = {{An approach for prioritizing software features based on node centrality in probability network}},
volume = {9679},
year = {2016}
}
@article{Ries2016,
abstract = {In this work we discuss the problem of automatically determining bounding box annotations for objects in images whereas we only assume weak labeling in the form of global image labels. We therefore are only given a set of positive images all containing at least one instance of a desired object and a negative set of images which represent background. Our goal is then to determine the locations of the object instances within the positive images by bounding boxes. We also describe and analyze a method for automatic bounding box annotation which consists of two major steps. First, we apply a statistical model for determining visual features which are likely to be indicative for the respective object class. Based on these feature models we infer preliminary estimations for bounding boxes. Second, we use a CCCP training algorithm for latent structured SVM in order to improve the initial estimations by using them as initializations for latent variables modeling the optimal bounding box positions. We evaluate our approach on three publicly available datasets.},
author = {Ries, Christian X. and Richter, Fabian and Lienhart, Rainer},
doi = {10.1007/s11042-014-2434-z},
issn = {15737721},
month = {jun},
number = {11},
pages = {6091--6118},
publisher = {Springer New York LLC},
title = {{Towards automatic bounding box annotations from weakly labeled images}},
volume = {75},
year = {2016}
}
@inproceedings{Schroter2016,
abstract = {Today's software systems are often customizable by means of load-time or compile-time configuration options. These options are typically not independent and their dependencies can be specified by means of feature models. As many industrial systems contain thousands of options, the maintenance and utilization of feature models is a challenge for all stakeholders. In the last two decades, numerous approaches have been presented to support stakeholders in analyzing feature models. Such analyses are commonly reduced to satis fiability problems, which suffer from the growing number of options. While first attempts have been made to decompose feature models into smaller parts, they still require to compose all parts for analysis. We propose the concept of a feature-model interface that only consists of a subset of features, typically selected by experts, and hides all other features and dependencies. Based on a formalization of feature-model interfaces, we prove compositionality properties. We evaluate feature-model interfaces using a three-month history of an industrial feature model from the automotive domain with 18,616 features. Our results indicate performance benefits especially under evolution as often only parts of the feature model need to be analyzed again.},
author = {Schr{\"{o}}ter, Reimar and Krieter, Sebastian and Th{\"{u}}m, Thomas and Benduhn, Fabian and Saake, Gunter},
doi = {10.1145/2884781.2884823},
isbn = {9781450339001},
issn = {02705257},
month = {may},
pages = {667--678},
publisher = {IEEE Computer Society},
title = {{Feature-model interfaces: The highway to compositional analyses of highly-configurable systems}},
year = {2016}
}
@inproceedings{Sriman2016,
abstract = {Achieving high accuracy for classifying foreground and background is an interesting challenge in the field of scene image analysis because of the wide range of illumination, complex background, and scale changes. Classifying foreground and background using bag-of-feature model gives a good result. However, the performance of the classifier depends on designed features. Therefore, this paper presents an alternative classification method based on three categories of object-attributes features namely object description, color distribution and gradient strength. Each feature is computed to a classifier model. The robustness of the method has been tested on the ICDAR2015 dataset. The experimental results show that the performance of the proposed method performs competitively against the results of existing methods in term of precision and recall.},
author = {Sriman, Bowornrat and Schomaker, Lambert},
doi = {10.1109/ACPR.2015.7486604},
isbn = {9781479961009},
month = {jun},
pages = {755--759},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Explicit foreground and background modeling in the classification of text blocks in scene images}},
year = {2016}
}
@article{Sun2016,
abstract = {For the multi-split variable refrigerant flow (VRF) system, the key of efficient operation is to achieve the appropriate refrigerant charge amount (RCA). However, it is difficult to achieve because of the complexity of VRF systems. To overcome the difficulty, this paper presents a hybrid RCA fault diagnosis model combined support vector machine (SVM) with wavelet de-noising (WD) and improved max-relevance and min-redundancy (mRMR) algorithm. WD is responsible for improving the quality of collected VRF experimental data. In addition, mRMR is firstly used to rank all the variables in descending order in terms of their importance for identify RCA faults. After top-ranked variable is determined, correlation analysis of features is implemented for further feature selection removing the redundant variables in linkage to the variable at the top. Finally, a subset of seven features are selected to develop the SVM model. Results indicate that fault diagnosis accuracy of the seven-feature SVM model decreases only 2.14{\%} compared with the initial eighteen-feature model. The proposed wavelet de-noising-max-relevance and min-redundancy-support vector machine (WD-mRMR-SVM) model shows good fault diagnosis performance for RCA faults.},
author = {Sun, Kaizheng and Li, Guannan and Chen, Huanxin and Liu, Jiangyan and Li, Jiong and Hu, Wenju},
doi = {10.1016/j.applthermaleng.2016.07.109},
issn = {13594311},
month = {sep},
pages = {989--998},
publisher = {Elsevier Ltd},
title = {{A novel efficient SVM-based fault diagnosis method for multi-split air conditioning system's refrigerant charge fault amount}},
volume = {108},
year = {2016}
}
@article{Tanhaei2016,
abstract = {Context: Feature model is an appropriate and indispensable tool for modeling similarities and differences among products of the Software Product Line (SPL). It not only exposes the validity of the products' configurations in an SPL but also changes in the course of time to support new requirements of the SPL. Modifications made on the feature model in the course of time raise a number of issues. Useless enlargements of the feature model, the existence of dead features, and violated constraints in the feature model are some of the key problems that make its maintenance difficult. Objective: The initial approach to dealing with the above-mentioned problems and improving maintainability of the feature model is refactoring. Refactoring modifies software artifacts in a way that their externally visible behavior does not change. Method: We introduce a method for defining refactoring rules and executing them on the feature model. We use the ATL model transformation language to define the refactoring rules. Moreover, we provide an Alloy model to check the feature model and the safety of the refactorings that are performed on it. Results: In this research, we propose a safe framework for refactoring a feature model. This framework enables users to perform automatic and semi-automatic refactoring on the feature model. Conclusions: Automated tool support for refactoring is a key issue for adopting approaches such as utilizing feature models and integrating them into the software development process of companies. In this work, we define some of the important refactoring rules on the feature model and provide tools that enable users to add new rules using the ATL M2M language. Our framework assesses the correctness of the refactorings using the Alloy language.},
author = {Tanhaei, Mohammad and Habibi, Jafar and Mirian-Hosseinabadi, Seyed Hassan},
doi = {10.1016/j.infsof.2016.08.011},
issn = {09505849},
month = {dec},
pages = {138--157},
publisher = {Elsevier},
title = {{Automating feature model refactoring: A Model transformation approach}},
volume = {80},
year = {2016}
}
@article{Venckauskas2016,
abstract = {This paper introduces the sensor-networked IoT model as a prototype to support the design of Body Area Network (BAN) applications for healthcare. Using the model, we analyze the synergistic effect of the functional requirements (data collection from the human body and transferring it to the top level) and non-functional requirements (trade-offs between energy-security-environmental factors, treated as Quality-of-Service (QoS)). We use feature models to represent the requirements at the earliest stage for the analysis and describe a model-driven methodology to design the possible BAN applications. Firstly, we specify the requirements as the problem domain (PD) variability model for the BAN applications. Next, we introduce the generative technology (meta-programming as the solution domain (SD)) and the mapping procedure to map the PD feature-based variability model onto the SD feature model. Finally, we create an executable meta-specification that represents the BAN functionality to describe the variability of the problem domain though transformations. The meta-specification (along with the meta-language processor) is a software generator for multiple BAN-oriented applications. We validate the methodology with experiments and a case study to generate a family of programs for the BAN sensor controllers. This enables to obtain the adequate measure of QoS efficiently through the interactive adjustment of the meta-parameter values and re-generation process for the concrete BAN application.},
author = {Ven{\v{c}}kauskas, Algimantas and {\v{S}}tuikys, Vytautas and Jusas, Nerijus and Burbaitė, Renata},
doi = {10.3390/s16050670},
issn = {14248220},
month = {may},
number = {5},
publisher = {MDPI AG},
title = {{Model-driven approach for body area network application development}},
volume = {16},
year = {2016}
}
@article{Vijaya2016,
abstract = {Cloud Computing is an evolving technology as it offers significant benefits like pay only for what you use, scale the resources according to the needs and less in-house staff and resources. These benefits have resulted in tremendous increase in the number of applications and services hosted in the cloud which inturn has resulted in increase in the number of cloud providers in the market. Cloud service providers have a lot of heterogeneity in the resources they use. They have their own servers, different cloud infrastructures, API's and methods to access the cloud resources. Despite its benefits; lack of standards among service providers has caused a high level of vendor lock-in when a software developer tries to change its cloud provider. In this paper we give an overview on the ongoing and current trends in the area of cloud service portability and we also propose a new cloud portability platform. Our new platform is based on establishing feature models which offers the desired cloud portability. Our solution DSkyL uses feature models and domain model analysis to support development, customization and deployment of application components across multiple clouds. The main goal of our approach is to reduce the effort and time needed for porting applications across different clouds. This paper aims to give an overview on DSkyL.},
author = {Vijaya, Aparna and Neelanarayanan, V.},
doi = {10.11591/ijece.v6i1.8270},
issn = {20888708},
month = {apr},
number = {2},
pages = {708--716},
publisher = {Institute of Advanced Engineering and Science},
title = {{A model driven framework for portable cloud services}},
volume = {6},
year = {2016}
}
@misc{Wang2016,
abstract = {In the context of product lines, test case selection aims at obtaining a set of relevant test cases for a product from the entire set of test cases available for a product line. While working on a research-based innovation project on automated testing of product lines of Video Conferencing Systems (VCSs) developed by Cisco, we felt the need to devise a cost-effective way of selecting relevant test cases for a product. To fulfill such need, we propose a systematic and automated test selection methodology using: 1) Feature Model for Testing (FM{\_}T) to capture commonalities and variabilities of a product line; 2) Component Family Model for Testing (CFM{\_}T) to model the structure of test case repository; 3) A tool to automatically build restrictions from CFM{\_}T to FM{\_}T and traces from CFM{\_}T to the actual test cases. Using our methodology, a test engineer is only required to select relevant features through FM{\_}T at a higher level of abstraction for a product and the corresponding test cases will be obtained automatically. We evaluate our methodology by applying it to a VCS product line called Saturn with seven commercial products and the results show that our methodology can significantly reduce cost measured as test selection time and at the same time achieves higher effectiveness (feature coverage, feature pairwise coverage and fault detection) as compared with the current manual process. Moreover, we conduct a questionnaire-based study to solicit the views of test engineers who are involved in developing FM{\_}T and CFM{\_}T. The results show that test engineers are positive about adapting our methodology in their current practice. Finally, we present a set of lessons learnt while applying product line engineering at Cisco for test case selection.},
author = {Wang, Shuai and Ali, Shaukat and Gotlieb, Arnaud and Liaaen, Marius},
doi = {10.1007/s10664-014-9345-5},
issn = {15737616},
month = {aug},
number = {4},
pages = {1586--1622},
publisher = {Springer New York LLC},
title = {{A systematic test case selection methodology for product lines: results and insights from an industrial case study}},
volume = {21},
year = {2016}
}
@article{Wang2016a,
abstract = {According to the study on linear track and global distribution of plasma size signals in laser welding, it is found that the global signals distribution feature of plasma has a close relationship with the real laser welding conditions. As aresults anew method based on integral analysis for plasma size signals and its transform feature model has been established in this paper. The results show that this method can not only effectly remove the random noise signals and the negative influence of the signals fluctuation during data treating process, but also has special features on its repeatability and legibility. Therefore, it is a quick and accurate way in testing the change of plasma morphology in laser welding.},
author = {Wang, Xuyou and Sun, Qian and Wang, Wei and Li, Xiaoyu and Huang, Ruisheng},
issn = {0253360X},
month = {mar},
number = {3},
pages = {45--48},
publisher = {Harbin Research Institute of Welding},
title = {{Study on the changing ruler of plasma in laser welding and the quick testing method of blowhole defects - Integral analysis method for signals detection}},
volume = {37},
year = {2016}
}
@article{Wang2016b,
abstract = {With the proliferation of diversified social network services, understanding how the influence is propagated could help us apprehend the network evolution mechanism and the social impact of different kinds of information better. Most previous works have focused on the analysis of the influence propagation on the static network structure and the discovery of the subset of the most influential users. They fail to identify the user susceptibility delivered by user generated content. In this paper, we propose the InfoIBP (Influence propagation on Indian Buffet Process) model, a general framework for the latent influence propagation on social content with dynamic network structure, which based on the Indian buffet process. The influential users could be taken as the latent features in the social network and be found by different sampling algorithms based on numerical approximation. For the dynamic evolutional property of the network, hidden Markov model was adopted to describe the influence propagation in different time steps. A series of experiments for link prediction, preference prediction and running time evaluation are conducted on the DBLP and Digg datasets. The results show that the InfoIBP is more accurate and more efficient for modeling the latent influence propagation and discovering the influential users. It also can describe the dynamic evolutional property more comprehensively and achieve relatively accurate predictions for the future observations.},
author = {Wang, Zhen Jun and Wang, Shu Hui and Zhang, Wei Gang and Huang, Qing Ming},
doi = {10.11897/SP.J.1016.2016.01528},
issn = {02544164},
month = {aug},
number = {8},
pages = {1528--1540},
publisher = {Science Press},
title = {{Social content based latent influence propagation model}},
volume = {39},
year = {2016}
}
@inproceedings{Weckesser2016,
abstract = {Feature models are frequently used for specifying variability of user-configurable software systems, e.g., software product lines. Numerous approaches have been developed for automating feature model validation concerning constraint consistency and absence of anomalies. As a crucial extension to feature models, cardinality annotations and respective constraints allow for multiple, and even potentially unbounded occurrences of feature instances within configurations. This is of particular relevance for user-adjustable application resources as prevalent, e.g., in cloud computing. However, a precise semantic characterization and tool support for automated and scalable validation of cardinality-based feature models is still an open issue. In this paper, we present a comprehensive formalization of cardinality-based feature models with potentially unbounded feature multiplicities. We apply a combination of ILP and SMT solvers to automate consistency checking and anomaly detection, including novel anomalies, e.g., interval gaps.We present evaluation results gained from our tool implementation showing applicability and scalability to larger-scale models.},
author = {Weckesser, Markus and Lochau, Malte and Schnabel, Thomas and Richerzhagen, Bj{\"{o}}rn and Sch{\"{u}}rr, Andy},
doi = {10.1007/978-3-662-49665-7_10},
isbn = {9783662496640},
issn = {16113349},
pages = {158--175},
publisher = {Springer Verlag},
title = {{Mind the gap! automated anomaly detection for potentially unbounded cardinality-based feature models}},
volume = {9633},
year = {2016}
}
@article{Wen2016,
abstract = {A product quality oriented reliability modeling method was developed containing pin/hole (slot) tolerance for process system of sheet metal assembly. Firstly, considering locating pin tolerance, part hole (slot) tolerance and pin wear, a statistics feature model of assembly deviation was presented. Then the locating pin wear model was deduced and analyzed, according to analyze the relationship between assembly qualities and process system reliability, and considering the impacts of locating failures rate and process wear on the process system reliability, the structure reliability model and assembly quality oriented reliability model of process system were built, then the process system reliability modeling method was formed. An automotive body side panel assembly was given as an example, the assembly process system reliability was analyzed based on the modeling method. The results show that locating pin wear, fixture layout and tolerance of pin/hole (slot) are important factors that affect process system reliability of automotive body side assembly. The method provides a new way of process system reliability analyses for product assembly.},
author = {Wen, Zejun and Liu, Jijun and Zhao, Yanming and Hu, Zhongju and Liu, Zhan and Chen, Lifeng},
doi = {10.3969/j.issn.1004-132X. 2016.03.017},
issn = {1004132X},
month = {feb},
number = {3},
pages = {376--382},
publisher = {China Mechanical Engineering Magazine Office},
title = {{Reliability modeling and analysis for process system of sheet metal assembly}},
volume = {27},
year = {2016}
}
@inproceedings{Yang2016,
abstract = {Feature model is an important model of capturing domain requirements. The traditional feature modeling methods extract the characteristics from the extension of the system. However, the methods fail to describe feature in details especially for embedded software product line engineering. As for these deficiencies, this paper combines the characteristics of the embedded products, with the domain ontology as the basis of feature modeling, dividing the feature modeling into building domain ontology and the feature analysis. First, it will identify the concept of mutual recognition through acquiring, describing and denoting the related domain knowledge. Then, it will divide the feature into three parts which are concept, attribute and the relation, and describe the feature from connotation and extension. It defines and normalizes feature in a clear way. Finally, the effectiveness of the proposed modeling method will be verified through a ventilator embedded product line.},
author = {Yang, Guanzhong and Zhang, Yaru},
doi = {10.1109/FSKD.2015.7382185},
isbn = {9781467376822},
month = {jan},
pages = {1607--1612},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{A feature-oriented modeling approach for embedded product line engineering}},
year = {2016}
}
@article{Yang2016a,
abstract = {Volleyball athletes playing filed motion trajectory aggregation model refers to a group of movement patterns of the volleyball athletes on the playing field, which is usually used to predict the abnormal phenomenon that is occurred in the model. It is difficult for the existing research method to conduct mining on the specific aggregation model accurately and highly efficiently. For this purpose, a kind of athlete trajectory aggregation model mining method based on the spatial temporal features is put forward, which first makes the analysis on the trajectory data by the improved method of Spatial clustering algorithm (SPC), so as to obtain the playing field motion trajectory clustering; Then, by the application of the spatial temporal feature model to replace the method of separate storage for the trajectory data, to carry out real-time observation on the spatial and temporal variation features of the playing field motion trajectory clustering. Experimental results show that, the proposed method is significantly superior to other methods in view of the accuracy and efficiency for the mining of volleyball athlete trajectory aggregation model.},
author = {Yang, Jiancang},
doi = {10.21311/001.39.4.26},
issn = {02540770},
number = {4},
pages = {213--220},
publisher = {Revista Tecnica de la Facultad de Ingeniera},
title = {{Motion trajectory aggregation model of playing field based on spatial temporal features}},
volume = {39},
year = {2016}
}
@article{Yu2016,
abstract = {In intelligent service task, it is difficult for indoor mobile robots to obtain semantic information of complex environment. A semantic map based on semantic acquisition structure of environment is constructed by designing cloud semantic database. The robot can not only get the geometric description of environment, but also obtain the semantic map which contains objects relationship based on rich semantic database of complex environment. It solves the low reliability of adding semantic information, the error of updating map and the lack of scalability in the process of constructing the semantic map. It begins by presenting a semantic database construction project. Then semantic sub-databases are obtained by classifying the semantic database based on SVM (support vector machine) algorithm. On the base of semantic sub-databases, the feature model database is formed by extracting key feature points based on network text classification. By combining the semantic sub-database with the semantic classification list, the objects can be identified. Secondly, the implementation of cloud semantic map for the intelligent service task is discussed. Based on the multi-scale image segmentation and the analysis of disparity map, annotation database and belonging database are designed to describe the belonging relationship between objects. Finally, the semantic map is constructed and the classification efficiency of semantic database is analyzed in simulation experiments to verify the validity of the method.},
author = {Yu, Jinshan and Wu, Hao and Tian, Guohui and Xue, Yinghua and Zhao, Guixiang},
doi = {10.13973/j.cnki.robot.2016.0410},
issn = {10020446},
month = {jul},
number = {4},
pages = {410--419},
publisher = {Chinese Academy of Sciences},
title = {{Semantic database design and semantic map construction of robots based on the cloud}},
volume = {38},
year = {2016}
}
@misc{Yu2016a,
abstract = {One of the basic activities in domain-specific software reuse is product derivation, which is deriving individual software products from the reusable software artifacts produced beforehand in the domain. The efficiency of product derivation decides the benefits of software reuse. Among all of the factors affecting the efficiency of product derivation, derivation being carried out manually is a major aspect with negative impacts that reduces the benefits of software reuse as a result. To improve the efficiency of product derivation, some approaches have been proposed to automate the derivation activity. A widely adopted idea in the approaches is automating the derivation activity based on feature models. In the approaches sharing the idea above, the implementation methods differ widely from one to another. To provide better support for feature model-based automated product derivation, this paper proposes a framework for classifying and analyzing these approaches. The paper also points out the problems in the existing researches and the possible solutions to the problems.},
author = {Yu, Wen Jing and Zhao, Hai Yan and Zhang, Wei and Jin, Zhi},
doi = {10.13328/j.cnki.jos.004929},
issn = {10009825},
month = {jan},
number = {1},
pages = {26--44},
publisher = {Chinese Academy of Sciences},
title = {{A survey of the feature model based approaches to automated product derivation}},
volume = {27},
year = {2016}
}
@inproceedings{Zhan2016,
abstract = {Keyword information retrieval is the mainstream way of information retrieval at present. Users need to scan a large amount of text information in the search results to find the information they want, but the process causes inefficiency and poor user experience. In fact, images in the web pages can provide a direct-viewing and fast retrieval way. Through the research on the features of web subject-related image and achieve its automatic identification and extraction, we can display it in the thumbnail together with the page title and summary in the results pages. It can help users filter and browse information in a more convenient way. This paper establishes a web image attribute system from both HTML attributes and external attributes. Then through corresponding automatic extracting algorithms and data analysis it succeeds to gain 16 feature rules of web subject-related image, and finishes building a web subject-related image feature model. The model proves to obtain more than 99{\%} of the extraction rate and filtering rate while applying to the sample data, demonstrating its value in web information retrieval.},
author = {Zhan, Daning and Zou, Yongli},
doi = {10.1109/APSIPA.2015.7415452},
isbn = {9789881476807},
month = {feb},
pages = {1151--1154},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Features of web subject-related image and its retrieval significance}},
year = {2016}
}
@article{Zhao2016,
abstract = {Numerous methods for detecting audio splicing have been proposed. Environmental-signature-based methods are considered to be the most effective forgery detection methods. The performance of existing audio forensic analysis methods is generally measured in the absence of any anti-forensic attack. Effectiveness of these methods in the presence of anti-forensic attacks is therefore unknown. In this paper, we propose an effective anti-forensic attack for environmental-signature-based splicing detection method and countermeasures to detect the presence of the anti-forensic attack. For anti-forensic attack, dereverberation-based processing is proposed. Three dereverberation methods are considered to tamper with the acoustic environment signature. Experimental results indicate that the proposed dereverberation-based anti-forensic attack significantly degrades the performance of the selected splicing detection method. The proposed countermeasures exploit artifacts introduced by the anti-forensic processing. To detect the presence of potential anti-forensic processing, a machine learning-based framework is proposed. In particular, the proposed anti-forensic detection method uses a rich-feature model consisting of Fourier coefficients, spectral properties, high-order statistics of musical noise residuals, and modulation spectral coefficients to capture traces of dereverberation attacks. The performance of the proposed framework is evaluated on both synthetic data and real-world speech recordings. The experimental results show that the proposed rich-feature model can detect the presence of anti-forensic processing with an average accuracy of 95{\%}.},
author = {Zhao, Hong and Chen, Yifan and Wang, Rui and Malik, Hafiz},
doi = {10.1109/TIFS.2016.2543205},
issn = {15566013},
month = {jul},
number = {7},
pages = {1603--1617},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Anti-Forensics of Environmental-Signature-Based Audio Splicing Detection and Its Countermeasure via Rich-Features Classification}},
volume = {11},
year = {2016}
}
@inproceedings{Zhou2016,
abstract = {A feature model is able to identify commonality and variability within a product line, helping stakeholders configure product variants and seize opportunities for reuse. However, no direct customer preference information is incorporated in the feature model when it comes to the question-how many product variants are needed in order to satisfy individual customer needs. This paper proposes to mine customer preference information for individual product features by sentiment analysis of online product reviews. The features commented by the users of a product are used to augment a simple feature model predefined with customer opinionated preference information. In such a way, the customer preference information is considered as one attribute of the features in the model, helping designers make informed decisions when trading off between commonality and variability of a product line. Finally, we present a Kindle Fire tablet case study to demonstrate the proposed method.},
author = {Zhou, F. and Jiao, R. J.},
doi = {10.1109/IEEM.2015.7385935},
isbn = {9781467380669},
issn = {2157362X},
month = {jan},
pages = {1689--1693},
publisher = {IEEE Computer Society},
title = {{Feature model augmentation with sentiment analysis for product line planning}},
year = {2016}
}
@misc{,
isbn = {9789811007668},
issn = {21945357},
pages = {1--670},
publisher = {Springer Verlag},
title = {{International Congress on Information and Communication Technology, ICICT 2015}},
volume = {438},
year = {2016}
}
@misc{,
issn = {22128271},
publisher = {Elsevier},
title = {{Procedia CIRP}},
volume = {43},
year = {2016}
}
