@Article{Celik20132520,
	Title = {{S-IDE: A tool framework for optimizing deployment architecture of High Level Architecture based simulation systems}},
	Author = {{\c{C}}elik, Turgay and Tekinerdogan, Bedir},
	Journal = {Journal of Systems and Software},
	Year = {2013},
	Number = {10},
	Pages = {2520--2541},
	Volume = {86},
	Abstract = {Abstract One of the important problems in High Level Architecture (HLA) based distributed simulation systems is the allocation of the different simulation modules to the available physical resources. Usually, the deployment of the simulation modules to the physical resources can be done in many different ways, and each deployment alternative will have a different impact on the performance. Although different algorithmic solutions have been provided to optimize the allocation with respect to the performance, the problem has not been explicitly tackled from an architecture design perspective. Moreover, for optimizing the deployment of the simulation system, tool support is largely missing. In this paper we propose a method for automatically deriving deployment alternatives for {\{}HLA{\}} based distributed simulation systems. The method extends the {\{}IEEE{\}} Recommended Practice for High Level Architecture Federation Development and Execution Process by providing an approach for optimizing the allocation at the design level. The method is realized by the tool framework, S-IDE (Simulation-IDE) that we have developed to provide an integrated development environment for deriving a feasible deployment alternative based on the simulation system and the available physical resources at the design phase. The method and the tool support have been validated using a case study for the development of a traffic simulation system. },
	Doi = {https://doi.org/10.1016/j.jss.2013.03.013},
	ISSN = {0164-1212},
	Keywords = {Deployment model optimization,Distributed simulation,FEDEP,High Level Architecture (HLA),Metamodel based tool development,Metamodeling,Model transformations,Software architecture},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121213000599}
}

@Article{Stuikys201648,
	Title = {{Model-driven processes and tools to design robot-based generative learning objects for computer science education}},
	Author = {{\v{S}}tuikys, Vytautas and Burbaitė, Renata and Bespalova, Kristina and Ziberkas, Giedrius},
	Journal = {Science of Computer Programming},
	Year = {2016},
	Pages = {48--71},
	Volume = {129},
	Abstract = {Abstract In this paper, we introduce a methodology to design robot-oriented generative learning objects (GLOs) that are, in fact, heterogeneous meta-programs to teach computer science (CS) topics such as programming. The methodology includes {\{}CS{\}} learning variability modelling using the feature-based approaches borrowed from the {\{}SW{\}} engineering domain. Firstly, we define the {\{}CS{\}} learning domain using the known educational framework {\{}TPACK{\}} (Technology, Pedagogy And Content Knowledge). By learning variability we mean the attributes of the framework extracted and represented as feature models with multiple values. Therefore, the {\{}CS{\}} learning variability represents the problem domain. Meta-programming is considered as a solution domain. Both are represented by feature models. The {\{}GLO{\}} design task is formulated as mapping the problem domain model on the solution domain model. Next, we present the design framework to design {\{}GLOs{\}} manually or semi-automatically. The multi-level separation of concepts, model representation and transformation forms the conceptual background. Its theoretical background includes: (a) a formal definition of feature-based models; (b) a graph-based and set-based definition of meta-programming concepts; (c) transformation rules to support the model mapping; (d) a computational Abstract State Machine model to define the processes and design tool for developing GLOs. We present the architecture and some characteristics of the tool. The tool enables to improve the {\{}GLO{\}} design process significantly (in terms of time and quality) and to achieve a higher quality and functionality of {\{}GLOs{\}} themselves (in terms of the parameter space enlargement for reuse and adaptation). We demonstrate the appropriateness of the methodology in the real teaching setting. In this paper, we present the case study that analyses three robot-oriented {\{}GLOs{\}} as the higher-level specifications. Then, using the meta-language processor, we are able to produce, from the specifications, the concrete robot control programs on demand automatically and to demonstrate teaching algorithms visually by robot's actions. We evaluate the approach from technological and pedagogical perspectives using the known structural metrics. Also, we indicate the merits and demerits of the approach. The main contribution and originality of the paper is the seamless integration of two known technologies (feature modelling and meta-programming) in designing robot-oriented {\{}GLOs{\}} and their supporting tools. },
	Annote = {Special issue on eLearning Software Architectures},
	Doi = {https://doi.org/10.1016/j.scico.2016.03.009},
	ISSN = {0167-6423},
	Keywords = {Educational robots,Feature models,GLO design tool,Generative learning objects (GLOs),Model transformation},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642316300247}
}

@Article{vanderAalst2005129,
	Title = {{Case handling: a new paradigm for business process support}},
	Author = {van der Aalst, Wil M P and Weske, Mathias and Gr{\"{u}}nbauer, Dolf},
	Journal = {Data {\&} Knowledge Engineering},
	Year = {2005},
	Number = {2},
	Pages = {129--162},
	Volume = {53},
	Abstract = {Case handling is a new paradigm for supporting flexible and knowledge intensive business processes. It is strongly based on data as the typical product of these processes. Unlike workflow management, which uses predefined process control structures to determine what should be done during a workflow process, case handling focuses on what can be done to achieve a business goal. In case handling, the knowledge worker in charge of a particular case actively decides on how the goal of that case is reached, and the role of a case handling system is assisting rather than guiding her in doing so. In this paper, case handling is introduced as a new paradigm for supporting flexible business processes. It is motivated by comparing it to workflow management as the traditional way to support business processes. The main entities of case handling systems are identified and classified in a meta model. Finally, the basic functionality and usage of a case handling system is illustrated by an example. },
	Doi = {https://doi.org/10.1016/j.datak.2004.07.003},
	ISSN = {0169-023X},
	Keywords = {Adaptive workflow,Business process management,Case handling,Flexibility,Workflow management systems},
	Url = {http://www.sciencedirect.com/science/article/pii/S0169023X04001296}
}

@Article{Abate2013459,
	Title = {{A modular package manager architecture}},
	Author = {Abate, Pietro and Cosmo, Roberto Di and Treinen, Ralf and Zacchiroli, Stefano},
	Journal = {Information and Software Technology},
	Year = {2013},
	Number = {2},
	Pages = {459--474},
	Volume = {55},
	Abstract = {Context The success of modern software distributions in the Free and Open Source world can be explained, among other factors, by the availability of a large collection of software packages and the possibility to easily install and remove those components using state-of-the-art package managers. However, package managers are often built using a monolithic architecture and hard-wired and ad-hoc dependency solvers implementing some customized heuristics. Objective We aim at laying the foundation for improving on existing package managers. Package managers should be complete, that is find a solution whenever there exists one, and allow the user to specify complex criteria that define how to pick the best solution according to the user's preferences. Method In this paper we propose a modular architecture relying on precise interface formalisms that allows the system administrator to choose from a variety of dependency solvers and backends. Results We have built a working prototype–called MPM–following the design advocated in this paper, and we show how it largely outperforms a variety of current package managers. Conclusion We argue that a modular architecture, allowing for delegating the task of constraint solving to external solvers, is the path that leads to the next generation of package managers that will deliver better results, offer more expressive preference languages, and be easily adaptable to new platforms. },
	Annote = {Special Section: Component-Based Software Engineering (CBSE), 2011},
	Doi = {https://doi.org/10.1016/j.infsof.2012.09.002},
	ISSN = {0950-5849},
	Keywords = {Open source,Package manager,Software components,Software dependencies,Software repositories},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584912001851}
}

@Article{Abate20122228,
	Title = {{Dependency solving: A separate concern in component evolution management}},
	Author = {Abate, Pietro and Cosmo, Roberto Di and Treinen, Ralf and Zacchiroli, Stefano},
	Journal = {Journal of Systems and Software},
	Year = {2012},
	Number = {10},
	Pages = {2228--2240},
	Volume = {85},
	Abstract = {Maintenance of component-based software platforms often has to face rapid evolution of software components. Component dependencies, conflicts, and package managers with dependency solving capabilities are the key ingredients of prevalent software maintenance technologies that have been proposed to keep software installations synchronized with evolving component repositories. We review state-of-the-art package managers and their ability to keep up with evolution at the current growth rate of popular component-based platforms, and conclude that their dependency solving abilities are not up to the task. We show that the complexity of the underlying upgrade planning problem is NP-complete even for seemingly simple component models, and argue that the principal source of complexity lies in multiple available versions of components. We then discuss the need of expressive languages for user preferences, which makes the problem even more challenging. We propose to establish dependency solving as a separate concern from other upgrade aspects, and present {\{}CUDF{\}} as a formalism to describe upgrade scenarios. By analyzing the result of an international dependency solving competition, we provide evidence that the proposed approach is viable. },
	Annote = {Automated Software Evolution},
	Doi = {https://doi.org/10.1016/j.jss.2012.02.018},
	ISSN = {0164-1212},
	Keywords = {Competition,Component,Dependency solving,Open source,Package management,Software evolution},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121212000477}
}

@InProceedings{Abotsi:2011:SPL:2019136.2019171,
	Title = {{A Software Product Line-based Self-healing Strategy for Web-based Applications}},
	Author = {Abotsi, Komi S and Kurniadi, S Tonny and Alsawalqah, Hamad I and Lee, Danhyung},
	Booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
	Year = {2011},
	Address = {New York, NY, USA},
	Pages = {31:1----31:8},
	Publisher = {ACM},
	Series = {SPLC '11},
	Doi = {10.1145/2019136.2019171},
	ISBN = {978-1-4503-0789-5},
	Keywords = {context-based adaptation,knowledge modeling,product line engineering,self-healing},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2019136.2019171}
}

@Article{Abramov20121029,
	Title = {{Evaluation of the Pattern-based method for Secure Development (PbSD): A controlled experiment}},
	Author = {Abramov, Jenny and Sturm, Arnon and Shoval, Peretz},
	Journal = {Information and Software Technology},
	Year = {2012},
	Number = {9},
	Pages = {1029--1043},
	Volume = {54},
	Abstract = {Context Security in general, and database protection from unauthorized access in particular, are crucial for organizations. Although it has been long accepted that the important system requirements should be considered from the early stages of the development process, non-functional requirements such as security tend to get neglected or dealt with only at later stages of the development process. Objective We present an empirical study conducted to evaluate a Pattern-based method for Secure Development – PbSD – that aims to help developers, in particular database designers, to design database schemata that comply with the organizational security policies regarding authorization, from the early stages of development. The method provides a complete framework to guide, enforce and verify the correct implementation of security policies within a system design, and eventually generate a database schema from that design. Method The PbSD method was evaluated in comparison with a popular existing method that directly specifies the security requirements in {\{}SQL{\}} and Oracle's VPD. The two methods were compared with respect to the quality of the created access control specifications, the time it takes to complete the specification, and the perceived quality of the methods. Results We found that the quality of the access control specifications using the PbSD method for secure development were better with respect to privileges granted in the table, column and row granularity levels. Moreover, subjects who used the PbSD method completed the specification task in less time compared to subjects who used SQL. Finally, the subjects perceived the PbSD method clearer and more easy to use. Conclusion The pattern-based method for secure development can enhance the quality of security specification of databases, and decrease the software development time and cost. The results of the experiment may also indicate that the use of patterns in general has similar benefits; yet this requires further examinations. },
	Doi = {https://doi.org/10.1016/j.infsof.2012.04.001},
	ISSN = {0950-5849},
	Keywords = {Authorization,Controlled experiment,Database design,Model driven development,Secure software development,Security patterns},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584912000729}
}

@Article{Accioly2014615,
	Title = {{Controlled experiments comparing black-box testing strategies for software product lines}},
	Author = {Accioly, P and Borba, P and Bonif{\'{a}}cio, R},
	Journal = {Journal of Universal Computer Science},
	Year = {2014},
	Number = {5},
	Pages = {615--639},
	Volume = {20},
	Abstract = {SPL testing has been considered a challenging task, mainly due to the diversity of products that might be generated from an SPL. To deal with this problem, techniques for specifying and deriving product specific functional test cases have been proposed. However, there is little empirical evidence of the benefits and drawbacks of these techniques. To provide this kind of evidence, in a previous work we conducted an empirical study that compared two design techniques for black-box manual testing, a generic technique that we have observed in an industrial test execution environment, and a product specific technique whose functional test cases could be derived using any SPL approach that considers variations in functional tests. Besides revisiting the first study, here we present a second study that reinforce our findings and brings new insights to our investigation. Both studies indicate that specific test cases improve test execution productivity and quality. {\textcopyright} J.UCS.},
	Annote = {cited By 1},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904764309{\&}partnerID=40{\&}md5=112aba25b5f4f038fd0f7ce17b051afe}
}

@Article{Aceto20132468,
	Title = {{On the specification of modal systems: A comparison of three frameworks}},
	Author = {Aceto, Luca and F{\'{a}}bregas, Ignacio and de Frutos-Escrig, David and Ing{\'{o}}lfsd{\'{o}}ttir, Anna and Palomino, Miguel},
	Journal = {Science of Computer Programming},
	Year = {2013},
	Number = {12},
	Pages = {2468--2487},
	Volume = {78},
	Abstract = {Abstract This paper studies the relationships between three notions of behavioural preorder that have been proposed in the literature: refinement over modal transition systems, and the covariant–contravariant simulation and the partial bisimulation preorders over labelled transition systems. It is shown that there are mutual translations between modal transition systems and labelled transition systems that preserve, and reflect, refinement and the covariant–contravariant simulation preorder. The translations are also shown to preserve the modal properties that can be expressed in the logics that characterize those preorders. A translation from labelled transition systems modulo the partial bisimulation preorder into the same model modulo the covariant–contravariant simulation preorder is also offered, together with some evidence that the former model is less expressive than the latter. In order to gain more insight into the relationships between modal transition systems modulo refinement and labelled transition systems modulo the covariant–contravariant simulation preorder, their connections are also phrased and studied in the context of institutions. },
	Annote = {Special Section on International Software Product Line Conference 2010 and Fundamentals of Software Engineering (selected papers of {\{}FSEN{\}} 2011)},
	Doi = {https://doi.org/10.1016/j.scico.2013.02.004},
	ISSN = {0167-6423},
	Keywords = {Covariant–contravariant simulation,Modal refinement,Modal transition systems,Partial bisimulation,Simulation semantics},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642313000294}
}

@Article{Acher2013657,
	Title = {{FAMILIAR: A domain-specific language for large scale management of feature models}},
	Author = {Acher, Mathieu and Collet, Philippe and Lahire, Philippe and France, Robert B},
	Journal = {Science of Computer Programming},
	Year = {2013},
	Number = {6},
	Pages = {657--681},
	Volume = {78},
	Abstract = {The feature model formalism has become the de facto standard for managing variability in software product lines (SPLs). In practice, developing an {\{}SPL{\}} can involve modeling a large number of features representing different viewpoints, sub-systems or concerns of the software system. This activity is generally tedious and error-prone. In this article, we present {\{}FAMILIAR{\}} a Domain-Specific Language (DSL) that is dedicated to the large scale management of feature models and that complements existing tool support. The language provides a powerful support for separating concerns in feature modeling, through the provision of composition and decomposition operators, reasoning facilities and scripting capabilities with modularization mechanisms. We illustrate how an {\{}SPL{\}} consisting of medical imaging services can be practically managed using reusable {\{}FAMILIAR{\}} scripts that implement reasoning mechanisms. We also report on various usages and applications of {\{}FAMILIAR{\}} and its operators, to demonstrate their applicability to different domains and use for different purposes. },
	Annote = {Special section: The Programming Languages track at the 26th {\{}ACM{\}} Symposium on Applied Computing (SAC 2011) {\&} Special section on Agent-oriented Design Methods and Programming Techniques for Distributed Computing in Dynamic and Complex Environments},
	Doi = {https://doi.org/10.1016/j.scico.2012.12.004},
	ISSN = {0167-6423},
	Keywords = {Domain-specific language,Feature model,Model management,Software product lines,Variability},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642312002158}
}

@Article{Acher2012629,
	Title = {{Feature model differences}},
	Author = {Acher, M and Heymans, P and Collet, P and Quinton, C and Lahire, P and Merle, P},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2012},
	Pages = {629--645},
	Volume = {7328 LNCS},
	Abstract = {Feature models are a widespread means to represent commonality and variability in software product lines. As is the case for other kinds of models, computing and managing feature model differences is useful in various real-world situations. In this paper, we propose a set of novel differencing techniques that combine syntactic and semantic mechanisms, and automatically produce meaningful differences. Practitioners can exploit our results in various ways: to understand, manipulate, visualize and reason about differences. They can also combine them with existing feature model composition and decomposition operators. The proposed automations rely on satisfiability algorithms. They come with a dedicated language and a comprehensive environment. We illustrate and evaluate the practical usage of our techniques through a case study dealing with a configurable component framework. {\textcopyright} 2012 Springer-Verlag Berlin Heidelberg.},
	Annote = {cited By 12},
	Doi = {10.1007/978-3-642-31095-9_41},
	Keywords = {Commonality and variability; Component framework;,Information systems; Semantics; Systems engineeri,Software design},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867776734{\&}doi=10.1007{\%}2F978-3-642-31095-9{\_}41{\&}partnerID=40{\&}md5=80cb45d216ca14a21f459ac1bbecafdb}
}

@Conference{Acher2014,
	Title = {{A survey on teaching of software product lines}},
	Author = {Acher, M and Lopez-Herrejon, R E and Rabiser, R},
	Booktitle = {ACM International Conference Proceeding Series},
	Year = {2014},
	Abstract = {With around two decades of existence, the community of Software Product Line (SPL) researchers and practitioners is thriving as can be attested by the extensive research output and the numerous successful industrial projects. Education has a key role to support the next generation of engineers to build highly complex SPLs. Yet, it is unclear how SPLs are taught, what are the possible missing gaps and difficulties faced, what are the benefits, or what is the material available. In this paper, we carry out a survey with over 30 respondents with the purpose of capturing a snapshot of the state of teaching in our community. We report and discuss quantitative as well as qualitative results of the survey. We build upon them and sketch six concrete actions to continue improving the state of practice of SPL teaching. {\textcopyright} 2014 ACM.},
	Annote = {cited By 0},
	Doi = {10.1145/2556624.2556629},
	Keywords = {Computer software; Industrial research; Software,Industrial projects; Research outputs; Software Pr,Surveys},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897625511{\&}doi=10.1145{\%}2F2556624.2556629{\&}partnerID=40{\&}md5=9aea59c569076ecfd2a2b8b74be3b0cc}
}

@Article{SMR:SMR558,
	Title = {{From business processes to software services and vice versa—an improved transition through service-oriented requirements engineering}},
	Author = {Adam, Sebastian and Riegel, Norman and Doerr, Joerg and Uenalan, Oezguer and Kerkow, Daniel},
	Journal = {Journal of Software: Evolution and Process},
	Year = {2012},
	Number = {3},
	Pages = {237--258},
	Volume = {24},
	Abstract = {Business processes need to be agile and flexible to help organizations stay competitive. For this purpose, SOA promises the reuse of already existing information system functionality for enabling enterprises to change the business processes more quickly. However, many promises made by SOA authors have not found their way into practice, because methodological guidance that constructively assures a more successful SOA application is still missing. In this article, service-oriented requirements engineering is therefore introduced as a discipline aiming at a better and more systematic handling and alignment of SOA and Business Process Management. Our industrial experience in this regard is also described. Copyright {\textcopyright} 2011 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/smr.558},
	ISSN = {2047-7481},
	Keywords = {processes,service-oriented,transition},
	Publisher = {John Wiley {\&} Sons, Ltd},
	Url = {http://dx.doi.org/10.1002/smr.558}
}

@Article{Adam2013362,
	Title = {{Effective requirements elicitation in product line application engineering - An experiment}},
	Author = {Adam, S and Schmid, K},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2013},
	Pages = {362--378},
	Volume = {7830 LNCS},
	Abstract = {[Context {\&} Motivation] Developing new software systems based on a software product line (SPL) is still a time-consuming task and the benefits of using such an approach are often smaller than expected. One important reason for this are difficulties in systematically mapping customer requirements to characteristics of the SPL. [Question/problem] Even though it has been recognized that the success of reuse strongly depends on how requirements are treated, it remains unclear how to perform this in an optimal way. [Principal ideas/results] In this paper, we present a controlled experiment performed with 26 students that compared two requirements elicitation approaches when instantiating a given SPL. [Contribution] Our findings indicate that a novel, problem-oriented requirements approach that explicitly integrates the reuse of SPL requirements into the elicitation of customer-specific requirements is more effective than a traditional SPL requirements approach, which distinguishes requirements reuse and additional elicitation customer-specific requirements. {\textcopyright} 2013 Springer-Verlag.},
	Annote = {cited By 0},
	Doi = {10.1007/978-3-642-37422-7_26},
	Keywords = {Application engineering; Controlled experiment; Cu,Computer software selection and evaluation; Exper,Requirements engineering},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875865790{\&}doi=10.1007{\%}2F978-3-642-37422-7{\_}26{\&}partnerID=40{\&}md5=a8e9142d33bf17142e7cc1a548e61cae}
}

@Article{Afzal201630,
	Title = {{Intelligent software product line configurations: A literature review}},
	Author = {Afzal, Uzma and Mahmood, Tariq and Shaikh, Zubair},
	Journal = {Computer Standards {\&} Interfaces},
	Year = {2016},
	Pages = {30--48},
	Volume = {48},
	Abstract = {Abstract A software product line (SPL) is a set of industrial software-intensive systems for configuring similar software products in which personalized feature sets are configured by different business teams. The integration of these feature sets can generate inconsistencies that are typically resolved through manual deliberation. This is a time-consuming process and leads to a potential loss of business resources. Artificial intelligence (AI) techniques can provide the best solution to address this issue autonomously through more efficient configurations, lesser inconsistencies and optimized resources. This paper presents the first literature review of both research and industrial {\{}AI{\}} applications to {\{}SPL{\}} configuration issues. Our results reveal only 19 relevant research works which employ traditional {\{}AI{\}} techniques on small feature sets with no real-life testing or application in industry. We categorize these works in a typology by identifying 8 perspectives of SPL. We also show that only 2 standard industrial {\{}SPL{\}} tools employ {\{}AI{\}} in a limited way to resolve inconsistencies. To inject more interest and application in this domain, we motivate and present future research directions. Particularly, using real-world {\{}SPL{\}} data, we demonstrate how predictive analytics (a state of the art {\{}AI{\}} technique) can separately model inconsistent and consistent patterns, and then predict inconsistencies in advance to help {\{}SPL{\}} designers during the configuration of a product. },
	Annote = {Special Issue on Information System in Distributed Environment},
	Doi = {https://doi.org/10.1016/j.csi.2016.03.003},
	ISSN = {0920-5489},
	Keywords = {Artificial intelligence,Automated feature selection,Inconsistencies,Industrial {\{}SPL{\}} tools,Literature review,Predictive analytics,Software product line},
	Url = {http://www.sciencedirect.com/science/article/pii/S0920548916300198}
}

@Article{SMR:SMR1643,
	Title = {{Classification and comparison of architecture evolution reuse knowledge—a systematic review}},
	Author = {Ahmad, Aakash and Jamshidi, Pooyan and Pahl, Claus},
	Journal = {Journal of Software: Evolution and Process},
	Year = {2014},
	Number = {7},
	Pages = {654--691},
	Volume = {26},
	Abstract = {Context Architecture-centric software evolution (ACSE) enables changes in system's structure and behaviour while maintaining a global view of the software to address evolution-centric trade-offs. The existing research and practices for ACSE primarily focus on design-time evolution and runtime adaptations to accommodate changing requirements in existing architectures. Objectives We aim to identify, taxonomically classify and systematically compare the existing research focused on enabling or enhancing change reuse to support ACSE. Method We conducted a systematic literature review of 32 qualitatively selected studies and taxonomically classified these studies based on solutions that enable (i) empirical acquisition and (ii) systematic application of architecture evolution reuse knowledge (AERK) to guide ACSE. Results We identified six distinct research themes that support acquisition and application of AERK. We investigated (i) how evolution reuse knowledge is defined, classified and represented in the existing research to support ACSE and (ii) what are the existing methods, techniques and solutions to support empirical acquisition and systematic application of AERK. Conclusions Change patterns (34{\%} of selected studies) represent a predominant solution, followed by evolution styles (25{\%}) and adaptation strategies and policies (22{\%}) to enable application of reuse knowledge. Empirical methods for acquisition of reuse knowledge represent 19{\%} including pattern discovery, configuration analysis, evolution and maintenance prediction techniques (approximately 6{\%} each). A lack of focus on empirical acquisition of reuse knowledge suggests the need of solutions with architecture change mining as a complementary and integrated phase for architecture change execution. Copyright {\textcopyright} 2014 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/smr.1643},
	ISSN = {2047-7481},
	Keywords = {architecture evolution reuse knowledge,architecture-centric software evolution,evidence-based study in software evolution,research synthesis,software architecture,systematic literature review},
	Url = {http://dx.doi.org/10.1002/smr.1643}
}

@Article{Ahmed201720,
	Title = {{Handling constraints in combinatorial interaction testing in the presence of multi objective particle swarm and multithreading}},
	Author = {Ahmed, Bestoun S and Gambardella, Luca M and Afzal, Wasif and Zamli, Kamal Z},
	Journal = {Information and Software Technology},
	Year = {2017},
	Pages = {20--36},
	Volume = {86},
	Abstract = {AbstractContext Combinatorial testing strategies have lately received a lot of attention as a result of their diverse applications. In its simple form, a combinatorial strategy can reduce several input parameters (configurations) of a system into a small set based on their interaction (or combination). In practice, the input configurations of software systems are subjected to constraints, especially in case of highly configurable systems. To implement this feature within a strategy, many difficulties arise for construction. While there are many combinatorial interaction testing strategies nowadays, few of them support constraints. Objective This paper presents a new strategy, to construct combinatorial interaction test suites in the presence of constraints. Method The design and algorithms are provided in detail. To overcome the multi-judgement criteria for an optimal solution, the multi-objective particle swarm optimisation and multithreading are used. The strategy and its associated algorithms are evaluated extensively using different benchmarks and comparisons. Results Our results are promising as the evaluation results showed the efficiency and performance of each algorithm in the strategy. The benchmarking results also showed that the strategy can generate constrained test suites efficiently as compared to state-of-the-art strategies. Conclusion The proposed strategy can form a new way for constructing of constrained combinatorial interaction test suites. The strategy can form a new and effective base for future implementations. },
	Doi = {https://doi.org/10.1016/j.infsof.2017.02.004},
	ISSN = {0950-5849},
	Keywords = {Constrained combinatorial interaction,Multi-objective particle swarm optimisation,Search-based software engineering,Test case design techniques,Test generation tools},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584917301349}
}

@Article{Ahmed20081098,
	Title = {{The software product line architecture: An empirical investigation of key process activities}},
	Author = {Ahmed, F and Capretz, L F},
	Journal = {Information and Software Technology},
	Year = {2008},
	Number = {11},
	Pages = {1098--1113},
	Volume = {50},
	Abstract = {Software architecture has been a key area of concern in software industry due to its profound impact on the productivity and quality of software products. This is even more crucial in case of software product line, because it deals with the development of a line of products sharing common architecture and having controlled variability. The main contributions of this paper is to increase the understanding of the influence of key software product line architecture process activities on the overall performance of software product line by conducting a comprehensive empirical investigation covering a broad range of organizations currently involved in the business of software product lines. This is the first study to empirically investigate and demonstrate the relationships between some of the software product line architecture process activities and the overall software product line performance of an organization at the best of our knowledge. The results of this investigation provide empirical evidence that software product line architecture process activities play a significant role in successfully developing and managing a software product line. {\textcopyright} 2007 Elsevier B.V. All rights reserved.},
	Annote = {cited By 18},
	Doi = {10.1016/j.infsof.2007.10.013},
	Keywords = {Architecture; Network architecture; Product develo,Domain engineering; Empirical evidences; Empirica,Software architecture},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-49549123847{\&}doi=10.1016{\%}2Fj.infsof.2007.10.013{\&}partnerID=40{\&}md5=63a5afabfc5f33af279aa8e53b82a03c}
}

@Article{Ahmed2007194,
	Title = {{Managing the business of software product line: An empirical investigation of key business factors}},
	Author = {Ahmed, F and Capretz, L F},
	Journal = {Information and Software Technology},
	Year = {2007},
	Number = {2},
	Pages = {194--208},
	Volume = {49},
	Abstract = {Business has been highlighted as a one of the critical dimensions of software product line engineering. This paper's main contribution is to increase the understanding of the influence of key business factors by showing empirically that they play an imperative role in managing a successful software product line. A quantitative survey of software organizations currently involved in the business of developing software product lines over a wide range of operations, including consumer electronics, telecommunications, avionics, and information technology, was designed to test the conceptual model and hypotheses of the study. This is the first study to demonstrate the relationships between the key business factors and software product lines. The results provide evidence that organizations in the business of software product line development have to cope with multiple key business factors to improve the overall performance of the business, in addition to their efforts in software development. The conclusions of this investigation reinforce current perceptions of the significance of key business factors in successful software product line business. {\textcopyright} 2006 Elsevier B.V. All rights reserved.},
	Annote = {cited By 15},
	Doi = {10.1016/j.infsof.2006.05.004},
	Keywords = {Avionics; Consumer electronics; Information techno,Key business factor; Marketing strategy; Software,Software engineering},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33751118800{\&}doi=10.1016{\%}2Fj.infsof.2006.05.004{\&}partnerID=40{\&}md5=84a05c338e8155168576c110f8c208f8}
}

@Article{Ahmed2007836,
	Title = {{Institutionalization of software product line: An empirical investigation of key organizational factors}},
	Author = {Ahmed, F and Capretz, L F and Sheikh, S A},
	Journal = {Journal of Systems and Software},
	Year = {2007},
	Number = {6},
	Pages = {836--849},
	Volume = {80},
	Abstract = {A good fit between the person and the organization is essential in a better organizational performance. This is even more crucial in case of institutionalization of a software product line practice within an organization. Employees' participation, organizational behavior and management contemplation play a vital role in successfully institutionalizing software product lines in a company. Organizational dimension has been weighted as one of the critical dimensions in software product line theory and practice. A comprehensive empirical investigation to study the impact of some organizational factors on the performance of software product line practice is presented in this work. This is the first study to empirically investigate and demonstrate the relationships between some of the key organizational factors and software product line performance of an organization. The results of this investigation provide empirical evidence and further support the theoretical foundations that in order to institutionalize software product lines within an organization, organizational factors play an important role. {\textcopyright} 2006 Elsevier Inc. All rights reserved.},
	Annote = {cited By 16},
	Doi = {10.1016/j.jss.2006.09.010},
	File = {:Users/mac/Downloads/Sciencedirect{\_}articles{\_}13Jun2017{\_}14-36-55.006/Institutionalization-of-software-product-line-An-empirical-investigation-of-key-organizational-factors{\_}2007{\_}Journal-of-Systems-and-Software.pdf:pdf},
	Keywords = {Computer software; Industrial management; Societie,Organizational behavior; Organizational theory; S,Software engineering},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947384858{\&}doi=10.1016{\%}2Fj.jss.2006.09.010{\&}partnerID=40{\&}md5=bbde19a109b7058e813ca72d3401c217}
}

@Conference{Ahmed2008,
	Title = {{Software product line based approach towards the software product quality improvement}},
	Author = {Ahmed, Z},
	Booktitle = {SoMeT{\_}08 - The 7th International Conference on Software Methodologies, Tools and Techniques},
	Year = {2008},
	Abstract = {In this research paper two intensive problems to the software rose by the software industry .i.e., identification of fault proneness and increase in rate of variability's in traditional and product line applications are discussed. To contribute in the field of software product development and to mitigate the aforementioned hurdles, a measurement analysis based approach is proposed. The proposed solution is based on the concepts of analyzing preprocessed source code characteristics, identification of the level of complexity by several procedural measurements and objects oriented source metrics and visualize the results in two and three dimensional diagrams. Furthermore, the capabilities, features, potential and effectiveness of implemented solution are validated by means of an experiment.},
	Annote = {cited By 0},
	Keywords = {Fault proneness; Measurement analysis; Product-lin,Product development,Software engineering},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858806764{\&}partnerID=40{\&}md5=f164dbc393b809c6c927b1bc034cd61f}
}

@Article{Ajila20081784,
	Title = {{Evolution support mechanisms for software product line process}},
	Author = {Ajila, Samuel A and Kaba, Ali B},
	Journal = {Journal of Systems and Software},
	Year = {2008},
	Number = {10},
	Pages = {1784--1801},
	Volume = {81},
	Abstract = {Software product family process evolution needs specific support for incremental change. Product line process evolution involves in addition to identifying new requirements the building of a meta-process describing the migration from the old process to the new one. This paper presents basic mechanisms to support software product line process evolution. These mechanisms share four strategies – change identification, change impact, change propagation, and change validation. It also examines three kinds of evolution processes – architecture, product line, and product. In addition, change management mechanisms are identified. Specifically we propose support mechanisms for static local entity evolution and complex entity evolution including transient evolution process. An evolution model prototype based on dependency relationships structure of the various product line artifacts is developed. },
	Annote = {Selected papers from the 30th Annual International Computer Software and Applications Conference (COMPSAC), Chicago, September 7–21, 2006},
	Doi = {https://doi.org/10.1016/j.jss.2007.12.797},
	ISSN = {0164-1212},
	Keywords = {Feature-based object oriented model,Meta-process,Product line architecture,Software development process,Software product line process evolution,Transient process,Use case modeling},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121208000083}
}

@Article{Ajoudanian2015640,
	Title = {{Automatic promotional specialization, generalization and analysis of extended feature models with cardinalities in Alloy}},
	Author = {Ajoudanian, Shohreh and Hosseinabadi, Seyed-Hassan Mirian},
	Journal = {Journal of Logical and Algebraic Methods in Programming},
	Year = {2015},
	Number = {5},
	Pages = {640--667},
	Volume = {84},
	Abstract = {Abstract Software product line engineering is a method of producing a set of related products that share more commonalities than variability in a cost-effective approach. Software product lines provide systematic reuse within a product family. Extended feature models with cardinalities are widely used for managing variability and commonality in the software product line domains. In this paper, we use promotion technique in Alloy to formalize constraint based extended feature models with cardinalities and their specialization and generalization. This technique has a significant influence on applying analysis operations on feature models. To show the benefits of the promotion technique, we calculate the reuse ratio of a feature in a large scale software product line. In the presented method, in addition to feature and group cardinalities, we consider different combinations of cardinalities with each other as well as feature cloning. },
	Doi = {https://doi.org/10.1016/j.jlamp.2014.11.005},
	ISSN = {2352-2208},
	Keywords = {Extended feature model with cardinality,Multiple multi-level promotions in Alloy,Specialization and generalization of SPLS},
	Url = {http://www.sciencedirect.com/science/article/pii/S2352220814000959}
}

@Article{SMR:SMR308,
	Title = {{Reusing class-based test cases for testing object-oriented framework interface classes}},
	Author = {{Al Dallal}, Jehad and Sorenson, Paul},
	Journal = {Journal of Software Maintenance and Evolution: Research and Practice},
	Year = {2005},
	Number = {3},
	Pages = {169--196},
	Volume = {17},
	Abstract = {An application framework provides a reusable design and implementation for a family of software systems. Frameworks are introduced to reduce the cost of a product line (i.e., family of products that share the common features) and to increase the maintainability of software products through the deployment of reliable large-scale reusable components. A key challenge with frameworks is the development, evolution and maintenance of test cases to ensure the framework operates appropriately in a given application or product. Reusable test cases increase the maintainability of the software products because an entirely new set of test cases does not have to be generated each time the framework is deployed. At the framework deployment stage, the application developers (i.e., framework users) may need the flexibility to ignore or modify part of the specification used to generate the reusable class-based test cases. This paper addresses how to deal effectively with the different modification forms such that the use of the test cases becomes easy and straightforward in testing the framework interface classes (FICs) developed at the application development stage. Finally, the paper discusses the fault coverage and experimentally examines the specification coverage of the reusable test cases. Copyright {\textcopyright} 2005 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/smr.308},
	ISSN = {1532-0618},
	Keywords = {class testing,framework interface classes (FICs),object-oriented framework,reusable test cases,specification-based testing},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/smr.308}
}

@Article{Alam201543,
	Title = {{Impact analysis and change propagation in service-oriented enterprises: A systematic review}},
	Author = {Alam, Khubaib Amjad and Ahmad, Rodina and Akhunzada, Adnan and Nasir, Mohd Hairul Nizam Md and Khan, Samee U},
	Journal = {Information Systems},
	Year = {2015},
	Pages = {43--73},
	Volume = {54},
	Abstract = {AbstractContext The adoption of Service-oriented Architecture (SOA) and Business Process Management (BPM) is fairly recent. The major concern is now shifting towards the maintenance and evolution of service-based business information systems. Moreover, these systems are highly dynamic and frequent changes are anticipated across multiple levels of abstraction. Impact analysis and change propagation are identified as potential research areas in this regard. Objective The aim of this study is to systematically review extant research on impact analysis and propagation in the {\{}BPM{\}} and {\{}SOA{\}} domains. Identifying, categorizing and synthesizing relevant solutions are the main study objectives. Method Through careful review and screening, we identified 60 studies relevant to 4 research questions. Two classification schemes served to comprehend and analyze the anatomy of existing solutions. {\{}BPM{\}} is considered at the business level for business operations and processes, while {\{}SOA{\}} is considered at the service level as deployment architecture. We focused on both horizontal and vertical impacts of changes across multiple abstraction layers. Results Impact analysis solutions were mainly divided into dependency analysis, traceability analysis and history mining. Dependency analysis is the most frequently adopted technique followed by traceability analysis. Further categorization of dependency analysis indicates that graph-based techniques are extensively used, followed by formal dependency modeling. While considering hierarchical coverage, inter-process and inter-service change analyses have received considerable attention from the research community, whereas bottom-up analysis has been the most neglected research area. The majority of change propagation solutions are top-down and semi-automated. Conclusions This study concludes with new insight suggestions for future research. Although, the evolution of service-based systems is becoming of grave concern, existing solutions in this field are less mature. Studies on hierarchical change impact are scarce. Complex relationships of services with business processes and semantic dependencies are poorly understood and require more attention from the research community. },
	Doi = {https://doi.org/10.1016/j.is.2015.06.003},
	ISSN = {0306-4379},
	Keywords = {BPM,CIA,Change propagation,Dependency analysis,MSR,SOA,SOC,Semantic annotation,Systematic literature review (SLR),Web service},
	Url = {http://www.sciencedirect.com/science/article/pii/S0306437915001179}
}

@Article{SPE:SPE2463,
	Title = {{Modelling a family of systems for crisis management with concern-oriented reuse}},
	Author = {Alam, Omar and Kienzle, J{\"{o}}rg and Mussbacher, Gunter},
	Journal = {Software: Practice and Experience},
	Year = {2017},
	Number = {7},
	Pages = {985--999},
	Volume = {47},
	Abstract = {Concern-oriented reuse (CORE) proposes the concern as a new unit of model-based reuse encapsulating software artefacts pertaining to a domain of interest that span multiple development phases and levels of abstraction. With CORE, a concern encapsulates multiple reusable features, while allowing its generic models to be customized to problem-specific contexts. We report on our experience of designing a family of crisis management systems (CMS) with the help of reusable concern libraries. The collected metrics show a considerable amount of reuse in our CMS design. The study provides encouraging evidence that CORE's vision to create large-scale, generic and reusable entities that are expressed with the most appropriate modelling formalisms at the right level of abstraction is feasible. We present our experience in the design of the CMS and elaborate on the advantages as well as the efforts required to adopt CORE in an industrial setting. Copyright {\textcopyright} 2016 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spe.2463},
	ISSN = {1097-024X},
	Keywords = {SPL,aspect-oriented modelling,bCMS,concern-oriented reuse,crisis management system,goal modelling,reusable concern library,software product line},
	Url = {http://dx.doi.org/10.1002/spe.2463}
}

@Article{Albert201591,
	Title = {{A multi-domain incremental analysis engine and its application to incremental resource analysis}},
	Author = {Albert, Elvira and Correas, Jes{\'{u}}s and Puebla, Germ{\'{a}}n and Rom{\'{a}}n-D{\'{i}}ez, Guillermo},
	Journal = {Theoretical Computer Science},
	Year = {2015},
	Pages = {91--114},
	Volume = {585},
	Abstract = {Abstract The aim of incremental analysis is, given a program, its analysis results, and a series of changes to the program, to obtain the new analysis results as efficiently as possible and, ideally, without having to (re-)analyze fragments of code which are not affected by the changes. Incremental analysis can significantly reduce both the time and the memory requirements of analysis. The first contribution of this article is a multi-domain incremental fixed-point algorithm for a sequential Java-like language. The algorithm is multi-domain in the sense that it interleaves the (re-)analysis for multiple domains by taking into account dependencies among them. Importantly, this allows the incremental analyzer to invalidate only those analysis results previously inferred by certain dependent domains. The second contribution is an incremental resource usage analysis which, in its first phase, uses the multi-domain incremental fixed-point algorithm to carry out all global pre-analyses required to infer cost in an interleaved way. Such resource analysis is parametric on the cost metrics one wants to measure (e.g., number of executed instructions, number of objects created, etc.). Besides, we present a novel form of cost summaries which allows us to incrementally reconstruct only those components of cost functions affected by the changes. Experimental results in the costa system show that the proposed incremental analysis provides significant performance gains, ranging from a speedup of 1.48 up to 5.13 times faster than non-incremental analysis. },
	Annote = {Developments in Implicit Complexity},
	Doi = {https://doi.org/10.1016/j.tcs.2015.03.002},
	ISSN = {0304-3975},
	Keywords = {Cost analysis,Incremental analysis,Resource usage analysis,Static analysis},
	Url = {http://www.sciencedirect.com/science/article/pii/S0304397515001954}
}

@Article{Albuquerque2015245,
	Title = {{Quantifying usability of domain-specific languages: An empirical study on software maintenance}},
	Author = {Albuquerque, Diego and Cafeo, Bruno and Garcia, Alessandro and Barbosa, Simone and Abrah{\~{a}}o, Silvia and Ribeiro, Ant{\'{o}}nio},
	Journal = {Journal of Systems and Software},
	Year = {2015},
	Pages = {245--259},
	Volume = {101},
	Abstract = {Abstract A domain-specific language (DSL) aims to support software development by offering abstractions to a particular domain. It is expected that {\{}DSLs{\}} improve the maintainability of artifacts otherwise produced with general-purpose languages. However, the maintainability of the {\{}DSL{\}} artifacts and, hence, their adoption in mainstream development, is largely dependent on the usability of the language itself. Unfortunately, it is often hard to identify their usability strengths and weaknesses early, as there is no guidance on how to objectively reveal them. Usability is a multi-faceted quality characteristic, which is challenging to quantify beforehand by {\{}DSL{\}} stakeholders. There is even less support on how to quantitatively evaluate the usability of {\{}DSLs{\}} used in maintenance tasks. In this context, this paper reports a study to compare the usability of textual {\{}DSLs{\}} under the perspective of software maintenance. A usability measurement framework was developed based on the cognitive dimensions of notations. The framework was evaluated both qualitatively and quantitatively using two {\{}DSLs{\}} in the context of two evolving object-oriented systems. The results suggested that the proposed metrics were useful: (1) to early identify {\{}DSL{\}} usability limitations, (2) to reveal specific {\{}DSL{\}} features favoring maintenance tasks, and (3) to successfully analyze eight critical {\{}DSL{\}} usability dimensions. },
	Doi = {https://doi.org/10.1016/j.jss.2014.11.051},
	ISSN = {0164-1212},
	Keywords = {DSL,Metrics,Usability},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121214002799}
}

@Article{Aleem201655,
	Title = {{A Digital Game Maturity Model (DGMM)}},
	Author = {Aleem, Saiqa and Capretz, Luiz Fernando and Ahmed, Faheem},
	Journal = {Entertainment Computing},
	Year = {2016},
	Pages = {55--73},
	Volume = {17},
	Abstract = {Abstract Game development is an interdisciplinary concept that embraces artistic, software engineering, management, and business disciplines. This research facilitates a better understanding of important dimensions of digital game development methodology. Game development is considered as one of the most complex tasks in software engineering. The increased popularity of digital games, the challenges faced by game development organizations in developing quality games, and high competition in the digital game industry demand a game development maturity assessment. Consequently, this study presents a Digital Game Maturity Model to evaluate the current development methodology in an organization. The framework of this model consists of assessment questionnaires, a performance scale, and a rating method. The main goal of the questionnaires is to collect information about current processes and practices. In general, this research contributes towards formulating a comprehensive and unified strategy for game development maturity evaluation. Two case studies were conducted and their assessment results reported. These demonstrate the level of maturity of current development practices in two organizations. },
	Doi = {https://doi.org/10.1016/j.entcom.2016.08.004},
	ISSN = {1875-9521},
	Keywords = {Game development methodology,Game performance,Online game,Process assessment,Software game,Software process improvement,Video game},
	Url = {http://www.sciencedirect.com/science/article/pii/S1875952116300246}
}

@Article{Alegre201655,
	Title = {{Engineering context-aware systems and applications: A survey}},
	Author = {Alegre, Unai and Augusto, Juan Carlos and Clark, Tony},
	Journal = {Journal of Systems and Software},
	Year = {2016},
	Pages = {55--83},
	Volume = {117},
	Abstract = {Abstract Context-awareness is an essential component of systems developed in areas like Intelligent Environments, Pervasive {\&} Ubiquitous Computing and Ambient Intelligence. In these emerging fields, there is a need for computerized systems to have a higher understanding of the situations in which to provide services or functionalities, to adapt accordingly. The literature shows that researchers modify existing engineering methods in order to better fit the needs of context-aware computing. These efforts are typically disconnected from each other and generally focus on solving specific development issues. We encourage the creation of a more holistic and unified engineering process that is tailored for the demands of these systems. For this purpose, we study the state-of-the-art in the development of context-aware systems, focusing on: (A) Methodologies for developing context-aware systems, analyzing the reasons behind their lack of adoption and features that the community wish they can use; (B) Context-aware system engineering challenges and techniques applied during the most common development stages; (C) Context-aware systems conceptualization. },
	Doi = {https://doi.org/10.1016/j.jss.2016.02.010},
	ISSN = {0164-1212},
	Keywords = {Ambient Intelligence,Context-Aware Systems Engineering,Context-aware computing,Context-awareness,Context-sensitive,Intelligent Environments,Pervasive {\&} Ubiquitous Computing,Sentient computing,Software engineering},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121216000467}
}

@Article{Alferez201424,
	Title = {{Dynamic adaptation of service compositions with variability models}},
	Author = {Alf{\'{e}}rez, G H and Pelechano, V and Mazo, R and Salinesi, C and Diaz, D},
	Journal = {Journal of Systems and Software},
	Year = {2014},
	Pages = {24--47},
	Volume = {91},
	Abstract = {Abstract Web services run in complex contexts where arising events may compromise the quality of the whole system. Thus, it is desirable to count on autonomic mechanisms to guide the self-adaptation of service compositions according to changes in the computing infrastructure. One way to achieve this goal is by implementing variability constructs at the language level. However, this approach may become tedious, difficult to manage, and error-prone. In this paper, we propose a solution based on a semantically rich variability model to support the dynamic adaptation of service compositions. When a problematic event arises in the context, this model is leveraged for decision-making. The activation and deactivation of features in the variability model result in changes in a composition model that abstracts the underlying service composition. These changes are reflected into the service composition by adding or removing fragments of Business Process Execution Language (WS-BPEL) code, which can be deployed at runtime. In order to reach optimum adaptations, the variability model and its possible configurations are verified at design time using Constraint Programming. An evaluation demonstrates several benefits of our approach, both at design time and at runtime. },
	Doi = {https://doi.org/10.1016/j.jss.2013.06.034},
	ISSN = {0164-1212},
	Keywords = {Autonomic computing,Constraint programming,Dynamic adaptation,Dynamic software product line,Models at runtime,Variability,Verification,Web service composition},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121213001465}
}

@Article{Alferez2014355,
	Title = {{Evaluating scenario-based SPL requirements approaches: the case for modularity, stability and expressiveness}},
	Author = {Alf{\'{e}}rez, M and Bonif{\'{a}}cio, R and Teixeira, L and Accioly, P and Kulesza, U and Moreira, A and Ara{\'{u}}jo, J and Borba, P},
	Journal = {Requirements Engineering},
	Year = {2014},
	Number = {4},
	Pages = {355--376},
	Volume = {19},
	Abstract = {Software product lines (SPL) provide support for productivity gains through systematic reuse. Among the various quality attributes supporting these goals, modularity, stability and expressiveness of feature specifications, their composition and configuration knowledge emerge as strategic values in modern software development paradigms. This paper presents a metric-based evaluation aiming at assessing how well the chosen qualities are supported by scenario-based SPL requirementsapproaches. The selected approaches for this study span from type of notation (textual or graphical based), style to support variability (annotation or composition based), and specification expressiveness. They are compared using the metrics developed in a set of releases from an exemplar case study. Our major findings indicate that composition-based approaches have greater potential to support modularity and stability, and that quantification mechanisms simplify and increase expressiveness of configuration knowledge and composition specifications. {\textcopyright} 2013, Springer-Verlag London.},
	Annote = {cited By 3},
	Doi = {10.1007/s00766-013-0184-5},
	Keywords = {Computer software reusability; Productivity; Speci,Productivity gain; Quality attributes; Requiremen,Software design},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920253202{\&}doi=10.1007{\%}2Fs00766-013-0184-5{\&}partnerID=40{\&}md5=26bd7eec7ee2caf73ab4fc950066efee}
}

@Article{Alferez2010103,
	Title = {{Multi-view composition language for software product line requirements}},
	Author = {Alf{\'{e}}rez, M and Santos, J and Moreira, A and Garcia, A and Kulesza, U and Ara{\'{u}}jo, J and Amaral, V},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2010},
	Pages = {103--122},
	Volume = {5969 LNCS},
	Abstract = {Composition of requirements models in Software Product Line (SPL) development enables stakeholders to derive the requirements of target software products and, very important, to reason about them. Given the growing complexity of SPL development and the various stakeholders involved, their requirements are often specified from heterogeneous, partial views. However, existing requirements composition languages are very limited to generate specific requirements views for SPL products. They do not provide specialized composition rules for referencing and composing elements in recurring requirements models, such as use cases and activity models. This paper presents a multi-view composition language for SPL requirements, the Variability Modeling Language for Requirements (VML4RE). This language describes how requirements elements expressed in different models should be composed to generate a specific SPL product. The use of VML4RE is illustrated with UML-based requirements models defined for a home automation SPL case study. The language is evaluated with additional case studies from different application domains, such as mobile phones and sales management. {\textcopyright} 2010 Springer-Verlag.},
	Annote = {cited By 19},
	Doi = {10.1007/978-3-642-12107-4_8},
	Keywords = {Activity models; Application domains; Composition,Computer software reusability; Network architectu,Linguistics},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951642464{\&}doi=10.1007{\%}2F978-3-642-12107-4{\_}8{\&}partnerID=40{\&}md5=bc504d6b8baecd0db67ea5835a2541ea}
}

@Article{Al-Hajjaji:2016:IEP:3093335.2993253,
	Title = {{IncLing: Efficient Product-line Testing Using Incremental Pairwise Sampling}},
	Author = {Al-Hajjaji, Mustafa and Krieter, Sebastian and Th{\"{u}}m, Thomas and Lochau, Malte and Saake, Gunter},
	Journal = {SIGPLAN Not.},
	Year = {2016},
	Number = {3},
	Pages = {144--155},
	Volume = {52},
	Address = {New York, NY, USA},
	Doi = {10.1145/3093335.2993253},
	ISSN = {0362-1340},
	Keywords = {Software product lines,combinatorial interaction testing,model-based testing,sampling},
	Publisher = {ACM},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/3093335.2993253}
}

@Article{Ali2010871,
	Title = {{A systematic review of comparative evidence of aspect-oriented programming}},
	Author = {Ali, Muhammad Sarmad and Babar, Muhammad Ali and Chen, Lianping and Stol, Klaas-Jan},
	Journal = {Information and Software Technology},
	Year = {2010},
	Number = {9},
	Pages = {871--887},
	Volume = {52},
	Abstract = {Context Aspect-oriented programming (AOP) promises to improve many facets of software quality by providing better modularization and separation of concerns, which may have system wide affect. There have been numerous claims in favor and against {\{}AOP{\}} compared with traditional programming languages such as Objective Oriented and Structured Programming Languages. However, there has been no attempt to systematically review and report the available evidence in the literature to support the claims made in favor or against {\{}AOP{\}} compared with non-AOP approaches. Objective This research aimed to systematically identify, analyze, and report the evidence published in the literature to support the claims made in favor or against {\{}AOP{\}} compared with non-AOP approaches. Method We performed a systematic literature review of empirical studies of {\{}AOP{\}} based development, published in major software engineering journals and conference proceedings. Results Our search strategy identified 3307 papers, of which 22 were identified as reporting empirical studies comparing {\{}AOP{\}} with non-AOP approaches. Based on the analysis of the data extracted from those 22 papers, our findings show that for performance, code size, modularity, and evolution related characteristics, a majority of the studies reported positive effects, a few studies reported insignificant effects, and no study reported negative effects; however, for cognition and language mechanism, negative effects were reported. Conclusion {\{}AOP{\}} is likely to have positive effect on performance, code size, modularity, and evolution. However its effect on cognition and language mechanism is less likely to be positive. Care should be taken using {\{}AOP{\}} outside the context in which it has been validated. },
	Doi = {https://doi.org/10.1016/j.infsof.2010.05.003},
	ISSN = {0950-5849},
	Keywords = {Aspect-oriented programming,Evidence-based software engineering,Systematic literature review},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584910000819}
}

@Article{Alonso2012170,
	Title = {{Automatic code generation for real-time systems: A development approach based on components, models, and frameworks [Generaci{\'{o}}n autom{\'{a}}tica de software para sistemas de tiempo real: Un enfoque basado en componentes, modelos y frameworks]}},
	Author = {Alonso, D and Pastor, J A and S{\'{a}}nchez, P and {\'{A}}lvarez, B and Vicente-Chicote, C},
	Journal = {RIAI - Revista Iberoamericana de Automatica e Informatica Industrial},
	Year = {2012},
	Number = {2},
	Pages = {170--181},
	Volume = {9},
	Abstract = {Real-Time Systems have some characteristics that make them particularly sensitive to architectural decisions. The use of Frameworks and Components has proven effective in improving productivity and software quality, especially when combined with Software Product Line approaches. However, the results in terms of software reuse and standardization make the lack of portability of both the design and componentbased implementations clear. This article, based on the Model- Driven Software Development paradigm, presents an approach that separates the component-based description of real-time applications from their possible implementations on different platforms. This separation is supported by the automatic integration of the code obtained from the input models into object-oriented frameworks. The article also details the architectural decisions taken in the implementation of one of such frameworks, which is used as a case study to illustrate the proposed approach. Finally, a comparison with other alternative approaches is made in terms of development cost. {\textcopyright} 2012 CEA. Publicado por Elsevier Espa{\~{n}}a, S.L.},
	Annote = {cited By 4},
	Doi = {10.1016/j.riai.2012.02.010},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863736214{\&}doi=10.1016{\%}2Fj.riai.2012.02.010{\&}partnerID=40{\&}md5=fd2181decbe551cbf2c5afe5f1455f04}
}

@Article{Alsawalqah201479,
	Title = {{A method to optimize the scope of a software product platform based on end-user features}},
	Author = {Alsawalqah, H I and Kang, S and Lee, J},
	Journal = {Journal of Systems and Software},
	Year = {2014},
	Pages = {79--106},
	Volume = {98},
	Abstract = {Context: Due to increased competition and the advent of mass customization, many software firms are utilizing product families - groups of related products derived from a product platform - to provide product variety in a cost-effective manner. The key to designing a successful software product family is the product platform, so it is important to determine the most appropriate product platform scope related to business objectives, for product line development. Aim: This paper proposes a novel method to find the optimized scope of a software product platform based on end-user features. Method: The proposed method, PPSMS (Product Platform Scoping Method for Software Product Lines), mathematically formulates the product platform scope selection as an optimization problem. The problem formulation targets identification of an optimized product platform scope that will maximize life cycle cost savings and the amount of commonality, while meeting the goals and needs of the envisioned customers' segments. A simulated annealing based algorithm that can solve problems heuristically is then used to help the decision maker in selecting a scope for the product platform, by performing tradeoff analysis of the commonality and cost savings objectives. Results In a case study, PPSMS helped in identifying 5 non-dominated solutions considered to be of highest preference for decision making, taking into account both cost savings and commonality objectives. A quantitative and qualitative analysis indicated that human experts perceived value in adopting the method in practice, and that it was effective in identifying appropriate product platform scope. {\textcopyright} 2014 Elsevier Inc. All rights reserved.},
	Annote = {cited By 5},
	Doi = {10.1016/j.jss.2014.08.034},
	Keywords = {End users; Software product line engineerings; Sof},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908291258{\&}doi=10.1016{\%}2Fj.jss.2014.08.034{\&}partnerID=40{\&}md5=b78a56f37674d45f046b08e09e784143}
}

@Article{IIS2:IIS203132,
	Title = {{1.2.3 Towards a semantic-based representation and computation of quantitative indexes for quality management of requirements}},
	Author = {Alvarez-Rodr{\'{i}}guez, Jose Mar{\'{i}}a and Llorens, Juan and Alejandres, Manuela and Fuentes, Jose Miguel},
	Journal = {INCOSE International Symposium},
	Year = {2014},
	Number = {1},
	Pages = {27--40},
	Volume = {24},
	Abstract = {Quality management of requirements has seen a dramatic increase of the amount of applications, management platforms, data, etc. gaining momentum in the Systems Engineering area and more specifically in the deployment of the next wave of critical system. In this context, one of the next big things lies in the creation of quality functions that can automatically detect and make decisions according to natural-language based requirements specifications and models. In this sense quality indicator of requirements seeks for providing an intelligent environment for detecting values of such as correctness, consistency and completeness based on domain knowledge in which both functional and nonfunctional properties of system components can be validated and verified easing the transition to a smart system environment. Thus the testing of critical systems based on requirements quality can be seen as a special kind of policy-making strategy that must compile several key indicators to summarize data and information and obtain an objective quantitative measure. Nevertheless the quantitative analysis of several quality indicators is becoming a challenging task due to natural language ambiguities and a tangled/heterogeneous environment of data, providers, etc. Existing tools and techniques based on traditional processes of quality assessment are preventing a proper use of the new dynamic and data environment avoiding more timely, adaptable and flexible (on-demand) quantitative index creation and, as a consequence, more accurate decisions. On the other hand, semantic-based technologies emerge to provide the adequate building blocks to represent domain-knowledge and process data in a flexible fashion using a common and shared data model. That is why the present paper introduces a Resource Description Framework (RDF) vocabulary to semantically represent and compute quantitative indexes as part of the implementation of the Open Services for Lifecycle Collaboration initiative (OSLC) Quality Management specification. Finally some discussion, conclusions and future work are also outlined.},
	Doi = {10.1002/j.2334-5837.2014.tb03132.x},
	ISSN = {2334-5837},
	Url = {http://dx.doi.org/10.1002/j.2334-5837.2014.tb03132.x}
}

@Article{STVR:STVR1603,
	Title = {{Prioritizing test cases for early detection of refactoring faults}},
	Author = {Alves, Everton L G and Machado, Patr{\'{i}}cia D L and Massoni, Tiago and Kim, Miryung},
	Journal = {Software Testing, Verification and Reliability},
	Year = {2016},
	Number = {5},
	Pages = {402--426},
	Volume = {26},
	Abstract = {Refactoring edits are error-prone, requiring cost-effective testing. Regression test suites are often used as a safety net for decreasing the chances of behavioural changes. Because of the high costs related to handling massive test suites, prioritization techniques can be applied to reorder test case execution, fostering early fault detection. However, traditional prioritization techniques are not specifically designed for detecting refactoring-related faults. This article proposes refactoring-based approach (RBA), a refactoring-aware strategy for prioritizing regression test cases. RBA reorders an existing test sequence, using a set of proposed refactoring fault models that define the refactoring's impact on program methods.Refactoring-based approach's evaluation shows that it promotes early detection of refactoring faults and outperforms well-known prioritization techniques in 71{\%} of the cases.Moreover, it prioritizes fault-revealing test cases close to one another in 73{\%} of the cases, which can be useful for fault localization. Those findings show that RBA can considerably improve prioritization of test cases during perfective evolution, both by increasing fault-detection rates as well as by helping to pinpoint defects introduced by an incorrect refactoring. Copyright {\textcopyright} 2016 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/stvr.1603},
	ISSN = {1099-1689},
	Keywords = {automated software testing,refactoring,test case prioritization},
	Url = {http://dx.doi.org/10.1002/stvr.1603}
}

@Article{Alves2010806,
	Title = {{Requirements engineering for software product lines: A systematic literature review}},
	Author = {Alves, Vander and Niu, Nan and Alves, Carina and Valen{\c{c}}a, George},
	Journal = {Information and Software Technology},
	Year = {2010},
	Number = {8},
	Pages = {806--820},
	Volume = {52},
	Abstract = {Context Software product line engineering (SPLE) is a growing area showing promising results in research and practice. In order to foster its further development and acceptance in industry, it is necessary to assess the quality of the research so that proper evidence for adoption and validity are ensured. This holds in particular for requirements engineering (RE) within SPLE, where a growing number of approaches have been proposed. Objective This paper focuses on {\{}RE{\}} within {\{}SPLE{\}} and has the following goals: assess research quality, synthesize evidence to suggest important implications for practice, and identify research trends, open problems, and areas for improvement. Method A systematic literature review was conducted with three research questions and assessed 49 studies, dated from 1990 to 2009. Results The evidence for adoption of the methods is not mature, given the primary focus on toy examples. The proposed approaches still have serious limitations in terms of rigor, credibility, and validity of their findings. Additionally, most approaches still lack tool support addressing the heterogeneity and mostly textual nature of requirements formats as well as address only the proactive {\{}SPLE{\}} adoption strategy. Conclusions Further empirical studies should be performed with sufficient rigor to enhance the body of evidence in {\{}RE{\}} within SPLE. In this context, there is a clear need for conducting studies comparing alternative methods. In order to address scalability and popularization of the approaches, future research should be invested in tool support and in addressing combined {\{}SPLE{\}} adoption strategies. },
	Doi = {https://doi.org/10.1016/j.infsof.2010.03.014},
	ISSN = {0950-5849},
	Keywords = {Requirements engineering,Software product lines,Systematic literature review},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584910000625}
}

@Article{Alvim201731,
	Title = {{A preliminary assessment of variability implementation mechanisms in service-oriented computing}},
	Author = {Alvim, L F M and Machado, I C and de Almeida, E S},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2017},
	Pages = {31--47},
	Volume = {10221 LNCS},
	Abstract = {Service-Oriented Computing and Software Product Lines are software development strategies capable to provide a systematic means to reuse existing software assets, rather than repeatedly developing them from scratch, for every new software system. The inherent characteristics of both strategies has led the research community to combine them, in what is commonly referred to as Service-Oriented Product Lines (SOPL) strategies. Despite the perceived potential of such a combination, there are many challenges to confront in order to provide a practical generalizable solution. In particular, there is a lack of empirical evidence on the actual support of variability implementation mechanisms, typical in SPL engineering, and their suitability for SOPL. In line with such a challenge, this paper presents a preliminary assessment aimed to identify variability implementation mechanisms which may improve measures of complexity, instability and modularity, quality attributes particularly important for modular and reusable software systems, as is the case of SOPL. Based on the results of these evaluations, an initial decision model is developed to provide software engineers with an adequate support for the selection of variability mechanisms. {\textcopyright} Springer International Publishing AG 2017.},
	Annote = {cited By 0},
	Doi = {10.1007/978-3-319-56856-0_3},
	Keywords = {Computer software reusability,Computer software; Distributed computer systems; S,Decision modeling; Implementation mechanisms; Inh},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019257326{\&}doi=10.1007{\%}2F978-3-319-56856-0{\_}3{\&}partnerID=40{\&}md5=bc005d44a2bb58ab1d2c387c316dbcdc}
}

@Article{Amalio20113,
	Title = {{Platform-variant applications from platform-independent models via templates}},
	Author = {Am{\'{a}}lio, N and Glodt, C and Pinto, F and Kelsen, P},
	Journal = {Electronic Notes in Theoretical Computer Science},
	Year = {2011},
	Number = {3},
	Pages = {3--25},
	Volume = {279},
	Abstract = {By raising the level of abstraction from code to models, model-driven development (MDD) emphasises design rather than implementation and platform-specificity. This paper presents an experiment with a MDD approach, which takes platform-independent models and generates code for various platforms from them. The platform code is generated from templates. Our approach is based on EP, a formal executable modelling language, supplemented with OCL, and FTL, a formal language of templates. The paper's experiment generates code for the mobile platforms Android and iPhone from the same abstract functional model of a case study. The experiment shows the feasibility of MDD to tackle present day problems, highlighting many benefits of the MDD approach and opportunities for improvement. {\textcopyright} 2011 Elsevier B.V. All rights reserved.},
	Annote = {cited By 0},
	Doi = {10.1016/j.entcs.2011.11.035},
	Keywords = {Abstracting; Experiments; Formal languages,Codes (symbols),Executable model; Functional model; Level of abstr},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855696791{\&}doi=10.1016{\%}2Fj.entcs.2011.11.035{\&}partnerID=40{\&}md5=0f6226b96c53c65c7ed20c03c28ac926}
}

@InProceedings{Aman:2014:EAC:2652524.2652592,
	Title = {{Empirical Analysis of Comments and Fault-proneness in Methods: Can Comments Point to Faulty Methods?}},
	Author = {Aman, Hirohisa and Sasaki, Takashi and Amasaki, Sousuke and Kawahara, Minoru},
	Booktitle = {Proceedings of the 8th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
	Year = {2014},
	Address = {New York, NY, USA},
	Pages = {63:1----63:1},
	Publisher = {ACM},
	Series = {ESEM '14},
	Doi = {10.1145/2652524.2652592},
	ISBN = {978-1-4503-2774-9},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2652524.2652592}
}

@Article{ISJ:ISJ332,
	Title = {{A socio-cognitive interpretation of the potential effects of downsizing on software quality performance}},
	Author = {Ambrose, Paul J and Chiravuri, Ananth},
	Journal = {Information Systems Journal},
	Year = {2010},
	Number = {3},
	Pages = {239--265},
	Volume = {20},
	Abstract = {Organizational downsizing research indicates that downsizing does not always realize its strategic intent and may, in fact, have a detrimental impact on organizational performance. In this paper, we extend the notion that downsizing negatively impacts performance and argue that organizational downsizing can potentially be detrimental to software quality performance. Using social cognitive theory (SCT), we primarily interpret the negative impacts of downsizing on software quality performance by arguing that downsizing results in a realignment of social networks (environmental factors), thereby affecting the self-efficacy and outcome expectations of a software professional (personal factors), which, in turn, affect software quality performance (outcome of behaviour undertaken). We synthesize relevant literature from the software quality, SCT and downsizing research streams and develop a conceptual model. Two major impacts of downsizing are hypothesized in the conceptual model. First, downsizing destroys formal and informal social networks in organizations, which, in turn, negatively impacts software developers' self-efficacy and outcome expectations through their antecedents, with consequent negative impacts on software development process efficiency and software product quality, the two major components of software quality performance. Second, downsizing negatively affects antecedents of software development process efficiency, namely top management leadership, management infrastructure sophistication, process management efficacy and stakeholder participation with consequent negative impacts on software quality performance. This theoretically grounded discourse can help demonstrate how organizational downsizing can potentially impact software quality performance through key intervening constructs. We also discuss how downsizing and other intervening constructs can be managed to mitigate the negative impacts of downsizing on software quality performance.},
	Doi = {10.1111/j.1365-2575.2009.00332.x},
	ISSN = {1365-2575},
	Keywords = {downsizing,social cognitive theory,software quality,theory development},
	Publisher = {Blackwell Publishing Ltd},
	Url = {http://dx.doi.org/10.1111/j.1365-2575.2009.00332.x}
}

@Article{Ameller201542,
	Title = {{Development of service-oriented architectures using model-driven development: A mapping study}},
	Author = {Ameller, David and Burgu{\'{e}}s, Xavier and Collell, Oriol and Costal, Dolors and Franch, Xavier and Papazoglou, Mike P},
	Journal = {Information and Software Technology},
	Year = {2015},
	Pages = {42--66},
	Volume = {62},
	Abstract = {AbstractContext Model-Driven Development (MDD) and Service-Oriented Architecture (SOA) are two challenging research areas in software engineering. {\{}MDD{\}} is about improving software development whilst {\{}SOA{\}} is a service-based conceptual development style, therefore investigating the available proposals in the literature to use {\{}MDD{\}} when developing {\{}SOA{\}} may be insightful. However, no studies have been found with this purpose. Objective This work aims at assessing the state of the art in {\{}MDD{\}} for {\{}SOA{\}} systems. It mainly focuses on: what are the characteristics of {\{}MDD{\}} approaches that support SOA; what types of {\{}SOA{\}} are supported; how do they handle non-functional requirements. Method We conducted a mapping study following a rigorous protocol. We identified the representative set of venues that should be included in the study. We applied a search string over the set of selected venues. As result, 129 papers were selected and analysed (both frequency analysis and correlation analysis) with respect to the defined classification criteria derived from the research questions. Threats to validity were identified and mitigated whenever possible. Results The analysis allows us to answer the research questions. We highlight: (1) predominance of papers from Europe and written by researchers only; (2) predominance of top-down transformation in software development activities; (3) inexistence of consolidated methods; (4) significant percentage of works without tool support; (5) {\{}SOA{\}} systems and service compositions more targeted than single services and {\{}SOA{\}} enterprise systems; (6) limited use of metamodels; (7) very limited use of NFRs; and (8) limited application in real cases. Conclusion This mapping study does not just provide the state of the art in the topic, but also identifies several issues that deserve investigation in the future, for instance the need of methods for activities other than software development (e.g., migration) or the need of conducting more real case studies. },
	Doi = {https://doi.org/10.1016/j.infsof.2015.02.006},
	ISSN = {0950-5849},
	Keywords = {MDD,Mapping study,Model-driven development,SOA,Service-oriented architecture,State of the art},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584915000361}
}

@Article{SPIP:SPIP220,
	Title = {{Scenario-based decision making for architectural variability in product families}},
	Author = {America, Pierre and Hammer, Dieter and Ionita, Mugurel T and Obbink, Henk and Rommes, Eelco},
	Journal = {Software Process: Improvement and Practice},
	Year = {2005},
	Number = {2},
	Pages = {171--187},
	Volume = {10},
	Abstract = {In this article, we present a systematic approach towards decision making for variability in product families in the context of uncertainty. Our approach consists of the following ingredients: A suitable set of architectural views that bridge the gap between customer needs and available technology, a multiview variation modeling technique, the selection of several scenarios of different kinds, and a quantitative analysis of quality aspects for these scenarios. Copyright {\textcopyright} 2005 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spip.220},
	ISSN = {1099-1670},
	Keywords = {decision making,scenarios,system architecture,variability,variation models},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/spip.220}
}

@Article{Amoui20122720,
	Title = {{Achieving dynamic adaptation via management and interpretation of runtime models}},
	Author = {Amoui, Mehdi and Derakhshanmanesh, Mahdi and Ebert, J{\"{u}}rgen and Tahvildari, Ladan},
	Journal = {Journal of Systems and Software},
	Year = {2012},
	Number = {12},
	Pages = {2720--2737},
	Volume = {85},
	Abstract = {In this article, we present a generic model-centric approach for realizing fine-grained dynamic adaptation in software systems by managing and interpreting graph-based models of software at runtime. We implemented this approach as the Graph-based Runtime Adaptation Framework (GRAF), which is particularly tailored to facilitate and simplify the process of evolving and adapting current software towards runtime adaptivity. As a proof of concept, we present case study results that show how to achieve runtime adaptivity with {\{}GRAF{\}} and sketch the framework's capabilities for facilitating the evolution of real-world applications towards self-adaptive software. The case studies also provide some details of the {\{}GRAF{\}} implementation and examine the usability and performance of the approach. },
	Annote = {Self-Adaptive Systems},
	Doi = {https://doi.org/10.1016/j.jss.2012.05.033},
	ISSN = {0164-1212},
	Keywords = {Adaptation framework,Model transformation,Models at runtime,Runtime adaptivity,Self-adaptive software},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121212001458}
}

@Article{Ampatzoglou20112265,
	Title = {{An empirical investigation on the reusability of design patterns and software packages}},
	Author = {Ampatzoglou, Apostolos and Kritikos, Apostolos and Kakarontzas, George and Stamelos, Ioannis},
	Journal = {Journal of Systems and Software},
	Year = {2011},
	Number = {12},
	Pages = {2265--2283},
	Volume = {84},
	Abstract = {Nowadays open-source software communities are thriving. Successful open-source projects are competitive and the amount of source code that is freely available offers great reuse opportunities to software developers. Thus, it is expected that several requirements can be implemented based on open source software reuse. Additionally, design patterns, i.e. well-known solution to common design problems, are introduced as elements of reuse. This study attempts to empirically investigate the reusability of design patterns, classes and software packages. Thus, the results can help developers to identify the most beneficial starting points for white box reuse, which is quite popular among open source communities. In order to achieve this goal we conducted a case study on one hundred (100) open source projects. More specifically, we identified 27,461 classes that participate in design patterns and compared the reusability of each of these classes with the reusability of the pattern and the package that this class belongs to. In more than 40{\%} of the cases investigated, design pattern based class selection, offers the most reusable starting point for white-box reuse. However there are several cases when package based selection might be preferable. The results suggest that each pattern has different level of reusability. },
	Doi = {https://doi.org/10.1016/j.jss.2011.06.047},
	ISSN = {0164-1212},
	Keywords = {Design,Design patterns,Empirical approach,Quality,Reusability},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121211001592}
}

@Article{Ampatzoglou2010888,
	Title = {{Software engineering research for computer games: A systematic review}},
	Author = {Ampatzoglou, Apostolos and Stamelos, Ioannis},
	Journal = {Information and Software Technology},
	Year = {2010},
	Number = {9},
	Pages = {888--901},
	Volume = {52},
	Abstract = {Context Currently, computer game development is one of the fastest growing industries in the worldwide economy. In addition to that, computer games are rapidly evolving in the sense that newer game versions arrive in a very short interval. Thus, software engineering techniques are needed for game development in order to achieve greater flexibility and maintainability, less cost and effort, better design, etc. In addition, games present several characteristics that differentiate their development from classical software development. Objective This study aims to assess the state of the art on research concerning software engineering for computer games and discuss possible important areas for future research. Method We employed a standard methodology for systematic literature reviews using four well known digital libraries. Results Software engineering for computer games is a research domain that has doubled its research activity during the last 5 years. The dominant research topic has proven to be requirements engineering, while topics such as software verification and maintenance have been neglected up to now. Conclusion The results of the study suggest that software engineering for computer games is a field that embraces many techniques and methods from conventional software engineering and adapts them so as to fit the specific requirements of game development. In addition to that, the study proposes the employment of more elaborate empirical methods, i.e. controlled experiments and case studies, in game software engineering research, which, have not been extensively used up to now. },
	Doi = {https://doi.org/10.1016/j.infsof.2010.05.004},
	ISSN = {0950-5849},
	Keywords = {Computer games,Software engineering,Systematic review},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584910000820}
}

@Article{Anand20131978,
	Title = {{An orchestrated survey of methodologies for automated software test case generation}},
	Author = {Anand, Saswat and Burke, Edmund K and Chen, Tsong Yueh and Clark, John and Cohen, Myra B and Grieskamp, Wolfgang and Harman, Mark and Harrold, Mary Jean and McMinn, Phil and Bertolino, Antonia and Li, J Jenny and Zhu, Hong},
	Journal = {Journal of Systems and Software},
	Year = {2013},
	Number = {8},
	Pages = {1978--2001},
	Volume = {86},
	Abstract = {Abstract Test case generation is among the most labour-intensive tasks in software testing. It also has a strong impact on the effectiveness and efficiency of software testing. For these reasons, it has been one of the most active research topics in software testing for several decades, resulting in many different approaches and tools. This paper presents an orchestrated survey of the most prominent techniques for automatic generation of software test cases, reviewed in self-standing sections. The techniques presented include: (a) structural testing using symbolic execution, (b) model-based testing, (c) combinatorial testing, (d) random testing and its variant of adaptive random testing, and (e) search-based testing. Each section is contributed by world-renowned active researchers on the technique, and briefly covers the basic ideas underlying the method, the current state of the art, a discussion of the open research problems, and a perspective of the future development of the approach. As a whole, the paper aims at giving an introductory, up-to-date and (relatively) short overview of research in automatic test case generation, while ensuring a comprehensive and authoritative treatment. },
	Doi = {https://doi.org/10.1016/j.jss.2013.02.061},
	ISSN = {0164-1212},
	Keywords = {Adaptive random testing,Combinatorial testing,Model-based testing,Orchestrated survey,Search-based software testing,Software testing,Symbolic execution,Test automation,Test case generation},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121213000563}
}

@Article{Anastasopoulos2004141,
	Title = {{An evaluation of aspect-oriented programming as a product line implementation technology}},
	Author = {Anastasopoulos, M and Muthig, D},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2004},
	Pages = {141--156},
	Volume = {3107},
	Abstract = {A systematic approach for implementing software product lines is more than just a selection of techniques. Its selection should be based on a systematic analysis of technical requirements and constraints, as well as of the types of variabilities, which occur in a particular application domain and are relevant for the planned product line (PL). In addition, each technique should provide a set of guidelines and criteria that support developers in applying the techniques in a systematic and unified way. This paper presents a case study that was performed to evaluate aspect-oriented programming (AOP) as a PL implementation technology. The systematical evaluation is organized along a general evaluation schema for PL implementation technologies. {\textcopyright} Springer-Verlag 2004.},
	Annote = {cited By 19},
	Keywords = {Aspect oriented programming,Aspect-Oriented Programming (AOP); Product-lines;,Computer software reusability},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947708260{\&}partnerID=40{\&}md5=b158dc68c45534fe5851c245e48ff223}
}

@Article{Anaya201099,
	Title = {{The Unified Enterprise Modelling Language—Overview and further work}},
	Author = {Anaya, Victor and Berio, Giuseppe and Harzallah, Mounira and Heymans, Patrick and Matulevi{\v{c}}ius, Raimundas and Opdahl, Andreas L and Panetto, Herv{\'{e}} and Verdecho, Maria Jose},
	Journal = {Computers in Industry},
	Year = {2010},
	Number = {2},
	Pages = {99--111},
	Volume = {61},
	Abstract = {The Unified Enterprise Modelling Language (UEML) aims at supporting integrated use of enterprise and {\{}IS{\}} models expressed using different languages. To achieve this aim, {\{}UEML{\}} offers a hub through which modelling languages can be connected, thereby paving the way for also connecting the models expressed in those languages. This paper motivates and presents the most central parts of the {\{}UEML{\}} approach: a structured path to describing enterprise and {\{}IS{\}} modelling constructs; a common ontology to interrelate construct descriptions at the semantic level; a correspondence analysis approach to estimate semantic construct similarity; a quality framework to aid selection of languages; a meta-meta model to integrate the different parts of the approach; and a set of tools to aid its use and evolution. The paper also discusses the benefits of {\{}UEML{\}} and points to paths for further work. },
	Annote = {Integration and Information in Networked Enterprises},
	Doi = {https://doi.org/10.1016/j.compind.2009.10.013},
	ISSN = {0166-3615},
	Keywords = {Bunge–Wand–Weber Model,Enterprise modelling,Information Systems Modelling,Interoperability,Modelling Languages,OWL,Ontology,Unified Enterprise Modelling Language (UEML)},
	Url = {http://www.sciencedirect.com/science/article/pii/S0166361509002061}
}

@Article{Andersson2013595,
	Title = {{Experience from model and software reuse in aircraft simulator product line engineering}},
	Author = {Andersson, Henric and Herzog, Erik and {\"{O}}lvander, Johan},
	Journal = {Information and Software Technology},
	Year = {2013},
	Number = {3},
	Pages = {595--606},
	Volume = {55},
	Abstract = {Context “Reuse? and “Model Based Development? are two prominent trends for improving industrial development efficiency. Product lines are used to reduce the time to create product variants by reusing components. The model based approach provides the opportunity to enhance knowledge capture for a system in the early stages in order to be reused throughout its lifecycle. This paper describes how these two trends are combined to support development and support of a simulator product line for the {\{}SAAB{\}} 39 Gripen fighter aircraft. Objective The work aims at improving the support (in terms of efficiency and quality) when creating simulation model configurations. Software based simulators are flexible so variants and versions of included models may easily be exchanged. The objective is to increase the reuse when combining models for usage in a range of development and training simulators. Method The research has been conducted with an interactive approach using prototyping and demonstrations, and the evaluation is based on an iterative and a retrospective method. Results A product line of simulator models for the {\{}SAAB{\}} 39 Gripen aircraft has been analyzed and defined in a Product Variant Master. A configurator system has been implemented for creation, integration, and customization of stringent simulator model configurations. The system is currently under incorporation in the standard development process at {\{}SAAB{\}} Aeronautics. Conclusion The explicit and visual description of products and their variability through a configurator system enables better insights and a common understanding so that collaboration on possible product configurations improves and the potential of software reuse increases. The combination of application fields imposes constraints on how traditional tools and methods may be utilized. Solutions for Design Automation and Knowledge Based Engineering are available, but their application has limitations for Software Product Line engineering and the reuse of simulation models. },
	Annote = {Special Issue on Software Reuse and Product LinesSpecial Issue on Software Reuse and Product Lines},
	Doi = {https://doi.org/10.1016/j.infsof.2012.06.014},
	ISSN = {0950-5849},
	Keywords = {Configurator,Knowledge Based Engineering,Model Based Development,PDM,SPL,Software Product Line},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584912001280}
}

@Conference{Andersson200515,
	Title = {{Development and use of dynamic product-line architectures}},
	Author = {Andersson, J and Bosch, J},
	Booktitle = {IEE Proceedings: Software},
	Year = {2005},
	Number = {1},
	Pages = {15--28},
	Volume = {152},
	Abstract = {Software product families are used to shorten time-to-market, improve quality and lower cost, by means of effective reuse. The paper presents the results of case study conducted at four Swedish companies that are involved in either the development of or development with a software product family. Several issues, such as inter-organisational development of platforms, platforms that employ dynamic reconfiguration, and platforms as a vehicle to achieve certain quality attributes, are identified and discussed. Issues are analysed, and it is demonstrated how these can be deduced as shortcomings in scoping and variability management for nonfunctional quality attributes and dynamic architectures. {\textcopyright} IEE, 2004.},
	Annote = {cited By 8},
	Doi = {10.1049/ip-sen:20041007},
	Keywords = {Architectural design; Object oriented programming;,Platform; Software architecture (SA); Software pr,Software engineering},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-14844341296{\&}doi=10.1049{\%}2Fip-sen{\%}3A20041007{\&}partnerID=40{\&}md5=6d01eb963507944f1d8f401542be5a04}
}

@Article{Andres20131925,
	Title = {{A formal framework for software product lines}},
	Author = {Andr{\'{e}}s, C{\'{e}}sar and Camacho, Carlos and Llana, Luis},
	Journal = {Information and Software Technology},
	Year = {2013},
	Number = {11},
	Pages = {1925--1947},
	Volume = {55},
	Abstract = {AbstractContext A Software Product Line is a set of software systems that are built from a common set of features. These systems are developed in a prescribed way and they can be adapted to fit the needs of customers. Feature models specify the properties of the systems that are meaningful to customers. A semantics that models the feature level has the potential to support the automatic analysis of entire software product lines. Objective The objective of this paper is to define a formal framework for Software Product Lines. This framework needs to be general enough to provide a formal semantics for existing frameworks like {\{}FODA{\}} (Feature Oriented Domain Analysis), but also to be easily adaptable to new problems. Method We define an algebraic language, called SPLA, to describe Software Product Lines. We provide the semantics for the algebra in three different ways. The approach followed to give the semantics is inspired by the semantics of process algebras. First we define an operational semantics, next a denotational semantics, and finally an axiomatic semantics. We also have defined a representation of the algebra into propositional logic. Results We prove that the three semantics are equivalent. We also show how {\{}FODA{\}} diagrams can be automatically translated into SPLA. Furthermore, we have developed our tool, called AT, that implements the formal framework presented in this paper. This tool uses a SAT-solver to check the satisfiability of an SPL. Conclusion This paper defines a general formal framework for software product lines. We have defined three different semantics that are equivalent; this means that depending on the context we can choose the most convenient approach: operational, denotational or axiomatic. The framework is flexible enough because it is closely related to process algebras. Process algebras are a well-known paradigm for which many extensions have been defined. },
	Doi = {https://doi.org/10.1016/j.infsof.2013.05.005},
	ISSN = {0950-5849},
	Keywords = {Feature models,Formal methods,Software product lines},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584913001262}
}

@Article{Angelov2012417,
	Title = {{A framework for analysis and design of software reference architectures}},
	Author = {Angelov, Samuil and Grefen, Paul and Greefhorst, Danny},
	Journal = {Information and Software Technology},
	Year = {2012},
	Number = {4},
	Pages = {417--431},
	Volume = {54},
	Abstract = {Context A software reference architecture is a generic architecture for a class of systems that is used as a foundation for the design of concrete architectures from this class. The generic nature of reference architectures leads to a less defined architecture design and application contexts, which makes the architecture goal definition and architecture design non-trivial steps, rooted in uncertainty. Objective The paper presents a structured and comprehensive study on the congruence between context, goals, and design of software reference architectures. It proposes a tool for the design of congruent reference architectures and for the analysis of the level of congruence of existing reference architectures. Method We define a framework for congruent reference architectures. The framework is based on state of the art results from literature and practice. We validate our framework and its quality as analytical tool by applying it for the analysis of 24 reference architectures. The conclusions from our analysis are compared to the opinions of experts on these reference architectures documented in literature and dedicated communication. Results Our framework consists of a multi-dimensional classification space and of five types of reference architectures that are formed by combining specific values from the multi-dimensional classification space. Reference architectures that can be classified in one of these types have better chances to become a success. The validation of our framework confirms its quality as a tool for the analysis of the congruence of software reference architectures. Conclusion This paper facilitates software architects and scientists in the inception, design, and application of congruent software reference architectures. The application of the tool improves the chance for success of a reference architecture. },
	Doi = {https://doi.org/10.1016/j.infsof.2011.11.009},
	ISSN = {0950-5849},
	Keywords = {Software architecture design,Software domain architecture,Software product line architecture,Software reference architecture},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584911002333}
}

@Article{vanAngeren2016430,
	Title = {{Can we ask you to collaborate? Analyzing app developer relationships in commercial platform ecosystems}},
	Author = {van Angeren, Joey and Alves, Carina and Jansen, Slinger},
	Journal = {Journal of Systems and Software},
	Year = {2016},
	Pages = {430--445},
	Volume = {113},
	Abstract = {Abstract Previous studies have emphasized the necessity for software platform owners to govern their platform ecosystem in order to create durable opportunities for themselves and the app developers that surround the platform. To date, platform ecosystems have been widely analyzed from the perspective of platform owners. However, how and to what extent app developers collaborate with their peers needs to be investigated further. In this article, we study the interfirm relationships among app developers in commercial platform ecosystems and explore the causes of variation in the network structure of these ecosystems. By means of a comparative study of four commercial platform ecosystems of Google (Google Apps and Google Chrome) and Microsoft (Microsoft Office365 and Internet Explorer), we illustrate substantial variation in the extent to which app developers initiated interfirm relationships. Further, we analyze how the degree of enforced entry barriers to the app store, the use of a partnership model, and the domain of the software platform that underpins the ecosystem affect the properties of these commercial platform ecosystems. We present subsequent explanations as a set of propositions that can be tested in future empirical research. },
	Doi = {https://doi.org/10.1016/j.jss.2015.11.025},
	ISSN = {0164-1212},
	Keywords = {Case study,Interfirm network analysis,Software ecosystem},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121215002502}
}

@Article{Anjorin2013441,
	Title = {{Model-driven rapid prototyping with programmed graph transformations}},
	Author = {Anjorin, A and Saller, K and Reimund, I and Oster, S and Zorcic, I and Sch{\"{u}}rr, A},
	Journal = {Journal of Visual Languages and Computing},
	Year = {2013},
	Number = {6},
	Pages = {441--462},
	Volume = {24},
	Abstract = {Modern software systems are constantly increasing in complexity and supporting the rapid prototyping of such systems has become crucial to check the feasibility of extensions and optimizations, thereby reducing risks and, consequently, the cost of development. As modern software systems are also expected to be reused, extended, and adapted over a much longer lifetime than ever before, ensuring the maintainability of such systems is equally gaining relevance. In this paper, we present the development, optimization and maintenance of MoSo-PoLiTe, a framework for Software Product Line (SPL) testing, as a novel case study for rapid prototyping via metamodelling and programmed graph transformations. The first part of the case study evaluates the use of programmed graph transformations for optimizing an existing, hand-written system (MoSo-PoLiTe) via rapid prototyping of various strategies. In the second part, we present a complete re-engineering of the hand-written system with programmed graph transformations and provide a critical comparison of both implementations.Our results and conclusions indicate that metamodelling and programmed graph transformation are not only suitable techniques for rapid prototyping, but also lead to more maintainable systems. {\textcopyright} 2013 Elsevier Ltd.},
	Annote = {cited By 1},
	Doi = {10.1016/j.jvlc.2013.08.001},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888819910{\&}doi=10.1016{\%}2Fj.jvlc.2013.08.001{\&}partnerID=40{\&}md5=67b44619de5e40fa303fed2e7e82fe10}
}

@Article{Apel20094,
	Title = {{Model superimposition in software product lines}},
	Author = {Apel, S and Janda, F and Trujillo, S and K{\"{a}}stner, C},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2009},
	Pages = {4--19},
	Volume = {5563 LNCS},
	Abstract = {In software product line engineering, feature composition generates software tailored to specific requirements from a common set of artifacts. Superimposition is a technique to merge code pieces belonging to different features. The advent of model-driven development raises the question of how to support the variability of software product lines in modeling techniques. We propose to use superimposition as a model composition technique in order to support variability. We analyze the feasibility of superimposition for model composition, offer corresponding tool support, and discuss our experiences with three case studies (including an industrial case study). {\textcopyright} 2009 Springer Berlin Heidelberg.},
	Annote = {cited By 38},
	Doi = {10.1007/978-3-642-02408-5_2},
	Keywords = {Computer software,Industrial case study; Model composition; Model dr,Network architecture; Production engineering; Res},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349538082{\&}doi=10.1007{\%}2F978-3-642-02408-5{\_}2{\&}partnerID=40{\&}md5=62082203ab17f8f5a582a05aaab53238}
}

@Article{Apel2012174,
	Title = {{Access control in feature-oriented programming}},
	Author = {Apel, Sven and Kolesnikov, Sergiy and Liebig, J{\"{o}}rg and K{\"{a}}stner, Christian and Kuhlemann, Martin and Leich, Thomas},
	Journal = {Science of Computer Programming},
	Year = {2012},
	Number = {3},
	Pages = {174--187},
	Volume = {77},
	Abstract = {In feature-oriented programming (FOP) a programmer decomposes a program in terms of features. Ideally, features are implemented modularly so that they can be developed in isolation. Access control mechanisms in the form of access or visibility modifiers are an important ingredient to attain feature modularity as they allow programmers to hide and expose internal details of a module's implementation. But developers of contemporary feature-oriented languages have not considered access control mechanisms so far. The absence of a well-defined access control model for {\{}FOP{\}} breaks encapsulation of feature code and leads to unexpected program behaviors and inadvertent type errors. We raise awareness of this problem, propose three feature-oriented access modifiers, and present a corresponding access modifier model. We offer an implementation of the model on the basis of a fully-fledged feature-oriented compiler. Finally, by analyzing ten feature-oriented programs, we explore the potential of feature-oriented modifiers in FOP. },
	Annote = {Feature-Oriented Software Development (FOSD 2009)},
	Doi = {https://doi.org/10.1016/j.scico.2010.07.005},
	ISSN = {0167-6423},
	Keywords = {Access control,Access modifier model,Feature modularity,Feature-oriented programming,Fuji},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642310001528}
}

@Article{Apel2008162,
	Title = {{Aspectual feature modules}},
	Author = {Apel, S. and Leich, T. and Saake, G.},
	Journal = {IEEE Transactions on Software Engineering},
	Year = {2008},
	Month = {mar},
	Number = {2},
	Pages = {162--180},
	Volume = {34},
	Abstract = {Two programming paradigms are gaining attention in the overlapping fields of software product lines (SPLs) and incremental software development (ISD). Feature-oriented programming (FOP) aims at large-scale compositional programming and feature modularity in SPLs using ISD. Aspect-oriented programming (AOP) focuses on the modularization of crosscutting concerns in complex software. While feature modules, the main abstraction mechanisms of FOP, perform well in implementing large-scale software building blocks, they are incapable of modularizing certain kinds of crosscutting concerns. This weakness is exactly the strength of aspects, the main abstraction mechanisms of AOP. In this article we contribute a systematic evaluation and comparison of FOP and AOP. It reveals that aspects and feature modules are complementary techniques. Consequently, we propose the symbiosis of FOP and AOP and aspectual feature modules (AFMs), a programming technique that integrates feature modules and aspects. We provide a set of tools that support implementing AFMs on top of Java and C++. We apply AFMs to a non-trivial case study demonstrating their practical applicability and to justify our design choices. {\textcopyright} 2008 IEEE.},
	Annote = {From Duplicate 1 (Aspectual feature modules - Apel, S; Leich, T; Saake, G)
		cited By 107},
	Doi = {10.1109/TSE.2007.70770},
	File = {:Users/mac/Downloads/bulk-download (8)/Aspectual Feature Modules.pdf:pdf},
	ISSN = {0098-5589},
	Keywords = {,Aspect-oriented programming,C,Collaboration-based,Computer program listings,Computer programming,Software engineering},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-42549143913{\&}doi=10.1109{\%}2FTSE.2007.70770{\&}partnerID=40{\&}md5=ca2f4187fd47f2bb362fb20b2d669b4b http://ieeexplore.ieee.org/document/4407729/}
}

@Article{Apel20132399,
	Title = {{Feature-interaction detection based on feature-based specifications}},
	Author = {Apel, Sven and von Rhein, Alexander and Th{\"{u}}m, Thomas and K{\"{a}}stner, Christian},
	Journal = {Computer Networks},
	Year = {2013},
	Number = {12},
	Pages = {2399--2409},
	Volume = {57},
	Abstract = {Abstract Formal specification and verification techniques have been used successfully to detect feature interactions. We investigate whether feature-based specifications can be used for this task. Feature-based specifications are a special class of specifications that aim at modularity in open-world, feature-oriented systems. The question we address is whether modularity of specifications impairs the ability to detect feature interactions, which cut across feature boundaries. In an exploratory study on 10 feature-oriented systems, we found that the majority of feature interactions could be detected based on feature-based specifications, but some specifications have not been modularized properly and require undesirable workarounds to modularization. Based on the study, we discuss the merits and limitations of feature-based specifications, as well as open issues and perspectives. A goal that underlies our work is to raise awareness of the importance and challenges of feature-based specification. },
	Annote = {Feature Interaction in Communications and Software Systems},
	Doi = {https://doi.org/10.1016/j.comnet.2013.02.025},
	ISSN = {1389-1286},
	Keywords = {Feature interaction,Feature orientation,Feature-based specification,Modularity,Software product lines},
	Url = {http://www.sciencedirect.com/science/article/pii/S1389128613001102}
}

@Article{April200973,
	Title = {{A Software Maintenance Maturity Model (S3M): Measurement Practices at Maturity Levels 3 and 4}},
	Author = {April, Alain and Abran, Alain},
	Journal = {Electronic Notes in Theoretical Computer Science},
	Year = {2009},
	Pages = {73--87},
	Volume = {233},
	Abstract = {Evaluation and continuous improvement of software maintenance are key contributors to improving software quality. The software maintenance function suffers from a scarcity of the management models that would facilitate these functions. This paper presents an overview of the measurement practices that are being introduced for level 3 and higher to the Software Maintenance Maturity Model (S3M). },
	Annote = {Proceedings of the International Workshop on Software Quality and Maintainability (SQM 2008)},
	Doi = {https://doi.org/10.1016/j.entcs.2009.02.062},
	ISSN = {1571-0661},
	Keywords = {Maturity Model,Process Assessment,Process Improvement,Product Assessment,Software Maintenance},
	Url = {http://www.sciencedirect.com/science/article/pii/S157106610900067X}
}

@InProceedings{Aquino:2010:UEM:1852786.1852826,
	Title = {{Usability Evaluation of Multi-device/Platform User Interfaces Generated by Model-driven Engineering}},
	Author = {Aquino, Nathalie and Vanderdonckt, Jean and Condori-Fern{\'{a}}ndez, Nelly and Dieste, {\'{O}}scar and Pastor, {\'{O}}scar},
	Booktitle = {Proceedings of the 2010 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
	Year = {2010},
	Address = {New York, NY, USA},
	Pages = {30:1----30:10},
	Publisher = {ACM},
	Series = {ESEM '10},
	Doi = {10.1145/1852786.1852826},
	ISBN = {978-1-4503-0039-1},
	Keywords = {effectiveness,efficiency,interaction with small and large screens,model-driven engineering,multi-device interface,multi-platform interface,satisfaction,usability evaluation},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/1852786.1852826}
}

@Article{Arcaini201752,
	Title = {{A novel use of equivalent mutants for static anomaly detection in software artifacts}},
	Author = {Arcaini, Paolo and Gargantini, Angelo and Riccobene, Elvinia and Vavassori, Paolo},
	Journal = {Information and Software Technology},
	Year = {2017},
	Pages = {52--64},
	Volume = {81},
	Abstract = {Abstract Context: In mutation analysis, a mutant of a software artifact, either a program or a model, is said equivalent if it leaves the artifact meaning unchanged. Equivalent mutants are usually seen as an inconvenience and they reduce the applicability of mutation analysis. Objective: Instead, we here claim that equivalent mutants can be useful to define, detect, and remove static anomalies, i.e., deficiencies of given qualities: If an equivalent mutant has a better quality value than the original artifact, then an anomaly has been found and removed. Method: We present a process for detecting static anomalies based on mutation, equivalence checking, and quality measurement. Results: Our proposal and the originating technique are applicable to different kinds of software artifacts. We present anomalies and conduct several experiments in different contexts, at specification, design, and implementation level. Conclusion: We claim that in mutation analysis a new research direction should be followed, in which equivalent mutants and operators generating them are welcome. },
	Doi = {https://doi.org/10.1016/j.infsof.2016.01.019},
	ISSN = {0950-5849},
	Keywords = {Equivalent mutant,Quality measure,Static anomaly},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584916300180}
}

@InProceedings{Arcega2016,
	Title = {{Achieving Knowledge Evolution in Dynamic Software Product Lines}},
	Author = {Arcega, Lorena and Font, Jaime and Haugen, Oystein and Cetina, Carlos},
	Booktitle = {2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering (SANER)},
	Year = {2016},
	Month = {mar},
	Pages = {505--516},
	Publisher = {IEEE},
	Doi = {10.1109/SANER.2016.24},
	ISBN = {978-1-5090-1855-0},
	Url = {http://ieeexplore.ieee.org/document/7476670/}
}

@Article{Ardis2000825,
	Title = {{Software product lines: A case study}},
	Author = {Ardis, M and Daley, N and Hoffman, D and Siy, H and Weiss, D},
	Journal = {Software - Practice and Experience},
	Year = {2000},
	Number = {7},
	Pages = {825--847},
	Volume = {30},
	Abstract = {A software product line is a family of products that share common features to meet the needs of a market area. Systematic processes have been developed to dramatically reduce the cost of a product line. Such product-line engineering processes have proven practical and effective in industrial use, but are not widely understood. The Family-Oriented Abstraction, Specification and Translation (FAST) process has been used successfully at Lucent Technologies in over 25 domains, providing productivity improvements of as much as four to one. In this paper, we show how to use FAST to document precisely the key abstractions in a domain, exploit design patterns in a generic product-line architecture, generate documentation and Java code, and automate testing to reduce costs. The paper is based on a detailed case study covering all aspects from domain analysis through testing.},
	Annote = {cited By 33},
	Doi = {10.1002/(SICI)1097-024X(200006)30:7<825::AID-SPE322>3.0.CO;2-1},
	Keywords = {Computer architecture; Computer software selection,Domain engineering; Software product lines,Software engineering},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033688589{\&}doi=10.1002{\%}2F{\%}28SICI{\%}291097-024X{\%}28200006{\%}2930{\%}3A7{\%}3C825{\%}3A{\%}3AAID-SPE322{\%}3E3.0.CO{\%}3B2-1{\&}partnerID=40{\&}md5=9146e51ed24f7399c09731f8d034fbf4}
}

@Article{SMR:SMR1790,
	Title = {{Pragmatic source code reuse via execution record and replay}},
	Author = {Armaly, Ameer and McMillan, Collin},
	Journal = {Journal of Software: Evolution and Process},
	Year = {2016},
	Number = {8},
	Pages = {642--664},
	Volume = {28},
	Abstract = {A key problem during copy–paste source code reuse is that, to reuse even a small section of code from a program as opposed to an API, a programmer must include a huge amount of additional source code from elsewhere in the same program. This additional code is notoriously large and complex, and portions can only be identified at runtime. In this paper, we propose execution record/replay as a solution to this problem. We describe a novel reuse technique that allows programmers to reuse functions from a C or C++ program, by recording the execution of the program and selectively modifying how its functions are replayed. We have implemented our technique and evaluated it in an empirical study in which eight programmers used our tool to complete four tasks over four hours each. The participants found our technique to be easier than manually reusing the code as part of their project. We also found that the resulting code was smaller and less complex than it would have been had the participants manually reused the code. Copyright {\textcopyright} 2016 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/smr.1790},
	ISSN = {2047-7481},
	Keywords = {execution record and replay,reusable execution,source code reuse},
	Url = {http://dx.doi.org/10.1002/smr.1790}
}

@Article{SPIP:SPIP412,
	Title = {{Scoping software process lines}},
	Author = {Armbrust, Ove and Katahira, Masafumi and Miyamoto, Yuko and M{\"{u}}nch, J{\"{u}}rgen and Nakao, Haruka and Ocampo, Alexis},
	Journal = {Software Process: Improvement and Practice},
	Year = {2009},
	Number = {3},
	Pages = {181--197},
	Volume = {14},
	Abstract = {Defining organization-specific process standards by integrating, harmonizing, and standardizing heterogeneous and often implicit processes is an important task, especially for large development organizations. On the one hand, such a standard must be generic enough to cover all of the organization's development activities; on the other hand, it must be as detailed and precise as possible to support employees' daily work. Today, organizations typically maintain and advance a plethora of individual processes, each addressing specific problems. This requires enormous effort, which could be spent more efficiently. This article introduces an approach for developing a Software Process Line that, similar to a Software Product Line, promises to reduce the complexity and thus, the effort required for managing the processes of a software organization. We propose Scoping, Modeling, and Architecting the Software Process Line as major steps, and describe in detail the Scoping approach we recommend, based on an analysis of the potential products to be produced in the future, the projects expected in the future, and the respective process capabilities needed. In addition, the article sketches experience from determining the scope of space process standards for satellite software development. Finally, it discusses the approach, draws conclusions, and gives an outlook on future work. Copyright {\textcopyright} 2009 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spip.412},
	ISSN = {1099-1670},
	Keywords = {process analysis,process selection,scoping,software process line,software product line},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/spip.412}
}

@Article{SMR:SMR1598,
	Title = {{Predicting dependences using domain-based coupling}},
	Author = {Aryani, Amir and Perin, Fabrizio and Lungu, Mircea and Mahmood, Abdun Naser and Nierstrasz, Oscar},
	Journal = {Journal of Software: Evolution and Process},
	Year = {2014},
	Number = {1},
	Pages = {50--76},
	Volume = {26},
	Abstract = {Software dependences play a vital role in programme comprehension, change impact analysis and other software maintenance activities. Traditionally, these activities are supported by source code analysis; however, the source code is sometimes inaccessible or difficult to analyse, as in hybrid systems composed of source code in multiple languages using various paradigms (e.g. object-oriented programming and relational databases). Moreover, not all stakeholders have adequate knowledge to perform such analyses. For example, non-technical domain experts and consultants raise most maintenance requests; however, they cannot predict the cost and impact of the requested changes without the support of the developers. We propose a novel approach to predicting software dependences by exploiting the coupling present in domain-level information. Our approach is independent of the software implementation; hence, it can be used to approximate architectural dependences without access to the source code or the database. As such, it can be applied to hybrid systems with heterogeneous source code or legacy systems with missing source code. In addition, this approach is based solely on information visible and understandable to domain users; therefore, it can be efficiently used by domain experts without the support of software developers. We evaluate our approach with a case study on a large-scale enterprise system, in which we demonstrate how up to 65{\%} of the source code dependences and 77{\%} of the database dependences are predicted solely based on domain information. Copyright {\textcopyright} 2013 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/smr.1598},
	ISSN = {2047-7481},
	Keywords = {architectural dependences,database dependences,domain-based coupling,programme comprehension,source code analysis},
	Url = {http://dx.doi.org/10.1002/smr.1598}
}

@Article{Asadi2016257,
	Title = {{Goal-oriented modeling and verification of feature-oriented product lines}},
	Author = {Asadi, M and Gr{\"{o}}ner, G and Mohabbati, B and Ga{\v{s}}evi{\'{c}}, D},
	Journal = {Software and Systems Modeling},
	Year = {2016},
	Number = {1},
	Pages = {257--279},
	Volume = {15},
	Abstract = {Goal models represent requirements and intentions of a software system. They play an important role in the development life cycle of software product lines (SPLs). In the domain engineering phase, goal models guide the development of variability in SPLs by providing the rationale for the variability, while they are used for the configuration of SPLs in the application engineering phase. However, variability in SPLs, which is represented by feature models, usually has design and implementation-induced constraints. When those constraints are not aligned with variability in goal models, the configuration with goal models becomes error prone. To remedy this problem, we propose a description logic (DL)-based approach to represent both models and their relations in a common DL knowledge base. Moreover, we apply reasoning to detect inconsistencies in the variability of goal and feature models. A formal proof is provided to demonstrate the correctness of the reasoning approach. An empirical evaluation shows computational tractability of the inconsistency detection. {\textcopyright} 2014, Springer-Verlag Berlin Heidelberg.},
	Annote = {cited By 3},
	Doi = {10.1007/s10270-014-0402-8},
	Keywords = {Application engineering; Computational tractabili,Computation theory; Computer circuits; Computer so,Verification},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956612126{\&}doi=10.1007{\%}2Fs10270-014-0402-8{\&}partnerID=40{\&}md5=f9e90df907b717cae8579d320572f8ae}
}

@Article{Asadi201473,
	Title = {{Development and validation of customized process models}},
	Author = {Asadi, Mohsen and Mohabbati, Bardia and Gr{\"{o}}ner, Gerd and Gasevic, Dragan},
	Journal = {Journal of Systems and Software},
	Year = {2014},
	Pages = {73--92},
	Volume = {96},
	Abstract = {Abstract Configurable reference process models encompass common and variable processes of organizations from different business domains. These reference process models are designed and reused to guide and derive customized business processes according to the requirements of stakeholders. The customization process is generally initiated by a configuration step, selecting a subset of the reference process model. Configuration is followed by a customization step, which assumes adapting or extending the configured business process based on the specific or unforeseen requirements. Hence, it is crucial to validate the correctness and compliance of the final customized business process with respect to the patterns and business constraints that are specified in the reference model. In this paper, we firstly introduce a technique to develop a customized process model and then present a set of identified inconsistency patterns that may happen during the configuration of a reference model and the customization of configured process models. Furthermore, we describe our proposed approach including formal representations and algorithms that provide logical reasoning and enable automatic inconsistency detection by leveraging description logic. In order to explore the scalability of the approach, we designed the experiments with various process models sizes and inconsistency distributions. The results of the experiments revealed the scalability of our approach with large size process models (500 activities). },
	Doi = {https://doi.org/10.1016/j.jss.2014.05.063},
	ISSN = {0164-1212},
	Keywords = {Description logics,Feature models,Reference process models},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121214001344}
}

@Article{Asadi20161706,
	Title = {{The effects of visualization and interaction techniques on feature model configuration}},
	Author = {Asadi, M and Soltani, S and Ga{\v{s}}evi{\'{c}}, D and Hatala, M},
	Journal = {Empirical Software Engineering},
	Year = {2016},
	Number = {4},
	Pages = {1706--1743},
	Volume = {21},
	Abstract = {A Software Product Line is a set of software systems of a domain, which share some common features but also have significant variability. A feature model is a variability modeling artifact which represents differences among software products with respect to variability relationships among their features. Having a feature model along with a reference model developed in the domain engineering lifecycle, a concrete product of the family is derived by selecting features in the feature model (referred to as the configuration process) and by instantiating the reference model. However, feature model configuration can be a cumbersome task because: 1) feature models may consist of a large number of features, which are hard to comprehend and maintain; and 2) many factors including technical limitations, implementation costs, stakeholders' requirements and expectations must be considered in the configuration process. Recognizing these issues, a significant amount of research efforts has been dedicated to different aspects of feature model configuration such as automating the configuration process. Several approaches have been proposed to alleviate the feature model configuration challenges through applying visualization and interaction techniques. However, there have been limited empirical insights available into the impact of visualization and interaction techniques on the feature model configuration process. In this paper, we present a set of visualization and interaction interventions for representing and configuring feature models, which are then empirically validated to measure the impact of the proposed interventions. An empirical study was conducted by following the principles of control experiments in software engineering and by applying the well-known software quality standard ISO 9126 to operationalize the variables investigated in the experiment. The results of the empirical study revealed that the employed visualization and interaction interventions significantly improved completion time of comprehension and changing of the feature model configuration. Additionally, according to results, the proposed interventions are easy-to-use and easy-to-learn for the participants. {\textcopyright} 2015, Springer Science+Business Media New York.},
	Annote = {cited By 0},
	Doi = {10.1007/s10664-014-9353-5},
	Keywords = {Computer software; Computer software selection and,Configuration process; Controlled experiment; Imp,Visualization},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925275030{\&}doi=10.1007{\%}2Fs10664-014-9353-5{\&}partnerID=40{\&}md5=985f277899e4fd1ea4af6e2b70d33f32}
}

@Article{Asadi20141144,
	Title = {{Toward automated feature model configuration with optimizing non-functional requirements}},
	Author = {Asadi, Mohsen and Soltani, Samaneh and Gasevic, Dragan and Hatala, Marek and Bagheri, Ebrahim},
	Journal = {Information and Software Technology},
	Year = {2014},
	Number = {9},
	Pages = {1144--1165},
	Volume = {56},
	Abstract = {AbstractContext A software product line is a family of software systems that share some common features but also have significant variabilities. A feature model is a variability modeling artifact, which represents differences among software products with respect to the variability relationships among their features. Having a feature model along with a reference model developed in the domain engineering lifecycle, a concrete product of the family is derived by binding the variation points in the feature model (called configuration process) and by instantiating the reference model. Objective In this work we address the feature model configuration problem and propose a framework to automatically select suitable features that satisfy both the functional and non-functional preferences and constraints of stakeholders. Additionally, interdependencies between various non-functional properties are taken into account in the framework. Method The proposed framework combines Analytical Hierarchy Process (AHP) and Fuzzy Cognitive Maps (FCM) to compute the non-functional properties weights based on stakeholders' preferences and interdependencies between non-functional properties. Afterwards, Hierarchical Task Network (HTN) planning is applied to find the optimal feature model configuration. Result Our approach improves state-of-art of feature model configuration by considering positive or negative impacts of the features on non-functional properties, the stakeholders' preferences, and non-functional interdependencies. The approach presented in this paper extends earlier work presented in [1] from several distinct perspectives including mechanisms handling interdependencies between non-functional properties, proposing a novel tooling architecture, and offering visualization and interaction techniques for representing functional and non-functional aspects of feature models. Conclusion our experiments show the scalability of our configuration approach when considering both functional and non-functional requirements of stakeholders. },
	Annote = {Special Sections from “Asia-Pacific Software Engineering Conference (APSEC), 2012? and “ Software Product Line conference (SPLC), 2012?},
	Doi = {https://doi.org/10.1016/j.infsof.2014.03.005},
	ISSN = {0950-5849},
	Keywords = {Feature model configuration,Non-functional interdependencies,Software product lines,Stakeholders' preferences},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584914000640}
}

@Article{IIS2:IIS201434,
	Title = {{Agile Collaborative Systems Engineering -Motivation for a Novel Approach to Systems Engineering}},
	Author = {Asan, Emrah and Bilgen, Semih},
	Journal = {INCOSE International Symposium},
	Year = {2012},
	Number = {1},
	Pages = {1746--1760},
	Volume = {22},
	Abstract = {Well-established systems engineering approaches are becoming more inadequate as today's systems are becoming more complex, more global, more software intensive, more COTS/re-use based and more evolving. The changes in the organization and management of such systems of systems projects make it even more difficult for traditional systems engineering approaches. Increased level of outsourcing, significant amount of subcontractors, more integration than development, reduced project cycles, ecosystem like collaborative developments, software product lines and global development are some of the changes in the project life cycle approaches. In this paper, we gather and synthesize the current literature to highlight the need for an agile systems engineering approach and to characterize its agile properties in order to support our motivation for a novel systems engineering approach for large scale software intensive systems of systems projects.},
	Doi = {10.1002/j.2334-5837.2012.tb01434.x},
	ISSN = {2334-5837},
	Url = {http://dx.doi.org/10.1002/j.2334-5837.2012.tb01434.x}
}

@Article{Assuncao2014119,
	Title = {{A multi-objective optimization approach for the integration and test order problem}},
	Author = {Assun{\c{c}}{\~{a}}o, Wesley Klewerton Guez and Colanzi, Thelma Elita and Vergilio, Silvia Regina and Pozo, Aurora},
	Journal = {Information Sciences},
	Year = {2014},
	Pages = {119--139},
	Volume = {267},
	Abstract = {Abstract A common problem found during the integration testing is to determine an order to integrate and test the units. Important factors related to stubbing costs and constraints regarding to the software development context must be considered. To solve this problem, the most promising results were obtained with multi-objective algorithms, however few algorithms and contexts have been addressed by existing works. Considering such fact, this paper aims at introducing a generic approach based on multi-objective optimization to be applied in different development contexts and with distinct multi-objective algorithms. The approach is instantiated in the object and aspect-oriented contexts, and evaluated with real systems and three algorithms: NSGA-II, {\{}SPEA2{\}} and PAES. The algorithms are compared by using different number of objectives and four quality indicators. Results point out that the characteristics of the systems, the instantiation context and the number of objectives influence on the behavior of the algorithms. Although for more complex systems, {\{}PAES{\}} reaches better results, NSGA-II is more suitable to solve the referred problem in general cases, considering all systems and indicators. },
	Doi = {https://doi.org/10.1016/j.ins.2013.12.040},
	ISSN = {0020-0255},
	Keywords = {Integration testing,Multi-objective optimization,Search-based algorithm},
	Url = {http://www.sciencedirect.com/science/article/pii/S0020025513008967}
}

@Article{IIS2:IIS203012,
	Title = {{9.2.1 Developing Product Lines in Engine Control Systems: Systems Engineering Challenges}},
	Author = {Atherton, Malvern J and Collins, Shawn T},
	Journal = {INCOSE International Symposium},
	Year = {2013},
	Number = {1},
	Pages = {184--198},
	Volume = {23},
	Abstract = {Rolls-Royce is developing a Full Authority Digital Electronic Control (FADEC) product line for helicopter and light turboprop applications. This is driven by market demand to reduce the proportional cost of control systems relative to the engine, and to field applications in timescales that preclude traditional clone-and-own approaches. The goal is to develop reusable control system architectures, requirements, and verification evidence, which can be used on a variety of applications. Key challenges include addressing military and commercial constraints with the same architecture, designing in flexibility for future applications, and leveraging global company capability in processes, tools, and supply chain strategy. This paper describes Rolls-Royce's experience adapting existing software product line principles to apply at the control system (hardware and embedded software) level in light of certification guidelines from DO-178B and ARP4574A. Lessons learned from moving through the “V? life cycle provide insight into how Systems Engineering adds value in this context.},
	Doi = {10.1002/j.2334-5837.2013.tb03012.x},
	ISSN = {2334-5837},
	Url = {http://dx.doi.org/10.1002/j.2334-5837.2013.tb03012.x}
}

@Article{Atkinson2015289,
	Title = {{A multi-level approach to modeling language extension in the Enterprise Systems Domain}},
	Author = {Atkinson, Colin and Gerbig, Ralph and Fritzsche, Mathias},
	Journal = {Information Systems},
	Year = {2015},
	Pages = {289--307},
	Volume = {54},
	Abstract = {Abstract As the number and diversity of technologies involved in building enterprise systems continues to grow so does the importance of modeling tools that are able to present customized views of enterprise systems to different stakeholders according to their needs and skills. Moreover, since the range of required view types is continuously evolving, it must be possible to extend and enhance the languages and services offered by such tools on an ongoing basis. However, this can be difficult with today׳s modeling tools because the meta-models that define the languages, views and services they support are usually hardwired and thus not amenable to extensions. In practice, therefore, various workarounds have to be used to extend a tool׳s underlying meta-model. Some of these are built into the implemented modeling standards (e.g. {\{}UML{\}} 2, {\{}BPMN{\}} 2.0 and ArchiMate 2.0) while others have to be applied by complementary, external tools (e.g. annotation models). These techniques not only increase accidental complexity, they also reduce the ability of the modeling tool to ensure adherence to enterprise rules and constraints. In this paper we discuss the strengths and weaknesses of the various approaches for language extension and propose a modeling framework best able to support the main extension scenarios currently found in practice today. },
	Doi = {https://doi.org/10.1016/j.is.2015.01.003},
	ISSN = {0306-4379},
	Keywords = {Linguistic classification,Model language extension,Multi-level modeling,Ontological classification,Orthogonal classification architecture},
	Url = {http://www.sciencedirect.com/science/article/pii/S0306437915000137}
}

@Article{Autili2013987,
	Title = {{A hybrid approach for resource-based comparison of adaptable Java applications}},
	Author = {Autili, Marco and Benedetto, Paolo Di and Inverardi, Paola},
	Journal = {Science of Computer Programming},
	Year = {2013},
	Number = {8},
	Pages = {987--1009},
	Volume = {78},
	Abstract = {During the last decade, context-awareness and adaptation have been receiving significant attention in many research areas. For application developers, the heterogeneity of resource-constrained mobile terminals creates serious problems for the development of mobile applications able to run properly on a large number of different devices. Thus, resource awareness plays a crucial role when developing such applications. It identifies the capability of being aware of the resources offered by an execution environment, in order to decide whether that environment is suited to receive and execute the application. Within this line of research, we propose Chameleon, a framework that provides both an integrated development environment and a proper context-aware support to adaptable Java applications for limited devices. In this paper we present the novel hybrid (from static to dynamic) analysis approach that Chameleon uses for inspecting (adaptable) Java programs with respect to their resource consumption in a given execution environment. This analysis permits to quantitatively compare alternative versions of the same program. The analysis is based on a resource model for specifying resource provisions and consumptions, and a parametric transition system that performs the actual analysis. },
	Annote = {Special section on software evolution, adaptability, and maintenance {\&} Special section on the Brazilian Symposium on Programming Languages},
	Doi = {https://doi.org/10.1016/j.scico.2012.01.005},
	ISSN = {0167-6423},
	Keywords = {Adaptable applications,Analysis,Tool support},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642312000147}
}

@Article{Axelsson20141457,
	Title = {{Characteristics of software ecosystems for Federated Embedded Systems: A case study}},
	Author = {Axelsson, Jakob and Papatheocharous, Efi and Andersson, Jesper},
	Journal = {Information and Software Technology},
	Year = {2014},
	Number = {11},
	Pages = {1457--1475},
	Volume = {56},
	Abstract = {AbstractContext Traditionally, Embedded Systems (ES) are tightly linked to physical products, and closed both for communication to the surrounding world and to additions or modifications by third parties. New technical solutions are however emerging that allow addition of plug-in software, as well as external communication for both software installation and data exchange. These mechanisms in combination will allow for the construction of Federated Embedded Systems (FES). Expected benefits include the possibility of third-party actors developing add-on functionality; a shorter time to market for new functions; and the ability to upgrade existing products in the field. This will however require not only new technical solutions, but also a transformation of the software ecosystems for ES. Objective This paper aims at providing an initial characterization of the mechanisms that need to be present to make a {\{}FES{\}} ecosystem successful. This includes identification of the actors, the possible business models, the effects on product development processes, methods and tools, as well as on the product architecture. Method The research was carried out as an explorative case study based on interviews with 15 senior staff members at 9 companies related to {\{}ES{\}} that represent different roles in a future ecosystem for FES. The interview data was analyzed and the findings were mapped according to the Business Model Canvas (BMC). Results The findings from the study describe the main characteristics of a {\{}FES{\}} ecosystem, and identify the challenges for future research and practice. Conclusions The case study indicates that new actors exist in the {\{}FES{\}} ecosystem compared to a traditional supply chain, and that their roles and relations are redefined. The business models include new revenue streams and services, but also create the need for trade-offs between, e.g., openness and dependability in the architecture, as well as new ways of working. },
	Annote = {Special issue on Software Ecosystems},
	Doi = {https://doi.org/10.1016/j.infsof.2014.03.011},
	ISSN = {0950-5849},
	Keywords = {Architecture,Case study,Embedded Systems,Software ecosystems,Systems-of-Systems},
	Url = {http://www.sciencedirect.com/science/article/pii/S095058491400072X}
}

@Article{Axelsson201669,
	Title = {{Quality assurance in software ecosystems: A systematic literature mapping and research agenda}},
	Author = {Axelsson, Jakob and Skoglund, Mats},
	Journal = {Journal of Systems and Software},
	Year = {2016},
	Pages = {69--81},
	Volume = {114},
	Abstract = {Abstract Software ecosystems are becoming a common model for software development in which different actors cooperate around a shared platform. However, it is not clear what the implications are on software quality when moving from a traditional approach to an ecosystem, and this is becoming increasingly important as ecosystems emerge in critical domains such as embedded applications. Therefore, this paper investigates the challenges related to quality assurance in software ecosystems, and identifies what approaches have been proposed in the literature. The research method used is a systematic literature mapping, which however only resulted in a small set of six papers. The literature findings are complemented with a constructive approach where areas are identified that merit further research, resulting in a set of research topics that form a research agenda for quality assurance in software ecosystems. The agenda spans the entire system life-cycle, and focuses on challenges particular to an ecosystem setting, which are mainly the results of the interactions across organizational borders, and the dynamic system integration being controlled by the users. },
	Doi = {https://doi.org/10.1016/j.jss.2015.12.020},
	ISSN = {0164-1212},
	Keywords = {Quality,Software ecosystems,Testing,Verification},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121215002861}
}

@Article{Ayora2015248,
	Title = {{VIVACE: A framework for the systematic evaluation of variability support in process-aware information systems}},
	Author = {Ayora, Clara and Torres, Victoria and Weber, Barbara and Reichert, Manfred and Pelechano, Vicente},
	Journal = {Information and Software Technology},
	Year = {2015},
	Pages = {248--276},
	Volume = {57},
	Abstract = {abstractContext The increasing adoption of process-aware information systems (PAISs) such as workflow management systems, enterprise resource planning systems, or case management systems, together with the high variability in business processes (e.g., sales processes may vary depending on the respective products and countries), has resulted in large industrial process model repositories. To cope with this business process variability, the proper management of process variants along the entire process lifecycle becomes crucial. Objective The goal of this paper is to develop a fundamental understanding of business process variability. In particular, the paper will provide a framework for assessing and comparing process variability approaches and the support they provide for the different phases of the business process lifecycle (i.e., process analysis and design, configuration, enactment, diagnosis, and evolution). Method We conducted a systematic literature review (SLR) in order to discover how process variability is supported by existing approaches. Results The {\{}SLR{\}} resulted in 63 primary studies which were deeply analyzed. Based on this analysis, we derived the {\{}VIVACE{\}} framework. {\{}VIVACE{\}} allows assessing the expressiveness of a process modeling language regarding the explicit specification of process variability. Furthermore, the support provided by a process-aware information system to properly deal with process model variants can be assessed with {\{}VIVACE{\}} as well. Conclusions {\{}VIVACE{\}} provides an empirically-grounded framework for process engineers that enables them to evaluate existing process variability approaches as well as to select that variability approach meeting their requirements best. Finally, it helps process engineers in implementing {\{}PAISs{\}} supporting process variability along the entire process lifecycle. },
	Doi = {https://doi.org/10.1016/j.infsof.2014.05.009},
	ISSN = {0950-5849},
	Keywords = {Business process,Business process variability,Process family,Process-aware information systems,Systematic literature review},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584914001268}
}

@Article{Azevedo2009411,
	Title = {{Refinement of software product line architectures through recursive modeling techniques}},
	Author = {Azevedo, S and Machado, R J and Muthig, D and Ribeiro, H},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2009},
	Pages = {411--422},
	Volume = {5872 LNCS},
	Abstract = {Currently, modeling methods applicable to software product line architectures do not explicitly comprise refinement, which implies dealing with a lot of complexity during their application to a high number of requirements. This paper suggests the extension of a modeling method applicable to product line architectural modeling, the 4SRS (Four Step Rule Set), to support the refinement of product lines. We have used the GoPhone case study to illustrate the approach and the recursion capability of the method as a solution to the challenges of modeling product line architectures. The strength of our approach resides in its stepwise nature and in allowing the modeler to work at the user requirements level without delving into lower abstraction concerns. {\textcopyright} Springer-Verlag 2009.},
	Annote = {cited By 7},
	Doi = {10.1007/978-3-642-05290-3_53},
	Keywords = {Architectural modeling; Logical architecture; Mode,Cams,Internet},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650724841{\&}doi=10.1007{\%}2F978-3-642-05290-3{\_}53{\&}partnerID=40{\&}md5=25a169ce1bda25c517b599c500478ac1}
}

@Article{Becan20161794,
	Title = {{Breathing ontological knowledge into feature model synthesis: an empirical study}},
	Author = {B{\'{e}}can, G and Acher, M and Baudry, B and Nasr, S B},
	Journal = {Empirical Software Engineering},
	Year = {2016},
	Number = {4},
	Pages = {1794--1841},
	Volume = {21},
	Abstract = {Feature Models (FMs) are a popular formalism for modeling and reasoning about the configurations of a software product line. As the manual construction of an FM is time-consuming and error-prone, management operations have been developed for reverse engineering, merging, slicing, or refactoring FMs from a set of configurations/dependencies. Yet the synthesis of meaningless ontological relations in the FM – as defined by its feature hierarchy and feature groups – may arise and cause severe difficulties when reading, maintaining or exploiting it. Numerous synthesis techniques and tools have been proposed, but only a few consider both configuration and ontological semantics of an FM. There are also few empirical studies investigating ontological aspects when synthesizing FMs. In this article, we define a generic, ontologic-aware synthesis procedure that computes the likely siblings or parent candidates for a given feature. We develop six heuristics for clustering and weighting the logical, syntactical and semantical relationships between feature names. We then perform an empirical evaluation on hundreds of FMs, coming from the SPLOT repository and Wikipedia. We provide evidence that a fully automated synthesis (i.e., without any user intervention) is likely to produce FMs far from the ground truths. As the role of the user is crucial, we empirically analyze the strengths and weaknesses of heuristics for computing ranking lists and different kinds of clusters. We show that a hybrid approach mixing logical and ontological techniques outperforms state-of-the-art solutions. We believe our approach, environment, and empirical results support researchers and practitioners working on reverse engineering and management of FMs. {\textcopyright} 2015, Springer Science+Business Media New York.},
	Annote = {cited By 2},
	Doi = {10.1007/s10664-014-9357-1},
	Keywords = {Computer software; Reverse engineering; Semantics;,Feature modeling; Model management; Refactorings;,Ontology},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924266514{\&}doi=10.1007{\%}2Fs10664-014-9357-1{\&}partnerID=40{\&}md5=a8bb299ac1e292ee13e93d3b3aaf87ad}
}

@Article{Borger2012939,
	Title = {{Ambient Abstract State Machines with applications}},
	Author = {B{\"{o}}rger, Egon and Cisternino, Antonio and Gervasi, Vincenzo},
	Journal = {Journal of Computer and System Sciences},
	Year = {2012},
	Number = {3},
	Pages = {939--959},
	Volume = {78},
	Abstract = {We define a flexible abstract ambient concept which turned out to support current programming practice, in fact can be instantiated to apparently any environment paradigm in use in frameworks for distributed computing with heterogeneous components. For the sake of generality and to also support rigorous high-level system design practice we give the definition in terms of Abstract State Machines. We show the definition to uniformly capture the common static and dynamic disciplines for isolating states or concurrent behavior (e.g. handling of multiple threads for Java) as well as for sharing memory, patterns of object-oriented programming (e.g. for delegation, incremental refinement, encapsulation, views) and agent mobility. },
	Annote = {In Commemoration of Amir Pnueli},
	Doi = {https://doi.org/10.1016/j.jcss.2011.08.004},
	ISSN = {0022-0000},
	Keywords = {Abstract State Machines,Ambient concept,Memory sharing disciplines,Mobile agents,Naming disciplines,Object-oriented design patterns},
	Url = {http://www.sciencedirect.com/science/article/pii/S0022000011000833}
}

@Article{Burdek2016687,
	Title = {{Reasoning about product-line evolution using complex feature model differences}},
	Author = {B{\"{u}}rdek, J and Kehrer, T and Lochau, M and Reuling, D and Kelter, U and Sch{\"{u}}rr, A},
	Journal = {Automated Software Engineering},
	Year = {2016},
	Number = {4},
	Pages = {687--733},
	Volume = {23},
	Abstract = {Features define common and variable parts of the members of a (software) product line. Feature models are used to specify the set of all valid feature combinations. Feature models not only enjoy an intuitive tree-like graphical syntax, but also a precise formal semantics, which can be denoted as propositional formulae over Boolean feature variables. A product line usually constitutes a long-term investment and, therefore, has to undergo continuous evolution to meet ever-changing requirements. First of all, product-line evolution leads to changes of the feature model due to its central role in the product-line paradigm. As a result, product-line engineers are often faced with the problems that (1) feature models are changed in an ad-hoc manner without proper documentation, and (2) the semantic impact of feature diagram changes is unclear. In this article, we propose a comprehensive approach to tackle both challenges. For (1), our approach compares the old and new version of the diagram representation of a feature model and specifies the changes using complex edit operations on feature diagrams. In this way, feature model changes are automatically detected and formally documented. For (2), we propose an approach for reasoning about the semantic impact of diagram changes. We present a set of edit operations on feature diagrams, where complex operations are primarily derived from evolution scenarios observed in a real-world case study, i.e., a product line from the automation engineering domain. We evaluated our approach to demonstrate its applicability with respect to the case study, as well as its scalability concerning experimental data sets. {\textcopyright} 2015, Springer Science+Business Media New York.},
	Annote = {cited By 0},
	Doi = {10.1007/s10515-015-0185-3},
	Keywords = {Automation engineering; Feature combination; Feat,Boolean algebra; Boolean functions; Formal methods,Software design},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944615031{\&}doi=10.1007{\%}2Fs10515-015-0185-3{\&}partnerID=40{\&}md5=dedba32d55a76719cbfda76e77866829}
}

@Conference{Burdek2014,
	Title = {{Staged configuration of dynamic software product lines with complex binding time constraints}},
	Author = {B{\"{u}}rdek, J and Lity, S and Lochau, M and Berens, M and Goltz, U and Sch{\"{u}}rr, A},
	Booktitle = {ACM International Conference Proceeding Series},
	Year = {2014},
	Abstract = {Dynamic software product lines (DSPL) constitute a promising approach for developing highly-configurable, runtime-adaptive systems in a feature-oriented way. A DSPL integrates both variability in time and space in a unified conceptual framework. For this, domain features are equipped with additional binding time information to distinguish between static configuration parameters and dynamically (re-) configurable features. Until now, little support exists to specify and validate staged (re-)configuration semantics for DSPLs in a concise way. In this paper, we propose conservative extensions to domain feature models comprising variable feature binding times together with different kinds of binding time constraints. Those extensions are motivated by a real-world industrial case study from the automation engineering domain. Our implementation performs a model transformation into plain feature models treatable by corresponding state-of-the-art analysis tools. We conducted an evaluation of our approach concerning the case study. {\textcopyright} 2014 ACM.},
	Annote = {cited By 1},
	Doi = {10.1145/2556624.2556627},
	Keywords = {Automation engineering; Conceptual frameworks; Con,Computer software; Industrial applications; Seman,Software design},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897611358{\&}doi=10.1145{\%}2F2556624.2556627{\&}partnerID=40{\&}md5=ed39770b9cb6a9158dcfc8b4a20009c3}
}

@Article{Baghdadi201695,
	Title = {{A framework for social commerce design}},
	Author = {Baghdadi, Youcef},
	Journal = {Information Systems},
	Year = {2016},
	Pages = {95--113},
	Volume = {60},
	Abstract = {Abstract Interaction features of social web sites, including social networks and social media, enable a new kind of commerce referred to as social commerce (s-commerce). It refers to doing commerce in a collaborative and participative way, through a uniform and interactive enterprise interface, by extending current social web sites initially designed for social interactions of individuals, to promote new business models. On one hand, none of the major social networks or social media providers has yet figured out how to bring commercial transactions directly to their platforms. On the other hand, there is a lack of a comprehensive framework to shape social commerce from both business and {\{}IT{\}} perspectives, which would guide a design process of s-commerce platforms. Indeed, s-commerce platforms differ from e-commerce web sites in many aspects from both business and {\{}IT{\}} perspectives and has more challenges in terms of (i) business models, architectures, principles, and even theories, (ii) complex constructs in terms of participants, interaction features, communities, and content, and (iii) issues such as social, control, security, and privacy issues. Therefore, there is a need for framing the elements of s-commerce, focusing on enterprise social interactions as first class citizens, in an abstract model that guides the architecture, the requirement engineering, the design, and the implementation of a uniform and interactive enterprise interface. Only this enterprise social interaction-enabling interface would promote the emerging knowledge and intelligence that are required for value (co-) creation in s-commerce model. This work fills the gap by proposing a framework that guides a design process to develop s-commerce. },
	Doi = {https://doi.org/10.1016/j.is.2016.03.007},
	ISSN = {0306-4379},
	Keywords = {Enterprise social interactions,Social commerce,Social commerce design process,Social commerce framework,Social design},
	Url = {http://www.sciencedirect.com/science/article/pii/S0306437916301144}
}

@Article{Bagheri2010300,
	Title = {{Stratified analytic hierarchy process: Prioritization and selection of software features}},
	Author = {Bagheri, E and Asadi, M and Gasevic, D and Soltani, S},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2010},
	Pages = {300--315},
	Volume = {6287 LNCS},
	Abstract = {Product line engineering allows for the rapid development of variants of a domain specific application by using a common set of reusable assets often known as core assets. Variability modeling is a critical issue in product line engineering, where the use of feature modeling is one of most commonly used formalisms. To support an effective and automated derivation of concrete products for a product family, staged configuration has been proposed in the research literature. In this paper, we propose the integration of well-known requirements engineering principles into stage configuration. Being inspired by the well-established Preview requirements engineering framework, we initially propose an extension of feature models with capabilities for capturing business oriented requirements. This representation enables a more effective capturing of stakeholders' preferences over the business requirements and objectives (e.g.,. implementation costs or security) in the form of fuzzy linguistic variables (e.g., high, medium, and low). On top of this extension, we propose a novel method, the Stratified Analytic Hierarchy process, which first helps to rank and select the most relevant high level business objectives for the target stakeholders (e.g., security over implementation costs), and then helps to rank and select the most relevant features from the feature model to be used as the starting point in the staged configuration process. Besides a complete formalization of the process, we define the place of our proposal in existing software product line lifecycles as well as demonstrate the use of our proposal on the widely-used e-Shop case study. Finally, we report on the results of our user study, which indicates a high appreciation of the proposed method by the participating industrial software developers. The tool support for S-AHP is also introduced. {\textcopyright} 2010 Springer-Verlag Berlin Heidelberg.},
	Annote = {cited By 12},
	Doi = {10.1007/978-3-642-15579-6_21},
	Keywords = {Analytic hierarchy process; Computer software reu,Business objectives; Business requirement; Busines,Feature extraction},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049414588{\&}doi=10.1007{\%}2F978-3-642-15579-6{\_}21{\&}partnerID=40{\&}md5=1c964be89a3d87cc725a2a54b3f70a9b}
}

@Article{Bagheri2014187,
	Title = {{Dynamic decision models for staged software product line configuration}},
	Author = {Bagheri, E and Ensan, F},
	Journal = {Requirements Engineering},
	Year = {2014},
	Number = {2},
	Pages = {187--212},
	Volume = {19},
	Abstract = {Software product line engineering practices offer desirable characteristics such as rapid product development, reduced time-to-market, and more affordable development costs as a result of systematic representation of the variabilities of a domain of discourse that leads to methodical reuse of software assets. The development lifecycle of a product line consists of two main phases: domain engineering, which deals with the understanding and formally modeling of the target domain, and application engineering that is concerned with the configuration of a product line into one concrete product based on the preferences and requirements of the stakeholders. The work presented in this paper focuses on the application engineering phase and builds both the theoretical and technological tools to assist the stakeholders in (a) understanding the complex interactions of the features of a product line; (b) eliciting the utility of each feature for the stakeholders and hence exposing the stakeholders' otherwise implicit preferences in a way that they can more easily make decisions; and (c) dynamically building a decision model through interaction with the stakeholders and by considering the structural characteristics of software product line feature models, which will guide the stakeholders through the product configuration process. Initial exploratory empirical experiments that we have performed show that our proposed approach for helping stakeholders understand their feature preferences and its associated staged feature model configuration process is able to positively impact the quality of the end results of the application engineering process within the context of the limited number of participants. In addition, it has been observed that the offered tooling support is able to ease the staged feature model configuration process. {\textcopyright} 2013 Springer-Verlag London.},
	Annote = {cited By 7},
	Doi = {10.1007/s00766-013-0165-8},
	Keywords = {Computer software reusability; Concrete products;,Feature models; Rapid product development; Softwa,Software design},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900872964{\&}doi=10.1007{\%}2Fs00766-013-0165-8{\&}partnerID=40{\&}md5=8dc990fc51cc488a06456ca6e52fbe4a}
}

@Article{Bagheri2011109,
	Title = {{Modular feature models: Representation and configuration}},
	Author = {Bagheri, E and Ensan, F and Ga{\v{s}}evic, D and Bo{\v{s}}kovic, M},
	Journal = {Journal of Research and Practice in Information Technology},
	Year = {2011},
	Number = {2},
	Pages = {109--140},
	Volume = {43},
	Abstract = {Within the realm of software product line engineering, feature modeling is one of the widely used techniques for modeling commonality as well as variability. Feature models incorporate the entire domain application configuration space, and are therefore developed collectively by teams of domain experts. In large scale industrial domains, feature models become too complex both in terms of maintenance and configuration. In order to make the maintenance and configuration of feature models feasible, we propose to modularize feature models based on the well-established Distributed Description Logics formalism. Modular feature models provide for an enhanced collaborative/ distributed feature model design, more efficient feature model evolution and better reusability of feature model structure. We also develop methods for the configuration and configuration verification of a modular feature model based on standard inference mechanisms. We describe and evaluate our proposed approach through a case study on an online electronic store application domain. {\textcopyright} 2011, Australian Computer Society Inc.},
	Annote = {cited By 4},
	Keywords = {Application domains; Configuration space; Configur,Data description; Maintenance; Reusability,Software design},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860433186{\&}partnerID=40{\&}md5=05447b5ad7d0b50d1200456b1f58c390}
}

@InProceedings{Bagheri:2012:GTG:2399776.2399785,
	Title = {{Grammar-based Test Generation for Software Product Line Feature Models}},
	Author = {Bagheri, Ebrahim and Ensan, Faezeh and Gasevic, Dragan},
	Booktitle = {Proceedings of the 2012 Conference of the Center for Advanced Studies on Collaborative Research},
	Year = {2012},
	Address = {Riverton, NJ, USA},
	Pages = {87--101},
	Publisher = {IBM Corp.},
	Series = {CASCON '12},
	Url = {http://0-dl.acm.org.fama.us.es/citation.cfm?id=2399776.2399785}
}

@Article{Bagheri2011579,
	Title = {{Assessing the maintainability of software product line feature models using structural metrics}},
	Author = {Bagheri, E and Gasevic, D},
	Journal = {Software Quality Journal},
	Year = {2011},
	Number = {3},
	Pages = {579--612},
	Volume = {19},
	Abstract = {A software product line is a unified representation of a set of conceptually similar software systems that share many common features and satisfy the requirements of a particular domain. Within the context of software product lines, feature models are tree-like structures that are widely used for modeling and representing the inherent commonality and variability of software product lines. Given the fact that many different software systems can be spawned from a single software product line, it can be anticipated that a low-quality design can ripple through to many spawned software systems. Therefore, the need for early indicators of external quality attributes is recognized in order to avoid the implications of defective and low-quality design during the late stages of production. In this paper, we propose a set of structural metrics for software product line feature models and theoretically validate them using valid measurement-theoretic principles. Further, we investigate through controlled experimentation whether these structural metrics can be good predictors (early indicators) of the three main subcharacteristics of maintainability: analyzability, changeability, and understandability. More specifically, a four-step analysis is conducted: (1) investigating whether feature model structural metrics are correlated with feature model maintainability through the employment of classical statistical correlation techniques; (2) understanding how well each of the structural metrics can serve as discriminatory references for maintainability; (3) identifying the sufficient set of structural metrics for evaluating each of the subcharacteristics of maintainability; and (4) evaluating how well different prediction models based on the proposed structural metrics can perform in indicating the maintainability of a feature model. Results obtained from the controlled experiment support the idea that useful prediction models can be built for the purpose of evaluating feature model maintainability using early structural metrics. Some of the structural metrics show significant correlation with the subjective perception of the subjects about the maintainability of the feature models. {\textcopyright} 2010 Springer Science+Business Media, LLC.},
	Annote = {cited By 60},
	Doi = {10.1007/s11219-010-9127-2},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79958248566{\&}doi=10.1007{\%}2Fs11219-010-9127-2{\&}partnerID=40{\&}md5=e0e2572c8e9a517f7df77ecfd32da223}
}

@Article{SMR:SMR534,
	Title = {{Formalizing interactive staged feature model configuration}},
	Author = {Bagheri, Ebrahim and Noia, Tommaso Di and Gasevic, Dragan and Ragone, Azzurra},
	Journal = {Journal of Software: Evolution and Process},
	Year = {2012},
	Number = {4},
	Pages = {375--400},
	Volume = {24},
	Doi = {10.1002/smr.534},
	ISSN = {2047-7481},
	Keywords = {feature models,logic languages,soft constraints,software product lines,variability},
	Publisher = {John Wiley {\&} Sons, Ltd},
	Url = {http://dx.doi.org/10.1002/smr.534}
}

@Article{Bakar2015132,
	Title = {{Feature extraction approaches from natural language requirements for reuse in software product lines: A systematic literature review}},
	Author = {Bakar, Noor Hasrina and Kasirun, Zarinah M and Salleh, Norsaremah},
	Journal = {Journal of Systems and Software},
	Year = {2015},
	Pages = {132--149},
	Volume = {106},
	Abstract = {Abstract Requirements for implemented system can be extracted and reused for a production of a new similar system. Extraction of common and variable features from requirements leverages the benefits of the software product lines engineering (SPLE). Although various approaches have been proposed in feature extractions from natural language (NL) requirements, no related literature review has been published to date for this topic. This paper provides a systematic literature review (SLR) of the state-of-the-art approaches in feature extractions from {\{}NL{\}} requirements for reuse in SPLE. We have included 13 studies in our synthesis of evidence and the results showed that hybrid natural language processing approaches were found to be in common for overall feature extraction process. A mixture of automated and semi-automated feature clustering approaches from data mining and information retrieval were also used to group common features, with only some approaches coming with support tools. However, most of the support tools proposed in the selected studies were not made available publicly and thus making it hard for practitioners' adoption. As for the evaluation, this {\{}SLR{\}} reveals that not all studies employed software metrics as ways to validate experiments and case studies. Finally, the quality assessment conducted confirms that practitioners' guidelines were absent in the selected studies. },
	Doi = {https://doi.org/10.1016/j.jss.2015.05.006},
	ISSN = {0164-1212},
	Keywords = {Feature extractions,Natural language requirements,Requirements reuse,Software product lines,Systematic literature review},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121215001004}
}

@Article{Bakar20161297,
	Title = {{Extracting features from online software reviews to aid requirements reuse}},
	Author = {Bakar, N H and Kasirun, Z M and Salleh, N and Jalab, H A},
	Journal = {Applied Soft Computing Journal},
	Year = {2016},
	Pages = {1297--1315},
	Volume = {49},
	Abstract = {Sets of common features are essential assets to be reused in fulfilling specific needs in software product line methodology. In Requirements Reuse (RR), the extraction of software features from Software Requirement Specifications (SRS) is viable only to practitioners who have access to these software artefacts. Due to organisational privacy, SRS are always kept confidential and not easily available to the public. As alternatives, researchers opted to use the publicly available software descriptions such as product brochures and online software descriptions to identify potential software features to initiate the RR process. The aim of this paper is to propose a semi-automated approach, known as Feature Extraction for Reuse of Natural Language requirements (FENL), to extract phrases that can represent software features from software reviews in the absence of SRS as a way to initiate the RR process. FENL is composed of four stages, which depend on keyword occurrences from several combinations of nouns, verbs, and/or adjectives. In the experiment conducted, phrases that could reflect software features, which reside within online software reviews were extracted by utilising the techniques from information retrieval (IR) area. As a way to demonstrate the feature groupings phase, a semi-automated approach to group the extracted features were then conducted with the assistance of a modified word overlap algorithm. As for the evaluation, the proposed extraction approach is evaluated through experiments against the truth data set created manually. The performance results obtained from the feature extraction phase indicates that the proposed approach performed comparably with related works in terms of recall, precision, and F-Measure. {\textcopyright} 2016 Elsevier B.V.},
	Annote = {cited By 0},
	Doi = {10.1016/j.asoc.2016.07.048},
	Keywords = {Automation; Computer software selection and evalua,Computer software reusability,Extracting features; Latent Semantic Analysis; NA},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997124422{\&}doi=10.1016{\%}2Fj.asoc.2016.07.048{\&}partnerID=40{\&}md5=e7c61585c2eae8eaf3f9c8cca858e5a2}
}

@InProceedings{Barcellos:2010:ESM:1852786.1852822,
	Title = {{Evaluating the Suitability of a Measurement Repository for Statistical Process Control}},
	Author = {Barcellos, Monalessa Perini and Rocha, Ana Regina and {de Almeida Falbo}, Ricardo},
	Booktitle = {Proceedings of the 2010 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
	Year = {2010},
	Address = {New York, NY, USA},
	Pages = {27:1----27:10},
	Publisher = {ACM},
	Series = {ESEM '10},
	Doi = {10.1145/1852786.1852822},
	ISBN = {978-1-4503-0039-1},
	Keywords = {high maturity,measurement,statistical process control},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/1852786.1852822}
}

@Article{Baresi:2007:TES:1276933.1276936,
	Title = {{Three Empirical Studies on Estimating the Design Effort of Web Applications}},
	Author = {Baresi, Luciano and Morasca, Sandro},
	Journal = {ACM Trans. Softw. Eng. Methodol.},
	Year = {2007},
	Number = {4},
	Volume = {16},
	Address = {New York, NY, USA},
	Doi = {10.1145/1276933.1276936},
	ISSN = {1049-331X},
	Keywords = {W2000,Web application design,effort estimation,empirical study},
	Publisher = {ACM},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/1276933.1276936}
}

@Article{SPE:SPE2224,
	Title = {{Aspect-oriented programming and pluggable software units: a comparison based on design patterns}},
	Author = {Barros, Fernando J},
	Journal = {Software: Practice and Experience},
	Year = {2015},
	Number = {3},
	Pages = {289--314},
	Volume = {45},
	Abstract = {The support for software reuse has been a major goal in the design of programming languages. This goal, however, has proven difficult to reach, being only partially enabled by current software tools. In particular, reuse is not fully supported by object-oriented programming (OOP). Aspect-oriented programming (AOP) has introduced new operators that extend OOP, enabling a superior support for reusability. However, AOP operators exhibit limitations in supporting software reuse and more powerful constructs are still required. We consider the ability to define software in an independent manner as the key construct to enable systematic software reuse. To bridge the gap between independence and practical software tools, we have developed the concept of Independent and Pluggable Software Unit (PU), a construct that supports the definition of software topologies. In this paper, we compare PUs with AOP in their support for reusable software. To enable comparison, we employ some well described problems addressed by Software Design Patterns (SDPs). We provide PU and AOP versions of several SDPs, including, Observer, Composite, Command, Chain of Responsibility, and Proxy. In particular, we show that, whereas PUs provide a unified representation of design patterns, AOP representations do not achieve this unification. We also show that AOP solutions do not promote independent and reusable software.Copyright {\textcopyright} 2013 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spe.2224},
	ISSN = {1097-024X},
	Keywords = {aspect programming,design patterns,software composition},
	Url = {http://dx.doi.org/10.1002/spe.2224}
}

@Article{Barros20111355,
	Title = {{Steps, techniques, and technologies for the development of intelligent applications based on Semantic Web Services: A case study in e-learning systems}},
	Author = {Barros, Heitor and Silva, Alan and Costa, Evandro and Bittencourt, Ig Ibert and Holanda, Olavo and Sales, Leandro},
	Journal = {Engineering Applications of Artificial Intelligence},
	Year = {2011},
	Number = {8},
	Pages = {1355--1367},
	Volume = {24},
	Abstract = {Semantic Web Services domain has gained special attention in academia and industry. It has been adopted as a promise to enable automation of all aspects of Web Services provision and uses, such as service creation, selection, discovery, composition, and invocation. However, the development of intelligent systems based on Semantic Web Services (SWS) is still a complex and time-consuming task, mainly with respect to the choice and integration of technologies. In this paper, we discuss some empirical issues associated with the development process for such systems and propose a systematic way for building intelligent applications based on {\{}SWS{\}} by providing the development process with steps, techniques and technologies. In addition, one experiment concerning the implementation of a real e-learning system using the proposed approach is described. The evaluation results from this experiment showed that our approach has been effective and relevant in terms of improvements in the development process of intelligent applications based on SWS. },
	Annote = {Semantic-based Information and Engineering Systems},
	Doi = {https://doi.org/10.1016/j.engappai.2011.05.007},
	ISSN = {0952-1976},
	Keywords = {Grinv Middleware,Intelligent Tutoring System,Ontology,Semantic Web Services},
	Url = {http://www.sciencedirect.com/science/article/pii/S0952197611000893}
}

@Article{Bashari2017191,
	Title = {{Dynamic Software Product Line Engineering: A Reference Framework}},
	Author = {Bashari, M and Bagheri, E and Du, W},
	Journal = {International Journal of Software Engineering and Knowledge Engineering},
	Year = {2017},
	Number = {2},
	Pages = {191--234},
	Volume = {27},
	Abstract = {Runtime adaptive systems are able to dynamically transform their internal structure, and hence their behavior, in response to internal or external changes. Such transformations provide the basis for new functionalities or improvements of the non-functional properties that match operational requirements and standards. Software Product Line Engineering (SPLE) has introduced several models and mechanisms for variability modeling and management. Dynamic software product lines (DSPL) engineering exploits the knowledge acquired in SPLE to develop systems that can be context-aware, post-deployment reconfigurable, or runtime adaptive. This paper focuses on DSPL engineering approaches for developing runtime adaptive systems and proposes a framework for classifying and comparing these approaches from two distinct perspectives: adaptation properties and adaptation realization. These two perspectives are linked together by a series of guidelines that help to select a suitable adaptation realization approach based on desired adaptation types. {\textcopyright} 2017 World Scientific Publishing Company.},
	Annote = {cited By 0},
	Doi = {10.1142/S0218194017500085},
	Keywords = {Adaptive systems; Computer software; Surveying,Dynamic software product lines; Non functional pr,Software design},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016321904{\&}doi=10.1142{\%}2FS0218194017500085{\&}partnerID=40{\&}md5=9e3707cf66288cf1fe8eb164fb6575b5}
}

@Article{Bashroush:2017:CTS:3058791.3034827,
	Title = {{CASE Tool Support for Variability Management in Software Product Lines}},
	Author = {Bashroush, Rabih and Garba, Muhammad and Rabiser, Rick and Groher, Iris and Botterweck, Goetz},
	Journal = {ACM Comput. Surv.},
	Year = {2017},
	Number = {1},
	Pages = {14:1----14:45},
	Volume = {50},
	Address = {New York, NY, USA},
	Doi = {10.1145/3034827},
	ISSN = {0360-0300},
	Keywords = {Software engineering,computer-aided software engineering,software variability},
	Publisher = {ACM},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/3034827}
}

@Article{Bass20161,
	Title = {{Artefacts and agile method tailoring in large-scale offshore software development programmes}},
	Author = {Bass, Julian M},
	Journal = {Information and Software Technology},
	Year = {2016},
	Pages = {1--16},
	Volume = {75},
	Abstract = {Abstract Context: Large-scale offshore software development programmes are complex, with challenging deadlines and a high risk of failure. Agile methods are being adopted, despite the challenges of coordinating multiple development teams. Agile processes are tailored to support team coordination. Artefacts are tangible products of the software development process, intended to ensure consistency in the approach of teams on the same development programme. Objective: This study aims to increase understanding of how development processes are tailored to meet the needs of large-scale offshore software development programmes, by focusing on artefact inventories used in the development process. Method: A grounded theory approach using 46 practitioner interviews, supplemented with documentary sources and observations, in nine international companies was adopted. The grounded theory concepts of open coding, memoing, constant comparison and saturation were used in data analysis. Results: The study has identified 25 artefacts, organised into five categories: feature, sprint, release, product and corporate governance. It was discovered that conventional agile artefacts are enriched with artefacts associated with plan-based methods in order to provide governance. The empirical evidence collected in the study has been used to identify a primary owner of each artefact and map each artefact to specific activities within each of the agile roles. Conclusion: The development programmes in this study create agile and plan-based artefacts to improve compliance with enterprise quality standards and technology strategies, whilst also mitigating risk of failure. Management of these additional artefacts is currently improvised because agile development processes lack corresponding ceremonies. },
	Doi = {https://doi.org/10.1016/j.infsof.2016.03.001},
	ISSN = {0950-5849},
	Keywords = {Agile software development,Enterprise,Grounded theory,Large-scale,Offshore,Outsourced,Process tailoring,Scrum,Software development artefacts},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584916300350}
}

@Article{Bassiliades2017203,
	Title = {{A semantic recommendation algorithm for the PaaSport platform-as-a-service marketplace}},
	Author = {Bassiliades, Nick and Symeonidis, Moisis and Meditskos, Georgios and Kontopoulos, Efstratios and Gouvas, Panagiotis and Vlahavas, Ioannis},
	Journal = {Expert Systems with Applications},
	Year = {2017},
	Pages = {203--227},
	Volume = {67},
	Abstract = {Abstract Platform as a service (PaaS) is one of the Cloud computing services that provide a computing platform in the Cloud, allowing customers to develop, run, and manage web applications without the complexity of building and maintaining the infrastructure. The primary disadvantage for an {\{}SME{\}} to enter the emerging PaaS market is the possibility of being locked into a certain platform, mostly provided by the market's giants. The PaaSport project focuses on facilitating {\{}SMEs{\}} to deploy business applications on the best-matching Cloud PaaS offering and to seamlessly migrate these applications on demand, via a thin, non-intrusive Cloud-broker, in the form of a Cloud PaaS Marketplace. PaaSport enables PaaS provider {\{}SMEs{\}} to roll out semantically interoperable PaaS offerings, by annotating them using a unified PaaS semantic model that has been defined as an {\{}OWL{\}} ontology. In this paper we focus on the recommendation algorithm that has been developed on top of the ontology, for providing the application developer with recommendations about the best-matching Cloud PaaS offering. The algorithm consists of: a) a matchmaking part, where the functional parameters of the application are taken into account to rule out inconsistent offerings, and b) a ranking part, where the non-functional parameters of the application are considered to score and rank offerings. Τhe algorithm is extensively evaluated showing linear scalability to the number of offerings and application requirements. Furthermore, it is extensible upon future semantic model extensions, because it is agnostic to domain specific concepts and parameters, using {\{}SPARQL{\}} template queries. },
	Doi = {https://doi.org/10.1016/j.eswa.2016.09.032},
	ISSN = {0957-4174},
	Keywords = {Cloud application,Cloud computing,Platform offering,Platform-as-a-service,Ranking,Recommendation,Semantic interoperability,Semantic matchmaking},
	Url = {http://www.sciencedirect.com/science/article/pii/S0957417416305164}
}

@Article{Basso2016612,
	Title = {{Automated design of multi-layered web information systems}},
	Author = {Basso, F{\'{a}}bio Paulo and Pillat, Raquel Mainardi and Oliveira, Toacy Cavalcante and Roos-Frantz, Fabricia and Frantz, Rafael Z},
	Journal = {Journal of Systems and Software},
	Year = {2016},
	Pages = {612--637},
	Volume = {117},
	Abstract = {Abstract In the development of web information systems, design tasks are commonly used in approaches for Model-Driven Web Engineering (MDWE) to represent models. To generate fully implemented prototypes, these models require a rich representation of the semantics for actions (e.g., database persistence operations). In the development of some use case scenarios for the multi-layered development of web information systems, these design tasks may consume weeks of work even for experienced designers. The literature pointed out that the impossibility for executing a software project with short iterations hampers the adoption of some approaches for design in some contexts, such as start-up companies. A possible solution to introduce design tasks in short iterations is the use of automated design techniques, which assist the production of models by means of transformation tasks and refinements. This paper details our methodology for MDWE, which is supported by automated design techniques strictly associated with use case patterns of type CRUD. The novelty relies on iterations that are possible for execution with short time-scales. This is a benefit from automated design techniques not observed in {\{}MDWE{\}} approaches based on manual design tasks. We also report on previous experiences and address open questions relevant for the theory and practice of MDWE. },
	Doi = {https://doi.org/10.1016/j.jss.2016.04.060},
	ISSN = {0164-1212},
	Keywords = {Automated design,Domain-specific language,Experience report,Mockup,Model-driven web engineering,Prototyping,Rapid application prototype},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121216300358}
}

@Article{Bastos2017112,
	Title = {{Software product lines adoption in small organizations}},
	Author = {Bastos, Jonatas Ferreira and {da Mota Silveira Neto}, Paulo Anselmo and O'Leary, P{\'{a}}draig and de Almeida, Eduardo Santana and {de Lemos Meira}, Silvio Romero},
	Journal = {Journal of Systems and Software},
	Year = {2017},
	Pages = {112--128},
	Volume = {131},
	Abstract = {AbstractContext An increasing number of studies has demonstrated improvements in product quality, and time-to-market reductions when Software Product Line (SPL) engineering is introduced. However, despite the amount of successful stories about the use of {\{}SPL{\}} engineering, there is a lack of guidelines to support its adoption, especially to small-sized software organizations. Objective The aim of this study is to investigate {\{}SPL{\}} adoption in small organizations and to improve the generalization of evidence through the use of a multi-method approach. Method This paper reports on a multi-method study, where results from a mapping study, industrial case study and also expert opinion survey were considered to identify a set of findings. Results The study provides a better understanding of {\{}SPL{\}} adoption in the context of small to medium-sized organizations, by documenting evidence observed during the transition from single-system development to an {\{}SPL{\}} approach. This evidence is strengthened by the use of different research methods, which results in 22 findings regarding to the {\{}SPL{\}} adoption. Conclusion This research has synthesized the available evidence in {\{}SPL{\}} adoption and identifies gaps between required strategies, organizational structures, maturity level and existing adoption barriers. These findings are an important step to establish guidelines for {\{}SPL{\}} adoption. },
	Doi = {https://doi.org/10.1016/j.jss.2017.05.052},
	File = {:Users/mac/Downloads/Sciencedirect{\_}articles{\_}13Jun2017{\_}14-36-55.006/Software-product-lines-adoption-in-small-organizations{\_}2017{\_}Journal-of-Systems-and-Software.pdf:pdf},
	ISSN = {0164-1212},
	Keywords = {Adoption Barriers,Adoption barriers,Case Study,Case study,Mapping Study,Mapping study,Multi-Method Approach,Multi-method approach,SPL Adoption,SPL adoption,Software Product Lines,Software product lines,Survey},
	Publisher = {Elsevier Inc.},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121217300997}
}

@Article{Batory20082059,
	Title = {{Modularizing theorems for software product lines: The jbook case study}},
	Author = {Batory, D and B{\"{o}}rger, E},
	Journal = {Journal of Universal Computer Science},
	Year = {2008},
	Number = {12},
	Pages = {2059--2082},
	Volume = {14},
	Abstract = {A gvvoal of software product lines is the economical assembly of programs in a family of programs. In this paper, we explore how theorems about program properties may be integrated into feature-based development of software product lines. As a case study, we analyze an existing Java/JVM compilation correctness proof for defining, interpreting, compiling, and executing bytecode for the Java language. We show how features modularize program source, theorem statements and their proofs. By composing features, the source code, theorem statements and proofs for a program are assembled. The investigation in this paper reveals a striking similarity of the refinement concepts used in Abstract State Machines (ASM) based system development and Feature-Oriented Programming (FOP) of software product lines. We suggest to exploit this observation for a fruitful interaction of researchers in the two communities. {\textcopyright} J.UCS.},
	Annote = {cited By 22},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-55249084004{\&}partnerID=40{\&}md5=05f8438cdd2224729c25937b88e8b009}
}

@Article{Bauer2016545,
	Title = {{Comparing reuse practices in two large software-producing companies}},
	Author = {Bauer, Veronika and Vetro', Antonio},
	Journal = {Journal of Systems and Software},
	Year = {2016},
	Pages = {545--582},
	Volume = {117},
	Abstract = {AbstractContext Reuse can improve productivity and maintainability in software development. Research has proposed a wide range of methods and techniques. Are these successfully adopted in practice? Objective We propose a preliminary answer by integrating two in-depth empirical studies on software reuse at two large software-producing companies. Method We compare and interpret the study results with a focus on reuse practices, effects, and context. Results Both companies perform pragmatic reuse of code produced within the company, not leveraging other available artefacts. Reusable entities are retrieved from a central repository, if present. Otherwise, direct communication with trusted colleagues is crucial for access. Reuse processes remain implicit and reflect the development style. In a homogeneous infrastructure-supported context, participants strongly agreed on higher development pace and less maintenance effort as reuse benefits. In a heterogeneous context with fragmented infrastructure, these benefits did not materialize. Neither case reports statistically significant evidence of negative side effects of reuse nor inhibitors. In both cases, a lack of reuse led to duplicate implementations. Conclusion Technological advances have improved the way reuse concepts can be applied in practice. Homogeneity in development process and tool support seem necessary preconditions. Developing and adopting adequate reuse strategies in heterogeneous contexts remains challenging. },
	Doi = {https://doi.org/10.1016/j.jss.2016.03.067},
	ISSN = {0164-1212},
	Keywords = {Empirical,Software engineering,Software reuse,Survey research,Technology transfer},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121216300176}
}

@Article{Bazi201787,
	Title = {{A comprehensive framework for cloud computing migration using Meta-synthesis approach}},
	Author = {reza Bazi, Hamid and Hassanzadeh, Alireza and Moeini, Ali},
	Journal = {Journal of Systems and Software},
	Year = {2017},
	Pages = {87--105},
	Volume = {128},
	Abstract = {Abstract Migration to the cloud computing environment is a strategic organizational decision. Using a reliable framework for migration ensures managers to mitigate risks in the cloud computing technology. Therefore, organizations always search for cloud migration frameworks with dynamic nature as well as integrity beside their simplicity. In previous studies, these important features have received less attention and have not been achieved in an integrated and comprehensive way. The aim of this study is to use a meta-synthesis method for the first time for analysis and synthesis of previous published studies and suggests a comprehensive cloud migration framework. We review more than 657 papers from relevant journals and conference proceedings. The concepts which are extracted from these papers are classified to related sub-categories and categories. Then, our proposed framework based on these concepts and categories is developed. It includes seven main phases (categories) and fifteen sub-categories. To improve the migration process a maturity model called “ClM3? is introduced. Finally, proposed framework and maturity model is evaluated by forming different focus group meetings and taking advantages of the cloud experts' opinion. The results of this research can help managers have a safe and effective migration to cloud computing environment. },
	Doi = {https://doi.org/10.1016/j.jss.2017.02.049},
	ISSN = {0164-1212},
	Keywords = {Cloud computing,Meta-synthesis,Migration framework,Process maturity model},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121217300456}
}

@Article{IIS2:IIS203191,
	Title = {{9.2.2 When “Yes? is the Wrong Answer}},
	Author = {Beasley, Richard and Nolan, Andy J and Pickard, Andrew C},
	Journal = {INCOSE International Symposium},
	Year = {2014},
	Number = {1},
	Pages = {938--952},
	Volume = {24},
	Abstract = {Systems Engineering's value comes from doing effective pre-work to avoid later, expensive rework. There are many barriers to uptake of Systems Engineering, including the difficulty of abstract and holistic thinking and project time pressures. This paper focuses on the time pressures, and the usual desire to show positive progress in any form of review of a project. This leads to a behavior where there is a tendency to say “yes? in answer to a question because we know it is the desired answer. Inappropriate “yes? statements to questions like “Are the requirements complete?? result in a tendency to stop the pre-work, and start the solution stage pre-maturely or with false confidence. The paper proposes as a heuristic that the Systems Engineer recognizes that there are implicit dangers in answering “yes? to many review type questions.},
	Doi = {10.1002/j.2334-5837.2014.tb03191.x},
	ISSN = {2334-5837},
	Url = {http://dx.doi.org/10.1002/j.2334-5837.2014.tb03191.x}
}

@Article{terBeek2014351,
	Title = {{Challenges in modelling and analyzing quantitative aspects of bike-sharing systems}},
	Author = {ter Beek, M H and Fantechi, A and Gnesi, S},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2014},
	Pages = {351--367},
	Volume = {8802},
	Abstract = {Bike-sharing systems are becoming popular not only as a sustainable means of transportation in the urban environment, but also as a challenging case study that presents interesting run-time optimization problems. As a side-study within a research project aimed at quantitative analysis that used such a case study, we have observed how the deployed systems enjoy a wide variety of different features. We have therefore applied variability analysis to define a family of bike-sharing systems, and we have sought support in available tools. We have so established a tool chain that includes (academic) tools that provide different functionalities regarding the analysis of software product lines, from feature modelling to product derivation and from quantitative evaluation of the attributes of products to model checking value-passing modal specifications. The tool chain is currently experimented inside the mentioned project as a complement to more sophisticated product-based analysis techniques. {\textcopyright} Springer-Verlag Berlin Heidelberg 2014.},
	Annote = {cited By 8},
	Keywords = {Analysis techniques; Means of transportations; Pr,Bicycles; Chains; Formal methods; Model checking;,Time sharing systems},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910613014{\&}partnerID=40{\&}md5=e5550e42d36a4fb96dd5b584ee979d72}
}

@Article{Beg2015674,
	Title = {{On the reliability estimation of nano-circuits using neural networks}},
	Author = {Beg, Azam and Awwad, Falah and Ibrahim, Walid and Ahmed, Faheem},
	Journal = {Microprocessors and Microsystems},
	Year = {2015},
	Number = {8},
	Pages = {674--685},
	Volume = {39},
	Abstract = {Abstract As the integrated circuit geometries shrink, it becomes important for the designers to take into consideration the reliability of the circuits. Different techniques can be used for reliability calculation or estimation. Some of these techniques are accurate but time-consuming while others are quick but not accurate. For example, using a set of mathematical equations for reliability estimation is very fast but not precise enough for large systems. Alternatively, Monte Carlo simulations are highly accurate, but very time-intensive. This work presents three different neural network models for estimating circuit reliability. The models provide better prediction accuracies than the mathematical technique. A reasonably large number of combinational circuits were simulated over a wide range of device reliabilities to collect the training data for the models. Multiple slices of an ISCAS-85 benchmark circuit were used to validate the models' prediction results. },
	Doi = {https://doi.org/10.1016/j.micpro.2015.09.008},
	ISSN = {0141-9331},
	Keywords = {Digital circuit,Modeling,Nano-electronics,Neural network,Reliability},
	Url = {http://www.sciencedirect.com/science/article/pii/S0141933115001507}
}

@Article{SMR:SMR1568,
	Title = {{Embedded software product lines: domain and application engineering model-based analysis processes}},
	Author = {Belategi, Lorea and Sagardui, Goiuria and Etxeberria, Leire and Azanza, Maider},
	Journal = {Journal of Software: Evolution and Process},
	Year = {2014},
	Number = {4},
	Pages = {419--433},
	Volume = {26},
	Abstract = {Nowadays, embedded systems are gaining importance. At the same time, the development of their software is increasing its complexity, having to deal with quality, cost, and time-to-market issues among others. With stringent quality requirements such as performance, early verification and validation become critical in these systems. In this regard, advanced development paradigms such as model-driven engineering and software product line engineering bring considerable benefits to the development and validation of embedded system software. However, these benefits come at the cost of increasing process complexity. This work presents a process based on UML and MARTE for the analysis of embedded model-driven product lines. It specifies the tasks, the involved roles, and the workproducts that form the process and how it is integrated in the more general development process. Existing tools that support the tasks to be performed in the process are also described. A classification of such tools and a study of traceability among them are provided, allowing engineering teams to choose the most adequate chain of tools to support the process. Copyright {\textcopyright} 2012 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/smr.1568},
	ISSN = {2047-7481},
	Keywords = {model-based analysis process,model-driven development,performance,quality attributes,software product line},
	Url = {http://dx.doi.org/10.1002/smr.1568}
}

@Conference{Belder201514,
	Title = {{Coherent branching feature bisimulation}},
	Author = {Belder, T and {Ter Beek}, M H and {De Vink}, E P},
	Booktitle = {Electronic Proceedings in Theoretical Computer Science, EPTCS},
	Year = {2015},
	Pages = {14--30},
	Volume = {182},
	Abstract = {Progress in the behavioral analysis of software product lines at the family level benefits from further development of the underlying semantical theory. Here, we propose a behavioral equivalence for feature transition systems (FTS) generalizing branching bisimulation for labeled transition systems (LTS). We prove that branching feature bisimulation for an FTS of a family of products coincides with branching bisimulation for the LTS projection of each the individual products. For a restricted notion of coherent branching feature bisimulation we furthermore present a minimization algorithm and show its correctness. Although the minimization problem for coherent branching feature bisimulation is shown to be intractable, application of the algorithm in the setting of a small case study results in a significant speed-up of model checking of behavioral properties. {\textcopyright} T. Belder, M.H. ter Beek {\&} E.P. de Vink.},
	Annote = {cited By 2},
	Doi = {10.4204/EPTCS.182.2},
	Keywords = {Behavioral analysis; Behavioral equivalence; Beha,Formal methods,Model checking},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014879559{\&}doi=10.4204{\%}2FEPTCS.182.2{\&}partnerID=40{\&}md5=9e48dfb377d1888fbee8c38cf11cbf37}
}

@Article{Belli201625,
	Title = {{Model-based mutation testing—Approach and case studies}},
	Author = {Belli, Fevzi and Budnik, Christof J and Hollmann, Axel and Tuglular, Tugkan and Wong, W Eric},
	Journal = {Science of Computer Programming},
	Year = {2016},
	Pages = {25--48},
	Volume = {120},
	Abstract = {Abstract This paper rigorously introduces the concept of model-based mutation testing (MBMT) and positions it in the landscape of mutation testing. Two elementary mutation operators, insertion and omission, are exemplarily applied to a hierarchy of graph-based models of increasing expressive power including directed graphs, event sequence graphs, finite-state machines and statecharts. Test cases generated based on the mutated models (mutants) are used to determine not only whether each mutant can be killed but also whether there are any faults in the corresponding system under consideration (SUC) developed based on the original model. Novelties of our approach are: (1) evaluation of the fault detection capability (in terms of revealing faults in the SUC) of test sets generated based on the mutated models, and (2) superseding of the great variety of existing mutation operators by iterations and combinations of the two proposed elementary operators. Three case studies were conducted on industrial and commercial real-life systems to demonstrate the feasibility of using the proposed {\{}MBMT{\}} approach in detecting faults in SUC, and to analyze its characteristic features. Our experimental data suggest that test sets generated based on the mutated models created by insertion operators are more effective in revealing faults in {\{}SUC{\}} than those generated by omission operators. Worth noting is that test sets following the {\{}MBMT{\}} approach were able to detect faults in the systems that were tested by manufacturers and independent testing organizations before they were released. },
	Doi = {https://doi.org/10.1016/j.scico.2016.01.003},
	ISSN = {0167-6423},
	Keywords = {Fault detection capability,Model-based mutation testing,Model-based testing,Mutation operator,Mutation testing},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642316000137}
}

@Article{Beloglazov201575,
	Title = {{Improving Productivity in Design and Development of Information Technology (IT) Service Delivery Simulation Models}},
	Author = {Beloglazov, A and Banerjee, D and Hartman, A and Buyya, R},
	Journal = {Journal of Service Research},
	Year = {2015},
	Number = {1},
	Pages = {75--89},
	Volume = {18},
	Abstract = {The unprecedented scale of Information Technology (IT) service delivery requires careful analysis and optimization of service systems. The simulation is an efficient way to handle the complexity of modeling and optimization of real-world service delivery systems. However, typically developed custom simulation models lack standard architectures and limit the reuse of design and implementation artifacts across multiple models. In this work, following the design science research methodology, based on a formal model of service delivery systems and applying an adapted software product line (SPL) approach, we create a design artifact for building product lines of IT service delivery simulation models, which vastly simplify and reduce the cost of simulation model design and development. We evaluate the design artifact by constructing a product line of simulation models for a set of IBM's IT service delivery systems. We validate the proposed approach by comparing the simulation results obtained using our models with the results from the corresponding custom simulation models. The case study demonstrates that the proposed approach leads to 5–8 times reductions in the time required to design and develop related simulation models. The potential implications of the application of the proposed approach within an organization are quicker responses to changes in the business environment, more information to assist in managerial decisions, and reduced workload on the process reengineering specialists. {\textcopyright} The Author(s) 2014.},
	Annote = {cited By 0},
	Doi = {10.1177/1094670514541002},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921056881{\&}doi=10.1177{\%}2F1094670514541002{\&}partnerID=40{\&}md5=f275d49946d43f6e2a61aadd9364b21b}
}

@TechReport{Benavides2014,
	author = {Benavides, David and Galindo, Jos{\'{e}} A},
	title = {{Variability Management in an unaware software product line company . An experience report}},
	year = {2014},
	file = {:Users/mac/Library/Application Support/Mendeley Desktop/Downloaded/Benavides, Galindo - 2014 - Variability Management in an unaware software product line company . An experience report.pdf:pdf},
	isbn = {9781450325561}
}

@Article{Benavides2010,
	Title = {{Automated analysis of feature models 20 years later: A literature review}},
	Author = {Benavides, David and Segura, Sergio and Ruiz-Cort??s, Antonio},
	Journal = {Information Systems},
	Year = {2010},
	Number = {6},
	Pages = {615--636},
	Volume = {35},
	Abstract = {Software product line engineering is about producing a set of related products that share more commonalities than variabilities. Feature models are widely used for variability and commonality management in software product lines. Feature models are information models where a set of products are represented as a set of features in a single model. The automated analysis of feature models deals with the computer-aided extraction of information from feature models. The literature on this topic has contributed with a set of operations, techniques, tools and empirical results which have not been surveyed until now. This paper provides a comprehensive literature review on the automated analysis of feature models 20 years after of their invention. This paper contributes by bringing together previously disparate streams of work to help shed light on this thriving area. We also present a conceptual framework to understand the different proposals as well as categorise future contributions. We finally discuss the different studies and propose some challenges to be faced in the future. ?? 2010 Elsevier B.V. All rights reserved.},
	Doi = {10.1016/j.is.2010.01.001},
	File = {:Users/mac/Downloads/Sciencedirect{\_}articles{\_}13Jun2017{\_}15-33-20.965/Automated-analysis-of-feature-models-20-years-later-A-literature-review{\_}2010{\_}Information-Systems.pdf:pdf},
	ISBN = {0306-4379},
	ISSN = {03064379},
	Keywords = {Automated analyses,Feature models,Literature review,Software product lines}
}

@Article{SMR:SMR412,
	Title = {{Understanding software maintenance and evolution by analyzing individual changes: a literature review}},
	Author = {Benestad, Hans Christian and Anda, Bente and Arisholm, Erik},
	Journal = {Journal of Software Maintenance and Evolution: Research and Practice},
	Year = {2009},
	Number = {6},
	Pages = {349--378},
	Volume = {21},
	Abstract = {Understanding, managing and reducing costs and risks inherent in change are key challenges of software maintenance and evolution, addressed in empirical studies with many different research approaches. Change-based studies analyze data that describes the individual changes made to software systems. This approach can be effective in order to discover cost and risk factors that are hidden at more aggregated levels. However, it is not trivial to derive appropriate measures of individual changes for specific measurement goals. The purpose of this review is to improve change-based studies by (1) summarizing how attributes of changes have been measured to reach specific study goals and (2) describing current achievements and challenges, leading to a guide for future change-based studies. Thirty-four papers conformed to the inclusion criteria. Forty-three attributes of changes were identified, and classified according to a conceptual model developed for the purpose of this classification. The goal of each study was to either characterize the evolution process, to assess causal factors of cost and risk, or to predict costs and risks. Effective accumulation of knowledge across change-based studies requires precise definitions of attributes and measures of change. We recommend that new change-based studies base such definitions on the proposed conceptual model. Copyright {\textcopyright} 2009 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/smr.412},
	ISSN = {1532-0618},
	Keywords = {change-based studies,conceptual model,software change},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/smr.412}
}

@Article{Benlarabi2015550,
	Title = {{Analyzing trends in software product lines evolution using a cladistics based approach}},
	Author = {Benlarabi, A and Khtira, A and {El Asri}, B},
	Journal = {Information (Switzerland)},
	Year = {2015},
	Number = {3},
	Pages = {550--563},
	Volume = {6},
	Abstract = {Abstract: A software product line is a complex system the aim of which is to provide a platform dedicated to large reuse. It necessitates a great investment. Thus, its ability to cope with customers' ever-changing requirements is among its key success factors. Great effort has been made to deal with the software product line evolution. In our previous works, we carried out a classification of these works to provide an overview of the used techniques. We also identified the following key challenges of software product lines evolution: the ability to predict future changes, the ability to define the impact of a change easily and the improvement in understanding the change. We have already tackled the second and the third challenges. The objective of this paper is to deal with the first challenge. We use the cladistics classification which was used in biology to understand the evolution of organisms sharing the same ancestor and their process of descent at the aim of predicting their future changes. By analogy, we consider a population of applications for media management on mobile devices derived from the same platform and we use cladistics to construct their evolutionary tree. We conducted an analysis to show how to identify the evolution trends of the case study products and to predict future changes. {\textcopyright} 2015 by the authors.},
	Annote = {cited By 1},
	Doi = {10.3390/info6030550},
	Keywords = {Biology,Cladistics; Evolution; Evolution trend; Evolution,Computer software; Computer software reusability;},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943561816{\&}doi=10.3390{\%}2Finfo6030550{\&}partnerID=40{\&}md5=892c17009c76fbae807d486836450566}
}

@Article{Beohar20161131,
	Title = {{Input–output conformance testing for software product lines}},
	Author = {Beohar, Harsh and Mousavi, Mohammad Reza},
	Journal = {Journal of Logical and Algebraic Methods in Programming},
	Year = {2016},
	Number = {6},
	Pages = {1131--1153},
	Volume = {85},
	Abstract = {Abstract We extend the theory of input–output conformance (IOCO) testing to accommodate behavioral models of software product lines (SPLs). We present the notions of residual and spinal testing. These notions allow for structuring the test process for {\{}SPLs{\}} by taking variability into account and extracting separate test suites for common and specific features of an SPL. The introduced notions of residual and spinal test suites allow for focusing on the newly introduced behavior and avoiding unnecessary re-test of the old one. Residual test suites are very conservative in that they require retesting the old behavior that can reach to new behavior. However, spinal test suites more aggressively prune the old tests and only focus on those test sequences that are necessary in reaching the new behavior. We show that residual testing is complete but does not usually lead to much reduction in the test-suite. In contrast, spinal testing is not necessarily complete but does reduce the test-suite. We give sufficient conditions on the implementation to guarantee completeness of spinal testing. Finally, we specify and analyze an example regarding the Ceiling Speed Monitoring Function from the European Train Control System. },
	Annote = {{\{}NWPT{\}} 2013},
	Doi = {https://doi.org/10.1016/j.jlamp.2016.09.007},
	ISSN = {2352-2208},
	Keywords = {Input–output conformance testing,Input–output featured transition systems,Model based testing,Software product lines},
	Url = {http://www.sciencedirect.com/science/article/pii/S2352220816301171}
}

@Article{Beohar201642,
	Title = {{Basic behavioral models for software product lines: Expressiveness and testing pre-orders}},
	Author = {Beohar, Harsh and Varshosaz, Mahsa and Mousavi, Mohammad Reza},
	Journal = {Science of Computer Programming},
	Year = {2016},
	Pages = {42--60},
	Volume = {123},
	Abstract = {Abstract In order to provide a rigorous foundation for Software Product Lines (SPLs), several fundamental approaches have been proposed to their formal behavioral modeling. In this paper, we provide a structured overview of those formalisms based on labeled transition systems and compare their expressiveness in terms of the set of products they can specify. Moreover, we define the notion of tests for each of these formalisms and show that our notions of testing precisely capture product derivation, i.e., all valid products will pass the set of test cases of the product line and each invalid product fails at least one test case of the product line. },
	Annote = {{\{}SELECTED{\}} {\{}AND{\}} {\{}EXTENDED{\}} {\{}PAPERS{\}} {\{}FROM{\}} {\{}ACM{\}} {\{}SVT{\}} 2014},
	Doi = {https://doi.org/10.1016/j.scico.2015.06.005},
	ISSN = {0167-6423},
	Keywords = {Behavioral specification,Calculus of communicating systems (CCS),Featured transition systems,Formal specification,Labeled transition systems,Modal transition systems,Product line {\{}CCS{\}} (PL-CCS),Software product lines},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642315001288}
}

@Conference{Berger2014,
	Title = {{Towards system analysis with variability model metrics}},
	Author = {Berger, T and Guo, J},
	Booktitle = {ACM International Conference Proceeding Series},
	Year = {2014},
	Abstract = {Variability models are central artifacts in highly configurable systems. They aim at planning, developing, and configuring systems by describing configuration knowledge at different levels of formality. The existence of large models using a variety of modeling concepts in heterogeneous languages with intricate semantics calls for a unified measuring approach. In this position paper, we attempt to take a first step towards such a measurement. We discuss perspectives of metrics, define low-level measurement goals, and conceive and implement metrics based on variability modeling concepts found in real-world languages and models. An evaluation of these metrics with real-world models and codebases provides insight into the benefits of such metrics for the defined perspectives. {\textcopyright} 2014 ACM.},
	Annote = {cited By 0},
	Doi = {10.1145/2556624.2556641},
	Keywords = {Empirical Software Engineering; Feature modeling;,Semantics,Software engineering},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897660283{\&}doi=10.1145{\%}2F2556624.2556641{\&}partnerID=40{\&}md5=5110ec5f740020ec5753e6bf5ab219d7}
}

@Article{Berger2014302,
	Title = {{Three cases of feature-based variability modeling in industry}},
	Author = {Berger, T and Nair, D and Rublack, R and Atlee, J M and Czarnecki, K and W{\c{a}}sowski, A},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2014},
	Pages = {302--319},
	Volume = {8767},
	Abstract = {Large software product lines need to manage complex variability. A common approach is variability modeling—creating and maintaining models that abstract over the variabilities inherent in such systems. While many variability modeling techniques and notations have been proposed, little is known about industrial practices and how industry values or criticizes this class of modeling. We attempt to address this gap with an exploratory case study of three companies that apply variability modeling. Among others, our study shows that variability models are valued for their capability to organize knowledge and to achieve an overview understanding of codebases. We observe centralized model governance, pragmatic versioning, and surprisingly little constraint modeling, indicating that the effort of declaring and maintaining constraints does not always pay off. {\textcopyright} Springer International Publishing Switzerland 2014.},
	Annote = {cited By 1},
	Keywords = {Artificial intelligence,Centralized models; Constraint model; Exploratory,Computers},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921634156{\&}partnerID=40{\&}md5=ba4de4c74bcc80924ed6d87cf2fb9984}
}

@Article{Berger20141520,
	Title = {{Variability mechanisms in software ecosystems}},
	Author = {Berger, T and Pfeiffer, R.-H. and Tartler, R and Dienst, S and Czarnecki, K and Wa̧sowski, A and She, S},
	Journal = {Information and Software Technology},
	Year = {2014},
	Number = {11},
	Pages = {1520--1535},
	Volume = {56},
	Abstract = {Context Software ecosystems are increasingly popular for their economic, strategic, and technical advantages. Application platforms such as Android or iOS allow users to highly customize a system by selecting desired functionality from a large variety of assets. This customization is achieved using variability mechanisms. Objective Variability mechanisms are well-researched in the context of software product lines. Although software ecosystems are often seen as conceptual successors, the technology that sustains their success and growth is much less understood. Our objective is to improve empirical understanding of variability mechanisms used in successful software ecosystems. Method We analyze five ecosystems, ranging from the Linux kernel through Eclipse to Android. A qualitative analysis identifies and characterizes variability mechanisms together with their organizational context. This analysis leads to a conceptual framework that unifies ecosystem-specific aspects using a common terminology. A quantitative analysis investigates scales, growth rates, and - most importantly - dependency structures of the ecosystems. Results In all the studied ecosystems, we identify rich dependency languages and variability descriptions that declare many direct and indirect dependencies. Indirect dependencies to abstract capabilities, as opposed to concrete variability units, are used predominantly in fast-growing ecosystems. We also find that variability models - while providing system-wide abstractions over code - work best in centralized variability management and are, thus, absent in ecosystems with large free markets. These latter ecosystems tend to emphasize maintaining capabilities and common vocabularies, dynamic discovery, and binding with strong encapsulation of contributions, together with uniform distribution channels. Conclusion The use of specialized mechanisms in software ecosystems with large free markets, as opposed to software product lines, calls for recognition of a new discipline - variability encouragement. {\textcopyright} 2014 Elsevier B.V. All rights reserved.},
	Annote = {cited By 13},
	Doi = {10.1016/j.infsof.2014.05.005},
	Keywords = {Android (operating system); Commerce; Computer sof,Ecosystems,Empirical Software Engineering; Mining software r},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905441720{\&}doi=10.1016{\%}2Fj.infsof.2014.05.005{\&}partnerID=40{\&}md5=be0733bc2c3d76e97ea11b75ee18fb46}
}

@Article{Berger2013,
	Title = {{A Study of Variability Models and Languages in the Systems Software Domain}},
	Author = {Berger, Thorsten and She, Steven and Lotufo, Rafael and Wasowski, Andrzej and Czarnecki, Krzysztof},
	Journal = {IEEE Transactions on Software Engineering},
	Year = {2013},
	Month = {dec},
	Number = {12},
	Pages = {1611--1640},
	Volume = {39},
	Doi = {10.1109/TSE.2013.34},
	File = {:Users/mac/Downloads/bulk-download (2)/A Study of Variability Models and Languages in the Systems Software Domain.pdf:pdf},
	ISSN = {0098-5589},
	Url = {http://ieeexplore.ieee.org/document/6572787/}
}

@Article{ISJ:ISJ12036,
	Title = {{Persistent problems and practices in information systems development: a study of mobile applications development and distribution}},
	Author = {Bergvall-K{\aa}reborn, Birgitta and Howcroft, Debra},
	Journal = {Information Systems Journal},
	Year = {2014},
	Number = {5},
	Pages = {425--444},
	Volume = {24},
	Abstract = {The widespread uptake of mobile technologies has witnessed a re-structuring of the mobile market with major shifts in the predominance of particular firms and the emergence of new business models. These sociotechnical trends are significant in the ways that they are influencing and shaping the working lives of software professionals. Building on prior research investigating the persistent problems and practices of systems development, this paper examines mobile applications development and distribution. A qualitative study of 60 developers based in Sweden, the UK and the USA was analysed around the interrelated problems of diversity, knowledge and structure. The analysis revealed how platform-based development in an evolving mobile market represents significant changes at the business environment level. These changes ripple through and accentuate ongoing trends and developments, intensifying the persistent problems and challenges facing software developers.},
	Doi = {10.1111/isj.12036},
	ISSN = {1365-2575},
	Keywords = {Apple,Google,crowdsourcing,ecosystem,platform,systems developers},
	Url = {http://dx.doi.org/10.1111/isj.12036}
}

@Article{BerntssonSvensson2012175,
	Title = {{Software architecture as a means of communication in a globally distributed software development context}},
	Author = {{Berntsson Svensson}, R and Aurum, A and Paech, B and Gorschek, T and Sharma, D},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2012},
	Pages = {175--189},
	Volume = {7343 LNCS},
	Abstract = {The management and coordination of globally distributed development poses many new challenges, including compensating for informal implicit communication, which is aggravated by heterogeneous social and engineering traditions between development sites. Although much research has gone into identifying challenges and working with practical solutions, such as tools for communication, little research has focused on comparing communication mechanisms in terms of their ability to provide large volumes of rich information in a timely manner. Data was collected through in-depth interviews with eleven practitioners and twenty-eight responses through a web-based questionnaire from three product lines at an international software development organization. This paper assesses the relative importance of ten commonly used communication mechanisms and practices across local and global development sites. The results clearly indicate that some communication mechanisms are more important than others in providing large volumes of rich information in a timely manner. The prevalence of architecture in providing rich information in large volumes for both local and global communication can be clearly observed. {\textcopyright} 2012 Springer-Verlag.},
	Annote = {cited By 3},
	Doi = {10.1007/978-3-642-31063-8_14},
	Keywords = {Communication mechanisms; Distributed development;,Communication; Research; Software architecture,Software design},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862196503{\&}doi=10.1007{\%}2F978-3-642-31063-8{\_}14{\&}partnerID=40{\&}md5=1c6a9d0ce3ffb7ffdd95be625b0461ae}
}

@Article{IIS2:IIS2143,
	Title = {{Delta Rhapsody}},
	Author = {Berreteaga, Oskar and Sagardui, Goiuria and Etxeberria, Leire and Markiegi, Urtzi and Perez, Xabier},
	Journal = {INCOSE International Symposium},
	Year = {2016},
	Number = {1},
	Pages = {25--41},
	Volume = {26},
	Abstract = {Model Based System Engineering (MBSE) has become the pre-eminent paradigm used to improve the development of complex systems. Additionally, Delta Modelling provides an incremental approach to the design and maintenance of models when there is variability in the system. In this context, the market leading IBM Rhapsody{\textregistered} provides a professional and wide range of functionalities. Nevertheless, there is not support for Delta Modelling in IBM Rhapsody{\textregistered}. To meet this need, the Delta Rhapsody solution has been developed. With the tool a new variant model can be automatically generated applying deltas to a core model. The application of the solution in an industrial case study has been undertaken. From this, it was concluded that the Delta Rhapsody solution notably reduces the required time to develop a variant model. To further improve the Delta Rhapsody solution, some future works have also been identified.},
	Doi = {10.1002/j.2334-5837.2016.00143.x},
	ISSN = {2334-5837},
	Url = {http://dx.doi.org/10.1002/j.2334-5837.2016.00143.x}
}

@Article{Bertolino2015355,
	Title = {{Similarity testing for access control}},
	Author = {Bertolino, Antonia and Daoudagh, Said and Kateb, Donia El and Henard, Christopher and Traon, Yves Le and Lonetti, Francesca and Marchetti, Eda and Mouelhi, Tejeddine and Papadakis, Mike},
	Journal = {Information and Software Technology},
	Year = {2015},
	Pages = {355--372},
	Volume = {58},
	Abstract = {AbstractContext Access control is among the most important security mechanisms, and {\{}XACML{\}} is the de facto standard for specifying, storing and deploying access control policies. Since it is critical that enforced policies are correct, policy testing must be performed in an effective way to identify potential security flaws and bugs. In practice, exhaustive testing is impossible due to budget constraints. Therefore the tests need to be prioritized so that resources are focused on their most relevant subset. Objective This paper tackles the issue of access control test prioritization. It proposes a new approach for access control test prioritization that relies on similarity. Method The approach has been applied to several policies and the results have been compared to random prioritization (as a baseline). To assess the different prioritization criteria, we use mutation analysis and compute the mutation scores reached by each criterion. This helps assessing the rate of fault detection. Results The empirical results indicate that our proposed approach is effective and its rate of fault detection is higher than that of random prioritization. Conclusion We conclude that prioritization of access control test cases can be usefully based on similarity criteria. },
	Doi = {https://doi.org/10.1016/j.infsof.2014.07.003},
	ISSN = {0950-5849},
	Keywords = {Security policies,Similarity,Test prioritization},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584914001578}
}

@Article{Bessling2014217,
	Title = {{Towards formal safety analysis in feature-oriented product line development}},
	Author = {Bessling, S and Huhn, M},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2014},
	Pages = {217--235},
	Volume = {8315},
	Abstract = {Feature-orientation has proven beneficial in the development of software product lines. We investigate formal safety analysis and verification for product lines of software-intensive embedded systems. We show how to uniformly augment a feature-oriented, model-based design approach with the specification of safety requirements, failure models and fault injection. Therefore we analyze system hazards and identify the causes, i.e. failures and inadequate control systematically. As features are themain concept of functional decomposition in the product line approach, features also direct the safety analysis and the specification of systemlevel safety requirements: Safety (design) constraints are allocated to features. Subsequently, the behavior including possible faults is formally modeled. Then formal verification techniques are employed in order to prove that the safety constraints are satisfied and the system level hazards are prevented. We demonstrate our method using SCADE Suite for the model-based product line design of cardiac pacemakers. VIATRA is employed for the model graph transformation generating the individual products. Formal safety analysis is performed by using SCADE Design Verifier. The case study shows that our approach leads to a fine-grained safety analysis and is capable of uncovering unwanted feature interactions. {\textcopyright} Springer-Verlag Berlin Heidelberg 2014.},
	Annote = {cited By 3},
	Keywords = {Design; Embedded systems; Formal verification; Haz,Feature interactions; Functional decomposition; G,Safety engineering},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924330620{\&}partnerID=40{\&}md5=d2e3a6f4124aeb45d19f9f1fe5f6b65f}
}

@Article{Bettini2013218,
	Title = {{Combining traits with boxes and ownership types in a Java-like setting}},
	Author = {Bettini, Lorenzo and Damiani, Ferruccio and Geilmann, Kathrin and Sch{\"{a}}fer, Jan},
	Journal = {Science of Computer Programming},
	Year = {2013},
	Number = {2},
	Pages = {218--247},
	Volume = {78},
	Abstract = {The box model is a lightweight component model for the object-oriented paradigm, which structures the flat object-heap into hierarchical runtime components called boxes. Boxes have clear runtime boundaries that divide the objects of a box into objects that can be used to interact with the box (the boundary objects) and objects that are encapsulated and represent the state of the box (the local objects). The distinction into local and boundary objects is statically achieved by an ownership type system for boxes that uses domain annotations to classify objects into local and boundary objects and that guarantees that local objects can never be directly accessed by the context of a box. A trait is a set of methods divorced from any class hierarchy. Traits are units of fine-grained reuse that can be composed together to form classes or other traits. This paper integrates traits into an ownership type system for boxes. This combination is fruitful in two ways: it can statically guarantee encapsulation of objects and still provide fine-grained reuse among classes that goes beyond the possibilities of standard inheritance. It also solves a specific problem of the box ownership type system: namely that box classes cannot inherit from standard classes (and vice versa), and thus code sharing between these two kinds of classes was not possible in this setting so far. We present an ownership type system and the corresponding soundness proofs that guarantee encapsulation of objects in an object-oriented language with traits. },
	Annote = {Coordination 2010},
	Doi = {https://doi.org/10.1016/j.scico.2011.10.006},
	ISSN = {0167-6423},
	Keywords = {Boxes,Featherweight Java,Ownership types,Traits},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642311001833}
}

@Article{Bettini2013521,
	Title = {{TraitRecordJ: A programming language with traits and records}},
	Author = {Bettini, Lorenzo and Damiani, Ferruccio and Schaefer, Ina and Strocco, Fabio},
	Journal = {Science of Computer Programming},
	Year = {2013},
	Number = {5},
	Pages = {521--541},
	Volume = {78},
	Abstract = {Traits have been designed as units for fine-grained reuse of behavior in the object-oriented paradigm. Records have been devised to complement traits for fine-grained reuse of state. In this paper, we present the language TraitRecordJ, a Java dialect with records and traits. Records and traits can be composed by explicit linguistic operations, allowing code manipulations to achieve fine-grained code reuse. Classes are assembled from (composite) records and traits and instantiated to generate objects. We introduce the language through examples and illustrate the prototypical implementation of TraitRecordJ using Xtext, an Eclipse framework for the development of programming languages as well as other domain-specific languages. Our implementation comprises an Eclipse-based editor for TraitRecordJ with typical {\{}IDE{\}} functionalities, and a stand-alone compiler, which translates TraitRecordJ programs into standard Java programs. As a case study, we present the TraitRecordJ implementation of a part of the software used in a web-based information system previously implemented in Java. },
	Annote = {Special section: Principles and Practice of Programming in Java 2009/2010 {\&} Special section: Self-Organizing Coordination},
	Doi = {https://doi.org/10.1016/j.scico.2011.06.007},
	ISSN = {0167-6423},
	Keywords = {Eclipse,Implementation,Java,Trait,Type system},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642311001572}
}

@Article{Biffl20141533,
	Title = {{Systematic knowledge engineering: Building bodies of knowledge from published research}},
	Author = {Biffl, S and Kalinowski, M and Rabiser, R and Ekaputra, F and Winkler, D},
	Journal = {International Journal of Software Engineering and Knowledge Engineering},
	Year = {2014},
	Number = {10},
	Pages = {1533--1571},
	Volume = {24},
	Abstract = {Context. Software engineering researchers conduct systematic literature reviews (SLRs) to build bodies of knowledge (BoKs). Unfortunately, relevant knowledge collected in the SLR process is not publicly available, which considerably slows down building BoKs incrementally. Objective. We present and evaluate the Systematic Knowledge Engineering (SKE) process to support efficiently building BoKs from published research. Method. SKE is based on the SLR process and on Knowledge Engineering practices to build a Knowledge Base (KB) by reusing intermediate data extraction results from SLRs. We evaluated the feasibility of applying SKE by building a Software Inspection BoK KB from published experiments and a Software Product Line BoK KB from published experience reports. We compared the effort, benefits, and risks of building BoK KBs regarding the SKE and the traditional SLR processes. Results. The application of SKE for incrementally collecting and organizing knowledge in the context of a BoK was feasible for different domains and different types of evidence. While the efforts for conducting the SKE and traditional SLR processes are comparable, SKE provides significant benefits for building BoKs. Conclusions. SKE enables researchers in a scientific community to reuse and incrementally build knowledge in a BoK. SKE is ready to be evaluated in other software engineering domains. {\textcopyright} 2014 World Scientific Publishing Company.},
	Annote = {cited By 2},
	Doi = {10.1142/S021819401440018X},
	Keywords = {Body of knowledge; Empirical Software Engineering,Buildings,Computer software; Computer software selection and},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928562233{\&}doi=10.1142{\%}2FS021819401440018X{\&}partnerID=40{\&}md5=25738e99352373633d4b6ba562f575c9}
}

@Article{Binkley201530,
	Title = {{Enabling improved IR-based feature location}},
	Author = {Binkley, Dave and Lawrie, Dawn and Uehlinger, Christopher and Heinz, Daniel},
	Journal = {Journal of Systems and Software},
	Year = {2015},
	Pages = {30--42},
	Volume = {101},
	Abstract = {Abstract Recent solutions to software engineering problems have incorporated tools and techniques from information retrieval (IR). The use of {\{}IR{\}} requires choosing an appropriate retrieval model and deciding on a query that best captures a particular information need. Taking feature location as a representative example, three research questions are investigated: (1) the impact of query preprocessing, (2) the impact that different scraping techniques for queries have on retrieval performance, (3) the performance impact that the underlying retrieval model has on identifying the correct source-code functions (the correct documents). These research questions are addressed using the five open source projects released as part of the {\{}SEMERU{\}} dataset. In the experiments, five methods of scraping queries from modification requests and seven retrieval model instances are considered. Using the standard evaluation metric Mean Reciprocal Rank (MRR), the experimental analysis reveals that better retrieval models are not the ones commonly used by software engineering researchers. Results find that models based on query-likelihood perform about twice as well as models in common use in software engineering such as {\{}LSI{\}} and thus deserve greater attention. Furthermore, corpus preprocessing has a significant impact as the top performing setting is over 100{\%} better than the average. },
	Doi = {https://doi.org/10.1016/j.jss.2014.11.013},
	ISSN = {0164-1212},
	Keywords = {Feature location,Information retrieval models,Query formulation},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121214002428}
}

@Article{Bjarnason201661,
	Title = {{A multi-case study of agile requirements engineering and the use of test cases as requirements}},
	Author = {Bjarnason, Elizabeth and Unterkalmsteiner, Michael and Borg, Markus and Engstr{\"{o}}m, Emelie},
	Journal = {Information and Software Technology},
	Year = {2016},
	Pages = {61--79},
	Volume = {77},
	Abstract = {AbstractContext It is an enigma that agile projects can succeed ‘without requirements' when weak requirements engineering is a known cause for project failures. While agile development projects often manage well without extensive requirements test cases are commonly viewed as requirements and detailed requirements are documented as test cases. Objective We have investigated this agile practice of using test cases as requirements to understand how test cases can support the main requirements activities, and how this practice varies. Method We performed an iterative case study at three companies and collected data through 14 interviews and two focus groups. Results The use of test cases as requirements poses both benefits and challenges when eliciting, validating, verifying, and managing requirements, and when used as a documented agreement. We have identified five variants of the test-cases-as-requirements practice, namely de facto, behaviour-driven, story-test driven, stand-alone strict and stand-alone manual for which the application of the practice varies concerning the time frame of requirements documentation, the requirements format, the extent to which the test cases are a machine executable specification and the use of tools which provide specific support for the practice of using test cases as requirements. Conclusions The findings provide empirical insight into how agile development projects manage and communicate requirements. The identified variants of the practice of using test cases as requirements can be used to perform in-depth investigations into agile requirements engineering. Practitioners can use the provided recommendations as a guide in designing and improving their agile requirements practices based on project characteristics such as number of stakeholders and rate of change. },
	Doi = {https://doi.org/10.1016/j.infsof.2016.03.008},
	ISSN = {0950-5849},
	Keywords = {Acceptance test,Agile development,Behaviour-driven development,Case study,Empirical software engineering,Requirements,Test-driven development,Test-first development,Testing},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584916300544}
}

@Article{Bjarnason20121107,
	Title = {{Are you biting off more than you can chew? A case study on causes and effects of overscoping in large-scale software engineering}},
	Author = {Bjarnason, Elizabeth and Wnuk, Krzysztof and Regnell, Bj{\"{o}}rn},
	Journal = {Information and Software Technology},
	Year = {2012},
	Number = {10},
	Pages = {1107--1124},
	Volume = {54},
	Abstract = {Context Scope management is a core part of software release management and often a key factor in releasing successful software products to the market. In a market-driven case, when only a few requirements are known a priori, the risk of overscoping may increase. Objective This paper reports on findings from a case study aimed at understanding overscoping in large-scale, market-driven software development projects, and how agile requirements engineering practices may affect this situation. Method Based on a hypothesis of which factors that may be involved in an overscoping situation, semi-structured interviews were performed with nine practitioners at a large, market-driven software company. The results from the interviews were validated by six (other) practitioners at the case company via a questionnaire. Results The results provide a detailed picture of overscoping as a phenomenon including a number of causes, root causes and effects, and indicate that overscoping is mainly caused by operating in a fast-moving market-driven domain and how this ever-changing inflow of requirements is managed. Weak awareness of overall goals, in combination with low development involvement in early phases, may contribute to ‘biting off' more than a project can ‘chew'. Furthermore, overscoping may lead to a number of potentially serious and expensive consequences, including quality issues, delays and failure to meet customer expectations. Finally, the study indicates that overscoping occurs also when applying agile requirements engineering practices, though the overload is more manageable and perceived to result in less wasted effort when applying a continuous scope prioritization, in combination with gradual requirements detailing and a close cooperation within cross-functional teams. Conclusion The results provide an increased understanding of scoping as a complex and continuous activity, including an analysis of the causes, effects, and a discussion on possible impact of agile requirements engineering practices to the issue of overscoping. The results presented in this paper can be used to identify potential factors to address in order to achieve a more realistic project scope. },
	Doi = {https://doi.org/10.1016/j.infsof.2012.04.006},
	ISSN = {0950-5849},
	Keywords = {Agile requirements engineering,Case study,Empirical study,Requirements scoping,Software release planning},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584912000778}
}

@Article{Blake200531,
	Title = {{Agent-oriented compositional approaches to services-based cross-organizational workflow}},
	Author = {Blake, M.Brian and Gomaa, Hassan},
	Journal = {Decision Support Systems},
	Year = {2005},
	Number = {1},
	Pages = {31--50},
	Volume = {40},
	Abstract = {With the sophistication and maturity of distributed component-based services and semantic web services, the idea of specification-driven service composition is becoming a reality. One such approach is workflow composition of services that span multiple, distributed web-accessible locations. Given the dynamic nature of this domain, the adaptation of software agents represents a possible solution for the composition and enactment of cross-organizational services. This paper details design aspects of an architecture that would support this evolvable service-based workflow composition. The internal coordination and control aspects of such an architecture is addressed. These agent developmental processes are aligned with industry-standard software engineering processes. },
	Annote = {Web services and process management},
	Doi = {https://doi.org/10.1016/j.dss.2004.04.003},
	ISSN = {0167-9236},
	Keywords = {Agent architectures,Coordination,UML,Web services,Workflow modeling},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167923604000624}
}

@Article{Bodden2012162,
	Title = {{Delta-oriented monitor specification}},
	Author = {Bodden, E and Falzon, K and Pun, K I and Stolz, V},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2012},
	Number = {PART 1},
	Pages = {162--177},
	Volume = {7609 LNCS},
	Abstract = {Delta-oriented programming allows software developers to define software product lines as variations of a common code base, where variations are expressed as so-called program deltas. Monitor-oriented programming (MOP) provides a mechanism to execute functionality based on the execution history of the program; this is useful, e.g., for the purpose of runtime verification and for enforcing security policies. In this work we discuss how delta-oriented programming and MOP can benefit from each other in the Abstract Behavior Specification Language (ABS) through a new approach we call Delta-oriented Monitor Specification (DMS). We use deltas over monitor definitions to concisely capture protocol changes induced by feature combinations, and propose a notation to denote these deltas. In addition, we explore the design space for expressing runtime monitors as program deltas in ABS. A small case study shows that our approach successfully avoids code duplication in monitor specifications and that those specifications can evolve hand in hand with feature definitions. {\textcopyright} 2012 Springer-Verlag.},
	Annote = {cited By 1},
	Doi = {10.1007/978-3-642-34026-0_13},
	Keywords = {Behavior specifications; Code duplication; Design,Specification languages,Specifications},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868275919{\&}doi=10.1007{\%}2F978-3-642-34026-0{\_}13{\&}partnerID=40{\&}md5=81a914f78956478166d71c4d9472c433}
}

@Article{SPE:SPE2416,
	Title = {{Synergies and tradeoffs in software reuse – a systematic mapping study}},
	Author = {Bombonatti, Denise and Goul{\~{a}}o, Miguel and Moreira, Ana},
	Journal = {Software: Practice and Experience},
	Year = {2017},
	Number = {7},
	Pages = {943--957},
	Volume = {47},
	Abstract = {Software reuse is a broadly accepted practice to improve software development quality and productivity. Although an object of study in software engineering since the late sixties, achieving effective reuse remains challenging for many software development organizations. This paper reports a systematic mapping study on how reusability relates to other non-functional requirements and how different contextual factors influence the success of a reuse initiative. The conclusion is that the relationships are discussed rather informally, and that human, organizational, and technological domain factors are extremely relevant to a particular reuse context. This mapping study highlights the need for further research to better understand how exactly the different non-functional requirements and context factors affect reusability. Copyright {\textcopyright} 2016 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spe.2416},
	ISSN = {1097-024X},
	Keywords = {non-functional requirements,reusability,systematic mapping study},
	Url = {http://dx.doi.org/10.1002/spe.2416}
}

@InProceedings{Bonifacio:2009:MSV:1509239.1509258,
	Title = {{Modeling Scenario Variability As Crosscutting Mechanisms}},
	Author = {Bonif{\'{a}}cio, Rodrigo and Borba, Paulo},
	Booktitle = {Proceedings of the 8th ACM International Conference on Aspect-oriented Software Development},
	Year = {2009},
	Address = {New York, NY, USA},
	Pages = {125--136},
	Publisher = {ACM},
	Series = {AOSD '09},
	Doi = {10.1145/1509239.1509258},
	ISBN = {978-1-60558-442-3},
	Keywords = {requirements models,software product line,variability management},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/1509239.1509258}
}

@Article{Bonifacio201797,
	Title = {{Empirical assessment of two approaches for specifying software product line use case scenarios}},
	Author = {Bonif{\'{a}}cio, R and Borba, P and Ferraz, C and Accioly, P},
	Journal = {Software and Systems Modeling},
	Year = {2017},
	Number = {1},
	Pages = {97--123},
	Volume = {16},
	Abstract = {Modularity benefits, including the independent maintenance and comprehension of individual modules, have been widely advocated. However, empirical assessments to investigate those benefits have mostly focused on source code, and thus, the relevance of modularity to earlier artifacts is still not so clear (such as requirements and design models). In this paper, we use a multimethod technique, including designed experiments, to empirically evaluate the benefits of modularity in the context of two approaches for specifying product line use case scenarios: PLUSS and MSVCM. The first uses an annotative approach for specifying variability, whereas the second relies on aspect-oriented constructs for separating common and variant scenario specifications. After evaluating these approaches through the specifications of several systems, we find out that MSVCM reduces feature scattering and improves scenario cohesion. These results suggest that evolving a product line specification using MSVCM requires only localized changes. On the other hand, the results of six experiments reveal that MSVCM requires more time to derive the product line specifications and, contrasting with the modularity results, reduces the time to evolve a product line specification only when the subjects have been well trained and are used to the task of evolving product line specifications. {\textcopyright} 2015, Springer-Verlag Berlin Heidelberg.},
	Annote = {cited By 0},
	Doi = {10.1007/s10270-015-0471-3},
	Keywords = {Aspect-oriented; Designed experiments; Empirical,Computer software; Requirements engineering; Softw,Specifications},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929900234{\&}doi=10.1007{\%}2Fs10270-015-0471-3{\&}partnerID=40{\&}md5=f2635739cdc8744117607019ed6aa88a}
}

@Article{Borba20122,
	Title = {{A theory of software product line refinement}},
	Author = {Borba, Paulo and Teixeira, Leopoldo and Gheyi, Rohit},
	Journal = {Theoretical Computer Science},
	Year = {2012},
	Pages = {2--30},
	Volume = {455},
	Abstract = {To safely evolve a software product line, it is important to have a notion of product line refinement that assures behavior preservation of the original product line products. So in this article we present a language independent theory of product line refinement, establishing refinement properties that justify stepwise and compositional product line evolution. Moreover, we instantiate our theory with the formalization of specific languages for typical product lines artifacts, and then introduce and prove soundness of a number of associated product line refinement transformation templates. These templates can be used to reason about specific product lines and as a basis to derive comprehensive product line refinement catalogues. },
	Annote = {International Colloquium on Theoretical Aspects of Computing 2010},
	Doi = {https://doi.org/10.1016/j.tcs.2012.01.031},
	ISSN = {0304-3975},
	Keywords = {Refactoring,Refinement,Software evolution,Software product lines},
	Url = {http://www.sciencedirect.com/science/article/pii/S0304397512000679}
}

@Article{SPIP:SPIP221,
	Title = {{Staged adoption of software product families}},
	Author = {Bosch, Jan},
	Journal = {Software Process: Improvement and Practice},
	Year = {2005},
	Number = {2},
	Pages = {125--142},
	Volume = {10},
	Doi = {10.1002/spip.221},
	ISSN = {1099-1670},
	Keywords = {adoption framework,organizational structures,software architecture,software features,software product families},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/spip.221}
}

@InProceedings{Bosch:2001:SPL:381473.381599,
	Title = {{Software Product Lines and Software Architecture Design}},
	Author = {Bosch, Jan},
	Booktitle = {Proceedings of the 23rd International Conference on Software Engineering},
	Year = {2001},
	Address = {Washington, DC, USA},
	Pages = {717----},
	Publisher = {IEEE Computer Society},
	Series = {ICSE '01},
	ISBN = {0-7695-1050-7},
	Url = {http://0-dl.acm.org.fama.us.es/citation.cfm?id=381473.381599}
}

@Book{Bosch:2000:DUS:339362,
	Title = {{Design and Use of Software Architectures: Adopting and Evolving a Product-line Approach}},
	Author = {Bosch, Jan},
	Publisher = {ACM Press/Addison-Wesley Publishing Co.},
	Year = {2000},
	Address = {New York, NY, USA},
	ISBN = {0-201-67494-7}
}

@Article{Bosch2011871,
	Title = {{Introducing agile customer-centered development in a legacy software product line}},
	Author = {Bosch, J and Bosch-Sijtsema, P M},
	Journal = {Software - Practice and Experience},
	Year = {2011},
	Number = {8},
	Pages = {871--882},
	Volume = {41},
	Abstract = {The ability to rapidly respond to customer interest and to effectively prioritize development effort has been a long-standing challenge for mass-market software intensive products. This problem is exacerbated in the context of software product lines as functionality may easily fall over software asset and organizational boundaries with consequent losses in efficiency and nimbleness. Some companies facing these problems in their product line respond with a new development process. In this paper we discuss the developments within a single case study, Intuit's Quickbooks product line that combined agile software development, design thinking and self-organizing teams in a successful approach, which provided a significant improvement in terms of responsiveness and accuracy of building customer value. Copyright {\textcopyright} 2011 John Wiley {\&} Sons, Ltd.},
	Annote = {cited By 14},
	Doi = {10.1002/spe.1063},
	Keywords = {Agile software development; compositional software,Customer satisfaction; Network architecture; Prod,Software design},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79958247118{\&}doi=10.1002{\%}2Fspe.1063{\&}partnerID=40{\&}md5=b2bfc7317e1d83ae546384531cf74963}
}

@Article{Bosch2001147,
	Title = {{Product instantiation in software product lines: A case study}},
	Author = {Bosch, J and H{\"{o}}gstr{\"{o}}m, M},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2001},
	Pages = {147--161},
	Volume = {2177},
	Abstract = {Product instantiation is one of the less frequently studied activities in the domain of software product lines. In this paper, we present the results of a case study at Axis Communication AB on product instantiation in an industrial product line, i.e. five problems and three issues. The problems are concerned the insufficiency of functional commonality, features spanning multiple components, the exclusion of unwanted features, the evolution of product line components and the handling of initialization code. The issues discuss architectural compliance versus product instantiation effort, quick-fixes versus properly engineered extensions and component instantiation support versus product instantiation effort. The identified problems and issues are based on the case study, but have been generalized to apply to a wider context. {\textcopyright} Springer-Verlag Berlin Heidelberg 2001.},
	Annote = {cited By 9},
	Keywords = {Architectural compliances; Industrial product; Mu,Computer software,Software engineering},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646842621{\&}partnerID=40{\&}md5=d15c360028691650e4973473f5a8fd3f}
}

@Article{JPIM:JPIM12233,
	Title = {{User Involvement throughout the Innovation Process in High-Tech Industries}},
	Author = {Bosch-Sijtsema, Petra and Bosch, Jan},
	Journal = {Journal of Product Innovation Management},
	Year = {2015},
	Number = {5},
	Pages = {793--807},
	Volume = {32},
	Abstract = {The feedback and input of users have been an important part of product innovation in recent years. User input has been studied from different approaches and is applied through different methods in particular phases of the innovation process. However, these methods are not integrated into the whole innovation process and are used only in particular phases or on an ad hoc basis. New developments in technology, social media, and new ways of working closer with customers have opened up new possibilities for firms to gain user input throughout the whole innovation process. However, the impact that these new developments in technology offer for user input innovation in high-tech firms is unclear. Therefore, we study how high-tech firms collect and apply user feedback throughout the whole innovation process. The paper is based on a comparative case study of eight cases in the high-tech industry, in which qualitative data collection was applied. The key contribution of the paper is a conceptual framework on user data-driven innovation throughout the innovation cycle. This framework gives insight into user involvement types and approaches to collect and apply user feedback throughout the innovation process.},
	Doi = {10.1111/jpim.12233},
	ISSN = {1540-5885},
	Url = {http://dx.doi.org/10.1111/jpim.12233}
}

@Article{Bouarar2015332,
	Title = {{SPL driven approach for variability in database design}},
	Author = {Bouarar, S and Jean, S and Siegmund, N},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2015},
	Pages = {332--342},
	Volume = {9344},
	Abstract = {The evolution of computer technology has strongly impacted the database design. No phase was spared: several conceptual formalisms (e.g. ER, UML, ontological), various logical models (e.g. relational, object, key-value), a wide panoply of physical optimization structures and deployment platforms have been proposed. As a result, the database design process has become more complex involving more tasks and even more actors (as database architect or analyst). Getting inspired from software engineering in dealing with variable similar systems, we propose a methodological framework for a variability-aware design of databases, whereby this latter is henceforth devised as a Software Product Line. Doing so guarantees a high reuse, automation, and customizability in generating ready-to-be implemented databases. We also propose a solution to help users make a suitable choice among the wide panoply. Finally, a case study is presented. {\textcopyright} Springer International Publishing Switzerland 2015.},
	Annote = {cited By 1},
	Doi = {10.1007/978-3-319-23781-7_27},
	Keywords = {Computer software; Database systems; Design; Softw,Computer technology; Customizability; Database de,Product design},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951776613{\&}doi=10.1007{\%}2F978-3-319-23781-7{\_}27{\&}partnerID=40{\&}md5=da9040f7cdddcfd957d7f76bacf2c976}
}

@Article{Boucke20102108,
	Title = {{Composition of architectural models: Empirical analysis and language support}},
	Author = {Bouck{\'{e}}, Nelis and Weyns, Danny and Holvoet, Tom},
	Journal = {Journal of Systems and Software},
	Year = {2010},
	Number = {11},
	Pages = {2108--2127},
	Volume = {83},
	Abstract = {Managing the architectural description (AD) of a complex software system and maintaining consistency among the different models is a demanding task. To understand the underlying problems, we analyse several non-trivial software architectures. The empirical study shows that a substantial amount of information of {\{}ADs{\}} is repeated, mainly by integrating information of different models in new models. Closer examination reveals that the absence of rigorously specified dependencies among models and the lack of support for automated composition of models are primary causes of management and consistency problems in software architecture. To tackle these problems, we introduce an approach in which compositions of models, together with relations among models, are explicitly supported in the ADL. We introduce these concepts formally and discuss a proof-of-concept instantiation of composition in xADL and its supporting tools. The approach is evaluated by comparing the original and revised {\{}ADs{\}} in an empirical study. The study indicates that our approach reduces the number of manually specified elements by 29{\%}, and reduces the number of manual changes to elements for several realistic change scenarios by 52{\%}. },
	Annote = {Interplay between Usability Evaluation and Software Development},
	Doi = {https://doi.org/10.1016/j.jss.2010.06.011},
	ISSN = {0164-1212},
	Keywords = {Architectural description language (ADL),Architectural models,Composition,Empirical analysis,Relations,Software architecture},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121210001639}
}

@Article{Boudaa201717,
	Title = {{An aspect-oriented model-driven approach for building adaptable context-aware service-based applications}},
	Author = {Boudaa, Boudjemaa and Hammoudi, Slimane and Mebarki, Leila Amel and Bouguessa, Abdelkader and Chikh, Mohammed Amine},
	Journal = {Science of Computer Programming},
	Year = {2017},
	Pages = {17--42},
	Volume = {136},
	Abstract = {AbstractContext Context-aware service-based applications development has been considered among the most studied research fields in the last decade. The objective was to accompany the rapid technology evolution of mobile computing devices by providing customized services able to interact with different contextual situations of a pervasive environment. For this purpose, many research works have advocated Model-Driven Development (MDD) for building context-aware service-based applications. However, the proposed approaches have presented specific methodologies without using development standards, which may be followed by developers. In addition, most of them have ignored the dynamic adaptation aspect at runtime that should characterize such kind of applications and no adaptation strategy was considered in their proposals. Objective The current paper aims to propose a generic model-driven approach for context-aware service-based applications engineering with a software development methodology including a reconfiguration loop to achieve the dynamic adaptation of these applications. Method This approach focuses on the combination of {\{}MDD{\}} and Aspect Oriented Modelling (AOM) to take advantage of their benefits. {\{}AOM{\}} encapsulates different context-awareness logics separately in aspect models called ContextAspect that can be easily woven into the service's business logic according to the changing context over time. The proposed development methodology includes four phases (modelling, composition, transformation and adaptation) which act in conformance with the {\{}MDA{\}} technology. Results The main results gained by using the present approach are the possibility to combine the {\{}MDA{\}} technology with the aspect-oriented paradigm in a generic development methodology for context-aware service-based applications, and the handling of their dynamic adaptation at execution time according to the changes in the context. Conclusion The development of context-aware applications is a complex, cumbersome, and time-consuming task. However, the experience reached by implementing the proposed methodology leads us to believe that the involvement of {\{}MDD{\}} and {\{}AOM{\}} is significantly beneficial to overcome some recognised shortcomings of several existing approaches and to make this task simpler, easier and faster. },
	Doi = {https://doi.org/10.1016/j.scico.2016.08.009},
	ISSN = {0167-6423},
	Keywords = {Aspect weaving,Context-aware service-based application,ContextAspect,Dynamic adaptation,Model-driven methodology},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642316301253}
}

@InProceedings{Brabrand:2012:IDA:2162049.2162052,
	Title = {{Intraprocedural Dataflow Analysis for Software Product Lines}},
	Author = {Brabrand, Claus and Ribeiro, M{\'{a}}rcio and Tol{\^{e}}do, T{\'{a}}rsis and Borba, Paulo},
	Booktitle = {Proceedings of the 11th Annual International Conference on Aspect-oriented Software Development},
	Year = {2012},
	Address = {New York, NY, USA},
	Pages = {13--24},
	Publisher = {ACM},
	Series = {AOSD '12},
	Doi = {10.1145/2162049.2162052},
	ISBN = {978-1-4503-1092-5},
	Keywords = {dataflow analysis,software product lines},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2162049.2162052}
}

@Article{Breß201460,
	Title = {{Load-aware inter-co-processor parallelism in database query processing}},
	Author = {Bre{\ss}, Sebastian and Siegmund, Norbert and Heimel, Max and Saecker, Michael and Lauer, Tobias and Bellatreche, Ladjel and Saake, Gunter},
	Journal = {Data {\&} Knowledge Engineering},
	Year = {2014},
	Pages = {60--79},
	Volume = {93},
	Abstract = {Abstract For a decade, the database community has been exploring graphics processing units and other co-processors to accelerate query processing. While the developed algorithms often outperform their {\{}CPU{\}} counterparts, it is not beneficial to keep processing devices idle while overutilizing others. Therefore, an approach is needed that efficiently distributes a workload on available (co-)processors while providing accurate performance estimates for the query optimizer. In this paper, we contribute heuristics that optimize query processing for response time and throughput simultaneously via inter-device parallelism. Our empirical evaluation reveals that the new approach achieves speedups up to 1.85 compared to state-of-the-art approaches while preserving accurate performance estimations. In a further series of experiments, we evaluate our approach on two new use cases: joining and sorting. Furthermore, we use a simulation to assess the performance of our approach for systems with multiple co-processors and derive some general rules that impact performance in those systems. },
	Annote = {Selected Papers from the 17th East-¬-European Conference on Advances in Databases and Information Systems},
	Doi = {https://doi.org/10.1016/j.datak.2014.07.003},
	ISSN = {0169-023X},
	Keywords = {Co-processing,Query optimization,Query processing},
	Url = {http://www.sciencedirect.com/science/article/pii/S0169023X14000627}
}

@Article{Breivold201216,
	Title = {{A systematic review of software architecture evolution research}},
	Author = {Breivold, Hongyu Pei and Crnkovic, Ivica and Larsson, Magnus},
	Journal = {Information and Software Technology},
	Year = {2012},
	Number = {1},
	Pages = {16--40},
	Volume = {54},
	Abstract = {Context Software evolvability describes a software system's ability to easily accommodate future changes. It is a fundamental characteristic for making strategic decisions, and increasing economic value of software. For long-lived systems, there is a need to address evolvability explicitly during the entire software lifecycle in order to prolong the productive lifetime of software systems. For this reason, many research studies have been proposed in this area both by researchers and industry practitioners. These studies comprise a spectrum of particular techniques and practices, covering various activities in software lifecycle. However, no systematic review has been conducted previously to provide an extensive overview of software architecture evolvability research. Objective In this work, we present such a systematic review of architecting for software evolvability. The objective of this review is to obtain an overview of the existing approaches in analyzing and improving software evolvability at architectural level, and investigate impacts on research and practice. Method The identification of the primary studies in this review was based on a pre-defined search strategy and a multi-step selection process. Results Based on research topics in these studies, we have identified five main categories of themes: (i) techniques supporting quality consideration during software architecture design, (ii) architectural quality evaluation, (iii) economic valuation, (iv) architectural knowledge management, and (v) modeling techniques. A comprehensive overview of these categories and related studies is presented. Conclusion The findings of this review also reveal suggestions for further research and practice, such as (i) it is necessary to establish a theoretical foundation for software evolution research due to the fact that the expertise in this area is still built on the basis of case studies instead of generalized knowledge; (ii) it is necessary to combine appropriate techniques to address the multifaceted perspectives of software evolvability due to the fact that each technique has its specific focus and context for which it is appropriate in the entire software lifecycle. },
	Doi = {https://doi.org/10.1016/j.infsof.2011.06.002},
	ISSN = {0950-5849},
	Keywords = {Architecture analysis,Architecture evolution,Evolvability analysis,Software architecture,Software evolvability,Systematic review},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584911001376}
}

@InProceedings{Brennan:2010:APT:1852786.1852857,
	Title = {{Adaptability Performance Trade-off: A Controlled Experiment}},
	Author = {Brennan, Adam and Greer, Des and McDaid, Kevin},
	Booktitle = {Proceedings of the 2010 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
	Year = {2010},
	Address = {New York, NY, USA},
	Pages = {56:1----56:4},
	Publisher = {ACM},
	Series = {ESEM '10},
	Doi = {10.1145/1852786.1852857},
	ISBN = {978-1-4503-0039-1},
	Keywords = {adaptability,design patterns,performance},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/1852786.1852857}
}

@Article{SMR:SMR304,
	Title = {{Architectural support in industry: a reflection using C-POSH}},
	Author = {Bril, R J and Krikhaar, R L and Postma, A},
	Journal = {Journal of Software Maintenance and Evolution: Research and Practice},
	Year = {2005},
	Number = {1},
	Pages = {3--25},
	Volume = {17},
	Abstract = {Software architecture plays a vital role in the development (and hence maintenance) of large complex systems (containing millions of lines of code) with a long lifetime. It is therefore required that the software architecture is also maintained, i.e., sufficiently documented, clearly communicated, and explicitly controlled during its life-cycle. In our experience, these requirements cannot be met without appropriate support.Commercial-off-the-shelf support for architectural maintenance is still scarcely available, if at all, implying the need to develop appropriate proprietary means. In this paper, we reflect upon software architecture maintenance taken within three organizations within Philips that develop professional systems. We extensively describe the experience gained with introducing and embedding of architectural support in these three organizations. We focus on architectural support in the area of software architecture recovery, visualization, analysis, and verification.In our experience, the support must be carried by a number of pillars of software development, and all of these pillars have to go through a change process to ensure sustainable embedding. Managing these changes requires several key roles to be fulfilled in the organization: a champion, a company angel, a change agent, and a target. We call our reflection model C-POSH, which is an acronym for Change management of the four identified pillars of software development: Process, Organization, Software development environment, and Humans. Our experiences will be presented in terms of the C-POSH model. Copyright {\textcopyright} 2005 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/smr.304},
	ISSN = {1532-0618},
	Keywords = {change management,development process,organization,software architecture,software development environment},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/smr.304}
}

@Article{Brosch20103,
	Title = {{Combining Architecture-based Software Reliability Predictions with Financial Impact Calculations}},
	Author = {Brosch, Franz and Gitzel, Ralf and Koziolek, Heiko and Krug, Simone},
	Journal = {Electronic Notes in Theoretical Computer Science},
	Year = {2010},
	Number = {1},
	Pages = {3--17},
	Volume = {264},
	Abstract = {Software failures can lead to substantial costs for the user. Existing models for software reliability prediction do not provide much insight into this financial impact. Our approach presents a first step towards the integration of reliability prediction from the {\{}IT{\}} perspective and the business perspective. We show that failure impact should be taken into account not only at their date of occurrence but already in the design stage of the development. First we model cost relevant business processes as well as the associated {\{}IT{\}} layer and then connect them to failure probabilities. Based on this we conduct a reliability and cost estimation. The method is illustrated by a case study. },
	Annote = {Proceedings of the 7th International Workshop on Formal Engineering approaches to Software Components and Architectures (FESCA 2010)},
	Doi = {https://doi.org/10.1016/j.entcs.2010.07.002},
	ISSN = {1571-0661},
	Keywords = {Cost,Reliability,Software failure},
	Url = {http://www.sciencedirect.com/science/article/pii/S1571066110000617}
}

@Article{Brugali2012361,
	Title = {{A reuse-oriented development process for component-based robotic systems}},
	Author = {Brugali, D and Gherardi, L and Biziak, A and Luzzana, A and Zakharov, A},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2012},
	Pages = {361--374},
	Volume = {7628 LNAI},
	Abstract = {State of the art in robot software development mostly relies on class library reuse and only to a limited extent to component-based design. In the BRICS project we have defined a software development process that is based on the two most recent and promising approaches to software reuse, i.e. Software Product Line (SPL) and Model-Driven Engineering (MDE). The aim of this paper is to illustrate the whole software development process that we have defined for developing flexible and reusable component-based robotics libraries, to exemplify it with the case study of robust navigation functionality, and to present the software tools that we have developed for supporting the proposed process. {\textcopyright} 2012 Springer-Verlag Berlin Heidelberg.},
	Annote = {cited By 9},
	Doi = {10.1007/978-3-642-34327-8_33},
	Keywords = {Class libraries; Component based; Component based,Computer software reusability; Robots,Software design},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868026835{\&}doi=10.1007{\%}2F978-3-642-34327-8{\_}33{\&}partnerID=40{\&}md5=a0a0f5546d6072db1d495974364b898b}
}

@Article{BLTJ:BLTJ2220,
	Title = {{Automated software development with XML and the Java* language}},
	Author = {Bruns, Glenn R and Frey, Alan E and Mataga, Peter A and Tripp, Susan J},
	Journal = {Bell Labs Technical Journal},
	Year = {2000},
	Number = {2},
	Pages = {32--43},
	Volume = {5},
	Abstract = {In software development with domain-specific languages (DSLs), one defines a requirements language for an application domain and then develops a compiler to generate an implementation from a requirements document. Because DSLs and DSL compilers are expensive to develop, DSLs are seen as cost effective only when many products of the same domain will be developed. In this paper, we show how the cost of DSL design and DSL compiler development can be reduced by defining DSLs as Extensible-Markup-Language (XML) dialects and by developing DSL compilers using commercial XML tools and the Java* language. This approach is illustrated through the Call View Data Language (CDL), a new DSL that generates provisioning support code and database table definitions for Lucent Technologies' 7R/E™ Network Feature Server.},
	Doi = {10.1002/bltj.2220},
	ISSN = {1538-7305},
	Publisher = {Wiley Subscription Services, Inc., A Wiley Company},
	Url = {http://dx.doi.org/10.1002/bltj.2220}
}

@Article{Buccella20139,
	Title = {{Towards systematic software reuse of GIS: Insights from a case study}},
	Author = {Buccella, Agustina and Cechich, Alejandra and Arias, Maximiliano and Pol'la, Matias and Doldan, Maria del Socorro and Morsan, Enrique},
	Journal = {Computers and Geosciences},
	Year = {2013},
	Pages = {9--20},
	Volume = {54},
	Abstract = {With the development and adoption of geographic information systems, there is an increasingly amount of software resources being stored or recorded as products to be reused. At the same time, complexity of geographic services is addressed through standardization, which allows developers reaching higher quality levels. In this paper, we introduce our domain-oriented approach to developing geographic software product lines focusing on the experiences collected from a case study. It was developed in the Marine Ecology Domain (Patagonia, Argentina) and illustrates insights of the process. {\textcopyright} 2013.},
	Annote = {From Duplicate 1 (Towards systematic software reuse of GIS: Insights from a case study - Buccella, A; Cechich, A; Arias, M; Pol'la, M; Doldan, M D S; Morsan, E)
		cited By 7},
	Doi = {10.1016/j.cageo.2012.11.014},
	File = {:Users/mac/Downloads/Sciencedirect{\_}articles{\_}13Jun2017{\_}15-34-52.618/Towards-systematic-software-reuse-of-GIS-Insights-from-a-case-study{\_}2013{\_}Computers-Geosciences.pdf:pdf},
	ISBN = {0098-3004},
	ISSN = {00983004},
	Keywords = {Computer software,GIS,Geographic information systems,Geographic open source tools,ISO 19119 std,ISO 19119 std.,Marine ecology,Open source tools,Patagonia,Reused services,Software product lines,complexity,computer simulation,marine ecos},
	Publisher = {Elsevier},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874512859{\&}doi=10.1016{\%}2Fj.cageo.2012.11.014{\&}partnerID=40{\&}md5=0c1cf8fbaa740dfc5873d8f7e95167cc http://dx.doi.org/10.1016/j.cageo.2012.11.014}
}

@Article{Buchmann2013630,
	Title = {{MOD2-SCM: A model-driven product line for software configuration management systems}},
	Author = {Buchmann, Thomas and Dotor, Alexander and Westfechtel, Bernhard},
	Journal = {Information and Software Technology},
	Year = {2013},
	Number = {3},
	Pages = {630--650},
	Volume = {55},
	Abstract = {Context Software Configuration Management (SCM) is the discipline of controlling the evolution of large and complex software systems. Over the years many different {\{}SCM{\}} systems sharing similar concepts have been implemented from scratch. Since these concepts usually are hard-wired into the respective program code, reuse is hardly possible. Objective Our objective is to create a model-driven product line for {\{}SCM{\}} systems. By explicitly describing the different concepts using models, reuse can be performed on the modeling level. Since models are executable, the need for manual programming is eliminated. Furthermore, by providing a library of loosely coupled modules, we intend to support flexible composition of {\{}SCM{\}} systems. Method We developed a method and a tool set for model-driven software product line engineering which we applied to the {\{}SCM{\}} domain. For domain analysis, we applied the {\{}FORM{\}} method, resulting in a layered feature model for {\{}SCM{\}} systems. Furthermore, we developed an executable object-oriented domain model which was annotated with features from the feature model. A specific {\{}SCM{\}} system is configured by selecting features from the feature model and elements of the domain model realizing these features. Results Due to the orthogonality of both feature model and domain model, a very large number of {\{}SCM{\}} systems may be configured. We tested our approach by creating instances of the product line which mimic wide-spread systems such as CVS, GIT, Mercurial, and Subversion. Conclusion The experiences gained from this project demonstrate the feasibility of our approach to model-driven software product line engineering. Furthermore, our work advances the state of the art in the domain of {\{}SCM{\}} systems since it support the modular composition of {\{}SCM{\}} systems at the model rather than the code level. },
	Annote = {Special Issue on Software Reuse and Product LinesSpecial Issue on Software Reuse and Product Lines},
	Doi = {https://doi.org/10.1016/j.infsof.2012.07.010},
	ISSN = {0950-5849},
	Keywords = {Code generation,Executable models,Feature models,Model transformation,Model-driven software engineering,Software configuration management,Software product line engineering},
	Url = {http://www.sciencedirect.com/science/article/pii/S095058491200136X}
}

@Article{SMR:SMR364,
	Title = {{Encapsulating targeted component abstractions using software Reflexion Modelling}},
	Author = {Buckley, Jim and LeGear, Andrew P and Exton, Chris and Cadogan, Ross and Johnston, Trevor and Looby, Bill and Koschke, Rainer},
	Journal = {Journal of Software Maintenance and Evolution: Research and Practice},
	Year = {2008},
	Number = {2},
	Pages = {107--134},
	Volume = {20},
	Abstract = {Design abstractions such as components, modules, subsystems or packages are often not made explicit in the implementation of legacy systems. Indeed, often the abstractions that are made explicit turn out to be inappropriate for future evolution agendas. This can make the maintenance, evolution and refactoring of these systems difficult. In this publication, we carry out a fine-grained evaluation of Reflexion Modelling as a technique for encapsulating user-targeted components. This process is a prelude to component recovery, reuse and refactoring. The evaluation takes the form of two in vivo case studies, where two professional software developers encapsulate components in a large, commercial software system. The studies demonstrate the validity of this approach and offer several best-use guidelines. Specifically, they argue that users benefit from having a strong mental model of the system in advance of Reflexion Modelling, even if that model is flawed, and that users should expend effort exploring the expected relationships present in Reflexion Models. Copyright {\textcopyright} 2008 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/smr.364},
	ISSN = {1532-0618},
	Keywords = {architecture recovery,component recovery,re-engineering,reflexion,software maintenance},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/smr.364}
}

@Article{Burgareli20081,
	Title = {{A variability management strategy for software product lines of Brazilian satellite luncher vehicles}},
	Author = {Burgareli, L A and Melnikoff, S S S and Ferreira, M G V},
	Journal = {Studies in Computational Intelligence},
	Year = {2008},
	Pages = {1--14},
	Volume = {150},
	Abstract = {The Product Line approach offers to the software development benefits such as savings, large-scale productivity and increased product quality. The management of variability is a key and challenging issue in the development of the software product line and product derivation. This work presents a strategy for the variability management for software product line of Brazilian Satellite Launcher Vehicles. After modeling the variability, extracting them from use case diagrams and features, the proposed strategy uses a variation mechanism based on a set of Adaptive Design Patterns as support in the creation of variants. The proposed strategy uses as case study the software system of an existing specific vehicle, the Brazilian Satellite Launcher (BSL). {\textcopyright} Springer-Verlag Berlin Heidelberg 2008.},
	Annote = {cited By 1},
	Doi = {10.1007/978-3-540-70561-1_1},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-59549088931{\&}doi=10.1007{\%}2F978-3-540-70561-1{\_}1{\&}partnerID=40{\&}md5=48b5f80ab238e0fb937c5c2af0267953}
}

@Article{Camara2009116,
	Title = {{Facilitating controlled tests of website design changes using aspect-oriented software development and software product lines}},
	Author = {C{\'{a}}mara, J and Kobsa, A},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2009},
	Pages = {116--135},
	Volume = {5740 LNCS},
	Abstract = {Controlled online experiments in which envisaged changes to a website are first tested live with a small subset of site visitors have proven to predict the effects of these changes quite accurately. However, these experiments often require expensive infrastructure and are costly in terms of development effort. This paper advocates a systematic approach to the design and implementation of such experiments in order to overcome the aforementioned drawbacks by making use of Aspect-Oriented Software Development and Software Product Lines. {\textcopyright} 2009 Springer.},
	Annote = {cited By 1},
	Doi = {10.1007/978-3-642-03722-1_5},
	Keywords = {Aspect oriented software development; Controlled t,Computer software,Computer systems programming; Experiments; Softwa},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350587132{\&}doi=10.1007{\%}2F978-3-642-03722-1{\_}5{\&}partnerID=40{\&}md5=f0c491bcf7c149426d1da832cf42dee3}
}

@Article{Cabanillas201555,
	Title = {{Specification and automated design-time analysis of the business process human resource perspective}},
	Author = {Cabanillas, Cristina and Resinas, Manuel and Del-R{\'{i}}o-Ortega, Adela and Ruiz-Cort{\'{e}}s, Antonio},
	Journal = {Information Systems},
	Year = {2015},
	Pages = {55--82},
	Volume = {52},
	Abstract = {Abstract The human resource perspective of a business process is concerned with the relation between the activities of a process and the actors who take part in them. Unlike other process perspectives, such as control flow, for which many different types of analyses have been proposed, such as finding deadlocks, there is an important gap regarding the human resource perspective. Resource analysis in business processes has not been defined, and only a few analysis operations can be glimpsed in previous approaches. In this paper, we identify and formally define seven design-time analysis operations related to how resources are involved in process activities. Furthermore, we demonstrate that for a wide variety of resource-aware {\{}BP{\}} models, those analysis operations can be automated by leveraging Description Logic (DL) off-the-shelf reasoners. To this end, we rely on Resource Assignment Language (RAL), a domain-specific language that enables the definition of conditions to select the candidates to participate in a process activity. We provide a complete formal semantics for {\{}RAL{\}} based on {\{}DLs{\}} and extend it to address the operations, for which the control flow of the process must also be taken into consideration. A proof-of-concept implementation has been developed and integrated in a system called CRISTAL. As a result, we can give an automatic answer to different questions related to the management of resources in business processes at design time. },
	Annote = {Special Issue on Selected Papers from {\{}SISAP{\}} 2013},
	Doi = {https://doi.org/10.1016/j.is.2015.03.002},
	ISSN = {0306-4379},
	Keywords = {Analysis operation,Automated analysis,Business process management,Human resource perspective,RAL,Resource assignment},
	Url = {http://www.sciencedirect.com/science/article/pii/S0306437915000460}
}

@Article{Cabral2010241,
	Title = {{Improving the testing and testability of software product lines}},
	Author = {Cabral, I and Cohen, M B and Rothermel, G},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2010},
	Pages = {241--255},
	Volume = {6287 LNCS},
	Abstract = {Software Product Line (SPL) engineering offers several advantages in the development of families of software products. There is still a need, however, for better understanding of testability issues and for testing techniques that can operate cost-effectively on SPLs. In this paper we consider these testability issues and highlight some differences between optional versus alternative features. We then provide a graph based testing approach called the FIG Basis Path method that selects products and features for testing based on a feature dependency graph. We conduct a case study on several non-trivial SPLs and show that for these subjects, the FIG Basis Path method is as effective as testing all products, but tests no more than 24{\%} of the products in the SPL. {\textcopyright} 2010 Springer-Verlag Berlin Heidelberg.},
	Annote = {cited By 9},
	Doi = {10.1007/978-3-642-15579-6_17},
	Keywords = {Dependency graphs; Graph-based; Non-trivial; Path,Network architecture; Testing,Software design},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049355116{\&}doi=10.1007{\%}2F978-3-642-15579-6{\_}17{\&}partnerID=40{\&}md5=c5952bce2ebb6aa5b2e37ca2f83771da}
}

@Article{Cacho2014117,
	Title = {{Blending design patterns with aspects: A quantitative study}},
	Author = {Cacho, Nelio and Sant'anna, Claudio and Figueiredo, Eduardo and Dantas, Francisco and Garcia, Alessandro and Batista, Thais},
	Journal = {Journal of Systems and Software},
	Year = {2014},
	Pages = {117--139},
	Volume = {98},
	Abstract = {Abstract Design patterns often need to be blended (or composed) when they are instantiated in a software system. The composition of design patterns consists of assigning multiple pattern elements into overlapping sets of classes in a software system. Whenever the modularity of each design pattern is not preserved in the source code, their implementation becomes tangled with each other and with the classes' core responsibilities. As a consequence, the change or removal of each design pattern will be costly or prohibitive as the software system evolves. In fact, composing design patterns is much harder than instantiating them in an isolated manner. Previous studies have found design pattern implementations are naturally crosscutting in object-oriented systems, thereby making it difficult to modularly compose them. Therefore, aspect-oriented programming (AOP) has been pointed out as a natural alternative for modularizing and blending design patterns. However, there is little empirical knowledge on how {\{}AOP{\}} models influence the composability of widely used design patterns. This paper investigates the influence of using {\{}AOP{\}} models for composing the Gang-of-Four design patterns. Our study categorizes different forms of pattern composition and studies the benefits and drawbacks of {\{}AOP{\}} in these contexts. We performed assessments of several pair-wise compositions taken from 3 medium-sized systems implemented in Java and two {\{}AOP{\}} models, namely, AspectJ and Compose*. We also considered complex situations where more than two patterns involved in each composition, and the patterns were interacting with other aspects implementing other crosscutting concerns of the system. In general, we observed two dominant factors impacting the pattern composability with AOP: (i) the category of the pattern composition, and (ii) the AspectJ idioms used to implement the design patterns taking part in the composition. },
	Doi = {https://doi.org/10.1016/j.jss.2014.08.041},
	ISSN = {0164-1212},
	Keywords = {Aspect-oriented programming,Composability,Design patterns,Empirical studies,Metrics},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121214001885}
}

@InProceedings{Cacho:2006:CDP:1119655.1119672,
	Title = {{Composing Design Patterns: A Scalability Study of Aspect-oriented Programming}},
	Author = {Cacho, Nelio and Sant'Anna, Claudio and Figueiredo, Eduardo and Garcia, Alessandro and Batista, Thais and Lucena, Carlos},
	Booktitle = {Proceedings of the 5th International Conference on Aspect-oriented Software Development},
	Year = {2006},
	Address = {New York, NY, USA},
	Pages = {109--121},
	Publisher = {ACM},
	Series = {AOSD '06},
	Doi = {10.1145/1119655.1119672},
	ISBN = {1-59593-300-X},
	Keywords = {aspect-oriented programming,composability,design patterns,empirical studies,metrics},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/1119655.1119672}
}

@Article{Cafeo201637,
	Title = {{Feature dependencies as change propagators: An exploratory study of software product lines}},
	Author = {Cafeo, Bruno B P and Cirilo, Elder and Garcia, Alessandro and Dantas, Francisco and Lee, Jaejoon},
	Journal = {Information and Software Technology},
	Year = {2016},
	Pages = {37--49},
	Volume = {69},
	Abstract = {AbstractContext A Software Product Line (SPL) is a set of software systems that share common functionalities, so-called features. When features are related, we consider this relation a feature dependency. Whenever a new feature is added, the presence of feature dependencies in the source code may increase the maintenance effort. In particular, along the maintenance of {\{}SPL{\}} implementation, added features may induce changes in other features, the so-called change propagation. Change propagation is the set of ripple changes required to other features whenever a particular feature is added or changed. Objective The relationship between feature dependency and change propagation is not well understood. Therefore, the objective of our study is to examine the relation between feature dependency and change propagation. Method We investigate change propagation through feature dependencies in additive changes on five evolving SPLs. We analysed a wide range of additive changes in 21 representations of those SPLs. This analysis enabled us to understand whether and how features dependencies and change propagations are related. Results The results have empirically confirmed for the first time the strong relation between feature dependency and change propagation. We also identified what are the circumstances involving dependent features that are more likely to cause change propagation. Surprisingly, the results also suggested that the extent of change propagation across {\{}SPL{\}} features might be higher than the one found in previous studies of dependent modules in non-SPLs. We also found a concentration of change propagation in a few feature dependencies. Conclusion Even though the results show that there is a strong relation between feature dependencies and change propagation, such relation is not alike for all dependencies. This indicates that (i) a general feature dependency minimisation might not ameliorate the change propagation, and (ii) feature dependency properties must be analysed beforehand to drive maintenance effort to important dependencies. },
	Doi = {https://doi.org/10.1016/j.infsof.2015.08.009},
	ISSN = {0950-5849},
	Keywords = {Change propagation,Feature dependency,Maintenance,Software Product Line},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584915001512}
}

@InProceedings{Caneill:2014:DLH:2652524.2652528,
	Title = {{Debsources: Live and Historical Views on Macro-level Software Evolution}},
	Author = {Caneill, Matthieu and Zacchiroli, Stefano},
	Booktitle = {Proceedings of the 8th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
	Year = {2014},
	Address = {New York, NY, USA},
	Pages = {28:1----28:10},
	Publisher = {ACM},
	Series = {ESEM '14},
	Doi = {10.1145/2652524.2652528},
	ISBN = {978-1-4503-2774-9},
	Keywords = {debian,free software,open source,software evolution,source code},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2652524.2652528}
}

@Article{Capilla20143,
	Title = {{An overview of Dynamic Software Product Line architectures and techniques: Observations from research and industry}},
	Author = {Capilla, Rafael and Bosch, Jan and Trinidad, Pablo and Ruiz-Cort{\'{e}}s, Antonio and Hinchey, Mike},
	Journal = {Journal of Systems and Software},
	Year = {2014},
	Pages = {3--23},
	Volume = {91},
	Abstract = {Abstract Over the last two decades, software product lines have been used successfully in industry for building families of systems of related products, maximizing reuse, and exploiting their variable and configurable options. In a changing world, modern software demands more and more adaptive features, many of them performed dynamically, and the requirements on the software architecture to support adaptation capabilities of systems are increasing in importance. Today, many embedded system families and application domains such as ecosystems, service-based applications, and self-adaptive systems demand runtime capabilities for flexible adaptation, reconfiguration, and post-deployment activities. However, as traditional software product line architectures fail to provide mechanisms for runtime adaptation and behavior of products, there is a shift toward designing more dynamic software architectures and building more adaptable software able to handle autonomous decision-making, according to varying conditions. Recent development approaches such as Dynamic Software Product Lines (DSPLs) attempt to face the challenges of the dynamic conditions of such systems but the state of these solution architectures is still immature. In order to provide a more comprehensive treatment of {\{}DSPL{\}} models and their solution architectures, in this research work we provide an overview of the state of the art and current techniques that, partially, attempt to face the many challenges of runtime variability mechanisms in the context of Dynamic Software Product Lines. We also provide an integrated view of the challenges and solutions that are necessary to support runtime variability mechanisms in {\{}DSPL{\}} models and software architectures. },
	Doi = {https://doi.org/10.1016/j.jss.2013.12.038},
	ISSN = {0164-1212},
	Keywords = {Dynamic Software Product Lines,Dynamic variability,Feature models,Software architecture},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121214000119}
}

@Article{SYS:SYS20205,
	Title = {{Managing software development information in global configuration management activities}},
	Author = {Capilla, Rafael and Due{\~{n}}as, Juan C and Krikhaar, Ren{\'{e}}},
	Journal = {Systems Engineering},
	Year = {2012},
	Number = {3},
	Pages = {241--254},
	Volume = {15},
	Doi = {10.1002/sys.20205},
	ISSN = {1520-6858},
	Keywords = {collaboration,global software development,outsourcing,product data management,software configuration management,software product lines},
	Publisher = {Wiley Subscription Services, Inc., A Wiley Company},
	Url = {http://dx.doi.org/10.1002/sys.20205}
}

@Article{SMR:SMR419,
	Title = {{Viability for codifying and documenting architectural design decisions with tool support}},
	Author = {Capilla, Rafael and Due{\~{n}}as, Juan C and Nava, Francisco},
	Journal = {Journal of Software Maintenance and Evolution: Research and Practice},
	Year = {2010},
	Number = {2},
	Pages = {81--119},
	Volume = {22},
	Abstract = {Current software architecture practices have been focused on modeling and documenting the architecture of a software system by means of several architectural views. In practice, the standard architecture documentation lacks explicit description of the decisions made and their underlying rationale, which often leads to knowledge loss. This fact strongly affects the maintenance activities as we need to spend additional effort to replay the decisions made as well as to understand the changes performed in the design. Hence, codifying this architectural knowledge is a challenging task that requires adequate tool support. In this research, we test the capabilities of Architecture Design Decision Support System (ADDSS), a web-based tool for supporting the creation, maintenance, use, and documentation of architectural design decisions (ADD) with their architectures. We used ADDSS to codify architectural knowledge and to maintain those trace links between the design decisions and other software artefacts that would help in the maintenance operations. We illustrate the usage of the tool through four different experiences and discuss the potential benefits of using this architectural knowledge and its impact on the maintenance and evolution activities. Copyright {\textcopyright} 2009 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/smr.419},
	ISSN = {1532-0618},
	Keywords = {architectural knowledge,design decisions,design rationale,software architecture,software maintenance,traceability},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/smr.419}
}

@Article{Capilla2016191,
	Title = {{10 years of software architecture knowledge management: Practice and future}},
	Author = {Capilla, Rafael and Jansen, Anton and Tang, Antony and Avgeriou, Paris and Babar, Muhammad Ali},
	Journal = {Journal of Systems and Software},
	Year = {2016},
	Pages = {191--205},
	Volume = {116},
	Abstract = {Abstract The importance of architectural knowledge (AK) management for software development has been highlighted over the past ten years, where a significant amount of research has been done. Since the first systems using design rationale in the seventies and eighties to the more modern approaches using {\{}AK{\}} for designing software architectures, a variety of models, approaches, and research tools have leveraged the interests of researchers and practitioners in {\{}AK{\}} management (AKM). Capturing, sharing, and using {\{}AK{\}} has many benefits for software designers and maintainers, but the cost to capture this relevant knowledge hampers a widespread use by software companies. However, as the improvements made over the last decade didn't boost a wider adoption of {\{}AKM{\}} approaches, there is a need to identify the successes and shortcomings of current {\{}AK{\}} approaches and know what industry needs from AK. Therefore, as researchers and promoters of many of the {\{}AK{\}} research tools in the early stages where {\{}AK{\}} became relevant for the software architecture community, and based on our experience and observations, we provide in this research an informal retrospective analysis of what has been done and the challenges and trends for a future research agenda to promote {\{}AK{\}} use in modern software development practices. },
	Doi = {https://doi.org/10.1016/j.jss.2015.08.054},
	ISSN = {0164-1212},
	Keywords = {Agile development,Architectural design decisions,Architectural knowledge management},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121215002034}
}

@Article{Caporuscio200618,
	Title = {{Rapid system development via product line architecture implementation}},
	Author = {Caporuscio, M and Muccini, H and Pelliccione, P and Nisio, E D},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2006},
	Pages = {18--33},
	Volume = {3943 LNCS},
	Abstract = {Software Product Line (SPL) engineering allows designers to reason about an entire family of software applications, instead of a single product, with a strategic importance for the rapid development of new applications. While much effort has been spent so far in understanding and modeling SPLs and their architectures, very little attention has been given on how to systematically enforce SPL architectural decisions into the implementation step. In this paper we propose a methodological approach and an implementation framework, based on a plugin component-based development, which allows us to move from an architectural specification of the SPL to its implementation in a systematic way. We show the suitability of this framework through its application to the TOOL•one case study SPL. {\textcopyright} Springer-Verlag Berlin Heidelberg 2006.},
	Annote = {cited By 1},
	Doi = {10.1007/11751113_3},
	Keywords = {Architectural decisions; Methodological approach;,Computer applications; Computer architecture; Comp,Rapid prototyping},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745842249{\&}doi=10.1007{\%}2F11751113{\_}3{\&}partnerID=40{\&}md5=ea3cd525b8637603f29f49e1046dd6ca}
}

@Article{SPE:SPE2301,
	Title = {{Performance-driven instrumentation and mapping strategies using the LARA aspect-oriented programming approach}},
	Author = {Cardoso, Jo{\~{a}}o M P and Coutinho, Jos{\'{e}} G F and Carvalho, Tiago and Diniz, Pedro C and Petrov, Zlatko and Luk, Wayne and Gon{\c{c}}alves, Fernando},
	Journal = {Software: Practice and Experience},
	Year = {2016},
	Number = {2},
	Pages = {251--287},
	Volume = {46},
	Abstract = {The development of applications for high-performance embedded systems is a long and error-prone process because in addition to the required functionality, developers must consider various and often conflicting nonfunctional requirements such as performance and/or energy efficiency. The complexity of this process is further exacerbated by the multitude of target architectures and mapping tools. This article describes LARA, an aspect-oriented programming language that allows programmers to convey domain-specific knowledge and nonfunctional requirements to a toolchain composed of source-to-source transformers, compiler optimizers, and mapping/synthesis tools. LARA is sufficiently flexible to target different tools and host languages while also allowing the specification of compilation strategies to enable efficient generation of software code and hardware cores (using hardware description languages) for hybrid target architectures – a unique feature to the best of our knowledge not found in any other aspect-oriented programming language. A key feature of LARA is its ability to deal with different models of join points, actions, and attributes. In this article, we describe the LARA approach and evaluate its impact on code instrumentation and analysis and on selecting critical code sections to be migrated to hardware accelerators for two embedded applications from industry. Copyright {\textcopyright} 2014 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spe.2301},
	ISSN = {1097-024X},
	Keywords = {aspect-oriented programming,compilers,domain-specific languages,embedded systems,hardware/software systems,instrumenting and profiling,monitoring},
	Url = {http://dx.doi.org/10.1002/spe.2301}
}

@Article{Carromeu201639,
	Title = {{From e-Gov Web SPL to e-Gov Mobile SPL}},
	Author = {Carromeu, C and Paiva, D B and Cagnin, M I},
	Journal = {International Journal of Web Information Systems},
	Year = {2016},
	Number = {1},
	Pages = {39--61},
	Volume = {12},
	Abstract = {Purpose-This paper aims to discuss the motivation and present the evolution from a Software Product Line (SPL) in the e-Gov Web (e-Gov Web SPL) domain to a SPL in the mobile domain (e-Gov Mobile SPL). Design/methodology/approach-The evolution was supported by the Product Line UML-Based Software Engineering approach and the feature model. Findings-The authors were able to observe that it is feasible to evolve from a SPL for the Web platform to a SPL for the mobile platform, with the intent to port existing Web applications to mobile platforms such that users can have access to the main information and are able to interact with the most important functionalities of Web applications in a mobile device. Research limitations/implications-As for the main limitations, the authors can point out the small number of instantiations performed until the moment with the support of the e-Gov Mobile SPL, what prevented the conduction of an empirical study. Practical implications-Using e-Gov Mobile SPL, it is possible to reduce development time and cost. Originality/value-The existing SPLs do not worry about supporting the development of mobile applications corresponding to existing Web applications, as it is desirable to have access to the information and main features of these applications in mobile devices. We obtained some e-Gov Mobile SPL instantiations corresponding to e-Gov Web SPL instantiations to attend the demands of the Brazilian Agricultural Research Corporation Unit situated at Campo Grande, MS, Brazil. {\textcopyright} Emerald Group Publishing Limited.},
	Annote = {cited By 0},
	Doi = {10.1108/IJWIS-10-2015-0036},
	Keywords = {Agricultural research; Empirical studies; Evoluti,Computer software; Mobile devices; Mobile phones;,World Wide Web},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971351807{\&}doi=10.1108{\%}2FIJWIS-10-2015-0036{\&}partnerID=40{\&}md5=fd88b8ca73aeeaa55fe4c41f927b8a9e}
}

@Article{SPE:SPE939,
	Title = {{On the modularization and reuse of exception handling with aspects}},
	Author = {Castor, Fernando and Cacho, N{\'{e}}lio and Figueiredo, Eduardo and Garcia, Alessandro and Rubira, Cec{\'{i}}lia M F and de Amorim, Jefferson Silva and da Silva, H{\'{i}}talo Oliveira},
	Journal = {Software: Practice and Experience},
	Year = {2009},
	Number = {17},
	Pages = {1377--1417},
	Volume = {39},
	Abstract = {This paper presents an in-depth study of the adequacy of the AspectJ language for modularizing and reusing exception-handling code. The study consisted of refactoring existing applications so that the code responsible for implementing error-handling strategies was moved to newly created exception handler aspects. We have performed quantitative assessments of five systems—four object-oriented and one aspect-oriented—based on four key quality attributes, namely separation of concerns, coupling, cohesion, and conciseness. Our investigation also included a multi-perspective analysis of the refactored systems, including (i) the extent to which error-handling aspects can be reused, (ii) the beneficial and harmful aspectization scenarios for exception handling, and (iii) the scalability of AOP to support the modularization of exception handling in the presence of other aspects. Copyright {\textcopyright} 2009 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spe.939},
	ISSN = {1097-024X},
	Keywords = {AspectJ,aspect-oriented programming,exception handling,metrics,modularity,reuse},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/spe.939}
}

@Article{Castro2012463,
	Title = {{Changing attitudes towards the generation of architectural models}},
	Author = {Castro, Jaelson and Lucena, Marcia and Silva, Carla and Alencar, Fernanda and Santos, Emanuel and Pimentel, Jo{\~{a}}o},
	Journal = {Journal of Systems and Software},
	Year = {2012},
	Number = {3},
	Pages = {463--479},
	Volume = {85},
	Abstract = {Architectural design is an important activity, but the understanding of how it is related to requirements modeling is rather limited. It is worth noting that goal orientation is an increasingly recognized paradigm for eliciting, modeling, specifying, and analyzing software requirements. However, it is not clear how goal models are related to architectural models. In this paper we present an approach based on model transformations to derive architectural structural specifications from system goals. The source and target languages are respectively the i* (iStar) modeling language and the Acme architectural description language. A real case study is used to show the feasibility of our approach. },
	Annote = {Novel approaches in the design and implementation of systems/software architecture},
	Doi = {https://doi.org/10.1016/j.jss.2011.05.047},
	ISSN = {0164-1212},
	Keywords = {Architectural design,Model driven development,Model transformations,Requirements engineering},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121211001415}
}

@Article{Castro2011342,
	Title = {{F-STREAM: A flexible process for deriving architectures from requirements models}},
	Author = {Castro, J and Pimentel, J and Lucena, M and Santos, E and Dermeval, D},
	Journal = {Lecture Notes in Business Information Processing},
	Year = {2011},
	Pages = {342--353},
	Volume = {83 LNBIP},
	Abstract = {Some quality attributes are known to have an impact on the overall architecture of a system, requiring to be properly handled from the early stages of the software development. This led to the creation of different and unrelated approaches to handle specific attributes, such as security, performance, adaptability, etc. The challenge is to propose a flexible approach that could be configured to address multiple attributes of interest, promoting the reuse of best practices and reduction of development costs. We advocate the use of Software Product Line (SPL) principles to manage and customize variability in software processes targeted for the generation of architectural models from requirements models. Hence, in this paper we propose F-STREAM, a flexible and systematic process to derive architecture models from requirements. We define a common core process, its variation and extension points. The definition of this process was performed based on a survey of the existing approaches. As example, we instantiate a process for adaptive systems. {\textcopyright} 2011 Springer-Verlag.},
	Annote = {cited By 1},
	Doi = {10.1007/978-3-642-22056-2_37},
	Keywords = {Adaptive systems; Information systems; Software de,Architectural models; Architecture models; Develo,Models},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960303560{\&}doi=10.1007{\%}2F978-3-642-22056-2{\_}37{\&}partnerID=40{\&}md5=974f17abd2abf9f728cda7c67a294855}
}

@Article{EXSY:EXSY509,
	Title = {{Unlabelled extra data do not always mean extra performance for semi-supervised fault prediction}},
	Author = {Catal, Cagatay and Diri, Banu},
	Journal = {Expert Systems},
	Year = {2009},
	Number = {5},
	Pages = {458--471},
	Volume = {26},
	Abstract = {Abstract: This research focused on investigating and benchmarking several high performance classifiers called J48, random forests, naive Bayes, KStar and artificial immune recognition systems for software fault prediction with limited fault data. We also studied a recent semi-supervised classification algorithm called YATSI (Yet Another Two Stage Idea) and each classifier has been used in the first stage of YATSI. YATSI is a meta algorithm which allows different classifiers to be applied in the first stage. Furthermore, we proposed a semi-supervised classification algorithm which applies the artificial immune systems paradigm. Experimental results showed that YATSI does not always improve the performance of naive Bayes when unlabelled data are used together with labelled data. According to experiments we performed, the naive Bayes algorithm is the best choice to build a semi-supervised fault prediction model for small data sets and YATSI may improve the performance of naive Bayes for large data sets. In addition, the YATSI algorithm improved the performance of all the classifiers except naive Bayes on all the data sets.},
	Doi = {10.1111/j.1468-0394.2009.00509.x},
	ISSN = {1468-0394},
	Keywords = {YATSI,artificial immune systems,classification algorithms,machine learning,naive Bayes,random forests,semi-supervised classification,software fault prediction},
	Publisher = {Blackwell Publishing Ltd},
	Url = {http://dx.doi.org/10.1111/j.1468-0394.2009.00509.x}
}

@Article{Catala20131930,
	Title = {{A meta-model for dataflow-based rules in smart environments: Evaluating user comprehension and performance}},
	Author = {Catala, Alejandro and Pons, Patricia and Jaen, Javier and Mocholi, Jose A and Navarro, Elena},
	Journal = {Science of Computer Programming},
	Year = {2013},
	Number = {10},
	Pages = {1930--1950},
	Volume = {78},
	Abstract = {A considerable part of the behavior in smart environments relies on event-driven and rule specification. Rules are the mechanism most often used to enable user customization of the environment. However, the expressiveness of the rules available to users in editing and other tools is usually either limited or the available rule editing interfaces are not designed for end-users with low skills in programming. This means we have to look for interaction techniques and new ways to define user customization rules. This paper describes a generic and flexible meta-model to support expressive rules enhanced with data flow expressions that will graphically support the definition of rules without writing code. An empirical study was conducted on the ease of understanding of the visual data flow expressions, which are the key elements in our rule proposal. The visual dataflow language was compared to its corresponding textual version in terms of comprehension and ease of learning by teenagers in exercises involving calculations, modifications, writing and detecting equivalences in expressions in both languages. Although the subjects had some previous experience in editing mathematical expressions on spreadsheets, the study found their performance with visual dataflows to be significantly better in calculation and modification exercises. This makes our dataflow approach a promising mechanism for expressing user-customized reactive behavior in Ambient Intelligence (AmI) environments. The performance of the rule matching processor was validated by means of two stress tests to ensure that the meta-model approach adopted would be able to scale up with the number of types and instances in the space. },
	Annote = {Special section on Language Descriptions Tools and Applications (LDTA'08 {\&} '09) {\&} Special section on Software Engineering Aspects of Ubiquitous Computing and Ambient Intelligence (UCAmI 2011)},
	Doi = {https://doi.org/10.1016/j.scico.2012.06.010},
	ISSN = {0167-6423},
	Keywords = {Ambient intelligence,Customization,Dataflow,Event based,Non-expert programmer,Rule,Smart home,Visual language},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642312001232}
}

@Article{SMR:SMR1639,
	Title = {{Challenges and opportunities for software change request repositories: a systematic mapping study}},
	Author = {Cavalcanti, Yguarat{\~{a}} Cerqueira and {da Mota Silveira Neto}, Paulo Anselmo and Machado, Ivan do Carmo and Vale, Tassio Ferreira and de Almeida, Eduardo Santana and Meira, Silvio Romero de Lemos},
	Journal = {Journal of Software: Evolution and Process},
	Year = {2014},
	Number = {7},
	Pages = {620--653},
	Volume = {26},
	Abstract = {Software maintenance starts as soon as the first artifacts are delivered and is essential for the success of the software. However, keeping maintenance activities and their related artifacts on track comes at a high cost. In this respect, change request (CR) repositories are fundamental in software maintenance. They facilitate the management of CRs and are also the central point to coordinate activities and communication among stakeholders. However, the benefits of CR repositories do not come without issues, and commonly occurring ones should be dealt with, such as the following: duplicate CRs, the large number of CRs to assign, or poorly described CRs. Such issues have led researchers to an increased interest in investigating CR repositories, by considering different aspects of software development and CR management. In this paper, we performed a systematic mapping study to characterize this research field. We analyzed 142 studies, which we classified in two ways. First, we classified the studies into different topics and grouped them into two dimensions: challenges and opportunities. Second, the challenge topics were classified in accordance with an existing taxonomy for information retrieval models. In addition, we investigated tools and services for CR management, to understand whether and how they addressed the topics identified. Copyright {\textcopyright} 2013 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/smr.1639},
	ISSN = {2047-7481},
	Keywords = {bug report,bug tracking,change request repository,software evolution,software maintenance,software quality assurance},
	Url = {http://dx.doi.org/10.1002/smr.1639}
}

@Article{Cavalcanti201682,
	Title = {{Towards semi-automated assignment of software change requests}},
	Author = {Cavalcanti, Yguarat{\~{a}} Cerqueira and {do Carmo Machado}, Ivan and {da Motal S. Neto}, Paulo Anselmo and de Almeida, Eduardo Santana},
	Journal = {Journal of Systems and Software},
	Year = {2016},
	Pages = {82--101},
	Volume = {115},
	Abstract = {Abstract Change Requests (CRs) are key elements to software maintenance and evolution. Finding the appropriate developer to a {\{}CR{\}} is crucial for obtaining the lowest, economically feasible, fixing time. Nevertheless, assigning {\{}CRs{\}} is a labor-intensive and time consuming task. In this paper, we report on a questionnaire-based survey with practitioners to understand the characteristics of {\{}CR{\}} assignment, and on a semi-automated approach for {\{}CR{\}} assignment which combines rule-based and machine learning techniques. In accordance with the results of the survey, the proposed approach emphasizes the use of contextual information, essential to effective assignments, and puts the development team in control of the assignment rules, toward making its adoption easier. The assignment rules can be either extracted from the assignment history or created from scratch. An empirical validation was performed through an offline experiment with {\{}CRs{\}} from a large software project. The results pointed out that the approach is up to 46.5{\%} more accurate than other approaches which relying solely on machine learning techniques. This indicates that a rule-based approach is a viable and simple method to leverage {\{}CR{\}} assignments. },
	Doi = {https://doi.org/10.1016/j.jss.2016.01.038},
	ISSN = {0164-1212},
	Keywords = {Automatic change request assignment,Bug triage,Change request management,Software maintenance and evolution},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121216000352}
}

@Article{Cengarle2006141,
	Title = {{Semantics of {\{}UML{\}} 2.0 Interactions with Variabilities}},
	Author = {Cengarle, Mar{\'{i}}a Victoria and Graubmann, Peter and Wagner, Stefan},
	Journal = {Electronic Notes in Theoretical Computer Science},
	Year = {2006},
	Pages = {141--155},
	Volume = {160},
	Abstract = {Means for the representation of variability in {\{}UML{\}} 2.0 interactions, as presented in a previous work, are further formalised and given a mathematically formal semantics. In this way, {\{}UML{\}} 2.0 interactions can be used in the conception and development of system families within domain and application engineering tasks. Following the transition from domain to application engineering as a configuration endeavour, resolution of the variability according to a given configuration is captured by a denotational semantics for plain interactions extended to the features for the specification of variability. An example based on a previous case study explicates the semantics hereby defined. },
	Annote = {Proceedings of the International Workshop on Formal Aspects of Component Software (FACS 2005)Proceedings of the International Workshop on Formal Aspects of Component Software (FACS 2005)},
	Doi = {https://doi.org/10.1016/j.entcs.2006.05.020},
	ISSN = {1571-0661},
	Keywords = {formal semantics,product lines,system families,variability,{\{}UML{\}} interactions},
	Url = {http://www.sciencedirect.com/science/article/pii/S1571066106003823}
}

@Article{Cetina20132399,
	Title = {{Prototyping Dynamic Software Product Lines to evaluate run-time reconfigurations}},
	Author = {Cetina, C and Giner, P and Fons, J and Pelechano, V},
	Journal = {Science of Computer Programming},
	Year = {2013},
	Number = {12},
	Pages = {2399--2413},
	Volume = {78},
	Abstract = {Dynamic Software Product Lines (DSPL) encompass systems that are capable of modifying their own behavior with respect to changes in their operating environment by using run-time reconfigurations. A failure in these reconfigurations can directly impact the user experience since the reconfigurations are performed when the system is already under the users control. In this work, we prototype a Smart Hotel DSPL to evaluate the reliability-based risk of the DSPL reconfigurations, specifically, the probability of malfunctioning (Availability) and the consequences of malfunctioning (Severity). This DSPL prototype was performed with the participation of human subjects by means of a Smart Hotel case study which was deployed with real devices. Moreover, we successfully identified and addressed two challenges associated with the involvement of human subjects in DSPL prototyping: enabling participants to (1) trigger the run-time reconfigurations and to (2) understand the effects of the reconfigurations. The evaluation of the case study reveals positive results regarding both Availability and Severity. However, the participant feedback highlights issues with recovering from a failed reconfiguration or a reconfiguration triggered by mistake. To address these issues, we discuss some guidelines learned in the case study. Finally, although the results achieved by the DSPL may be considered satisfactory for its particular domain, DSPL engineers must provide users with more control over the reconfigurations or the users will not be comfortable with DSPLs. {\textcopyright} 2012 Elsevier B.V. All rights reserved.},
	Annote = {cited By 7},
	Doi = {10.1016/j.scico.2012.06.007},
	Keywords = {Computer software,Dynamic software product lines; Human subjects; Op,Hotels; Research; Software design},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884637085{\&}doi=10.1016{\%}2Fj.scico.2012.06.007{\&}partnerID=40{\&}md5=9983ebc28ad5519b3283a208a72f7271}
}

@Article{Cetina2010331,
	Title = {{Designing and prototyping dynamic software product lines: Techniques and guidelines}},
	Author = {Cetina, C and Giner, P and Fons, J and Pelechano, V},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2010},
	Pages = {331--345},
	Volume = {6287 LNCS},
	Abstract = {Dynamic Software Product Lines (DSPL) encompass systems that are capable of modifying their own configuration with respect to changes in their operating environment by using run-time reconfigurations. A failure in these reconfigurations can directly impact the user experience since the reconfigurations are performed when the system is already under the users control. Prototyping DSPLs at an early development stage can help to pinpoint potential issues and optimize design. In this work, we identify and addresses two challenges associated with the involvement of human subjects in DSPL prototyping: enabling DSPL users to (1) trigger the run-time reconfigurations and to (2) understand the effects of the reconfigurations. These techniques have been applied with the participation of human subjects by means of a Smart Hotel case study which was deployed with real devices. The application of these techniques reveals DSPL-design issues with recovering from a failed reconfiguration or a reconfiguration triggered by mistake. To address these issues, we discuss some guidelines learned in the Smart Hotel case study. {\textcopyright} 2010 Springer-Verlag Berlin Heidelberg.},
	Annote = {cited By 4},
	Doi = {10.1007/978-3-642-15579-6_23},
	Keywords = {Design issues; Development stages; Dynamic softwar,Software prototyping},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049359003{\&}doi=10.1007{\%}2F978-3-642-15579-6{\_}23{\&}partnerID=40{\&}md5=81df8821082a001b4b31b20a5abe04d6}
}

@Article{Chabridon20131912,
	Title = {{Building ubiquitous QoC-aware applications through model-driven software engineering}},
	Author = {Chabridon, Sophie and Conan, Denis and Abid, Zied and Taconet, Chantal},
	Journal = {Science of Computer Programming},
	Year = {2013},
	Number = {10},
	Pages = {1912--1929},
	Volume = {78},
	Abstract = {As every-day mobile devices can easily be equipped with multiple sensing capabilities, ubiquitous applications are expected to exploit the richness of the context information that can be collected by these devices in order to provide the service that is the most appropriate to the situation of the user. However, the design and implementation of such context-aware ubiquitous appplications remain challenging as there exist very few models and tools to guide application designers and developers in mastering the complexity of context information. This becomes even more crucial as context is by nature imperfect. One way to address this issue is to associate to context information meta-data representing its quality. We propose a generic and extensible design process for context-aware applications taking into account the quality of context (QoC). We demonstrate its use on a prototype application for sending flash sale offers to mobile users. We present extensive performance results in terms of memory and processing time of both elementary context management operations and the whole context policy implementing the Flash sale application. The cost of adding QoC management is also measured and appears to be limited to a few milliseconds. We show that a context policy with 120 QoC-aware nodes can be processed in less than 100 ms on a mobile phone. Moreover, a policy of almost 3000 nodes can be instantiated before exhausting the resources of the phone. This enables very rich application scenarios enhancing the user experience and will favor the development of new ubiquitous applications. },
	Annote = {Special section on Language Descriptions Tools and Applications (LDTA'08 {\&} '09) {\&} Special section on Software Engineering Aspects of Ubiquitous Computing and Ambient Intelligence (UCAmI 2011)},
	Doi = {https://doi.org/10.1016/j.scico.2012.07.019},
	ISSN = {0167-6423},
	Keywords = {Context,Domain specific language,Model-driven software engineering,Pervasive computing,Quality of context,Ubiquitous computing},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642312001475}
}

@InProceedings{Chapman:2008:CSS:1456659.1456662,
	Title = {{Contemplating Systematic Software Reuse in a Project-centric Company}},
	Author = {Chapman, Mark and van der Merwe, Alta},
	Booktitle = {Proceedings of the 2008 Annual Research Conference of the South African Institute of Computer Scientists and Information Technologists on IT Research in Developing Countries: Riding the Wave of Technology},
	Year = {2008},
	Address = {New York, NY, USA},
	Pages = {16--26},
	Publisher = {ACM},
	Series = {SAICSIT '08},
	Doi = {10.1145/1456659.1456662},
	ISBN = {978-1-60558-286-3},
	Keywords = {action research,ethnography,interpretive case study,project-centric,software product line engineering,software product lines,software reuse,systematic software reuse},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/1456659.1456662}
}

@InProceedings{Charalampidou:2015:SCM:2810146.2810155,
	Title = {{Size and Cohesion Metrics As Indicators of the Long Method Bad Smell: An Empirical Study}},
	Author = {Charalampidou, Sofia and Ampatzoglou, Apostolos and Avgeriou, Paris},
	Booktitle = {Proceedings of the 11th International Conference on Predictive Models and Data Analytics in Software Engineering},
	Year = {2015},
	Address = {New York, NY, USA},
	Pages = {8:1----8:10},
	Publisher = {ACM},
	Series = {PROMISE '15},
	Doi = {10.1145/2810146.2810155},
	ISBN = {978-1-4503-3715-1},
	Keywords = {Long method,case study,cohesion,metrics,size},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2810146.2810155}
}

@Article{Chen2014114,
	Title = {{Uncertainty handling in goal-driven self-optimization – Limiting the negative effect on adaptation}},
	Author = {Chen, Bihuan and Peng, Xin and Yu, Yijun and Zhao, Wenyun},
	Journal = {Journal of Systems and Software},
	Year = {2014},
	Pages = {114--127},
	Volume = {90},
	Abstract = {Abstract Goal-driven self-optimization through feedback loops has shown effectiveness in reducing oscillating utilities due to a large number of uncertain factors in the runtime environments. However, such self-optimization is less satisfactory when there contains uncertainty in the predefined requirements goal models, such as imprecise contributions and unknown quality preferences, or during the switches of goal solutions, such as lack of understanding about the time for the adaptation actions to take effect. In this paper, we propose to handle such uncertainty in goal-driven self-optimization without interrupting the services. Taking the monitored quality values as the feedback, and the estimated earned value as the global indicator of self-optimization, our approach dynamically updates the quantitative contributions from alternative functionalities to quality requirements, tunes the preferences of relevant quality requirements, and determines a proper timing delay for the last adaptation action to take effect. After applying these runtime measures to limit the negative effect of the uncertainty in goal models and their suggested switches, an experimental study on a real-life online shopping system shows the improvements over goal-driven self-optimization approaches without uncertainty handling. },
	Doi = {https://doi.org/10.1016/j.jss.2013.12.033},
	ISSN = {0164-1212},
	Keywords = {Goal-driven self-optimization,Requirements goal models,Uncertainty},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121214000065}
}

@Article{Chen20092051,
	Title = {{A holistic approach to managing software change impact}},
	Author = {Chen, Chung-Yang and Chen, Pei-Chi},
	Journal = {Journal of Systems and Software},
	Year = {2009},
	Number = {12},
	Pages = {2051--2067},
	Volume = {82},
	Abstract = {Change is inevitable in the software product lifecycle. When a software change occurs, all of the stakeholders and related artifacts should be considered in determining the success of the change action in a collaborative development environment such as {\{}JAD{\}} (joint application development). In this regard, current implementation-based or homogeneous impact analyses are insufficient; therefore, this paper presents a holistic approach to change impact analysis in handling not only software contents but also other items such as requirements, documents and data. This approach characterizes product contents and relates heterogeneous items by using attributes and linkages. It also uses an object-oriented propagation mechanism to handle dynamic looping in determining the impact of changes. A prototype, EPIC, was built to realize this approach and these concepts. A walkthrough example is provided in order to verify the work of the proposed approach. An empirical study is presented to discuss the benefits of the proposed approach and the application of {\{}EPIC{\}} in a software company. Lessons learned from the case study and improvement issues of the proposed approach and the tool are also discussed. },
	Doi = {https://doi.org/10.1016/j.jss.2009.06.052},
	ISSN = {0164-1212},
	Keywords = {Collaborative development,Holistic approach,Object technology,Software change management},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121209001654}
}

@Article{Chen201772,
	Title = {{Continuous Delivery: Overcoming adoption challenges}},
	Author = {Chen, Lianping},
	Journal = {Journal of Systems and Software},
	Year = {2017},
	Pages = {72--86},
	Volume = {128},
	Abstract = {Abstract Continuous Delivery (CD) is a relatively new software development approach. Companies that have adopted {\{}CD{\}} have reported significant benefits. Motivated by these benefits, many companies would like to adopt CD. However, adopting {\{}CD{\}} can be very challenging for a number of reasons, such as obtaining buy-in from a wide range of stakeholders whose goals may seemingly be different from—or even conflict with—our own; gaining sustained support in a dynamic complex enterprise environment; maintaining an application development team's momentum when their application's migration to {\{}CD{\}} requires an additional strenuous effort over a long period of time; and so on. To help overcome the adoption challenges, I present six strategies: (1) selling {\{}CD{\}} as a painkiller; (2) establishing a dedicated team with multi-disciplinary members; (3) continuous delivery of continuous delivery; (4) starting with the easy but important applications; (5) visual {\{}CD{\}} pipeline skeleton; (6) expert drop. These strategies were derived from four years of experience in implementing {\{}CD{\}} at a multi-billion-euro company. Additionally, our experience led to the identification of eight further challenges for research. The information contributes toward building a body of knowledge for {\{}CD{\}} adoption. },
	Doi = {https://doi.org/10.1016/j.jss.2017.02.013},
	ISSN = {0164-1212},
	Keywords = {Adoption,Agile Software Development,Continuous Delivery,Continuous Deployment,Continuous Software Engineering,DevOps},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121217300353}
}

@Article{Chen2011344,
	Title = {{A systematic review of evaluation of variability management approaches in software product lines}},
	Author = {Chen, L and {Ali Babar}, M},
	Journal = {Information and Software Technology},
	Year = {2011},
	Number = {4},
	Pages = {344--362},
	Volume = {53},
	Abstract = {Context: Variability management (VM) is one of the most important activities of software product-line engineering (SPLE), which intends to develop software-intensive systems using platforms and mass customization. VM encompasses the activities of eliciting and representing variability in software artefacts, establishing and managing dependencies among different variabilities, and supporting the exploitation of the variabilities for building and evolving a family of software systems. Software product line (SPL) community has allocated huge amount of effort to develop various approaches to dealing with variability related challenges during the last two decade. Several dozens of VM approaches have been reported. However, there has been no systematic effort to study how the reported VM approaches have been evaluated. Objective: The objectives of this research are to review the status of evaluation of reported VM approaches and to synthesize the available evidence about the effects of the reported approaches. Method: We carried out a systematic literature review of the VM approaches in SPLE reported from 1990s until December 2007. Results: We selected 97 papers according to our inclusion and exclusion criteria. The selected papers appeared in 56 publication venues. We found that only a small number of the reviewed approaches had been evaluated using rigorous scientific methods. A detailed investigation of the reviewed studies employing empirical research methods revealed significant quality deficiencies in various aspects of the used quality assessment criteria. The synthesis of the available evidence showed that all studies, except one, reported only positive effects. Conclusion: The findings from this systematic review show that a large majority of the reported VM approaches have not been sufficiently evaluated using scientifically rigorous methods. The available evidence is sparse and the quality of the presented evidence is quite low. The findings highlight the areas in need of improvement, i.e., rigorous evaluation of VM approaches. However, the reported evidence is quite consistent across different studies. That means the proposed approaches may be very beneficial when they are applied properly in appropriate situations. Hence, it can be concluded that further investigations need to pay more attention to the contexts under which different approaches can be more beneficial. {\textcopyright} 2010 Elsevier B.V. All rights reserved.},
	Annote = {cited By 94},
	Doi = {10.1016/j.infsof.2010.12.006},
	Keywords = {Empirical research method; Empirical studies; Mass,Network architecture; Software design,Rating},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951813796{\&}doi=10.1016{\%}2Fj.infsof.2010.12.006{\&}partnerID=40{\&}md5=30a609a648e0a56c33c9feb550e3df7d}
}

@Article{Chen2010166,
	Title = {{Variability management in software product lines: An investigation of contemporary industrial challenges}},
	Author = {Chen, L and Babar, M A},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2010},
	Pages = {166--180},
	Volume = {6287 LNCS},
	Abstract = {Variability management is critical for achieving the large scale reuse promised by the software product line paradigm. It has been studied for almost 20 years. We assert that it is important to explore how well the body of knowledge of variability management solves the challenges faced by industrial practitioners, and what are the remaining and (or) emerging challenges. To gain such understanding of the challenges of variability management faced by practitioners, we have conducted an empirical study using focus group as data collection method. The results of the study highlight several technical challenges that are often faced by practitioners in their daily practices. Different from previous studies, the results also reveal and shed light on several non-technical challenges that were almost neglected by existing research. {\textcopyright} 2010 Springer-Verlag Berlin Heidelberg.},
	Annote = {cited By 10},
	Doi = {10.1007/978-3-642-15579-6_12},
	Keywords = {Body of knowledge; Data collection method; Empiric,Computer software reusability,Network architecture; Software design},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049366809{\&}doi=10.1007{\%}2F978-3-642-15579-6{\_}12{\&}partnerID=40{\&}md5=7675a95c5d6d22be26b635caf04a0f06}
}

@InProceedings{Chen:2012:ETS:2364527.2364535,
	Title = {{An Error-tolerant Type System for Variational Lambda Calculus}},
	Author = {Chen, Sheng and Erwig, Martin and Walkingshaw, Eric},
	Booktitle = {Proceedings of the 17th ACM SIGPLAN International Conference on Functional Programming},
	Year = {2012},
	Address = {New York, NY, USA},
	Pages = {29--40},
	Publisher = {ACM},
	Series = {ICFP '12},
	Doi = {10.1145/2364527.2364535},
	ISBN = {978-1-4503-1054-3},
	Keywords = {error-tolerant type systems,variational lambda calculus,variational type inference,variational types},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2364527.2364535}
}

@Article{SPIP:SPIP281,
	Title = {{A software product line process simulator}},
	Author = {Chen, Yu and Gannod, Gerald C and Collofello, James S},
	Journal = {Software Process: Improvement and Practice},
	Year = {2006},
	Number = {4},
	Pages = {385--409},
	Volume = {11},
	Doi = {10.1002/spip.281},
	ISSN = {1099-1670},
	Keywords = {process simulation,product line economics,software product lines},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/spip.281}
}

@Article{Chien2007783,
	Title = {{Investigating the success of {\{}ERP{\}} systems: Case studies in three Taiwanese high-tech industries}},
	Author = {Chien, Shih-Wen and Tsaur, Shu-Ming},
	Journal = {Computers in Industry},
	Year = {2007},
	Number = {8–9},
	Pages = {783--793},
	Volume = {58},
	Abstract = {The measurement of enterprise resource planning (ERP) systems success or effectiveness is critical to our understanding of the value and efficacy of {\{}ERP{\}} investment and managerial actions. Whether traditional information systems success models can be extended to investigating {\{}ERP{\}} systems success is yet to be investigated. This paper proposes a partial extension and respecification of the DeLone and MacLean model of {\{}IS{\}} success to {\{}ERP{\}} systems. The purpose of the present research is to re-examine the updated DeLone and McLean model [W. DeLone, E. McLean, The DeLone McLean model of information system success: a ten-year update, Journal of Management Information Systems 19 (4) (2003) 3–9] of {\{}ERP{\}} systems success. The updated DeLone and McLean model was applied to collect data from the questionnaires answered by 204 users of {\{}ERP{\}} systems at three high-tech firms in Taiwan. Finally, this study suggests that system quality, service quality, and information quality are most important successful factors.},
	Doi = {https://doi.org/10.1016/j.compind.2007.02.001},
	ISSN = {0166-3615},
	Keywords = {Case Study,DeLone and McLean model,High-tech firms,{\{}ERP{\}} success model},
	Mendeley-tags = {Case Study},
	Url = {http://www.sciencedirect.com/science/article/pii/S0166361507000188}
}

@Article{Chitchyan2017,
	Title = {{Uncovering sustainability concerns in software product lines}},
	Author = {Chitchyan, R and Groher, I and Noppen, J},
	Journal = {Journal of Software: Evolution and Process},
	Year = {2017},
	Number = {2},
	Volume = {29},
	Abstract = {Sustainable living, ie, living within the bounds of the available environmental, social, and economic resources, is the focus of many present-day social and scientific discussions. But what does sustainability mean within the context of software engineering? In this paper, we undertake a comprehensive analysis of 8 case studies to address this question within the context of a specific software engineering approach, software product line engineering (SPLE). We identify the sustainability-related characteristics that arise in present-day studies that apply SPLE. We conclude that technical and economic sustainability are in prime focus on the present SPLE practice, with social sustainability issues, where they relate to organisations, also addressed to a good degree. On the other hand, the issues related to the personal sustainability are less prominent, and environmental considerations are nearly completely amiss. We present feature models and cross-relations that result from our analysis as a starting point for sustainability engineering through SPLE, suggesting that any new development should consider how these models would be instantiated and expanded for the intended sociotechnical system. The good representation of sustainability features in these models is also validated with 2 additional case studies. Copyright {\textcopyright} 2017 John Wiley {\&} Sons, Ltd.},
	Annote = {cited By 0},
	Doi = {10.1002/smr.1853},
	Keywords = {Comprehensive analysis; Economic sustainability;,Computer software; Software design; Software engin,Sustainable development},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013469369{\&}doi=10.1002{\%}2Fsmr.1853{\&}partnerID=40{\&}md5=795a2abe870c297214d65f4db6c81319}
}

@Article{Chuang201484,
	Title = {{Assessment of institutions, scholars, and contributions on agile software development (2001–2012)}},
	Author = {Chuang, Sun-Wen and Luor, Tainyi and Lu, Hsi-Peng},
	Journal = {Journal of Systems and Software},
	Year = {2014},
	Pages = {84--101},
	Volume = {93},
	Abstract = {Abstract The number of scholarly publications on agile software development has grown significantly in recent years. Several researchers reviewed and attempted to synthesize studies on agile software development. However, no work has ranked the contributions of scholars and institutions to publications using a thorough process. This study presents findings on top publications, institutions, and scholars in the agile software development field from 2001 to 2012 based on the publication of such works in Science Citation Index journals. This paper highlights the key outlets for agile research and summarizes the most influential researchers and institutions as well as the most studied research areas. This study concludes by providing directions for future research. },
	Doi = {https://doi.org/10.1016/j.jss.2014.03.006},
	ISSN = {0164-1212},
	Keywords = {Agile software development,Literature assessment,Research productivity},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121214000697}
}

@Article{Cirilo2012258,
	Title = {{Automating the product derivation process of multi-agent systems product lines}},
	Author = {Cirilo, Elder and Nunes, Ingrid and Kulesza, Uir{\'{a}} and Lucena, Carlos},
	Journal = {Journal of Systems and Software},
	Year = {2012},
	Number = {2},
	Pages = {258--276},
	Volume = {85},
	Abstract = {Agent-oriented software engineering and software product lines are two promising software engineering techniques. Recent research work has been exploring their integration, namely multi-agent systems product lines (MAS-PLs), to promote reuse and variability management in the context of complex software systems. However, current product derivation approaches do not provide specific mechanisms to deal with MAS-PLs. This is essential because they typically encompass several concerns (e.g., trust, coordination, transaction, state persistence) that are constructed on the basis of heterogeneous technologies (e.g., object-oriented frameworks and platforms). In this paper, we propose the use of multi-level models to support the configuration knowledge specification and automatic product derivation of MAS-PLs. Our approach provides an agent-specific architecture model that uses abstractions and instantiation rules that are relevant to this application domain. In order to evaluate the feasibility and effectiveness of the proposed approach, we have implemented it as an extension of an existing product derivation tool, called GenArch. The approach has also been evaluated through the automatic instantiation of two MAS-PLs, demonstrating its potential and benefits to product derivation and configuration knowledge specification. },
	Annote = {Special issue with selected papers from the 23rd Brazilian Symposium on Software Engineering},
	Doi = {https://doi.org/10.1016/j.jss.2011.04.066},
	ISSN = {0164-1212},
	Keywords = {Application engineering,Model-driven development,Multi-agent systems,Product derivation tool,Software product lines},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121211001075}
}

@Article{Classen20111130,
	Title = {{A text-based approach to feature modelling: Syntax and semantics of {\{}TVL{\}}}},
	Author = {Classen, Andreas and Boucher, Quentin and Heymans, Patrick},
	Journal = {Science of Computer Programming},
	Year = {2011},
	Number = {12},
	Pages = {1130--1143},
	Volume = {76},
	Abstract = {In the scientific community, feature models are the de-facto standard for representing variability in software product line engineering. This is different from industrial settings where they appear to be used much less frequently. We and other authors found that in a number of cases, they lack concision, naturalness and expressiveness. This is confirmed by industrial experience. When modelling variability, an efficient tool for making models intuitive and concise are feature attributes. Yet, the semantics of feature models with attributes is not well understood and most existing notations do not support them at all. Furthermore, the graphical nature of feature models' syntax also appears to be a barrier to industrial adoption, both psychological and rational. Existing tool support for graphical feature models is lacking or inadequate, and inferior in many regards to tool support for text-based formats. To overcome these shortcomings, we designed TVL, a text-based feature modelling language. In terms of expressiveness, {\{}TVL{\}} subsumes most existing dialects. The main goal of designing {\{}TVL{\}} was to provide engineers with a human-readable language with a rich syntax to make modelling easy and models natural, but also with a formal semantics to avoid ambiguity and allow powerful automation. },
	Annote = {Special Issue on Software Evolution, Adaptability and Variability},
	Doi = {https://doi.org/10.1016/j.scico.2010.10.005},
	ISSN = {0167-6423},
	Keywords = {Code,Feature models,Language,Modelling,Semantics,Software product lines,Syntax},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642310001899}
}

@Article{Classen2014416,
	Title = {{Formal semantics, modular specification, and symbolic verification of product-line behaviour}},
	Author = {Classen, Andreas and Cordy, Maxime and Heymans, Patrick and Legay, Axel and Schobbens, Pierre-Yves},
	Journal = {Science of Computer Programming},
	Year = {2014},
	Pages = {416--439},
	Volume = {80, Part B},
	Abstract = {Abstract Formal techniques for specifying and verifying Software Product Lines (SPL) are actively studied. While the foundations of this domain recently made significant progress with the introduction of Featured Transition Systems (FTSs) and associated algorithms, {\{}SPL{\}} model checking still faces the well-known state explosion problem. Moreover, there is a need for high-level specification languages usable in industry. We address the state explosion problem by applying the principles of symbolic model checking to FTS-based verification of SPLs. In order to specify properties on specific products only, we extend the temporal logic {\{}CTL{\}} with feature quantifiers. Next, we show how {\{}SPL{\}} behaviour can be specified with fSMV, a variant of SMV, the specification language of the industry-strength model checker NuSMV. fSMV is a feature-oriented extension of {\{}SMV{\}} originally introduced by Plath and Ryan. We prove that fSMV and {\{}FTSs{\}} are expressively equivalent. Finally, we connect these results to a NuSMV extension we developed for verifying {\{}SPLs{\}} against {\{}CTL{\}} properties. },
	Doi = {https://doi.org/10.1016/j.scico.2013.09.019},
	ISSN = {0167-6423},
	Keywords = {Feature,Language,Software product line,Specification,Verification},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642313002517}
}

@Article{Clemente20111032,
	Title = {{Managing crosscutting concerns in component based systems using a model driven development approach}},
	Author = {Clemente, Pedro J and Hern{\'{a}}ndez, Juan and Conejero, Jos{\'{e}} M and Ortiz, Guadalupe},
	Journal = {Journal of Systems and Software},
	Year = {2011},
	Number = {6},
	Pages = {1032--1053},
	Volume = {84},
	Abstract = {In the last few years, Model-Driven Development (MDD), Aspect-Oriented Software Development (AOSD), and Component-Based Software Development (CBSD) have become interesting alternatives for the design and construction of complex distributed applications. Although these methodological approaches share the principle of separation of concerns and their further integration as key factors to obtaining high-quality and evolvable large software systems, they usually each address this principle from their own particular perspective. In the present work, we combine Component-Based and Aspect-Oriented Software Developments in a Model Driven software process targeted at the development of complex systems. This process constitutes an enhancement of the separation of concerns by allowing the isolation of crosscutting concerns in both Platform Independent and Platform Specific models. Following a pure {\{}MDD{\}} philosophy, a set of model transformations are used to generate the system, from preliminary models to the final source code for the Corba Component Model platform. A twofold empirical analysis was used to evaluate the approach's benefits in terms of two internal quality attributes: modularity and complexity. Conclusions were drawn from this evaluation regarding other quality attributes correlated with these two – stability, changeability, error-proneness, and reusability. An Eclipse plug-in was developed to drive the development of the entire system from early modeling to late deployment stages. },
	Doi = {https://doi.org/10.1016/j.jss.2011.01.053},
	ISSN = {0164-1212},
	Keywords = {Aspect Oriented Software Development (AOSD),CORBA Component Model (CCM),Component Based Software Development (CBSD),Crosscutting concerns,Model Driven Development (MDD),Transformation models},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121211000434}
}

@Article{IIS2:IIS2131,
	Title = {{Product Line Engineering Comes to the Industrial Mainstream}},
	Author = {Clements, Paul C},
	Journal = {INCOSE International Symposium},
	Year = {2015},
	Number = {1},
	Pages = {1305--1319},
	Volume = {25},
	Abstract = {Product line engineering (PLE) is a systems engineering discipline to engineer a portfolio of related products in an efficient manner, taking full and ongoing advantage of the products' similarities while respecting and managing their differences. Managing a portfolio as a single entity with variation, as opposed to a multitude of separate products, brings enormous efficiencies in production and maintenance. This paper shows that PLE has now matured into a repeatable, industrial-strength engineering discipline. We define and explore the concepts central to modern product line engineering, and illustrate how it is becoming applied in two of the most challenging systems engineering domains of all: aerospace and defense, and automotive.},
	Doi = {10.1002/j.2334-5837.2015.00131.x},
	ISSN = {2334-5837},
	Url = {http://dx.doi.org/10.1002/j.2334-5837.2015.00131.x}
}

@Article{Coallier200573,
	Title = {{A Product Line engineering practices model}},
	Author = {Coallier, Fran{\c{c}}ois and Champagne, Roger},
	Journal = {Science of Computer Programming},
	Year = {2005},
	Number = {1},
	Pages = {73--87},
	Volume = {57},
	Abstract = {This paper describes work in progress towards the elaboration of a Product Line practices model that combines concepts proposed by various authors. The strengths of existing Product Line frameworks and models are summarized and a new model is proposed in the form of 31 Product Line practice areas, grouped in five categories. An important objective of this Product Line practices model is that it should be easily incorporated into existing development methodologies, while remaining aligned with existing systems engineering standards. },
	Annote = {System and Software Architectures3rd International Workshop on System/Software Architectures},
	Doi = {https://doi.org/10.1016/j.scico.2004.10.006},
	ISSN = {0167-6423},
	Keywords = {Modeling,Product Lines,Software engineering,System analysis and design},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642304001923}
}

@Article{Coarfa:2006:PAT:1124153.1124155,
	Title = {{Performance Analysis of TLS Web Servers}},
	Author = {Coarfa, Cristian and Druschel, Peter and Wallach, Dan S},
	Journal = {ACM Trans. Comput. Syst.},
	Year = {2006},
	Number = {1},
	Pages = {39--69},
	Volume = {24},
	Address = {New York, NY, USA},
	Doi = {10.1145/1124153.1124155},
	ISSN = {0734-2071},
	Keywords = {Internet,RSA accelerator,TLS,e-commerce,secure Web servers},
	Publisher = {ACM},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/1124153.1124155}
}

@Article{Coelho20112700,
	Title = {{Unveiling and taming liabilities of aspects in the presence of exceptions: A static analysis based approach}},
	Author = {Coelho, Roberta and von Staa, Arndt and Kulesza, Uir{\'{a}} and Rashid, Awais and Lucena, Carlos},
	Journal = {Information Sciences},
	Year = {2011},
	Number = {13},
	Pages = {2700--2720},
	Volume = {181},
	Abstract = {As aspects extend or replace existing functionality at specific join points in the code, their behavior may raise new exceptions, which can flow through the program execution in unexpected ways. Assuring the reliability of exception handling code in aspect-oriented (AO) systems is a challenging task. Testing the exception handling code is inherently difficult, since it is tricky to provoke all exceptions during tests, and the large number of different exceptions that can happen in a system may lead to the test-case explosion problem. Moreover, we have observed that some properties of {\{}AO{\}} programming (e.g., quantification, obliviousness) may conflict with characteristics of exception handling mechanisms, exacerbating existing problems (e.g., uncaught exceptions). The lack of verification approaches for exception handling code in {\{}AO{\}} systems stimulated the present work. This work presents a verification approach based on a static analysis tool, called SAFE, to check the reliability of exception handling code in AspectJ programs. We evaluated the effectiveness and feasibility of our approach in two complementary ways (i) by investigating if the {\{}SAFE{\}} tool is precise enough to uncover exception flow information and (ii) by applying the approach to three medium-sized ApectJ systems from different application domains. },
	Annote = {Including Special Section on Databases and Software Engineering},
	Doi = {https://doi.org/10.1016/j.ins.2010.06.002},
	ISSN = {0020-0255},
	Keywords = {Aspect-oriented programming,Exception flow analysis,Exception handling,Exception handling rules conformance,Static analysis},
	Url = {http://www.sciencedirect.com/science/article/pii/S0020025510002525}
}

@Article{Colanzi2016126,
	Title = {{A feature-driven crossover operator for multi-objective and evolutionary optimization of product line architectures}},
	Author = {Colanzi, Thelma Elita and Vergilio, Silvia Regina},
	Journal = {Journal of Systems and Software},
	Year = {2016},
	Pages = {126--143},
	Volume = {121},
	Abstract = {Abstract The optimization of a Product Line Architecture (PLA) design can be modeled as a multi-objective problem, influenced by many factors, such as feature modularization, extensibility and other design principles. Due to this it has been properly solved in the Search Based Software Engineering (SBSE) field. However, previous empirical studies optimized {\{}PLA{\}} design using the multi-objective and evolutionary algorithm NSGA-II, without applying one of the most important genetic operators: the crossover. To overcome this limitation, this paper presents a feature-driven crossover operator that aims at improving feature modularization in {\{}PLA{\}} design. The proposed operator was applied in two empirical studies using NSGA-II in comparison with another version of NSGA-II that uses only mutation operators. The results show the usefulness and applicability of the proposed operator. The NSGA-II version that applies the feature-driven crossover found a greater diversity of solutions (potential {\{}PLA{\}} designs), with higher feature-based cohesion, and less feature scattering and tangling. },
	Doi = {https://doi.org/10.1016/j.jss.2016.02.026},
	ISSN = {0164-1212},
	Keywords = {Crossover operator,Empirical study,Multi-objective genetic algorithm,Product line architecture design},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121216000583}
}

@Article{Colanzi2013970,
	Title = {{Search Based Software Engineering: Review and analysis of the field in Brazil}},
	Author = {Colanzi, Thelma Elita and Vergilio, Silvia Regina and Assun{\c{c}}{\~{a}}o, Wesley Klewerton Guez and Pozo, Aurora},
	Journal = {Journal of Systems and Software},
	Year = {2013},
	Number = {4},
	Pages = {970--984},
	Volume = {86},
	Abstract = {Search Based Software Engineering (SBSE) is the field of software engineering research and practice that applies search based techniques to solve different optimization problems from diverse software engineering areas. {\{}SBSE{\}} approaches allow software engineers to automatically obtain solutions for complex and labor-intensive tasks, contributing to reduce efforts and costs associated to the software development. The {\{}SBSE{\}} field is growing rapidly in Brazil. The number of published works and research groups has significantly increased in the last three years and a Brazilian {\{}SBSE{\}} community is emerging. This is mainly due to the Brazilian Workshop on Search Based Software Engineering (WOES), co-located with the Brazilian Symposium on Software Engineering (SBES). Considering these facts, this paper presents results of a mapping we have performed in order to provide an overview of the {\{}SBSE{\}} field in Brazil. The main goal is to map the Brazilian {\{}SBSE{\}} community on {\{}SBES{\}} by identifying the main researchers, focus of the published works, fora and frequency of publications. The paper also introduces {\{}SBSE{\}} concerns and discusses trends, challenges, and open research problems to this emergent area. We hope the work serves as a reference to this novel field, contributing to disseminate {\{}SBSE{\}} and to its consolidation in Brazil. },
	Annote = {{\{}SI{\}} : Software Engineering in Brazil: Retrospective and Prospective Views},
	Doi = {https://doi.org/10.1016/j.jss.2012.07.041},
	ISSN = {0164-1212},
	Keywords = {Metaheuristics,Search based algorithms,Software engineering},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121212002166}
}

@Article{Coman2014124,
	Title = {{Cooperation, collaboration and pair-programming: Field studies on backup behavior}},
	Author = {Coman, Irina D and Robillard, Pierre N and Sillitti, Alberto and Succi, Giancarlo},
	Journal = {Journal of Systems and Software},
	Year = {2014},
	Pages = {124--134},
	Volume = {91},
	Abstract = {Abstract Considering that pair programming has been extensively studied for more than a decade, it can seem quite surprising that there is such a lack of consensus on both its best use and its benefits. We argue that pair programming is not a replacement of usual developer interactions, but rather a formalization and enhancement of naturally occurring interactions. Consequently, we study and classify a broader range of developer interactions, evaluating them for type, purpose and patterns of occurrence, with the aim to identify situations in which pair programming is likely to be truly needed and thus most beneficial. We study the concrete pair programming practices in both academic and industrial settings. All interactions between teammates were recorded as backup behavior activities. In each of these two projects, developers were free to interact when needed. All team interactions were self-recorded by the teammates. The analysis of the interaction tokens shows two salient features: solo work is an important component of teamwork and team interactions have two main purposes, namely cooperation and collaboration. Cooperative backup behavior occurs when a developer provides help to a teammate. Collaborative backup behavior occurs when the teammates are sharing the same goal toward solving an issue. We found that collaborative backup behavior, which occurred much less often, is close to the formal definition of pair programming. This study suggests that mandatory pair programming may be less efficient in organizations where solo work could be done and when some interactions are for cooperative activities. Based on these results, we discussed the potential implications concerning the best use of pair programming in practice, a more effective evaluation of its use, its potential benefits and emerging directions of future research. },
	Doi = {https://doi.org/10.1016/j.jss.2013.12.037},
	ISSN = {0164-1212},
	Keywords = {Backup-behavior,Field study,Pair-programming},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121214000107}
}

@Article{Conejero2012212,
	Title = {{On the relationship of concern metrics and requirements maintainability}},
	Author = {Conejero, J M and Figueiredo, E and Garcia, A and Hern{\'{a}}ndez, J and Jurado, E},
	Journal = {Information and Software Technology},
	Year = {2012},
	Number = {2},
	Pages = {212--238},
	Volume = {54},
	Abstract = {Context: Maintainability has become one of the most essential attributes of software quality, as software maintenance has shown to be one of the most costly and time-consuming tasks of software development. Many studies reveal that maintainability is not often a major consideration in requirements and design stages, and software maintenance costs may be reduced by a more controlled design early in the software life cycle. Several problem factors have been identified as harmful for software maintainability, such as lack of upfront consideration of proper modularity choices. In that sense, the presence of crosscutting concerns is one of such modularity anomalies that possibly exert negative effects on software maintainability. However, to the date there is little or no knowledge about how characteristics of crosscutting concerns, observable in early artefacts, are correlated with maintainability. Objective: In this setting, this paper introduces an empirical analysis where the correlation between crosscutting properties and two ISO/IEC 9126 maintainability attributes, namely changeability and stability, is presented. Method: This correlation is based on the utilization of a set of concern metrics that allows the quantification of crosscutting, scattering and tangling. Results: Our study confirms that a change in a crosscutting concern is more difficult to be accomplished and that artefacts addressing crosscutting concerns are found to be less stable later as the system evolves. Moreover, our empirical analysis reveals that crosscutting properties introduce non-syntactic dependencies between software artefacts, thereby decreasing the quality of software in terms of changeability and stability as well. These subtle dependencies cannot be easily detected without the use of concern metrics. Conclusion: The correlation provides evidence that the presence of certain crosscutting properties negatively affects to changeability and stability. The whole analysis is performed using as target cases three software product lines, where maintainability properties are of upmost importance not only for individual products but also for the core architecture of the product line. {\textcopyright} 2011 Elsevier B.V. All rights reserved.},
	Annote = {cited By 9},
	Doi = {10.1016/j.infsof.2011.09.003},
	Keywords = {Computer software maintenance; Computer software,Concern metrics; Crosscutting; Crosscutting concer,Maintainability},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-81055140252{\&}doi=10.1016{\%}2Fj.infsof.2011.09.003{\&}partnerID=40{\&}md5=2eb396552b5d1d2a125d858d5e0c5f44}
}

@Conference{Cooke2010,
	Title = {{X-Tools: A case study in building world class software}},
	Author = {Cooke, A},
	Booktitle = {Proceedings of the International Telemetering Conference},
	Year = {2010},
	Volume = {46},
	Abstract = {X-Tools is a collection of utilities for validation, translation, editing and report generation designed to enable the Flight Test Instrumentation (FTI) community to quickly adopt the XidML 3.0 meta-data standard. This paper discusses the challenges of developing such software that meets the current and future needs of the FTI community, and meets the increasingly high quality standards expected of modern software. The paper first starts by discussing the needs of the FTI community and the specific functional requirements of software. These include the ability to fit in with legacy systems, the ability to handle many tens of thousands of parameters, support for new networked-based technologies and support for hardware from any vendor. The non-functional requirements of FTI orientated software are also described and it is suggested that the key non-functional requirements include testability, modifiability, extensibility and maintainability. Finally, as a case study, the X-Tools from ACRA CONTROL are presented. The paper discusses their design, and the tactics used to meet the functional and non-functional requirements of the FTI industry. The paper then outlines how the rigorous quality standards were met and describes the specific mechanisms used to verify the quality of the software. {\textcopyright} International Foundation for Telemetering, 2010.},
	Annote = {cited By 0},
	Keywords = {Computer software selection and evaluation; Exhib,Functional requirement; Non-functional requirement,Telemetering},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877941858{\&}partnerID=40{\&}md5=e503731ea3151bf371524e250f06b6a7}
}

@InProceedings{Corazza:2010:ETS:1868328.1868335,
	Title = {{How Effective is Tabu Search to Configure Support Vector Regression for Effort Estimation?}},
	Author = {Corazza, A and {Di Martino}, S and Ferrucci, F and Gravino, C and Sarro, F and Mendes, E},
	Booktitle = {Proceedings of the 6th International Conference on Predictive Models in Software Engineering},
	Year = {2010},
	Address = {New York, NY, USA},
	Pages = {4:1----4:10},
	Publisher = {ACM},
	Series = {PROMISE '10},
	Doi = {10.1145/1868328.1868335},
	ISBN = {978-1-4503-0404-7},
	Keywords = {Tabu search,development effort estimation,empirical studies,support vector machines,support vector regression},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/1868328.1868335}
}

@InProceedings{Cordy:2014:CGA:2635868.2635919,
	Title = {{Counterexample Guided Abstraction Refinement of Product-line Behavioural Models}},
	Author = {Cordy, Maxime and Heymans, Patrick and Legay, Axel and Schobbens, Pierre-Yves and Dawagne, Bruno and Leucker, Martin},
	Booktitle = {Proceedings of the 22Nd ACM SIGSOFT International Symposium on Foundations of Software Engineering},
	Year = {2014},
	Address = {New York, NY, USA},
	Pages = {190--201},
	Publisher = {ACM},
	Series = {FSE 2014},
	Doi = {10.1145/2635868.2635919},
	ISBN = {978-1-4503-3056-5},
	Keywords = {Abstraction,CEGAR,Model Checking,Software Product Lines},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2635868.2635919}
}

@Article{Costa2016156,
	Title = {{Evaluating {\{}REST{\}} architectures—Approach, tooling and guidelines}},
	Author = {Costa, Bruno and Pires, Paulo F and Delicato, Fl{\'{a}}via C and Merson, Paulo},
	Journal = {Journal of Systems and Software},
	Year = {2016},
	Pages = {156--180},
	Volume = {112},
	Abstract = {Abstract Architectural decisions determine the ability of the implemented system to satisfy functional and quality attribute requirements. The Representational State Transfer (REST) architectural style has been extensively used recently for integrating services and applications. Its adoption to build SOA-based distributed systems brings several benefits, but also poses new challenges and risks. Particularly important among those risks are failures to effectively address quality attribute requirements such as security, reliability, and performance. A proved efficient technique to identify and help mitigate those risks is the architecture evaluation. In this paper we propose an approach, tooling, and guidelines to aid architecture evaluation activities in REST-based systems. These guidelines can be systematically used along with evaluation methods to reason about design considerations and tradeoffs. To demonstrate how the guidelines can help architecture evaluators, we present a proof of concept describing how to use the guidelines in an {\{}ATAM{\}} (Architecture Tradeoff Analysis Method) evaluation. We also present the results of a survey conducted with industry specialists who have performed architecture evaluations in real world REST-based systems in order to gauge the suitability and utility of the proposed guidelines. Finally, the paper describes a Web tool developed to facilitate the use of the evaluation guidelines. },
	Doi = {https://doi.org/10.1016/j.jss.2015.09.039},
	ISSN = {0164-1212},
	Keywords = {REST,Scenario-based evaluation guidelines,Software architecture evaluation},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121215002150}
}

@Article{Costa2015239,
	Title = {{A Scientific Software Product Line for the Bioinformatics domain}},
	Author = {Costa, G C B and Braga, R and David, J M N and Campos, F},
	Journal = {Journal of Biomedical Informatics},
	Year = {2015},
	Pages = {239--264},
	Volume = {56},
	Abstract = {Context: Most specialized users (scientists) that use bioinformatics applications do not have suitable training on software development. Software Product Line (SPL) employs the concept of reuse considering that it is defined as a set of systems that are developed from a common set of base artifacts. In some contexts, such as in bioinformatics applications, it is advantageous to develop a collection of related software products, using SPL approach. If software products are similar enough, there is the possibility of predicting their commonalities, differences and then reuse these common features to support the development of new applications in the bioinformatics area. Objectives: This paper presents the PL-Science approach which considers the context of SPL and ontology in order to assist scientists to define a scientific experiment, and to specify a workflow that encompasses bioinformatics applications of a given experiment. This paper also focuses on the use of ontologies to enable the use of Software Product Line in biological domains. Method: In the context of this paper, Scientific Software Product Line (SSPL) differs from the Software Product Line due to the fact that SSPL uses an abstract scientific workflow model. This workflow is defined according to a scientific domain and using this abstract workflow model the products (scientific applications/algorithms) are instantiated. Results: Through the use of ontology as a knowledge representation model, we can provide domain restrictions as well as add semantic aspects in order to facilitate the selection and organization of bioinformatics workflows in a Scientific Software Product Line. The use of ontologies enables not only the expression of formal restrictions but also the inferences on these restrictions, considering that a scientific domain needs a formal specification. Conclusions: This paper presents the development of the PL-Science approach, encompassing a methodology and an infrastructure, and also presents an approach evaluation. This evaluation presents case studies in bioinformatics, which were conducted in two renowned research institutions in Brazil. {\textcopyright} 2015 Elsevier Inc.},
	Annote = {cited By 3},
	Doi = {10.1016/j.jbi.2015.05.014},
	File = {:Users/mac/Downloads/Sciencedirect{\_}articles{\_}13Jun2017{\_}15-33-20.965/A-Scientific-Software-Product-Line-for-the-Bioinformatics-domain{\_}2015{\_}Journal-of-Biomedical-Informatics.pdf:pdf},
	Keywords = {Algorithms; Brazil; Cloud Computing; Cluster Anal,Application programs; Bioinformatics; Computer sof,Bioinformatics applications; Bioinformatics workf,DNA; Software,Factual; Internet; Observer Variation; Programmin,Software design,algorithm; Article; bioinformatics; computer prog},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938568062{\&}doi=10.1016{\%}2Fj.jbi.2015.05.014{\&}partnerID=40{\&}md5=e2192bcdee6013c6539bb89f6a19253a}
}

@Article{SPE:SPE2306,
	Title = {{Toward the adaptation of component-based architectures by model transformation: behind smart user interfaces}},
	Author = {Criado, Javier and Rodr{\'{i}}guez-Gracia, Diego and Iribarne, Luis and Padilla, Nicol{\'{a}}s},
	Journal = {Software: Practice and Experience},
	Year = {2015},
	Number = {12},
	Pages = {1677--1718},
	Volume = {45},
	Abstract = {Graphical user interfaces are not always developed for remaining static. There are GUIs with the need of implementing some variability mechanisms. Component-based GUIs are an ideal target for incorporating this kind of operations, because they can adapt their functionality at run-time when their structure is updated by adding or removing components or by modifying the relationships between them. Mashup user interfaces are a good example of this type of GUI, and they allow to combine services through the assembly of graphical components. We intend to adapt component-based user interfaces for obtaining smart user interfaces. With this goal, our proposal attempts to adapt abstract component-based architectures by using model transformation. Our aim is to generate at run-time a dynamic model transformation, because the rules describing their behavior are not pre-set but are selected from a repository depending on the context. The proposal describes an adaptation schema based on model transformation providing a solution to this dynamic transformation. Context information is processed to select at run-time a rule subset from a repository. Selected rules are used to generate, through a higher-order transformation, the dynamic model transformation. This approach has been tested through a case study which applies different repositories to the same architecture and context. Moreover, a web tool has been developed for validation and demonstration of its applicability. The novelty of our proposal arises from the adaptation schema that creates a non pre-set transformation, which enables the dynamic adaptation of component-based architectures. Copyright {\textcopyright} 2014 Copyright {\textcopyright} 2014 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spe.2306},
	ISSN = {1097-024X},
	Keywords = {component-based architectures,higher-order transformations,mashup user interfaces,model transformations,run-time adaptation,smart user interfaces},
	Url = {http://dx.doi.org/10.1002/spe.2306}
}

@Article{Cruzes2011440,
	Title = {{Research synthesis in software engineering: A tertiary study}},
	Author = {Cruzes, Daniela S and Dyb{\aa}, Tore},
	Journal = {Information and Software Technology},
	Year = {2011},
	Number = {5},
	Pages = {440--455},
	Volume = {53},
	Abstract = {Context Comparing and contrasting evidence from multiple studies is necessary to build knowledge and reach conclusions about the empirical support for a phenomenon. Therefore, research synthesis is at the center of the scientific enterprise in the software engineering discipline. Objective The objective of this article is to contribute to a better understanding of the challenges in synthesizing software engineering research and their implications for the progress of research and practice. Method A tertiary study of journal articles and full proceedings papers from the inception of evidence-based software engineering was performed to assess the types and methods of research synthesis in systematic reviews in software engineering. Results As many as half of the 49 reviews included in the study did not contain any synthesis. Of the studies that did contain synthesis, two thirds performed a narrative or a thematic synthesis. Only a few studies adequately demonstrated a robust, academic approach to research synthesis. Conclusion We concluded that, despite the focus on systematic reviews, there is limited attention paid to research synthesis in software engineering. This trend needs to change and a repertoire of synthesis methods needs to be an integral part of systematic reviews to increase their significance and utility for research and practice. },
	Annote = {Special Section on Best Papers from {\{}XP2010{\}}},
	Doi = {https://doi.org/10.1016/j.infsof.2011.01.004},
	ISSN = {0950-5849},
	Keywords = {Empirical software engineering,Evidence-based software engineering,Mixed-methods,Qualitative methods,Systematic review},
	Url = {http://www.sciencedirect.com/science/article/pii/S095058491100005X}
}

@Article{Cubo2013326,
	Title = {{Composition and self-adaptation of service-based systems with feature models}},
	Author = {Cubo, J and Gamez, N and Fuentes, L and Pimentel, E},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2013},
	Pages = {326--342},
	Volume = {7925 LNCS},
	Abstract = {The adoption of mechanisms for reusing software in pervasive systems has not yet become standard practice. This is because the use of pre-existing software requires the selection, composition and adaptation of prefabricated software parts, as well as the management of some complex problems such as guaranteeing high levels of efficiency and safety in critical domains. In addition to the wide variety of services, pervasive systems are composed of many networked heterogeneous devices with embedded software. In this work, we promote the safe reuse of services in service-based systems using two complementary technologies, Service-Oriented Architecture and Software Product Lines. In order to do this, we extend both the service discovery and composition processes defined in the DAMASCo framework, which currently does not deal with the service variability that constitutes pervasive systems. We use feature models to represent the variability and to self-adapt the services during the composition in a safe way taking context changes into consideration. We illustrate our proposal with a case study related to the driving domain of an Intelligent Transportation System, handling the context information of the environment. {\textcopyright} 2013 Springer-Verlag Berlin Heidelberg.},
	Annote = {cited By 7},
	Doi = {10.1007/978-3-642-38977-1_25},
	Keywords = {Computer software reusability; Information servic,Context information; Feature models; Heterogeneous,Software design},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884877705{\&}doi=10.1007{\%}2F978-3-642-38977-1{\_}25{\&}partnerID=40{\&}md5=f81d9530e644ff736d29215fe0d9a293}
}

@Article{Cuesta2005177,
	Title = {{An “abstract process? approach to algebraic dynamic architecture description}},
	Author = {Cuesta, Carlos E and de la Fuente, Pablo and Barrio-Sol{\'{o}}rzano, Manuel and Beato, M Encarnaci{\'{o}}n},
	Journal = {The Journal of Logic and Algebraic Programming},
	Year = {2005},
	Number = {2},
	Pages = {177--214},
	Volume = {63},
	Abstract = {Current software development methodologies recognize the critical importance of the architectural concerns during the design phase. Software Architecture promises to be the solution for a number of recurring problems; but to do so, the first task is to be able to obtain a precise description of a system architecture. In late years, a number of specific architecture description languages (Adls) have been proposed in order to achieve the required precision. Most of them have solid formal foundations; among them, several process-algebraic Adls stand out for their popularity and expressive power. The algebraic approach to architecture description is probably the most successful in the field. There is a natural intuition relating the concepts of algebraic process and architectural component; anyway, none of the existing approaches seems to have found the right balance between them. This article explains what is the problem with them, and defines the informal concept of abstract process, trying to provide a reference for the right level of abstraction. After presenting the concept, the article presents a dynamic, reflective Adl named P i L ar, which has been designed using this notion. The syntax and semantics of this language are briefly summarized and explained. Finally, the classic example of the Gas Station is described in terms of P i L ar, and then compared to previous presentations in other Adls. },
	Annote = {Special Issue on Process Algebra and System Architecture},
	Doi = {https://doi.org/10.1016/j.jlap.2004.05.003},
	ISSN = {1567-8326},
	Keywords = {Abstract process,Architecture description language,Dynamic software architecture,Process algebra,Reflection,$\pi$-Calculus},
	Url = {http://www.sciencedirect.com/science/article/pii/S1567832604000360}
}

@Article{Cunha2016234,
	Title = {{Evaluating refactorings for spreadsheet models}},
	Author = {Cunha, J{\'{a}}come and Fernandes, Jo{\~{a}}o Paulo and Martins, Pedro and Mendes, Jorge and Pereira, Rui and Saraiva, Jo{\~{a}}o},
	Journal = {Journal of Systems and Software},
	Year = {2016},
	Pages = {234--250},
	Volume = {118},
	Abstract = {Abstract Software refactoring is a well-known technique that provides transformations on software artifacts with the aim of improving their overall quality. We have previously proposed a catalog of refactorings for spreadsheet models expressed in the ClassSheets modeling language, which allows us to specify the business logic of a spreadsheet in an object-oriented fashion. Reasoning about spreadsheets at the model level enhances a model-driven spreadsheet environment where a ClassSheet model and its conforming instance (spreadsheet data) automatically co-evolves after applying a refactoring at the model level. Research motivation was to improve the model and its conforming instance: the spreadsheet data. In this paper we define such refactorings using previously proposed evolution steps for models and instances. We also present an empirical study we designed and conducted in order to confirm our original intuition that these refactorings have a positive impact on end-user productivity, both in terms of effectiveness and efficiency. The results are not only presented in terms of productivity changes between refactored and non-refactored scenarios, but also the overall user satisfaction, relevance, and experience. In almost all cases the refactorings improved end-users productivity. Moreover, in most cases users were more engaged with the refactored version of the spreadsheets they worked with. },
	Doi = {https://doi.org/10.1016/j.jss.2016.04.043},
	ISSN = {0164-1212},
	Keywords = {Empirical study,Model-driven engineering,Software refactoring,Spreadsheets},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121216300280}
}

@Article{Cunningham2006147,
	Title = {{Using classic problems to teach Java framework design}},
	Author = {Cunningham, H Conrad and Liu, Yi and Zhang, Cuihua},
	Journal = {Science of Computer Programming},
	Year = {2006},
	Number = {1–2},
	Pages = {147--169},
	Volume = {59},
	Abstract = {All programmers should understand the concept of software families and know the techniques for constructing them. This paper suggests that classic problems, such as well-known algorithms and data structures, are good sources for examples to use in a study of software family design. The paper describes two case studies that can be used to introduce students in a Java software design course to the construction of software families using software frameworks. The first is the family of programs that use the well-known divide and conquer algorithmic strategy. The second is the family of programs that carry out traversals of binary trees. },
	Annote = {Special Issue on Principles and Practices of Programming in Java (PPPJ 2004)Special Issue on Principles and Practices of Programming in Java (PPPJ 2004)},
	Doi = {https://doi.org/10.1016/j.scico.2005.07.009},
	ISSN = {0167-6423},
	Keywords = {Design pattern,Divide and conquer,Hot spot,Software family,Software framework,Tree traversal},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642305000900}
}

@Article{SPIP:SPIP213,
	Title = {{Formalizing cardinality-based feature models and their specialization}},
	Author = {Czarnecki, Krzysztof and Helsen, Simon and Eisenecker, Ulrich},
	Journal = {Software Process: Improvement and Practice},
	Year = {2005},
	Number = {1},
	Pages = {7--29},
	Volume = {10},
	Abstract = {Feature modeling is an important approach to capture the commonalities and variabilities in system families and product lines. Cardinality-based feature modeling integrates a number of existing extensions of the original feature-modeling notation from Feature-Oriented Domain Analysis. Staged configuration is a process that allows the incremental configuration of cardinality-based feature models. It can be achieved by performing a step-wise specialization of the feature model. In this article, we argue that cardinality-based feature models can be interpreted as a special class of context-free grammars. We make this precise by specifying a translation from a feature model into a context-free grammar. Consequently, we provide a semantic interpretation for cardinality-based feature models by assigning an appropriate semantics to the language recognized by the corresponding grammar. Finally, we give an account on how feature model specialization can be formalized as transformations on the grammar equivalent of feature models. Copyright {\textcopyright} 2005 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spip.213},
	ISSN = {1099-1670},
	Keywords = {domain analysis,software configuration,software product lines,system families},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/spip.213}
}

@Article{SPIP:SPIP225,
	Title = {{Staged configuration through specialization and multilevel configuration of feature models}},
	Author = {Czarnecki, Krzysztof and Helsen, Simon and Eisenecker, Ulrich},
	Journal = {Software Process: Improvement and Practice},
	Year = {2005},
	Number = {2},
	Pages = {143--169},
	Volume = {10},
	Abstract = {Feature modeling is a key technique for capturing commonalities and variabilities in system families and product lines. In this article, we propose a cardinality-based notation for feature modeling, which integrates a number of existing extensions of previous approaches. We then introduce and motivate the novel concept of staged configuration. Staged configuration can be achieved by the stepwise specialization of feature models or by multilevel configuration, where the configuration choices available in each stage are defined by separate feature models. Staged configuration is important because, in a realistic development process, different groups and different people make product configuration choices in different stages. Finally, we also discuss how multilevel configuration avoids a breakdown between the different abstraction levels of individual features. This problem, sometimes referred to as 'analysis paralysis', easily occurs in feature modeling because features can denote entities at arbitrary levels of abstraction within a system family. Copyright {\textcopyright} 2005 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spip.225},
	ISSN = {1099-1670},
	Keywords = {domain analysis,software configuration,software product lines,system families},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/spip.225}
}

@Article{d’Amorim20121012,
	Title = {{Modularity analysis of use case implementations}},
	Author = {D'Amorim, Fernanda and Borba, Paulo},
	Journal = {Journal of Systems and Software},
	Year = {2012},
	Number = {4},
	Pages = {1012--1027},
	Volume = {85},
	Abstract = {A component-based decomposition can result in implementations having use cases code tangled with other concerns and scattered across components. Modularity mechanisms such as aspects, mixins, and virtual classes have been proposed to address this kind of problem. One can use such mechanisms to group together code related to a single use case. This paper quantitatively analyzes the impact of this kind of use case modularization. We apply one specific technique, aspect oriented programming, to modularize the use case implementations of two information systems that conform to the layered architecture pattern. We extract traditional and contemporary metrics – including cohesion, coupling, and separation of concerns – to analyze modularity in terms of quality attributes such as changeability, support for independent development, and pluggability. Our findings indicate that the results of a given modularity analysis depend on other factors beyond the chosen system, metrics, and the applied modularity technique. },
	Doi = {https://doi.org/10.1016/j.jss.2011.11.1025},
	ISSN = {0164-1212},
	Keywords = {Aspect-oriented programming,Empirical software engineering,Modularity,Use cases},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121211002950}
}

@Article{SPE:SPE1087,
	Title = {{Agile product line engineering—a systematic literature review}},
	Author = {D{\'{i}}az, Jessica and P{\'{e}}rez, Jennifer and Alarc{\'{o}}n, Pedro P and Garbajosa, Juan},
	Journal = {Software: Practice and Experience},
	Year = {2011},
	Number = {8},
	Pages = {921--941},
	Volume = {41},
	Doi = {10.1002/spe.1087},
	ISSN = {1097-024X},
	Keywords = {agile product line engineering (APLE),agile software development,software product line engineering,systematic literature review},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/spe.1087}
}

@Article{Diaz2015323,
	Title = {{A model for tracing variability from features to product-line architectures: A case study in smart grids}},
	Author = {D{\'{i}}az, J and P{\'{e}}rez, J and Garbajosa, J},
	Journal = {Requirements Engineering},
	Year = {2015},
	Number = {3},
	Pages = {323--343},
	Volume = {20},
	Abstract = {In current software systems with highly volatile requirements, traceability plays a key role to maintain the consistency between requirements and code. Traceability between artifacts involved in the development of software product line (SPL) is still more critical because it is necessary to guarantee that the selection of variants that realize the different SPL products meet the requirements. Current SPL traceability mechanisms trace from variability in features to variations in the configuration of product-line architecture (PLA) in terms of adding and removing components. However, it is not always possible to materialize the variable features of a SPL through adding or removing components, since sometimes they are materialized inside components, i.e., in part of their functionality: a class, a service, and/or an interface. Additionally, variations that happen inside components may crosscut several components of architecture. These kinds of variations are still challenging and their traceability is not currently well supported. Therefore, it is not possible to guarantee that those SPL products with these kinds of vriations meet the requirements. This paper presents a solution for tracing variability from features to PLA by taking these kinds of variations into account. This solution is based on models and traceability between models in order to automate SPL configuration by selecting the variants and realizing the product application. The FPLA modeling framework supports this solution which has been deployed in a software factory. Validation has consisted in putting the solution into practice to develop a product line of power metering management applications for smart grids. {\textcopyright} Springer-Verlag London 2014.},
	Annote = {cited By 3},
	Doi = {10.1007/s00766-014-0203-1},
	Keywords = {Computer software; Electric power transmission net,Management applications; Product applications; Pr,Smart power grids},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943353769{\&}doi=10.1007{\%}2Fs00766-014-0203-1{\&}partnerID=40{\&}md5=2b9700810f69105f1098e29cc8ca542c}
}

@Article{Diaz2014727,
	Title = {{Agile product-line architecting in practice: A case study in smart grids}},
	Author = {D{\'{i}}az, J and P{\'{e}}rez, J and Garbajosa, J},
	Journal = {Information and Software Technology},
	Year = {2014},
	Number = {7},
	Pages = {727--748},
	Volume = {56},
	Abstract = {Context Software Product Line Engineering implies the upfront design of a Product-Line Architecture (PLA) from which individual product applications can be engineered. The big upfront design associated with PLAs is in conflict with the current need of "being open to change". To make the development of product-lines more flexible and adaptable to changes, several companies are adopting Agile Product Line Engineering. However, to put Agile Product Line Engineering into practice it is still necessary to make mechanisms available to assist and guide the agile construction and evolution of PLAs. Objective This paper presents the validation of a process for "the agile construction and evolution of product-line architectures", called Agile Product-Line Architecting (APLA). The contribution of the APLA process is the integration of a set of models for describing, documenting, and tracing PLAs, as well as an algorithm for guiding the change decision-making process of PLAs. The APLA process is assessed to prove that assists Agile Product Line Engineering practitioners in the construction and evolution of PLAs. Method Validation is performed through a case study by using both quantitative and qualitative analysis. Quantitative analysis was performed using statistics, whereas qualitative analysis was performed through interviews using constant comparison, triangulation, and supporting tools. This case study was conducted according to the guidelines of Runeson and H{\"{o}}st in a software factory where three projects in the domain of Smart Grids were involved. Results APLA is deployed through the Flexible-PLA modeling framework. This framework supported the successful development and evolution of the PLA of a family of power metering management applications for Smart Grids. Conclusions APLA is a well-supported solution for the agile construction and evolution of PLAs. This case study illustrates that the proposed solution for the agile construction of PLAs is viable in an industry project on Smart Grids. {\textcopyright} 2014 Elsevier B.V. All rights reserved.},
	Annote = {cited By 9},
	Doi = {10.1016/j.infsof.2014.01.014},
	Keywords = {Agile product-line architecting; Architectural kn,Product design,Production engineering; Research; Smart power grid},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898810060{\&}doi=10.1016{\%}2Fj.infsof.2014.01.014{\&}partnerID=40{\&}md5=79cd1ad172a29071b1a41d63ef7c884c}
}

@Article{DaMotaSilveiraNeto2011407,
	Title = {{A systematic mapping study of software product lines testing}},
	Author = {{Da Mota Silveira Neto}, Paulo Anselmo and {Carmo MacHado}, Ivan Do and McGregor, John D. and {De Almeida}, Eduardo Santana and {De Lemos Meira}, Silvio Romero},
	Journal = {Information and Software Technology},
	Year = {2011},
	Number = {5},
	Pages = {407--423},
	Volume = {53},
	Abstract = {Context: In software development, Testing is an important mechanism both to identify defects and assure that completed products work as specified. This is a common practice in single-system development, and continues to hold in Software Product Lines (SPL). Even though extensive research has been done in the SPL Testing field, it is necessary to assess the current state of research and practice, in order to provide practitioners with evidence that enable fostering its further development. Objective: This paper focuses on Testing in SPL and has the following goals: investigate state-of-the-art testing practices, synthesize available evidence, and identify gaps between required techniques and existing approaches, available in the literature. Method: A systematic mapping study was conducted with a set of nine research questions, in which 120 studies, dated from 1993 to 2009, were evaluated. Results: Although several aspects regarding testing have been covered by single-system development approaches, many cannot be directly applied in the SPL context due to specific issues. In addition, particular aspects regarding SPL are not covered by the existing SPL approaches, and when the aspects are covered, the literature just gives brief overviews. This scenario indicates that additional investigation, empirical and practical, should be performed. Conclusion: The results can help to understand the needs in SPL Testing, by identifying points that still require additional investigation, since important aspects regarding particular points of software product lines have not been addressed yet. {\textcopyright} 2010 Elsevier B.V. All rights reserved.},
	Annote = {From Duplicate 1 (A systematic mapping study of software product lines testing - Da Mota Silveira Neto, Paulo Anselmo; Carmo MacHado, Ivan Do; McGregor, John D.; De Almeida, Eduardo Santana; De Lemos Meira, Silvio Romero)
		From Duplicate 1 (A systematic mapping study of software product lines testing - Da Mota Silveira Neto, P A; Carmo MacHado, I D; McGregor, J D; De Almeida, E S; De Lemos Meira, S R)
		cited By 87
		From Duplicate 2 (A systematic mapping study of software product lines testing - Da Mota Silveira Neto, P A; Carmo MacHado, I D; McGregor, J D; De Almeida, E S; De Lemos Meira, S R)
		cited By 87},
	Doi = {10.1016/j.infsof.2010.12.003},
	File = {:Users/mac/Downloads/Sciencedirect{\_}articles{\_}13Jun2017{\_}14-36-55.006/A-systematic-mapping-study-of-software-product-lines-testing{\_}2011{\_}Information-and-Software-Technology.pdf:pdf},
	ISBN = {0950-5849},
	ISSN = {09505849},
	Keywords = {,Computer software selection and evaluation,Further development,Mappi,Mapping studies,Mapping study,Research que,Software design,Software product lines,Software testing},
	Publisher = {Elsevier B.V.},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952451558{\&}doi=10.1016{\%}2Fj.infsof.2010.12.003{\&}partnerID=40{\&}md5=48146e279174abef9ae7749f41cec642 http://dx.doi.org/10.1016/j.infsof.2010.12.003}
}

@Article{SilveiraNeto2013872,
	Title = {{25 years of software engineering in Brazil: Beyond an insider's view}},
	Author = {{da Mota Silveira Neto}, Paulo Anselmo and Gomes, Jo{\'{a}}s Sousa and de Almeida, Eduardo Santana and Leite, Jair Cavalcanti and Batista, Thais Vasconcelos and Leite, Larissa},
	Journal = {Journal of Systems and Software},
	Year = {2013},
	Number = {4},
	Pages = {872--889},
	Volume = {86},
	Abstract = {The software engineering area is facing a growing number of challenges due to the continuing increase in software size and complexity. The challenges are addressed by the very relevant and high quality publications of the Brazilian Symposium on Software Engineering (SBES), in the past 25 editions. This article summarizes the findings from two different mapping studies about these 25 {\{}SBES{\}} editions. It also reports the results of an expert opinion survey with the most important Brazilian researchers in the software engineering (SE) area. The survey reinforces the findings of the mapping studies. It also provides guidance for future research. In addition, the studies report several findings that confirmed the validity of the research methods applied. All of these findings are important input to the current Brazilian {\{}SE{\}} scenario. Our findings also suggest that greater attention should be given to the {\{}SE{\}} area, by improving researchers' interaction with industry and increasing collaboration between researchers, especially internationally. },
	Annote = {{\{}SI{\}} : Software Engineering in Brazil: Retrospective and Prospective Views},
	Doi = {https://doi.org/10.1016/j.jss.2012.10.041},
	ISSN = {0164-1212},
	Keywords = {Expert opinion survey,Mapping study,Software engineering},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121212002981}
}

@Article{DaSilva2015,
	Title = {{Using a multi-method approach to understand agile software product lines}},
	Author = {{Da Silva}, Ivonei Freitas and {Da Mota Silveira Neto}, Paulo Anselmo and O'Leary, P??draig and {De Almeida}, Eduardo Santana and {De Lemos Meira}, Silvio Romero},
	Journal = {Information and Software Technology},
	Year = {2015},
	Number = {1},
	Pages = {527--542},
	Volume = {57},
	Abstract = {Context: Software product lines (SPLs) and Agile are approaches that share similar objectives. The main difference is the way in which these objectives are met. Typically evidence on what activities of Agile and SPL can be combined and how they can be integrated stems from different research methods performed separately. The generalizability of this evidence is low, as the research topic is still relatively new and previous studies have been conducted using only one research method. Objective: This study aims to increase understanding of Agile SPL and improve the generalizability of the identified evidence through the use of a multi-method approach. Method: Our multi-method research combines three complementary methods (Mapping Study, Case Study and Expert Opinion) to consolidate the evidence. Results: This combination results in 23 findings that provide evidence on how Agile and SPL could be combined. Conclusion: Although multi-method research is time consuming and requires a high degree of effort to plan, design, and perform, it helps to increase the understanding on Agile SPL and leads to more generalizable evidence. The findings confirm a synergy between Agile and SPL and serve to improve the body of evidence in Agile SPL. When researchers and practitioners develop new Agile SPL approaches, it will be important to consider these synergies.},
	Doi = {10.1016/j.infsof.2014.06.004},
	File = {:Users/mac/Downloads/Sciencedirect{\_}articles{\_}13Jun2017{\_}15-34-52.618/Using-a-multi-method-approach-to-understand-Agile-software-product-lines{\_}2015{\_}Information-and-Software-Technology.pdf:pdf},
	ISSN = {09505849},
	Keywords = {Agile,Case study,Expert opinion,Mapping study,Multi-method approach,Software product lines},
	Publisher = {Elsevier B.V.},
	Url = {http://dx.doi.org/10.1016/j.infsof.2014.06.004}
}

@Article{SPE:SPE452,
	Title = {{A framework for table driven testing of Java classes}},
	Author = {Daley, Nigel and Hoffman, Daniel and Strooper, Paul},
	Journal = {Software: Practice and Experience},
	Year = {2002},
	Number = {5},
	Pages = {465--493},
	Volume = {32},
	Abstract = {With the advent of object-oriented languages and the portability of Java, the development and use of class libraries has become widespread. Effective class reuse depends on class reliability which in turn depends on thorough testing. This paper describes a class testing approach based on modeling each test case with a tuple and then generating large numbers of tuples to thoroughly cover an input space with many interesting combinations of values. The testing approach is supported by the Roast framework for the testing of Java classes. Roast provides automated tuple generation based on boundary values, unit operations that support driver standardization, and test case templates used for code generation. Roast produces thorough, compact test drivers with low development and maintenance cost. The framework and tool support are illustrated on a number of non-trivial classes, including a graphical user interface policy manager. Quantitative results are presented to substantiate the practicality and effectiveness of the approach. Copyright {\textcopyright} 2002 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spe.452},
	ISSN = {1097-024X},
	Keywords = {Java,automated class testing,unit testing},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/spe.452}
}

@Article{Dam2016137,
	Title = {{Consistent merging of model versions}},
	Author = {Dam, Hoa Khanh and Egyed, Alexander and Winikoff, Michael and Reder, Alexander and Lopez-Herrejon, Roberto E},
	Journal = {Journal of Systems and Software},
	Year = {2016},
	Pages = {137--155},
	Volume = {112},
	Abstract = {Abstract While many engineering tasks can, and should be, manageable independently, it does place a great burden on explicit collaboration needs—including the need for frequent and incremental merging of artifacts that software engineers manipulate using these tools. State-of-the-art merging techniques are often limited to textual artifacts (e.g., source code) and they are unable to discover and resolve complex merging issues beyond simple conflicts. This work focuses on the merging of models where we consider not only conflicts but also arbitrary syntactic and semantic consistency issues. Consistent artifacts are merged fully automatically and only inconsistent/conflicting artifacts are brought to the users' attention, together with a systematic proposal of how to resolve them. Our approach is neutral with regard to who made the changes and hence reduces the bias caused by any individual engineer's limited point of view. Our approach also applies to arbitrary design or models, provided that they follow a well-defined metamodel with explicit constraints—the norm nowadays. The extensive empirical evaluation suggests that our approach scales to practical settings. },
	Doi = {https://doi.org/10.1016/j.jss.2015.06.044},
	ISSN = {0164-1212},
	Keywords = {Inconsistency management,Model merging,Model versioning},
	Url = {http://www.sciencedirect.com/science/article/pii/S016412121500134X}
}

@Article{SMR:SMR466,
	Title = {{Balancing uncertainty of context in ERP project estimation: an approach and a case study}},
	Author = {Daneva, Maya},
	Journal = {Journal of Software Maintenance and Evolution: Research and Practice},
	Year = {2010},
	Number = {5},
	Pages = {329--357},
	Volume = {22},
	Abstract = {The increasing demand for Enterprise Resource Planning (ERP) solutions as well as the high rates of troubled ERP implementations and outright cancellations calls for developing effort estimation practices to systematically deal with uncertainties in ERP projects. This paper describes an approach—and a case study—to balancing uncertainties of context in the very early project stages, when an ERP adopter initiates a request-for-proposal process and when alternative bids are to be compared for the purpose of choosing an implementation partner. The proposed empirical approach leverages the complementary application of three techniques, an algorithmic estimation model, Monte Carlo simulation, and portfolio management. Our case study findings show how the ability of our approach to model uncertainty allows practitioners to address the challenging question of how to adjust project context factors so that chances of project success are increased. We also include a discussion on the implications of our approach for practice as well as on the possible validity threats and what the practitioner could do to counterpart them. Copyright {\textcopyright} 2010 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/smr.466},
	ISSN = {1532-0618},
	Keywords = {COCOMO,Enterprise Resource Planning implementation,Monte Carlo simulation,portfolio management,project effort estimation},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/smr.466}
}

@Article{Dang2014135,
	Title = {{Exploring modal worlds}},
	Author = {Dang, Han-Hing and Gl{\"{u}}ck, Roland and M{\"{o}}ller, Bernhard and Roocks, Patrick and Zelend, Andreas},
	Journal = {Journal of Logical and Algebraic Methods in Programming},
	Year = {2014},
	Number = {2},
	Pages = {135--153},
	Volume = {83},
	Abstract = {Abstract Modal idempotent semirings cover a large set of different applications. The paper presents a small collection of these, ranging from algebraic logics for program correctness over bisimulation refinement, formal concept analysis, database preferences to feature oriented software development. We provide new results and/or views on these domains; the modal semiring setting allows a concise and unified treatment, while being more general than, e.g., standard relation algebra. },
	Annote = {Festschrift in Honour of Gunther Schmidt on the Occasion of his 75th Birthday},
	Doi = {https://doi.org/10.1016/j.jlap.2014.02.004},
	ISSN = {2352-2208},
	Keywords = {Bisimulation,Formal concept analysis,Pareto front,Rectangles,Separation logic,Software product lines},
	Url = {http://www.sciencedirect.com/science/article/pii/S1567832614000058}
}

@Article{IIS2:IIS200697,
	Title = {{4.4.1 A Hybrid Requirements Capture Process}},
	Author = {Daniels, Jesse and Botta, Rick and Bahill, Terry},
	Journal = {INCOSE International Symposium},
	Year = {2005},
	Number = {1},
	Pages = {654--667},
	Volume = {15},
	Abstract = {Systems engineers traditionally produce a system requirements specification containing shall-statement requirements. However, the rapid adoption of use case modeling for capturing functional requirements in the software community has caused systems engineers to examine the utility of use case models for capturing system-level functional requirements. A transition from traditional shall-statement requirements to use case modeling has raised some issues and questions. This paper advocates a unified requirements engineering method in which use case modeling and traditional shall-statement requirements are applied together to effectively express both functional and non-functional requirements for complex, hierarchical systems. This paper also presents a practical method for extracting requirements from the use case text to produce a robust requirements specification.},
	Doi = {10.1002/j.2334-5837.2005.tb00697.x},
	ISSN = {2334-5837},
	Url = {http://dx.doi.org/10.1002/j.2334-5837.2005.tb00697.x}
}

@Article{Dao2010377,
	Title = {{Mapping features to reusable components: A problem frames-based approach}},
	Author = {Dao, T M and Kang, K C},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2010},
	Pages = {377--392},
	Volume = {6287 LNCS},
	Abstract = {In software product line engineering (SPLE), feature modeling has been extensively used to represent commonality and variability between the products of a domain in terms of features, based on which reusable components are developed. However, the link between a feature model and product requirements, that fundamentally decide how the features are developed into reusable components, has not been adequately addressed in SPLE methods. This paper introduces an approach to combining feature modeling and problem frames in an attempt to address this problem. First, features are mapped to problem frames using heuristics derived from feature modeling and feature mapping units. Requirements are then identified and analyzed to ensure that they are fully satisfied. Finally, a solution modeling method maps the problem frames to architectural components. A Home Integration System (HIS) case study is used to demonstrate the feasibility of the approach. {\textcopyright} 2010 Springer-Verlag Berlin Heidelberg.},
	Annote = {cited By 5},
	Doi = {10.1007/978-3-642-15579-6_26},
	Keywords = {Architectural components; Commonality and variabil,Computer software reusability; Network architectu,Software design},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049354800{\&}doi=10.1007{\%}2F978-3-642-15579-6{\_}26{\&}partnerID=40{\&}md5=bc7e661954a4edda413019f40273c6bc}
}

@Article{IIS2:IIS201413,
	Title = {{10.1.2 Towards a framework for variability management and integration in Systems Engineering}},
	Author = {Dauron, Alain and Dumitrescu, Cosmin and Salinesi, Camille},
	Journal = {INCOSE International Symposium},
	Year = {2012},
	Number = {1},
	Pages = {1425--1438},
	Volume = {22},
	Abstract = {The automotive industry is faced today with the challenge of developing increasingly complex systems, but it also needs to integrate new emerging technologies while leveraging on existing designs, on previously developed assets. The concept of reuse is well described in different domains, such as software product line engineering, production, product development or marketing (as a mass customization strategy). In the automotive industry, we see systems engineering as the domain that can bridge the gap between high-level customer requirements diversity and basic component development. The challenge is to be able to ensure traceability from customer down to components while being able to match requirement variability to component diversity and to ensure an effective management of variability at each level of abstraction or decomposition. The current paper presents an on-going research that aims at developing a comprehensive framework for managing system variability under three main themes: development process conception, variability description and integration with current methods concerning vehicle development. Our paper will focus on the last two of these themes, presenting the needs for treating this issue and presenting a metamodel for variability representation and integration of external diversity.},
	Doi = {10.1002/j.2334-5837.2012.tb01413.x},
	ISSN = {2334-5837},
	Keywords = {asset reuse,commonality,product lines,systems engineering,systems variability,variability},
	Url = {http://dx.doi.org/10.1002/j.2334-5837.2012.tb01413.x}
}

@Article{Saraiva201585,
	Title = {{Classifying metrics for assessing Object-Oriented Software Maintainability: A family of metrics' catalogs}},
	Author = {{de A.G. Saraiva}, Juliana and de Fran{\c{c}}a, Micael S and Soares, S{\'{e}}rgio C B and Filho, Fernando J C L and de Souza, Renata M C R},
	Journal = {Journal of Systems and Software},
	Year = {2015},
	Pages = {85--101},
	Volume = {103},
	Abstract = {Abstract Object-Oriented Programming is one of the most used paradigms. Complementarily, the software maintainability is considered a software attribute playing an important role in quality level. In this context, Object-Oriented Software Maintainability (OOSM) has been studied through years, and many researchers have proposed a large number of metrics to measure it. Consequently, the decision-making process about which metrics can be adopted in experiments on {\{}OOSM{\}} is a hard task. Therefore, a metrics' categorization has been proposed to facilitate this process. As result, 7 categories and 17 subcategories were identified. These categories represent the scenarios of {\{}OOSM{\}} metrics adoption, and a family of {\{}OOSM{\}} metrics catalog was generated based on the selection of a metrics' categorization. Additionally, a quasi-experiment was conducted to check the coverage index of the catalogs generated using our approach over the catalogs suggested by experts. 90{\%} of coverage was obtained with 99{\%} of confidential level using the Wilcoxon Test. Complementarily, a survey was conducted to check the experts' opinion about the catalog generated by the portal when they were compared by the catalogs suggested by them. Therefore, this evaluation can be the first evidences of the usefulness of the family of the catalogs based on the metrics' categorization. },
	Doi = {https://doi.org/10.1016/j.jss.2015.01.014},
	ISSN = {0164-1212},
	Keywords = {Metrics,Object-Oriented Software Development,Software maintainability},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121215000126}
}

@Article{dosSantosNeto20161243,
	Title = {{A hybrid approach to suggest software product line portfolios}},
	Author = {{de Alc{\^{a}}ntara dos Santos Neto}, Pedro and Britto, Ricardo and {de Andrade Lira Rab{\^{e}}lo}, Ricardo and {de Almeida Cruz}, Jonathas Jivago and Lira, Werney Ayala Luz},
	Journal = {Applied Soft Computing},
	Year = {2016},
	Pages = {1243--1255},
	Volume = {49},
	Abstract = {Abstract Software product line (SPL) development is a new approach to software engineering which aims at the development of a whole range of products. However, as long as {\{}SPL{\}} can be useful, there are many challenges regarding the use of that approach. One of the main problems which hinders the adoption of software product line (SPL) is the complexity regarding product management. In that context, we can remark the scoping problem. One of the existent ways to deal with scoping is the product portfolio scoping (PPS). {\{}PPS{\}} aims to define the products that should be developed as well as their key features. In general, that approach is driven by marketing aspects, like cost of the product and customer satisfaction. Defining a product portfolio by using the many different available aspects is a NP-hard problem. This work presents an improved hybrid approach to solve the feature model selection problem, aiming at supporting product portfolio scoping. The proposal is based in a hybrid approach not dependent on any particular algorithm/technology. We have evaluated the usefulness and scalability of our approach using one real {\{}SPL{\}} (ArgoUML-SPL) and synthetic SPLs. As per the evaluation results, our approach is both useful from a practitioner's perspective and scalable. },
	Doi = {https://doi.org/10.1016/j.asoc.2016.08.024},
	ISSN = {1568-4946},
	Keywords = {Feature model selection problem,Fuzzy inference systems,NSGA-II,Product portfolio scoping,Search based feature model selection,Search based software engineering,Software product lines},
	Url = {http://www.sciencedirect.com/science/article/pii/S1568494616304185}
}

@Article{Maia20131023,
	Title = {{On the impact of trace-based feature location in the performance of software maintainers}},
	Author = {{de Almeida Maia}, Marcelo and Lafet{\'{a}}, Raquel Fialho},
	Journal = {Journal of Systems and Software},
	Year = {2013},
	Number = {4},
	Pages = {1023--1037},
	Volume = {86},
	Abstract = {Software maintainers frequently strive to locate source code related to specific software features. This situation is mostly observable when features are scattered in the code. Considering this problem, several approaches for feature location using execution traces have been developed. Nonetheless, the practice of post-mortem analysis based on execution traces is not fully incorporated in the daily practice of software maintainers. Empirical studies that reveal strengths and weaknesses on the use of execution traces in maintenance activities could better explain the role of execution traces in software maintenance. This study reports on a controlled experiment conducted with maintainers performing actual maintenance activities on systems of different sizes unknown to them. There are benefits from systematic use of execution traces: the reduction of the maintenance activity time and greater accuracy of the activity outcome. Other qualitative observations were the lower level of activity difficulty perceived by the participants that used execution trace information and that this kind of information seems to be less useful in maintenance activities where the problem of feature scattering does not occur clearly. },
	Annote = {{\{}SI{\}} : Software Engineering in Brazil: Retrospective and Prospective Views},
	Doi = {https://doi.org/10.1016/j.jss.2012.12.032},
	ISSN = {0164-1212},
	Keywords = {Empirical assessment,Execution traces,Feature location,Software maintenance},
	Url = {http://www.sciencedirect.com/science/article/pii/S016412121200341X}
}

@Article{deOMelo2013412,
	Title = {{Interpretative case studies on agile team productivity and management}},
	Author = {{de O. Melo}, Claudia and Cruzes, Daniela S and Kon, Fabio and Conradi, Reidar},
	Journal = {Information and Software Technology},
	Year = {2013},
	Number = {2},
	Pages = {412--427},
	Volume = {55},
	Abstract = {Context The management of software development productivity is a key issue in software organizations, where the major drivers are lower cost and shorter time-to-market. Agile methods, including Extreme Programming and Scrum, have evolved as “light? approaches that simplify the software development process, potentially leading to increased team productivity. However, little empirical research has examined which factors do have an impact on productivity and in what way, when using agile methods. Objective Our objective is to provide a better understanding of the factors and mediators that impact agile team productivity. Method We have conducted a multiple-case study for 6 months in three large Brazilian companies that have been using agile methods for over 2 years. We have focused on the main productivity factors perceived by team members through interviews, documentation from retrospectives, and non-participant observation. Results We developed a novel conceptual framework, using thematic analysis to understand the possible mechanisms behind such productivity factors. Agile team management was found to be the most influential factor in achieving agile team productivity. At the intra-team level, the main productivity factors were team design (structure and work allocation) and member turnover. At the inter-team level, the main productivity factors were how well teams could be effectively coordinated by proper interfaces and other dependencies and avoiding delays in providing promised software to dependent teams. Conclusion Teams should be aware of the influence and magnitude of turnover, which has been shown negative for agile team productivity. Team design choices remain an important factor impacting team productivity, even more pronounced on agile teams that rely on teamwork and people factors. The intra-team coordination processes must be adjusted to enable productive work by considering priorities and pace between teams. Finally, the revised conceptual framework for agile team productivity supports further tests through confirmatory studies. },
	Annote = {Special Section: Component-Based Software Engineering (CBSE), 2011},
	Doi = {https://doi.org/10.1016/j.infsof.2012.09.004},
	ISSN = {0950-5849},
	Keywords = {Agile software development,Industrial case studies,Team management,Team productivity factors,Thematic analysis},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584912001875}
}

@InProceedings{deOliveira:2005:VMP:1105634.1105651,
	Title = {{A Variability Management Process for Software Product Lines}},
	Author = {{de Oliveira Junior}, Edson Alves and Gimenes, Itana M S and Huzita, Elisa Hatsue Moriya and Maldonado, Jos{\'{e}} Carlos},
	Booktitle = {Proceedings of the 2005 Conference of the Centre for Advanced Studies on Collaborative Research},
	Year = {2005},
	Pages = {225--241},
	Publisher = {IBM Press},
	Series = {CASCON '05},
	Url = {http://0-dl.acm.org.fama.us.es/citation.cfm?id=1105634.1105651}
}

@Article{DeSouza2015319,
	Title = {{Product derivation in practice}},
	Author = {{De Souza}, L O and O'Leary, P and {De Almeida}, E S and {De Lemos Meira}, S R},
	Journal = {Information and Software Technology},
	Year = {2015},
	Pages = {319--337},
	Volume = {58},
	Abstract = {Context: The process of constructing a product from a product line of software assets is known product derivation. An effective product derivation process is important in order to ensure that the efforts required to develop these shared assets is lower than the benefits achieved through their use. Despite its importance, relatively little work has been dedicated to the product derivation process and the strategies applied in practice. Additionally, there is a lack of empirical reports describing product derivation in industrial settings, and, in general, where these reports are available, they have been conducted as informal studies. Objective: Our aim is to investigate how product derivation is performed in practice. Method: We apply a multi-case study design to two different industrial software product line projects with the goal of investigating how they derive their products in practice. The findings from our studies were individually analyzed using the Constant Comparison technique. In order to identify patterns across these studies, the findings were compared using a Cross-case analysis approach. Results: The research approach allowed us to examine the case study outcomes from different perspectives, capturing similarities and differences. From the cases, we identified context specific strategies for product derivation which are easier for practitioners to contextualise and implement. Conclusions: The case studies provide method-in-action insights into concepts explored in the literature, such as: iterative and incremental product derivation, instantiation and integration of platform components and derivation of product databases. Practitioners can use this work as a basis for defining, adapting or evaluating their own product derivation approaches. While researchers can use this work as a starting point for new industrial reports, presenting their experiences with product derivation. {\textcopyright} 2014 Elsevier B.V. All rights reserved.},
	Annote = {cited By 1},
	Doi = {10.1016/j.infsof.2014.07.004},
	Keywords = {Case analysis; Comparison analysis; Multiple-case,Computer software; Iterative methods; Software des,Product design},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84914144344{\&}doi=10.1016{\%}2Fj.infsof.2014.07.004{\&}partnerID=40{\&}md5=161e2856a77bae190e2fc6e05556a2fe}
}

@Article{Deb2016265,
	Title = {{Extracting finite state models from i* models}},
	Author = {Deb, Novarun and Chaki, Nabendu and Ghose, Aditya},
	Journal = {Journal of Systems and Software},
	Year = {2016},
	Pages = {265--280},
	Volume = {121},
	Abstract = {Abstract i* models are inherently sequence agnostic. This makes the process of cross-checking i* models against temporal properties quite impossible. There is an immediate industrial need to bridge the gap between such a sequence agnostic model and a standardized model verifier so that model checking can be performed in the requirement analysis phase itself. In this paper, we first spell out the Naive Algorithm that generates all possible finite state models corresponding to a given i* model. The growth of the finite state model space can be mapped to the problem of finding the number of possible paths between the Least Upper Bound (LUB) and the Greatest Lower Bound (GLB) of a k-dimensional hypercube lattice structure. The mathematics for doing a quantitative analysis of the space growth has also been presented. The Naive Algorithm has its main drawback in the hyperexponential growth of the model space. The Semantic Implosion Algorithm is proposed as a solution to the hyperexponential problem. This algorithm exploits the temporal information embedded within the i* model of an enterprise to reduce the rate of growth of the finite state model space. A comparative quantitative analysis between the two approaches concludes the superiority of the Semantic Implosion Algorithm. },
	Doi = {https://doi.org/10.1016/j.jss.2016.03.038},
	ISSN = {0164-1212},
	Keywords = {Model checking,Model transformation,i* model},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121216300048}
}

@Article{Deelstra2009195,
	Title = {{Variability assessment in software product families}},
	Author = {Deelstra, Sybren and Sinnema, Marco and Bosch, Jan},
	Journal = {Information and Software Technology},
	Year = {2009},
	Number = {1},
	Pages = {195--218},
	Volume = {51},
	Abstract = {Software variability management is a key factor in the success of software systems and software product families. An important aspect of software variability management is the evolution of variability in response to changing markets, business needs, and advances in technology. To be able to determine whether, when, and how variability should evolve, we have developed the {\{}COVAMOF{\}} software variability assessment method (COSVAM). The contribution of {\{}COSVAM{\}} is that it is a novel, and industry-strength assessment process that addresses the issues that are associated to the current variability assessment practice. In this paper, we present the successful validation of {\{}COSVAM{\}} in an industrial software product family. },
	Annote = {Special Section - Most Cited Articles in 2002 and Regular Research Papers},
	Doi = {https://doi.org/10.1016/j.infsof.2008.04.002},
	File = {:Users/mac/Downloads/Sciencedirect{\_}articles{\_}13Jun2017{\_}15-32-18.445/Variability-assessment-in-software-product-families{\_}2009{\_}Information-and-Software-Technology.pdf:pdf},
	ISSN = {0950-5849},
	Keywords = {Assessment,Evolution,Software product families,Variability},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584908000542}
}

@Article{Deelstra2005173,
	Title = {{Product derivation in software product families: A case study}},
	Author = {Deelstra, S and Sinnema, M and Bosch, J},
	Journal = {Journal of Systems and Software},
	Year = {2005},
	Number = {2 SPEC. ISS.},
	Pages = {173--194},
	Volume = {74},
	Abstract = {From our experience with several organizations that employ software product families, we have learned that, contrary to popular belief, deriving individual products from shared software assets is a time-consuming and expensive activity. In this paper we therefore present a study that investigated the source of those problems. We provide the reader with a framework of terminology and concepts regarding product derivation. In addition, we present several problems and issues we identified during a case study at two large industrial organizations that are relevant to other, for example, comparable or less mature organizations. {\textcopyright} 2003 Elsevier Inc. All rights reserved.},
	Annote = {cited By 135},
	Doi = {10.1016/j.jss.2003.11.012},
	Keywords = {Case study; Product derivation; Software product,Costs; Interfaces (computer); Investments; Problem,Software engineering},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-6444234242{\&}doi=10.1016{\%}2Fj.jss.2003.11.012{\&}partnerID=40{\&}md5=31f3032b01263f5e37e93d65ed64b827}
}

@Article{Deelstra2004165,
	Title = {{Experiences in Software Product Families: Problems and Issues during Product Derivation}},
	Author = {Deelstra, S and Sinnema, M and Bosch, J},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2004},
	Pages = {165--182},
	Volume = {3154},
	Abstract = {A fundamental reason for investing in product families is to minimize the application engineering costs. Several organizations that employ product families, however, are becoming increasingly aware of the fact that, despite the efforts in domain engineering, deriving individual products from their shared software assets is a time- and effort-consuming activity. In this paper, we present a collection of product derivation problems that we identified during a case study at two large and mature industrial organizations. These problems are attributed to the lack of methodological support for application engineering, and to underlying causes of complexity and implicit properties. For each problem, we provide a description and an example, while for each cause we present a description, consequences, solutions, and research issues. The discussions in this paper are relevant outside the context of the two companies, as the challenges they face arise in, for example, comparable or less mature organizations. {\textcopyright} Springer-Verlag 2004.},
	Annote = {cited By 24},
	Keywords = {Application engineering; Domain engineering; Indu,Cost engineering,Societies and institutions},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-35048863852{\&}partnerID=40{\&}md5=27964013b1c42ae6e016636d1126754e}
}

@Article{Deelstra2004473,
	Title = {{A product derivation framework for software product families}},
	Author = {Deelstra, S and Sinnema, M and Bosch, J},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2004},
	Pages = {473--484},
	Volume = {3014},
	Abstract = {From our experience with several organizations that employ software product families, we have learned that deriving individual products from shared software artifacts is a time-consuming and expensive activity. In the research community, product derivation methodologies are rather scarce, however. By studying product derivation, we believe we will be better able to provide and validate industrially practicable solutions for application engineering. In this paper, we present a framework of terminology and concepts regarding product derivation that serves as basis for further discussion. We exemplify this framework with two industrial case studies, i.e. Thales Nederland B.V. and Robert Bosch GmbH. {\textcopyright} Springer-Verlag Berlin Heidelberg 2004.},
	Annote = {cited By 1},
	Keywords = {Application engineering; Industrial case study; P,Artificial intelligence,Computer science; Computers},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-35048844224{\&}partnerID=40{\&}md5=ce618c240de4f8fd9b6e4420479cd5dd}
}

@Article{SMR:SMR337,
	Title = {{Continuous evolution through software architecture evaluation: a case study}},
	Author = {{Del Rosso}, Christian},
	Journal = {Journal of Software Maintenance and Evolution: Research and Practice},
	Year = {2006},
	Number = {5},
	Pages = {351--383},
	Volume = {18},
	Doi = {10.1002/smr.337},
	ISSN = {1532-0618},
	Keywords = {experience-based software assessment,scenario-based software architecture assessment,software architecture assessments,software performance assessment,software product family},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/smr.337}
}

@Article{Demuth2016281,
	Title = {{Co-evolution of metamodels and models through consistent change propagation}},
	Author = {Demuth, Andreas and Riedl-Ehrenleitner, Markus and Lopez-Herrejon, Roberto E and Egyed, Alexander},
	Journal = {Journal of Systems and Software},
	Year = {2016},
	Pages = {281--297},
	Volume = {111},
	Abstract = {Abstract In model-driven engineering (MDE), metamodels and domain-specific languages are key artifacts as they are used to define syntax and static semantics of domain models. However, metamodels are evolving over time, requiring existing domain models to be co-evolved. Though approaches have been proposed for performing such co-evolution automatically, those approaches typically support only specific metamodel changes. In this paper, we present a vision of co-evolution between metamodels and models through consistent change propagation. The approach addresses co-evolution issues without being limited to specific metamodels or evolution scenarios. It relies on incremental management of metamodel-based constraints that are used to detect co-evolution failures (i.e., inconsistencies between metamodel and model). After failure detection, the approach automatically generates suggestions for correction (i.e., repairs for inconsistencies). A case study with the {\{}UML{\}} metamodel and 23 {\{}UML{\}} models shows that the approach is technically feasible and also scalable. },
	Doi = {https://doi.org/10.1016/j.jss.2015.03.003},
	ISSN = {0164-1212},
	Keywords = {Consistency checking,Consistent change propagation,Metamodel co-evolution},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121215000564}
}

@InProceedings{Deneckere2015,
	Title = {{Method Association Approach: Situational construction and evaluation of an implementation method for software products}},
	Author = {Deneckere, Rebecca and Hug, Charlotte and Onderstal, Juliette and Brinkkemper, Sjaak},
	Booktitle = {2015 IEEE 9th International Conference on Research Challenges in Information Science (RCIS)},
	Year = {2015},
	Month = {may},
	Pages = {274--285},
	Publisher = {IEEE},
	Doi = {10.1109/RCIS.2015.7128888},
	ISBN = {978-1-4673-6630-4},
	Url = {http://ieeexplore.ieee.org/document/7128888/}
}

@Article{Deng2006247,
	Title = {{Addressing domain evolution challenges in software product lines}},
	Author = {Deng, G and Lenz, G and Schmidt, D C},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2006},
	Pages = {247--261},
	Volume = {3844 LNCS},
	Abstract = {It is hard to develop and evolve software product-line architectures (PLAs) for large-scale distributed real-time and embedded (DRE) systems. Although certain challenges of PLAs can be addressed by combining model-driven development (MDD) techniques with component frameworks, domain evolution problems remain largely unresolved. In particular, extending or refactoring existing software product-lines to handle unanticipated requirements or better satisfy current requirements requires significant effort. This paper describes techniques for minimizing such impacts on MDD-based PLAs for DRE systems through a case study that shows how a layered architecture and model-to-model transformation tool support can reduce the effort of PLA evolution. {\textcopyright} Springer-Verlag Berlin Heidelberg 2006.},
	Annote = {cited By 8},
	Keywords = {Computer architecture; Computer software; Distrib,Domains; Model Transformation; Model-driven develo,Software engineering},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745644065{\&}partnerID=40{\&}md5=f9b4f3ee8eaac0b2b74ead6f121a4e5e}
}

@Article{Deng20132537,
	Title = {{Compositional reasoning for weighted Markov decision processes}},
	Author = {Deng, Yuxin and Hennessy, Matthew},
	Journal = {Science of Computer Programming},
	Year = {2013},
	Number = {12},
	Pages = {2537--2579},
	Volume = {78},
	Abstract = {Abstract Weighted Markov decision processes (MDPs) have long been used to model quantitative aspects of systems in the presence of uncertainty. However, much of the literature on such {\{}MDPs{\}} takes a monolithic approach, by modelling a system as a particular MDP; properties of the system are then inferred by analysis of that particular MDP. In contrast in this paper we develop compositional methods for reasoning about weighted MDPs, as a possible basis for compositional reasoning about their quantitative behaviour. In particular we approach these systems from a process algebraic point of view. For these we define a coinductive simulation-based behavioural preorder which is compositional in the sense that it is preserved by structural operators for constructing weighted {\{}MDPs{\}} from components. For finitary convergent processes, which are finite-state and finitely branching systems without divergence, we provide two characterisations of the behavioural preorder. The first uses a novel quantitative probabilistic logic, while the second is in terms of a novel form of testing, in which benefits are accrued during the execution of tests. },
	Annote = {Special Section on International Software Product Line Conference 2010 and Fundamentals of Software Engineering (selected papers of {\{}FSEN{\}} 2011)},
	Doi = {https://doi.org/10.1016/j.scico.2013.02.009},
	ISSN = {0167-6423},
	Keywords = {Compositionality,Markov decision processes,Modal logic,Simulation,Testing preorder},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642313000440}
}

@Article{Derakhshanmanesh2014333,
	Title = {{Requirements-driven incremental adoption of variability management techniques and tools: an industrial experience report}},
	Author = {Derakhshanmanesh, M and Fox, J and Ebert, J},
	Journal = {Requirements Engineering},
	Year = {2014},
	Number = {4},
	Pages = {333--354},
	Volume = {19},
	Abstract = {In theory, software product line engineering has reached a mature state. In practice though, implementing a variability management approach remains a tough case-by-case challenge for any organization. To tame the complexity of this undertaking, it is inevitable to handle variability from multiple perspectives and to manage variability consistently across artifacts, tools, and workflows. Especially, a solid understanding and management of the requirements to be met by the products is an inevitable prerequisite. In this article, we share experiences from the ongoing incremental adoption of explicit variability management at TRW Automotive's department for automotive slip control systems—located in Koblenz, Germany. On the technical side, the three key drivers of this adoption effort are (a) domain modeling and scoping, (b) handling of variability in requirements and (c) tighter integration of software engineering focus areas (e.g., domain modeling, requirements engineering, architectural modeling) to make use of variability-related data. In addition to implementation challenges with using and integrating concrete third-party tools, social and workflow-related issues are covered as well. The lessons learned are presented, discussed, and thoroughly compared with the state of the art in research. {\textcopyright} 2013, Springer-Verlag London.},
	Annote = {cited By 4},
	Doi = {10.1007/s00766-013-0185-4},
	Keywords = {C (programming language),Computer software reusability; Software design,Features; Incremental adoption; Requirements; Reu},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920258824{\&}doi=10.1007{\%}2Fs00766-013-0185-4{\&}partnerID=40{\&}md5=87e9e513d18e8be3cc6ae95b7eccfec8}
}

@Article{Dermeval20154950,
	Title = {{Ontology-based feature modeling: An empirical study in changing scenarios}},
	Author = {Dermeval, D and Ten{\'{o}}rio, T and Bittencourt, I I and Silva, A and Isotani, S and Ribeiro, M},
	Journal = {Expert Systems with Applications},
	Year = {2015},
	Number = {11},
	Pages = {4950--4964},
	Volume = {42},
	Abstract = {A software product line (SPL) is a set of software systems that have a particular set of common features and that satisfy the needs of a particular market segment or mission. Feature modeling is one of the key activities involved in the design of SPLs. The feature diagram produced in this activity captures the commonalities and variabilities of SPLs. In some complex domains (e.g.; ubiquitous computing, autonomic systems and context-aware computing), it is difficult to foresee all functionalities and variabilities a specific SPL may require. Thus, Dynamic Software Product Lines (DSPLs) bind variation points at runtime to adapt to fluctuations in user needs as well as to adapt to changes in the environment. In this context, relying on formal representations of feature models is important to allow them to be automatically analyzed during system execution. Among the mechanisms used for representing and analyzing feature models, description logic (DL) based approaches demand to be better investigated in DSPLs since it provides capabilities, such as automated inconsistency detection, reasoning efficiency, scalability and expressivity. Ontology is the most common way to represent feature models knowledge based on DL reasoners. Previous works conceived ontologies for feature modeling either based on OWL classes and properties or based on OWL individuals. However, considering change or evolution scenarios of feature models, we need to compare whether a class-based or an individual-based feature modeling style is recommended to describe feature models to support SPLs, and especially its capabilities to deal with changes in feature models, as required by DSPLs. In this paper, we conduct a controlled experiment to empirically compare two approaches based on each one of these modeling styles in several changing scenarios (e.g.; add/remove mandatory feature, add/remove optional feature and so on). We measure time to perform changes, structural impact of changes (flexibility) and correctness for performing changes in our experiment. Our results indicate that using OWL individuals requires less time to change and is more flexible than using OWL classes and properties. These results provide insightful assumptions towards the definition of an approach relying on reasoning capabilities of ontologies that can effectively support products reconfiguration in the context of DSPL. {\textcopyright} 2015 Elsevier Ltd. All rights reserved.},
	Annote = {cited By 6},
	Doi = {10.1016/j.eswa.2015.02.020},
	Keywords = {Birds; Computation theory; Computer software; Data,Context-aware computing; Dynamic software product,Software design},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924777581{\&}doi=10.1016{\%}2Fj.eswa.2015.02.020{\&}partnerID=40{\&}md5=2fefeaf0ff5c921b30d1c9a4d9f75aa9}
}

@Article{SPIP:SPIP224,
	Title = {{Experiences with conducting project postmortems: reports versus stories}},
	Author = {Desouza, Kevin C and Dings{\o}yr, Torgeir and Awazu, Yukika},
	Journal = {Software Process: Improvement and Practice},
	Year = {2005},
	Number = {2},
	Pages = {203--215},
	Volume = {10},
	Abstract = {The most popular unit of work in organizations is a project. Managing knowledge in and about projects is salient for successful project management. In this article, we will discuss how postmortems can be used to capture tacit experiences in projects. Conducting a postmortem, either after a milestone or at the end of a project, is salient in order to gauge what has been learnt, what were the main issues faced, and what can be used to improve the processes of work in the future. The conducting of postmortems aids in articulation of tacit experiences into explicit forms. This enables for experiences to be better re-used in the future. Re-using of postmortem findings depends heavily on the nature of the postmortem outcome. We will compare two kinds of postmortem outcomes—traditional reports and stories. Both types have their pros and cons, and management must choose the right kind of postmortem report to calibrate, depending on the project and learning outcomes. The article will also highlight lessons learnt from conducting postmortem reviews in several software organizations. Copyright {\textcopyright} 2005 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spip.224},
	ISSN = {1099-1670},
	Keywords = {knowledge management,narratives,postmortem review,project,project management,projects,software engineering,stories},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/spip.224}
}

@Article{Devroey2017153,
	Title = {{Statistical prioritization for software product line testing: an experience report}},
	Author = {Devroey, X and Perrouin, G and Cordy, M and Samih, H and Legay, A and Schobbens, P.-Y. and Heymans, P},
	Journal = {Software and Systems Modeling},
	Year = {2017},
	Number = {1},
	Pages = {153--171},
	Volume = {16},
	Abstract = {Software product lines (SPLs) are families of software systems sharing common assets and exhibiting variabilities specific to each product member of the family. Commonalities and variabilities are often represented as features organized in a feature model. Due to combinatorial explosion of the number of products induced by possible features combinations, exhaustive testing of SPLs is intractable. Therefore, sampling and prioritization techniques have been proposed to generate sorted lists of products based on coverage criteria or weights assigned to features. Solely based on the feature model, these techniques do not take into account behavioural usage of such products as a source of prioritization. In this paper, we assess the feasibility of integrating usage models into the testing process to derive statistical testing approaches for SPLs. Usage models are given as Markov chains, enabling prioritization of probable/rare behaviours. We used featured transition systems, compactly modelling variability and behaviour for SPLs, to determine which products are realizing prioritized behaviours. Statistical prioritization can achieve a significant reduction in the state space, and modelling efforts can be rewarded by better automation. In particular, we used MaTeLo, a statistical test cases generation suite developed at ALL4TEC. We assess feasibility criteria on two systems: Claroline, a configurable course management system, and Sferion™, an embedded system providing helicopter landing assistance. {\textcopyright} 2015, Springer-Verlag Berlin Heidelberg.},
	Annote = {cited By 0},
	Doi = {10.1007/s10270-015-0479-8},
	Keywords = {Combinatorial explosion; Course management system,Computer software; Embedded systems; Markov proces,Integration testing},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937933560{\&}doi=10.1007{\%}2Fs10270-015-0479-8{\&}partnerID=40{\&}md5=05e5d292352e727f469b0cc635961e21}
}

@Article{Dey2017160,
	Title = {{REASSURE: Requirements elicitation for adaptive socio-technical systems using repertory grid}},
	Author = {Dey, Sangeeta and Lee, Seok-Won},
	Journal = {Information and Software Technology},
	Year = {2017},
	Pages = {160--179},
	Volume = {87},
	Abstract = {AbstractContext Socio-technical systems are expected to understand the dynamics of the execution environment and behave accordingly. Significant work has been done on formalizing and modeling requirements of such adaptive systems. However, not enough attention is paid on eliciting requirements from users and introducing flexibility in the system behavior at an early phase of requirements engineering. Most of the work is based on an assumption that general users' cognitive level would be able to support the inherent complexity of variability acquisition. Objective Our main focus is on providing help to the users with ordinary cognitive level to express their expectations from the complex system considering various contexts. This work also helps the designers to explore the design variability based on the general users' preferences. Method We explore the idea of using a cognitive technique Repertory Grid (RG) to acquire knowledge from users and experts along multiple dimensions of problem and design space. We propose {\{}REASSURE{\}} methodology which guides requirements engineers to explore the intentional and design variability in an organized way. We also provide a tool support to analyze the knowledge captured in multiple repertory grid files and detect potential conflicts in the intentional variability. Finally, we evaluate the proposed idea by performing an empirical study using smart home system domain. Results The result of our study shows that a greater number of requirements can be elicited after applying our approach. With the help of the provided tool support, it is even possible to detect a greater number of conflicts in user's requirements than the traditional practices. Conclusion We envision {\{}RG{\}} as a technique to filter design options based on the intentional variability in various contexts. The promising results of empirical study open up new research questions: “how to elicit requirements from multiple stakeholders and reach consensus for multi-dimensional problem domain?. },
	Doi = {https://doi.org/10.1016/j.infsof.2017.03.004},
	ISSN = {0950-5849},
	Keywords = {Adaptive systems,Repertory grid,Requirements elicitation,Socio-technical systems},
	Url = {http://www.sciencedirect.com/science/article/pii/S095058491730229X}
}

@Article{Dhungana20101108,
	Title = {{Structuring the modeling space and supporting evolution in software product line engineering}},
	Author = {Dhungana, Deepak and Gr{\"{u}}nbacher, Paul and Rabiser, Rick and Neumayer, Thomas},
	Journal = {Journal of Systems and Software},
	Year = {2010},
	Number = {7},
	Pages = {1108--1122},
	Volume = {83},
	Abstract = {The scale and complexity of product lines means that it is practically infeasible to develop a single model of the entire system, regardless of the languages or notations used. The dynamic nature of real-world systems means that product line models need to evolve continuously to meet new customer requirements and to reflect changes of product line artifacts. To address these challenges, product line engineers need to apply different strategies for structuring the modeling space to ease the creation and maintenance of models. This paper presents an approach that aims at reducing the maintenance effort by organizing product lines as a set of interrelated model fragments defining the variability of particular parts of the system. We provide support to semi-automatically merge fragments into complete product line models. We also provide support to automatically detect inconsistencies between product line artifacts and the models representing these artifacts after changes. Furthermore, our approach supports the co-evolution of models and their respective meta-models. We discuss strategies for structuring the modeling space and show the usefulness of our approach using real-world examples from our ongoing industry collaboration. },
	Annote = {{\{}SPLC{\}} 2008},
	Doi = {https://doi.org/10.1016/j.jss.2010.02.018},
	ISSN = {0164-1212},
	Keywords = {Model evolution,Product line engineering,Variability modeling},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121210000506}
}

@Article{Diaz20101970,
	Title = {{Generating blogs out of product catalogues: An {\{}MDE{\}} approach}},
	Author = {Diaz, Oscar and Villoria, Felipe M},
	Journal = {Journal of Systems and Software},
	Year = {2010},
	Number = {10},
	Pages = {1970--1982},
	Volume = {83},
	Abstract = {Blogs can be used as a conduit for customer opinions and, in so doing, building communities around products. We attempt to realise this vision by building blogs out of product catalogues. Unfortunately, the immaturity of blog engines makes this endeavour risky. This paper presents a model-driven approach to face this drawback. This implies the introduction of (meta)models: the catalogue model, based on the standard Open Catalog Format, and blog models, that elaborate on the use of blogs as conduits for virtual communities. Blog models end up being realised through blog engines. Specifically, we focus on two types of engines: a hosted blog platform and a standalone blog platform, both in Blojsom. However, the lack of standards in a broad and constantly evolving blog-engine space, hinders both the portability and the maintainability of the solution. Hence, we resort to the notion of “abstract platform? as a way to depart from the peculiarities of specific blog engines. Additionally, the paper measures the reuse gains brought by {\{}MDE{\}} in comparison with the manual coding of blogs. },
	Doi = {https://doi.org/10.1016/j.jss.2010.05.075},
	ISSN = {0164-1212},
	Keywords = {Blog,Catalogue,Model-Driven Engineering,PSM evolution},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121210001469}
}

@Article{Ding2014545,
	Title = {{Knowledge-based approaches in software documentation: A systematic literature review}},
	Author = {Ding, Wei and Liang, Peng and Tang, Antony and van Vliet, Hans},
	Journal = {Information and Software Technology},
	Year = {2014},
	Number = {6},
	Pages = {545--567},
	Volume = {56},
	Abstract = {AbstractContext Software documents are core artifacts produced and consumed in documentation activity in the software lifecycle. Meanwhile, knowledge-based approaches have been extensively used in software development for decades, however, the software engineering community lacks a comprehensive understanding on how knowledge-based approaches are used in software documentation, especially documentation of software architecture design. Objective The objective of this work is to explore how knowledge-based approaches are employed in software documentation, their influences to the quality of software documentation, and the costs and benefits of using these approaches. Method We use a systematic literature review method to identify the primary studies on knowledge-based approaches in software documentation, following a pre-defined review protocol. Results Sixty studies are finally selected, in which twelve quality attributes of software documents, four cost categories, and nine benefit categories of using knowledge-based approaches in software documentation are identified. Architecture understanding is the top benefit of using knowledge-based approaches in software documentation. The cost of retrieving information from documents is the major concern when using knowledge-based approaches in software documentation. Conclusions The findings of this review suggest several future research directions that are critical and promising but underexplored in current research and practice: (1) there is a need to use knowledge-based approaches to improve the quality attributes of software documents that receive less attention, especially credibility, conciseness, and unambiguity; (2) using knowledge-based approaches with the knowledge content in software documents which gets less attention in current applications of knowledge-based approaches in software documentation, to further improve the practice of software documentation activity; (3) putting more focus on the application of software documents using the knowledge-based approaches (knowledge reuse, retrieval, reasoning, and sharing) in order to make the most use of software documents; and (4) evaluating the costs and benefits of using knowledge-based approaches in software documentation qualitatively and quantitatively. },
	Doi = {https://doi.org/10.1016/j.infsof.2014.01.008},
	ISSN = {0950-5849},
	Keywords = {Knowledge activity,Knowledge-based approach,Software architecture design,Software documentation,Systematic literature review},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584914000196}
}

@Article{Machado20141183,
	Title = {{On strategies for testing software product lines: A systematic literature review}},
	Author = {{do Carmo Machado}, Ivan and McGregor, John D. and Cavalcanti, Yguarat{\~{a}} Cerqueira Yguarat?? Cerqueira and {De Almeida}, Eduardo Santana and Machado, Ivan Do Carmo and McGregor, John D. and Cavalcanti, Yguarat{\~{a}} Cerqueira Yguarat?? Cerqueira and {De Almeida}, Eduardo Santana},
	Journal = {Information and Software Technology},
	Year = {2014},
	Number = {10},
	Pages = {1183--1199},
	Volume = {56},
	Abstract = {AbstractContext Testing plays an important role in the quality assurance process for software product line engineering. There are many opportunities for economies of scope and scale in the testing activities, but techniques that can take advantage of these opportunities are still needed. Objective The objective of this study is to identify testing strategies that have the potential to achieve these economies, and to provide a synthesis of available research on {\{}SPL{\}} testing strategies, to be applied towards reaching higher defect detection rates and reduced quality assurance effort. Method We performed a literature review of two hundred seventy-six studies published from the year 1998 up to the 1 st semester of 2013. We used several filters to focus the review on the most relevant studies and we give detailed analyses of the core set of studies. Results The analysis of the reported strategies comprised two fundamental aspects for software product line testing: the selection of products for testing, and the actual test of products. Our findings indicate that the literature offers a large number of techniques to cope with such aspects. However, there is a lack of reports on realistic industrial experiences, which limits the inferences that can be drawn. Conclusion This study showed a number of leveraged strategies that can support both the selection of products, and the actual testing of products. Future research should also benefit from the problems and advantages identified in this study. },
	Doi = {https://doi.org/10.1016/j.infsof.2014.04.002},
	File = {:Users/mac/Downloads/Sciencedirect{\_}articles{\_}13Jun2017{\_}14-36-55.006/On-strategies-for-testing-software-product-lines-A-systematic-literature-review{\_}2014{\_}Information-and-Software-Technology.pdf:pdf},
	ISBN = {0840-8688 VO - 29},
	ISSN = {0950-5849},
	Keywords = {Software product lines,Software quality,Software testing,Systematic literature review},
	Pmid = {25125798},
	Publisher = {Elsevier B.V.},
	Url = {http://dx.doi.org/10.1016/j.infsof.2014.04.002 http://www.sciencedirect.com/science/article/pii/S0950584914000834}
}

@Article{SantosRocha20131355,
	Title = {{The use of software product lines for business process management: A systematic literature review}},
	Author = {{dos Santos Rocha}, Roberto and Fantinato, Marcelo},
	Journal = {Information and Software Technology},
	Year = {2013},
	Number = {8},
	Pages = {1355--1373},
	Volume = {55},
	Abstract = {AbstractContext Business Process Management (BPM) is a potential domain in which Software Product Line (PL) can be successfully applied. Including the support of Service-oriented Architecture (SOA), {\{}BPM{\}} and {\{}PL{\}} may help companies achieve strategic alignment between business and IT. Objective Presenting the results of a study undertaken to seek and assess {\{}PL{\}} approaches for {\{}BPM{\}} through a Systematic Literature Review (SLR). Moreover, identifying the existence of dynamic {\{}PL{\}} approaches for BPM. Method A {\{}SLR{\}} was conducted with four research questions formulated to evaluate {\{}PL{\}} approaches for BPM. Results 63 papers were selected as primary studies according to the criteria established. From these primary studies, only 15 papers address the specific dynamic aspects in the context evaluated. Moreover, it was found that {\{}PLs{\}} only partially address the {\{}BPM{\}} lifecycle since the last business process phase is not a current concern on the found approaches. Conclusions The found {\{}PL{\}} approaches for {\{}BPM{\}} only cover partially the {\{}BPM{\}} lifecycle, not taking into account the last phase which restarts the lifecycle. Moreover, no wide dynamic {\{}PL{\}} proposal was found for BPM, but only the treatment of specific dynamic aspects. The results indicate that {\{}PL{\}} approaches for {\{}BPM{\}} are still at an early stage and gaining maturity. },
	Doi = {https://doi.org/10.1016/j.infsof.2013.02.007},
	ISSN = {0950-5849},
	Keywords = {BPM,Business process management,PL,Software product line},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584913000402}
}

@InProceedings{Dubslaff:2014:PMC:2577080.2577095,
	Title = {{Probabilistic Model Checking for Energy Analysis in Software Product Lines}},
	Author = {Dubslaff, Clemens and Kl{\"{u}}ppelholz, Sascha and Baier, Christel},
	Booktitle = {Proceedings of the 13th International Conference on Modularity},
	Year = {2014},
	Address = {New York, NY, USA},
	Pages = {169--180},
	Publisher = {ACM},
	Series = {MODULARITY '14},
	Doi = {10.1145/2577080.2577095},
	ISBN = {978-1-4503-2772-5},
	Keywords = {dynamic features,energy analysis,probabilistic model checking,software product lines},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2577080.2577095}
}

@Article{Duque201229,
	Title = {{Integration of collaboration and interaction analysis mechanisms in a concern-based architecture for groupware systems}},
	Author = {Duque, Rafael and Rodr{\'{i}}guez, Mar{\'{i}}a Luisa and Hurtado, Mar{\'{i}}a Visitaci{\'{o}}n and Bravo, Crescencio and Rodr{\'{i}}guez-Dom{\'{i}}nguez, Carlos},
	Journal = {Science of Computer Programming},
	Year = {2012},
	Number = {1},
	Pages = {29--45},
	Volume = {77},
	Abstract = {Collaboration and interaction analysis allows for the characterization and study of the collaborative work performed by the users of a groupware system. The results of the analyzed processes allow problems in users' collaborative work and shortcomings in the functionalities of the groupware system to be identified. Therefore, automating collaboration and interaction analysis enables users' work to be assessed and groupware system support and behavior to be improved. This article proposes a concern-based architecture to be used by groupware developers as a guide to the integration of analysis subsystems into groupware systems. This architecture was followed to design the {\{}COLLECE{\}} groupware system, which supports collaborative programming practices and integrates an analysis subsystem that assesses different aspects of the work carried out by the programmers and adapts the functionality of the system under specific conditions. },
	Annote = {System and Software Solution Oriented Architectures},
	Doi = {https://doi.org/10.1016/j.scico.2010.05.003},
	ISSN = {0167-6423},
	Keywords = {CSCW,Collaboration and interaction analysis,Groupware,Software architectures},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642310000936}
}

@Article{Dyer2013148,
	Title = {{Language features for software evolution and aspect-oriented interfaces: An exploratory study}},
	Author = {Dyer, R and Rajan, H and Cai, Y},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2013},
	Pages = {148--183},
	Volume = {7800 LNCS},
	Abstract = {A variety of language features to modularize cross-cutting concerns have recently been discussed, e.g., open modules, annotation-based pointcuts, explicit join points, and quantified-typed events. All of these ideas are essentially a form of aspect-oriented interface between object-oriented and cross-cutting modules, but the representation of this interface differs. Previous works have studied maintenance benefits of AO programs compared to OO programs, by usually looking at a single AO interface. Other works have looked at several AO interfaces, but only on relatively small systems or systems with only one type of aspectual behavior. Thus, there is a need for a study that examines large, realistic systems for several AO interfaces to determine what problems arise and in which interface(s). The main contribution of this work is a rigorous empirical study that evaluates the effectiveness of these proposals for 4 different AO interfaces by applying them to 35 different releases of a software product line called MobileMedia and 50 different releases of a Web application called Health Watcher. In total, over 400k lines of code were studied across all releases. Our comparative analysis using quantitative metrics proposed by Chidamber and Kemerer shows the strengths and weaknesses of these AO interface proposals. Our change impact analysis shows the design stability provided by each of these recent proposals for AO interfaces. {\textcopyright} 2013 Springer-Verlag.},
	Annote = {cited By 1},
	Doi = {10.1007/978-3-642-36964-3_5},
	Keywords = {Artificial intelligence,Change impact analysis; Comparative analysis; Cros,Computer systems programming},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875108456{\&}doi=10.1007{\%}2F978-3-642-36964-3{\_}5{\&}partnerID=40{\&}md5=ab6d868048fbc5f3580720c4f6f5d809}
}

@InProceedings{Dyer:2012:ESD:2162049.2162067,
	Title = {{An Exploratory Study of the Design Impact of Language Features for Aspect-oriented Interfaces}},
	Author = {Dyer, Robert and Rajan, Hridesh and Cai, Yuanfang},
	Booktitle = {Proceedings of the 11th Annual International Conference on Aspect-oriented Software Development},
	Year = {2012},
	Address = {New York, NY, USA},
	Pages = {143--154},
	Publisher = {ACM},
	Series = {AOSD '12},
	Doi = {10.1145/2162049.2162067},
	ISBN = {978-1-4503-1092-5},
	Keywords = {AO interfaces,annotations,aspect-oriented,empirical study,events,implicit invocation,open modules},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2162049.2162067}
}

@Article{SMR:SMR453,
	Title = {{The impact of agile principles on market-driven software product development}},
	Author = {{Dzamashvili Fogelstr{\"{o}}m}, Nina and Gorschek, Tony and Svahnberg, Mikael and Olsson, Peo},
	Journal = {Journal of Software Maintenance and Evolution: Research and Practice},
	Year = {2010},
	Number = {1},
	Pages = {53--80},
	Volume = {22},
	Abstract = {Agile development methods such as extreme programming (XP), SCRUM, Lean Software Development (Lean SD) and others have gained much popularity during the last years. Agile methodologies promise faster time-to-market, satisfied customers and high quality software. While these prospects are appealing, the suitability of agile practices to different domains and business contexts still remains unclear. In this article we investigate the applicability of agile principles in the context of market-driven software product development (MDPD), focusing on pre-project activities. This article presents results of a comparison between typical properties of agile methods to the needs of MDPD, as well as findings of a case study conducted at Ericsson, an early adopter of agile product development. The results show misalignment between the agile principles and needs of pre-project activities in market-driven development. This misalignment threatens to subtract from the positive aspects of agile development, but maybe more importantly, threaten the overall product development by disabling effective product management. Copyright {\textcopyright} 2009 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spip.420},
	ISSN = {1532-0618},
	Keywords = {agile methods,case study,market-driven software development,software product management},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/spip.420}
}

@Article{Echeverria2016476,
	Title = {{Comprehensibility of variability in model fragments for product configuration}},
	Author = {Echeverr{\'{i}}a, J and P{\'{e}}rez, F and Cetina, C and Pastor, {\'{O}}},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2016},
	Pages = {476--490},
	Volume = {9694},
	Abstract = {The ability to manage variability in software has become crucial to overcome the complexity and variety of systems. To this end, a comprehensible representation of variability is important. Nevertheless, in previous works, difficulties have been detected to understand variability in an industrial environment. Specifically, domain experts had difficulty understanding variability in model fragments to produce the software for their products. Hence, the aim of this paper is to further investigate these difficulties by conducting an experiment in which participants deal with variability in order to achieve their desired product configurations. Our results show new insights into product configuration which suggest next steps to improve general variability modeling approaches, and therefore promoting the adoption of these approaches in industry. {\textcopyright} Springer International Publishing Switzerland 2016.},
	Annote = {cited By 0},
	Doi = {10.1007/978-3-319-39696-529},
	Keywords = {Domain experts; Industrial environments; Product,Information systems,Software design; Systems engineering},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976647589{\&}doi=10.1007{\%}2F978-3-319-39696-529{\&}partnerID=40{\&}md5=cc20ce4b870548c351a0f40e87a08572}
}

@InProceedings{Echeverria:2016:EBS:2961111.2962635,
	Title = {{Evaluating Bug-Fixing in Software Product Lines: An Industrial Case Study}},
	Author = {Echeverri{\'{a}}, J and P{\'{e}}rez, Francisca and Abellanas, Andr{\'{e}}s and Panach, Jose Ignacio and Cetina, Carlos and Pastor, O and Echeverr$\backslash$'$\backslash$ia, Jorge and P{\'{e}}rez, Francisca and Abellanas, Andr{\'{e}}s and Panach, Jose Ignacio and Cetina, Carlos and Pastor, {\'{O}}scar},
	Booktitle = {Proceedings of the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
	Year = {2016},
	Address = {New York, NY, USA},
	Pages = {24:1----24:6},
	Publisher = {ACM},
	Series = {ESEM '16},
	Volume = {08-09-Sept},
	Abstract = {[Background] Bug-fixing could be complex in industrial practice since thousands of products share features in their configuration. Despite the importance and complexity of bug-fixing, there is still a lack of empirical data about the difficulties found in industrial Software Product Lines (SPLs). [Aims] This paper aims to evaluate engineers' performance fixing errors and propagating the fixes to other configured products in the context of an industrial SPL. [Method] We designed and conducted an empirical study to collect data with regard to bug-fixing tasks within the context of a Induction Hob SPL in the BSH group, the largest manufacturer of home appliances in Europe. [Results] We found that effectiveness, efficiency and satisfaction got reached good values. Through interviews we also found difficulties related to unused features, cloning features unintentionally, detecting modified features, and propagating the fix when the source of the bug is the interaction between features. [Conclusions] The identified difficulties are relevant to know how to better apply SPLs in industry in the future. {\textcopyright} 2016 ACM.},
	Annote = {From Duplicate 2 (Evaluating Bug-Fixing in Software Product Lines: An Industrial Case Study - Echeverri{\'{a}}, J; P{\'{e}}rez, F; Abellanas, A; Panach, J I; Cetina, C; Pastor, O)
		cited By 0},
	Doi = {10.1145/2961111.2962635},
	ISBN = {978-1-4503-4427-2},
	Keywords = {,Computer software,Domestic appliances,Empirical studies,Induction hobs,Industrial cas,Software Product Line,Software design,Software e,Usability,Variability Modeling},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2961111.2962635 https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991579756{\&}doi=10.1145{\%}2F2961111.2962635{\&}partnerID=40{\&}md5=aa98b9304b197c6ff3666e22413e0087}
}

@InProceedings{Egorova:2015:UDP:2855667.2855679,
	Title = {{Usage of Design Patterns As a Kind of Components of Software Architecture}},
	Author = {Egorova, Inga and Itsykson, Vladimir},
	Booktitle = {Proceedings of the 11th Central {\&} Eastern European Software Engineering Conference in Russia},
	Year = {2015},
	Address = {New York, NY, USA},
	Pages = {11:1----11:9},
	Publisher = {ACM},
	Series = {CEE-SECR '15},
	Doi = {10.1145/2855667.2855679},
	ISBN = {978-1-4503-4130-1},
	Keywords = {design patterns,program design technologies,reusable architecture,role models,systematic usage of design patterns},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2855667.2855679}
}

@Article{Eichelberger2014163,
	Title = {{Flexible resource monitoring of Java programs}},
	Author = {Eichelberger, Holger and Schmid, Klaus},
	Journal = {Journal of Systems and Software},
	Year = {2014},
	Pages = {163--186},
	Volume = {93},
	Abstract = {Abstract Monitoring resource consumptions is fundamental in software engineering, e.g., in validation of quality requirements, performance engineering, or adaptive software systems. However, resource monitoring does not come for free as it typically leads to overhead in the observed program. Minimizing this overhead and increasing the reliability of the monitored data is a major goal in realizing resource monitoring tools. Typically, this is achieved by limiting capabilities, e.g., supported resources, granularity of the monitoring focus, or runtime access to results. Thus, in practice often several approaches must be combined to obtain relevant information. We describe SPASS-meter, a novel resource monitoring approach for Java and Android Apps, which combines these conflicting capabilities with low overhead. SPASS-meter supports a large set of resources, flexible configuration of the monitoring scope even for user-defined semantic units (components), runtime analysis and online access to monitoring results in a platform-independent way. We discuss the concepts of SPASS-meter, its architecture, realization and validation, the latter in terms of case studies and an overhead analysis based on performance experiments with SPASS-meter, OpenCore and Kieker. SPASS-meter provides a detailed view of the runtime resource consumption at reasonable overhead of less than 3{\%} processing power and 0.5{\%} memory consumption in our experiments. },
	Doi = {https://doi.org/10.1016/j.jss.2014.02.022},
	ISSN = {0164-1212},
	Keywords = {Empirical analysis,Java,Monitoring overhead,Performance engineering,Resource monitoring,Software components},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121214000533}
}

@Article{Eichelberger20091686,
	Title = {{Guidelines on the aesthetic quality of {\{}UML{\}} class diagrams}},
	Author = {Eichelberger, Holger and Schmid, Klaus},
	Journal = {Information and Software Technology},
	Year = {2009},
	Number = {12},
	Pages = {1686--1698},
	Volume = {51},
	Abstract = {In the past, formatting guidelines have proved to be a successful method to improve the readability of source code. With the increasing success of visual specification languages such as {\{}UML{\}} for model-driven software engineering visual guidelines are needed to standardize the presentation and the exchange of modeling diagrams with respect to human communication, understandability and readability. In this article, we introduce a new and encompassing taxonomy of visual guidelines capturing the aesthetic quality of {\{}UML{\}} class diagrams. We propose these guidelines as a framework to improve the aesthetic quality and thus the understandability of {\{}UML{\}} class diagrams. To validate this claim, we describe in detail a controlled experiment carried out as a pilot study to gather preliminary insights on the effects of some of the guideline rules on the understandability of {\{}UML{\}} class diagrams. },
	Annote = {Quality of {\{}UML{\}} Models},
	Doi = {https://doi.org/10.1016/j.infsof.2009.04.008},
	ISSN = {0950-5849},
	Keywords = {Aesthetic quality,Automatic layout,Layout guidelines,Modeling tools,Software engineering,UML class diagrams},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584909000421}
}

@Article{Eklund2014128,
	Title = {{Architecture for embedded open software ecosystems}},
	Author = {Eklund, Ulrik and Bosch, Jan},
	Journal = {Journal of Systems and Software},
	Year = {2014},
	Pages = {128--142},
	Volume = {92},
	Abstract = {Abstract Software is prevalent in embedded products and may be critical for the success of the products, but manufacturers may view software as a necessary evil rather than as a key strategic opportunity and business differentiator. One of the reasons for this can be extensive supplier and subcontractor relationships and the cost, effort or unpredictability of the deliverables from the subcontractors are experienced as a major problem. The paper proposes open software ecosystem as an alternative approach to develop software for embedded systems, and elaborates on the necessary quality attributes of an embedded platform underlying such an ecosystem. The paper then defines a reference architecture consisting of 17 key decisions together with four architectural patterns, and provides the rationale why they are essential for an open software ecosystem platform for embedded systems in general and automotive systems in particular. The reference architecture is validated through a prototypical platform implementation in an industrial setting, providing a deeper understanding of how the architecture could be realised in the automotive domain. Four potential existing platforms, all targeted at the embedded domain (Android, OKL4, {\{}AUTOSAR{\}} and Robocop), are evaluated against the identified quality attributes to see how they could serve as a basis for an open software ecosystem platform with the conclusion that while none of them is a perfect fit they all have fundamental mechanisms necessary for an open software ecosystem approach. },
	Doi = {https://doi.org/10.1016/j.jss.2014.01.009},
	ISSN = {0164-1212},
	Keywords = {Embedded software,Software architecture,Software ecosystem},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121214000211}
}

@Article{Eklund20132347,
	Title = {{Architecting automotive product lines: Industrial practice}},
	Author = {Eklund, Ulrik and Gustavsson, H{\aa}kan},
	Journal = {Science of Computer Programming},
	Year = {2013},
	Number = {12},
	Pages = {2347--2359},
	Volume = {78},
	Abstract = {This paper presents an in-depth view of how architects work with maintaining product line architectures at two internationally well-known automotive companies. The case study shows several interesting results. The process of managing architectural changes as well as the information the architects maintain and update is surprisingly similar between the two companies, despite that one has a strong line organisation and the other a strong project organisation. The architecting process found does not differ from what can be seen in other business domains. What does differ is that the architects studied see themselves interacting much more with other stakeholders than architects in general. The actual architectures are based on similar technology, e.g. CAN, but the network topology, S/W deployment and interfaces are totally different. The results indicate how the company's different core values influence the architects when defining and maintaining the architectures over time. One company maintains four similar architectures in parallel, each at a different stage in their respective life-cycle, while the other has a single architecture for all products since 2002. The organisational belonging of the architects in the former company has been turbulent in contrast to the latter and there is some speculation if this is correlated. },
	Annote = {Special Section on International Software Product Line Conference 2010 and Fundamentals of Software Engineering (selected papers of {\{}FSEN{\}} 2011)},
	Doi = {https://doi.org/10.1016/j.scico.2012.06.008},
	ISSN = {0167-6423},
	Keywords = {Architecting,Automotive industry,Case study,Process},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642312001190}
}

@Article{SPE:SPE1145,
	Title = {{Understanding design patterns — what is the problem?}},
	Author = {{El Boussaidi}, Ghizlane and Mili, Hafedh},
	Journal = {Software: Practice and Experience},
	Year = {2012},
	Number = {12},
	Pages = {1495--1529},
	Volume = {42},
	Abstract = {Design patterns codify proven solutions to recurring design problems. Their proper use within a development context requires that: (i) we understand them; (ii) we ascertain their applicability or relevance to the design problem at hand; and (iii) we apply them faithfully to the problem at hand. We argue that an explicit representation of the design problem solved by a design pattern is key to supporting the three tasks in an integrated fashion. We propose a model-driven representation of design patterns consisting of triples {\textless} MP, MS, T {\textgreater} where MP is a model of the problem solved by the pattern, MS is a model of the solution proposed by the pattern, and T is a model transformation of an instance of the problem into an instance of the solution. Given an object-oriented design model, we look for model fragments that match MP (call them instances of MP), and when one is found, we apply the transformation T yielding an instance of MS. Easier said than done. Experimentation with an Eclipse Modeling Framework-based implementation of our approach applied to a number of open-source software application's raised fundamental questions about: (i) the nature of design patterns in general, and the ones that lend themselves to our approach, and (ii) our understanding and codification of seemingly simple design patterns. In this paper, we present the principles behind our approach, report on the results of applying the approach to the Gang of Four (GoF) design patterns, and discuss the representability of design problems solved by these patterns. Copyright {\textcopyright} 2011 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spe.1145},
	ISSN = {1097-024X},
	Keywords = {design patterns,design problems,model-driven development,pattern detection},
	Publisher = {John Wiley {\&} Sons, Ltd},
	Url = {http://dx.doi.org/10.1002/spe.1145}
}

@Article{EXSY:EXSY12116,
	Title = {{A rule-based approach to detect and prevent inconsistency in the domain-engineering process}},
	Author = {Elfaki, Abdelrahman Osman},
	Journal = {Expert Systems},
	Year = {2016},
	Number = {1},
	Pages = {3--13},
	Volume = {33},
	Abstract = {A medium-sized domain-engineering process can contain thousands of features that all have constraint dependency rules between them. Therefore, the validation of the content of domain-engineering process is vital to produce high-quality software products. However, it is not feasible to do this manually. This paper aims to improve the quality of the software products generated by the domain-engineering process by ensuring the validity of the results of that process. We propose rules for two operations: inconsistency detection and inconsistency prevention. We introduce first-order logic (FOL) rules to detect three types of inconsistency and prevent the direct inconsistency in the domain-engineering process. Developing FOL rules to detect and prevent inconsistency in the domain-engineering process directly without the need to the configuration process is our main contribution. We performed some experiments to test the scalability and applicability of our approach on domain-engineered software product lines containing 1000 assets to 20000 assets. The results show that our approach is scalable and could be utilized to improve the domain-engineering process.},
	Doi = {10.1111/exsy.12116},
	ISSN = {1468-0394},
	Keywords = {automated validation,domain engineering,software product line},
	Url = {http://dx.doi.org/10.1111/exsy.12116}
}

@InProceedings{El-Sharkawy:2011:SHC:2019136.2019164,
	Title = {{Supporting Heterogeneous Compositional Multi Software Product Lines}},
	Author = {El-Sharkawy, Sascha and Kr{\"{o}}her, Christian and Schmid, Klaus},
	Booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
	Year = {2011},
	Address = {New York, NY, USA},
	Pages = {25:1----25:4},
	Publisher = {ACM},
	Series = {SPLC '11},
	Doi = {10.1145/2019136.2019164},
	ISBN = {978-1-4503-0789-5},
	Keywords = {decision modeling,distributed work,heterogeneous composition,multi software product lines,variability modeling},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2019136.2019164}
}

@Article{SYS:SYS10040,
	Title = {{A methodology for modeling VVT risks and costs}},
	Author = {Engel, Avner and Barad, Miryam},
	Journal = {Systems Engineering},
	Year = {2003},
	Number = {3},
	Pages = {135--151},
	Volume = {6},
	Abstract = {The cost of large systems' Verification, Validation, and Testing (VVT) is in the neighborhood of 40{\%} of the total life cycle cost. The cost associated with systems' failures is even more dramatic, often exceeding 10{\%} of industrial organizations turnover. There is a great potential benefit in streamlining and optimizing the VVT process. The first step in accomplishing this aim is to define a VVT strategy and then to quantify the cost and risk associated with carrying it out. This paper provides an overview of the methodologies for risk and cost monitoring for VVT and proposes a novel approach for modeling VVT strategies as decision problems. A quantitative VVT process and risk model is proposed. Due to the nondeterministic nature of risk, simulation is used to generate distributions of possible costs, schedules, and risk outcomes. These distributions represent a probabilistic approach and are analyzed in relation to impact events. The model provides means to explore different VVT strategies for optimizing relevant decision parameters. To demonstrate the proposed procedure the paper describes a case study depicting a planned avionics suite upgrade program for a fighter aircraft. Some simplified partial quantitative results are also presented. {\textcopyright} 2003 Wiley Periodicals, Inc. Syst Eng 6: 135–151, 2003},
	Doi = {10.1002/sys.10040},
	ISSN = {1520-6858},
	Keywords = {CVM,VVT,cost,life cycle,risk,systems,testing,validation,verification},
	Publisher = {Wiley Subscription Services, Inc., A Wiley Company},
	Url = {http://dx.doi.org/10.1002/sys.10040}
}

@Article{DECI:DECI12254,
	Title = {{Designing Products for Adaptability: Insights from Four Industrial Cases}},
	Author = {Engel, Avner and Browning, Tyson R and Reich, Yoram},
	Journal = {Decision Sciences},
	Year = {2016},
	Pages = {n/a----n/a},
	Abstract = {Developing products that are more easily adaptable to future requirements can increase their overall value. Product adaptability is largely determined by choices about product architecture, especially modularity. Because it is possible to be too modular and/or inappropriately modular, deciding how and where to be modular in a cost-effective way is an important managerial decision. In this article, we gather data from four case studies to model effects of firms' product architecture decisions at the component level. We optimize an architecture adaptability value (AAV) measure that accounts for both the benefits of more architecture options and the costs of interfaces. The optimal architecture prompted each firm to rearchitect an existing product to increase its expected future profitability. Several insights emerged from the case evidence during this research. (i) Although decomposing an architecture into an increasing number of modules increases product adaptability, the amount of modularity is an insufficient predictor of the adaptability value of a system. AAV, which also accounts for interface costs, provides an improved measure of appropriate modularity. (ii) Managers can influence the path of architectural evolution in the direction of increased value. This influence may diminish but does not disappear as products become more mature. Also, modularity and innovations coevolved, as the new modularizations suggested by AAV optimization prompted and guided searches for further innovations. (iii) When presented with the concepts of options, interface costs, and AAV, the firms' designers and managers were initially skeptical. However, in each case, the modelers were able to rearchitect an actual product not only with increased AAV by our model (theoretical improvement) but also with actual future benefits for their firm. Postproject reports from each firm confirmed that the AAV modeling and optimization approaches were indeed helpful, equipping them to increase the adaptability, cost-efficiency, lifespan, and overall value of actual products. The evidence suggests that firms can benefit from designing products for adaptability, but that how they do so matters. This study expands our understanding of modularity and adaptability by illuminating managerial decisions and insights about appropriate approaches to each.},
	Doi = {10.1111/deci.12254},
	ISSN = {1540-5915},
	Keywords = {Coordination Costs,Design for Adaptability,Modularity,Options,Product Architecture,Product Design,Product Development},
	Url = {http://dx.doi.org/10.1111/deci.12254}
}

@Article{SYS:SYS21312,
	Title = {{Advancing Architecture Options Theory: Six Industrial Case Studies}},
	Author = {Engel, Avner and Reich, Yoram},
	Journal = {Systems Engineering},
	Year = {2015},
	Number = {4},
	Pages = {396--414},
	Volume = {18},
	Abstract = {Systems provide value through their ability to fulfill stakeholders' needs. These needs evolve and often diverge from an original system's capabilities. Thus, a system's value to its stakeholders diminishes over time. Consequently, systems have to be periodically upgraded or replaced. Since replacement costs are often prohibitive, system adaptability is valuable. Adaptability entails the ability to modify an existing system or design of a system's architecture, such as changing, adding, removing, or replacing relevant elements as well as adjusting their reciprocal interactions.In 2008, Engel and Browning proposed a Design for Adaptability concept based on Architecture Option (AO) theory. AO fuses Financial Options and Transaction Cost theories, seeking to design systems for optimal lifetime value. They asserted that designers should balance the benefits of adaptability against its affordability. More modularity is not always better; the amount of modularity alone is an insufficient and even misleading cause of value.This follow-up paper reports on a project, aimed at enhancing the AO theory and validating its applicability within diverse industrial environments. Through interactions with practicing system developers, the AO model was simplified and a modified Black–Scholes model was adapted into the engineering domain and successfully practiced. Six case studies were conducted within: food packaging, machine tools, automotive, aerospace, communication, and optoelectronics industries. All six industrial participants estimated, among other improvements, over 15{\%} benefits in (1) reducing systems' lifetime cost, (2) reducing systems' upgrade cycle-time, and (3) increased systems' lifespan. These results demonstrate the industrial applicability and validity of the AO theory.},
	Doi = {10.1002/sys.21312},
	ISSN = {1520-6858},
	Keywords = {TRIZ,architecture adaptability value,architecture options,design for adaptability,design structure matrix,financial options,modified Black-Scholes equation,real options,transaction costs},
	Url = {http://dx.doi.org/10.1002/sys.21312}
}

@Article{Engstrom2013581,
	Title = {{Test overlay in an emerging software product line-An industrial case study}},
	Author = {Engstr{\"{o}}m, E and Runeson, P},
	Journal = {Information and Software Technology},
	Year = {2013},
	Number = {3},
	Pages = {581--594},
	Volume = {55},
	Abstract = {Context: In large software organizations with a product line development approach, system test planning and scope selection is a complex task. Due to repeated testing: across different testing levels, over time (test for regression) as well as of different variants, the risk of redundant testing is large as well as the risk of overlooking important tests, hidden by the huge amount of possible tests. Aims: This study assesses the amount and type of overlaid manual testing across feature, integration and system test in such context, it explores the causes of potential redundancy and elaborates on how to provide decision support in terms of visualization for the purpose of avoiding redundancy. Method: An in-depth case study was launched including both qualitative and quantitative observations. Results: A high degree of test overlay is identified originating from distributed test responsibilities, poor documentation and structure of test cases, parallel work and insufficient delta analysis. The amount of test overlay depends on which level of abstraction is studied. Conclusions: Avoiding redundancy requires tool support, e.g. visualization of test design coverage, test execution progress, priorities of coverage items as well as visualized priorities of variants to support test case selection. {\textcopyright} 2012 Elsevier B.V. All rights reserved.},
	Annote = {cited By 4},
	Doi = {10.1016/j.infsof.2012.04.009},
	Keywords = {Complex task; Decision supports; Industrial case s,Decision support systems; Efficiency; Industrial,Software testing},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872969667{\&}doi=10.1016{\%}2Fj.infsof.2012.04.009{\&}partnerID=40{\&}md5=f0761683155fbeab04ba6c98af239ed2}
}

@Article{Engstrom20112,
	Title = {{Software product line testing – A systematic mapping study}},
	Author = {Engstr{\"{o}}m, Emelie and Runeson, Per},
	Journal = {Information and Software Technology},
	Year = {2011},
	Number = {1},
	Pages = {2--13},
	Volume = {53},
	Abstract = {Context Software product lines (SPL) are used in industry to achieve more efficient software development. However, the testing side of {\{}SPL{\}} is underdeveloped. Objective This study aims at surveying existing research on {\{}SPL{\}} testing in order to identify useful approaches and needs for future research. Method A systematic mapping study is launched to find as much literature as possible, and the 64 papers found are classified with respect to focus, research type and contribution type. Results A majority of the papers are of proposal research types (64{\%}). System testing is the largest group with respect to research focus (40{\%}), followed by management (23{\%}). Method contributions are in majority. Conclusions More validation and evaluation research is needed to provide a better foundation for {\{}SPL{\}} testing. },
	Doi = {https://doi.org/10.1016/j.infsof.2010.05.011},
	File = {:Users/mac/Downloads/Sciencedirect{\_}articles{\_}13Jun2017{\_}14-36-55.006/Software-product-line-testing-A-systematic-mapping-study{\_}2011{\_}Information-and-Software-Technology.pdf:pdf},
	ISSN = {0950-5849},
	Keywords = {Software product line testing,Systematic literature review,Systematic mapping,Testing},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584910001709}
}

@Conference{Enriquez2017,
	Title = {{Development and evaluation of a framework for generation usability testing for mobile application}},
	Author = {Enriquez, J and Casas, S},
	Booktitle = {Proceedings of the 2016 42nd Latin American Computing Conference, CLEI 2016},
	Year = {2017},
	Abstract = {The widespread use of mobile devices and the heterogeneity of the users involve changes for usability evaluating of mobile applications. It is important to have methodologies and tools to perform usability specific studies for this type of applications in which the context of use is constantly changing. It is necessary that the usability tests are transparent for the user, usability and context data are automatically collected, the test code are not intrusive in the tested application and the tests support the variability of the devices. To achieve the above features, this article presents a framework called FUsAM (Framework of Usability for Mobile Applications) which is extensible and can generate and integrate usability testing in mobile applications. The design and implementation approach is based on Software Product Line (SPL) combined with Features Oriented Programming (FOP) and Aspect Oriented Programming (AOP). A case study is also presented to prove the functionality. {\textcopyright} 2016 IEEE.},
	Annote = {cited By 0},
	Doi = {10.1109/CLEI.2016.7833323},
	Keywords = {Application programs; Computer software; Mobile co,Aspect oriented programming,Aspect-Oriented Programming (AOP); Design and imp},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013893986{\&}doi=10.1109{\%}2FCLEI.2016.7833323{\&}partnerID=40{\&}md5=de47406d9dd05d370d6b8c53c7024f0b}
}

@Article{Erdweg201524,
	Title = {{Evaluating and comparing language workbenches: Existing results and benchmarks for the future}},
	Author = {Erdweg, Sebastian and van der Storm, Tijs and V{\"{o}}lter, Markus and Tratt, Laurence and Bosman, Remi and Cook, William R and Gerritsen, Albert and Hulshout, Angelo and Kelly, Steven and Loh, Alex and Konat, Gabri{\"{e}}l and Molina, Pedro J and Palatnik, Martin and Pohjonen, Risto and Schindler, Eugen and Schindler, Klemens and Solmi, Riccardo and Vergu, Vlad and Visser, Eelco and van der Vlist, Kevin and Wachsmuth, Guido and van der Woning, Jimi},
	Journal = {Computer Languages, Systems {\&} Structures},
	Year = {2015},
	Pages = {24--47},
	Volume = {44, Part A},
	Abstract = {Abstract Language workbenches are environments for simplifying the creation and use of computer languages. The annual Language Workbench Challenge (LWC) was launched in 2011 to allow the many academic and industrial researchers in this area an opportunity to quantitatively and qualitatively compare their approaches. We first describe all four {\{}LWCs{\}} to date, before focussing on the approaches used, and results generated, during the third LWC. We give various empirical data for ten approaches from the third LWC. We present a generic feature model within which the approaches can be understood and contrasted. Finally, based on our experiences of the existing LWCs, we propose a number of benchmark problems for future LWCs. },
	Annote = {Special issue on the 6th and 7th International Conference on Software Language Engineering (SLE 2013 and {\{}SLE{\}} 2014)},
	Doi = {https://doi.org/10.1016/j.cl.2015.08.007},
	ISSN = {1477-8424},
	Keywords = {Benchmarks,Domain-specific languages,Language workbenches,Questionnaire language,Survey},
	Url = {http://www.sciencedirect.com/science/article/pii/S1477842415000573}
}

@Article{Eriksson2009,
	Title = {{Managing requirements specifications for product lines - An approach and industry case study}},
	Author = {Eriksson, Magnus and B??rstler, J??rgen and Borg, Kjell},
	Journal = {Journal of Systems and Software},
	Year = {2009},
	Number = {3},
	Pages = {435--447},
	Volume = {82},
	Abstract = {Software product line development has emerged as a leading approach for software reuse. This paper describes an approach to manage natural-language requirements specifications in a software product line context. Variability in such product line specifications is modeled and managed using a feature model. The proposed approach has been introduced in the Swedish defense industry. We present a multiple-case study covering two different product lines with in total eight product instances. These were compared to experiences from previous projects in the organization employing clone-and-own reuse. We conclude that the proposed product line approach performs better than clone-and-own reuse of requirements specifications in this particular industrial context. ?? 2008 Elsevier Inc. All rights reserved.},
	Doi = {10.1016/j.jss.2008.07.046},
	File = {:Users/mac/Downloads/Sciencedirect{\_}articles{\_}13Jun2017{\_}15-33-20.965/Managing-requirements-specifications-for-product-lines-An-approach-and-industry-case-study{\_}2009{\_}Journal-of-Systems-and-Software.pdf:pdf},
	ISBN = {0164-1212},
	ISSN = {01641212},
	Keywords = {Feature model,Natural-language requirements specification,Software product line,Variability management},
	Publisher = {Elsevier Inc.},
	Url = {http://dx.doi.org/10.1016/j.jss.2008.07.046}
}

@Article{SYS:SYS20087,
	Title = {{Use Cases for Systems Engineering—An Approach and Empirical Evaluation}},
	Author = {Eriksson, Magnus and Borg, Kjell and B{\"{o}}rstler, J{\"{u}}rgen},
	Journal = {Systems Engineering},
	Year = {2008},
	Number = {1},
	Pages = {39--60},
	Volume = {11},
	Abstract = {This paper describes a use case driven approach for functional analysis/allocation and requirements flowdown. The approach utilizes use cases and use case realizations for functional architecture modeling, which in turn form the basis for design synthesis and requirements flowdown. We refer to this approach as the FAR (Functional Architecture by use case Realizations) approach. The FAR approach is currently applied in several large-scale defense projects within BAE Systems H{\"{a}}gglunds AB. This paper also presents an empirical study where FAR is applied and evaluated in two large-scale defense projects. Our results indicate that the FAR approach performs better than the previously used document based approach in the organization. {\textcopyright} 2007, Wiley Periodicals, Inc. Syst Eng},
	Doi = {10.1002/sys.20087},
	ISSN = {1520-6858},
	Keywords = {functional architecture,pilot projects,requirements flowdown,use case,use case realization},
	Publisher = {Wiley Subscription Services, Inc., A Wiley Company},
	Url = {http://dx.doi.org/10.1002/sys.20087}
}

@Article{Esfahani2012786,
	Title = {{Utilizing architectural styles to enhance the adaptation support of middleware platforms}},
	Author = {Esfahani, Naeem and Malek, Sam},
	Journal = {Information and Software Technology},
	Year = {2012},
	Number = {7},
	Pages = {786--801},
	Volume = {54},
	Abstract = {Context Modern middleware platforms provide the applications deployed on top of them with facilities for their adaptation. However, the level of adaptation support provided by the state-of-the-art middleware solutions is often limited to dynamically loading and off-loading of software components. Therefore, it is left to the application developers to handle the details of change such that the system's consistency is not jeopardized. Objective We aim to change the status quo by providing the middleware facilities necessary to ensure the consistency of software after adaptation. We would like these facilities to be reusable across different applications, such that the middleware can streamline the process of achieving safe adaptation. Method Our approach addresses the current shortcomings by utilizing the information encoded in a software system's architectural style. This information drives the development of reusable adaptation patterns. The patterns specify both the exact sequence of changes and the time at which those changes need to occur. We use the patterns to provide advanced adaptation support on top of an existing architectural middleware platform. Results Our experience shows the feasibility of deriving detailed adaptation patterns for several architectural styles. Applying the middleware to adapt two real-world software systems shows the approach is effective in consistently adapting these systems without jeopardizing their consistency. Conclusion We conclude the approach is effective in alleviating the application developers from the responsibility of managing the adaptation process at the application-level. Moreover, we believe this study provides the foundation for changing the way adaptation support is realized in middleware solutions. },
	Doi = {https://doi.org/10.1016/j.infsof.2012.02.001},
	ISSN = {0950-5849},
	Keywords = {Adaptation patterns,Architectural style,Middleware,Software architecture},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584912000328}
}

@Article{Fabry2016528,
	Title = {{AspectJ code analysis and verification with {\{}GASR{\}}}},
	Author = {Fabry, Johan and Roover, Coen De and Noguera, Carlos and Zschaler, Steffen and Rashid, Awais and Jonckers, Viviane},
	Journal = {Journal of Systems and Software},
	Year = {2016},
	Pages = {528--544},
	Volume = {117},
	Abstract = {Abstract Aspect-oriented programming languages extend existing languages with new features for supporting modularization of crosscutting concerns. These features however make existing source code analysis tools unable to reason over this code. Consequently, all code analysis efforts of aspect-oriented code that we are aware of have either built limited analysis tools or were performed manually. Given the significant complexity of building them or manual analysis, a lot of duplication of effort could have been avoided by using a general-purpose tool. To address this, in this paper we present Gasr: a source code analysis tool that reasons over AspectJ source code, which may contain metadata in the form of annotations. {\{}GASR{\}} provides multiple kinds of analyses that are general enough such that they are reusable, tailorable and can reason over annotations. We demonstrate the use of {\{}GASR{\}} in two ways: we first automate the recognition of previously identified aspectual source code assumptions. Second, we turn implicit assumptions into explicit assumptions through annotations and automate their verification. In both uses {\{}GASR{\}} performs detection and verification of aspect assumptions on two well-known case studies that were manually investigated in earlier work. {\{}GASR{\}} finds already known aspect assumptions and adds instances that had been previously overlooked. },
	Doi = {https://doi.org/10.1016/j.jss.2016.04.014},
	ISSN = {0164-1212},
	Keywords = {Aspect oriented programming,Logic program querying,Source code analysis},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121216300279}
}

@Conference{Falvo2015,
	Title = {{A contribution to the adoption of software product lines in the development of mobile learning applications}},
	Author = {{Falvo V.}, Jr. and Filho, N F D and {Oliveira E.}, Jr. and Barbosa, E F},
	Booktitle = {Proceedings - Frontiers in Education Conference, FIE},
	Year = {2015},
	Number = {February},
	Volume = {2015-Febru},
	Abstract = {The increasing presence of mobile devices in the society has motivated the use of these gadgets in several segments. This reality has contributed to the emergence of a new and innovative modality of learning - the mobile learning, or simply, m-learning. In short, mobile learning is based on the use of a set of specific mobile applications, which possesses similar features and specific characteristics according to the learning goals. In a different but related perspective, the emerging reuse technique of Software Product Line (SPL) allows mass customization and systematic derivation of products, such as mobile learning applications. Motivated by this scenario, we have worked on the establishment of M-SPLearning, a SPL to the mobile learning applications domain. M-SPLearning has been developed throughout a proactive adoption model, according to the basics of Service-Oriented Architecture (SOA). In this paper, we discuss the main aspects of the development of M-SPLearning, focusing on the implementation phase. Also, as a case study, we illustrate two products generated by the SPL proposed. The main results obtained suggest the practical feasibility of adopting M-SPLearning in the development of mobile learning applications. {\textcopyright} 2014 IEEE.},
	Annote = {cited By 0},
	Doi = {10.1109/FIE.2014.7044091},
	Keywords = {Adoption model; Applications domains; Learning go,Application programs,Computer software; Computer software reusability;},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938099350{\&}doi=10.1109{\%}2FFIE.2014.7044091{\&}partnerID=40{\&}md5=43961a3e129a5f60f7feb79783501d1e}
}

@Article{Famelis201582,
	Title = {{Migrating automotive product lines: A case study}},
	Author = {Famelis, M and L{\'{u}}cio, L and Selim, G and {Di Sandro}, A and Salay, R and Chechik, M and Cordy, J R and Dingel, J and Vangheluwe, H and Ramesh, S},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2015},
	Pages = {82--97},
	Volume = {9152},
	Abstract = {Software Product Lines (SPL) are widely used to manage variability in the automotive industry. In a rapidly changing industrial environment, model transformations are necessary to aid in automating the evolution of SPLs. However, existing transformation technologies are not well-suited to handling industrial-grade variability in software artifacts. We present a case study where we “lift? a previously developed migration transformation so that it becomes applicable to realistic industrial product lines. Our experience indicates that it is both feasible and scalable to lift transformations for industrial SPLs. {\textcopyright} Springer International Publishing Switzerland 2015.},
	Annote = {cited By 3},
	Doi = {10.1007/978-3-319-21155-8_7},
	Keywords = {Artificial intelligence; Computers,Automotive industry,Automotive products; Industrial environments; Ind},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952671657{\&}doi=10.1007{\%}2F978-3-319-21155-8{\_}7{\&}partnerID=40{\&}md5=b612409a8ad427664909fe1e65b1cd3d}
}

@InProceedings{Fang:2016:MMR:2976767.2976804,
	Title = {{Multi-variability Modeling and Realization for Software Derivation in Industrial Automation Management}},
	Author = {Fang, Miao and Leyh, Georg and Doerr, Joerg and Elsner, Christoph},
	Booktitle = {Proceedings of the ACM/IEEE 19th International Conference on Model Driven Engineering Languages and Systems},
	Year = {2016},
	Address = {New York, NY, USA},
	Pages = {2--12},
	Publisher = {ACM},
	Series = {MODELS '16},
	Doi = {10.1145/2976767.2976804},
	ISBN = {978-1-4503-4321-3},
	Keywords = {code generation,domain-specific modeling,model-based engineering,software derivation,software product line,variability modeling},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2976767.2976804}
}

@Article{Fantinato2006290,
	Title = {{Web service E-contract establishment using features}},
	Author = {Fantinato, M and {De S. Gimenes}, I M and {De Toledo}, M B F},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2006},
	Pages = {290--305},
	Volume = {4102 LNCS},
	Abstract = {Electronic contracts describe inter-organizational business processes in terms of supply and consumption of electronic services (commonly Web services). In a given contract domain, it is usually possible to identify a set of well-defined common and variation points. Feature modeling is an ontology-like technique that has been widely used for capturing and managing commonalities and variabilities of product families in the context of software product line. This paper proposes a feature-based approach in order to decrease the complexity in Web service e-contract establishment. The feasibility of the approach is shown by a case study carried out within the telecom context and based on experimental software engineering concepts. {\textcopyright} Springer-Verlag Berlin Heidelberg 2006.},
	Annote = {cited By 2},
	Keywords = {Computer simulation; Computer software; Computer s,Electronic commerce,e-contract; Feature modeling; Ontology; Web servi},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750013759{\&}partnerID=40{\&}md5=6eb690c8a307d019593534f057352d8b}
}

@Article{Fantinato2008373,
	Title = {{WS-contract establishment with QOS:}},
	Author = {Fantinato, M and {De Toledo}, M B F and {De Souza Gimenes}, I M},
	Journal = {International Journal of Cooperative Information Systems},
	Year = {2008},
	Number = {3},
	Pages = {373--407},
	Volume = {17},
	Abstract = {Electronic contracts describe inter-organizational business processes in terms of supply and consumption of electronic services (commonly Web services). The establishment of e-contracts in a particular business domain usually involves a set of well-defined common and variable properties. These properties are not fully exploited by the existing e-contract establishment approaches. Feature modeling is a software engineering technique that has been widely used for capturing and managing commonalities and variabilities of product families in the context of software product line. This paper presents a feature-based approach to support Web services e-contract (WS-contract) establishment. The approach aims at improving the information structure and reuse of WS-contracts, including the QoS attributes. Features are used to represent possible WS-contract elements in order to drive WS-contract template instantiation, thus acting as a configuration space manager. A toolkit named FeatureContract was developed to automatically support the proposed approach. A case study was carried out within the telecom context to show the approach feasibility. {\textcopyright} 2008 World Scientific Publishing Company.},
	Annote = {cited By 19},
	Doi = {10.1142/S0218843008001889},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-50949123633{\&}doi=10.1142{\%}2FS0218843008001889{\&}partnerID=40{\&}md5=5b4ae3194ddd2d62e7c64661813e9a4f}
}

@Article{Farahani2016301,
	Title = {{Comprehensive configuration management model for software product line}},
	Author = {Farahani, E D and Habibi, J},
	Journal = {International Journal of Control Theory and Applications},
	Year = {2016},
	Number = {25},
	Pages = {301--322},
	Volume = {9},
	Abstract = {In Software Product Line (SPL), Configuration Management (CM) is a multi-dimensional problem. On the one hand, the Core Assets that constitute a configuration need to be managed, and on the other hand, each product in the product line that is built using a configuration must be managed, and furthermore, the management of all these configurations must be coordinated under a single process. Therefore, CM for product lines is more complex than for single systems. The CM of any software system involves four closely related activities: Change Management (ChM), Version Management (VM), Build Management (BM) and Release Management (RM). The aim of this paper is to provide a comprehensive CM model comprising four main sub-models for all CM-related activities required for evolutionary based SPL system development and maintenance. The proposed models support any level of aggregation in SPLs and have been applied to Mobile SPL as a case study. {\textcopyright} International Science Press.},
	Annote = {cited By 0},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007028727{\&}partnerID=40{\&}md5=3164e6050cad5ccf1db519d1ff64c1c8}
}

@Article{Farahani2016433,
	Title = {{Configuration Management Model in Evolutionary Software Product Line}},
	Author = {Farahani, E D and Habibi, J},
	Journal = {International Journal of Software Engineering and Knowledge Engineering},
	Year = {2016},
	Number = {3},
	Pages = {433--455},
	Volume = {26},
	Abstract = {In Software Product Line (SPL), Configuration Management (CM) is a multi-dimensional problem. On the one hand, the Core Assets that constitute a configuration need to be managed, and on the other hand, each product in the product line that is built using a configuration must be managed, and furthermore, the management of all these configurations must be coordinated under a single process. Therefore, CM for product lines is more complex than for single systems. The CM of any software system involves four closely related activities: Change Management (ChM), Version Management (VM), System Building (SB) and Release Management (RM) [I. Sommerville, Software Engineering, 9th edn. (Addison-Wesley, 2010)]. The aim of this paper is to provide ChM and VM models for evolutionary-based SPL system development and maintenance. The proposed models support any level of aggregation in SPLs and have been applied to Mobile SPL as a case study. {\textcopyright} 2016 World Scientific Publishing Company.},
	Annote = {cited By 0},
	Doi = {10.1142/S0218194016500182},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968548424{\&}doi=10.1142{\%}2FS0218194016500182{\&}partnerID=40{\&}md5=04b82211594b36b88b4f29690caba20f}
}

@Article{Farahani2014545,
	Title = {{Methodologies for agile product line engineering: A survey and evaluation}},
	Author = {Farahani, F F and Ramsin, R},
	Journal = {Frontiers in Artificial Intelligence and Applications},
	Year = {2014},
	Pages = {545--564},
	Volume = {265},
	Abstract = {Agile Product Line Engineering (APLE) is a relatively new approach which has emerged as the result of combining two successful approaches: Software Product Line Engineering and Agile Software Development. The goal of this combined approach is to cover the weaknesses of each of the two approaches while maximizing the advantages of both. Several methodologies exist which provide a practical process for applying APLE. In this paper, a select set of these methodologies are evaluated using a criteria-based approach, the results of which highlight each methodology's strengths and weaknesses. The evaluation framework and the results can be helpful in selecting, comparing, and modifying APLE methodologies; they can also be used for developing bespoke APLE methodologies, tailored to fit the specific needs of organizations and projects. {\textcopyright} 2014 The authors and IOS Press. All rights reserved.},
	Annote = {cited By 0},
	Doi = {10.3233/978-1-61499-434-3-545},
	Keywords = {Agile software development; Criteria-based evalua,Software design,Software engineering},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948778166{\&}doi=10.3233{\%}2F978-1-61499-434-3-545{\&}partnerID=40{\&}md5=92a9ac3a5646d4296c1c8c4d4631fbe1}
}

@InProceedings{Farias:2010:AIA:1739230.1739240,
	Title = {{Assessing the Impact of Aspects on Model Composition Effort}},
	Author = {Farias, Kleinner and Garcia, Alessandro and Whittle, Jon},
	Booktitle = {Proceedings of the 9th International Conference on Aspect-Oriented Software Development},
	Year = {2010},
	Address = {New York, NY, USA},
	Pages = {73--84},
	Publisher = {ACM},
	Series = {AOSD '10},
	Doi = {10.1145/1739230.1739240},
	ISBN = {978-1-60558-958-9},
	Keywords = {empirical studies,model composition,software architecture,software metrics,software product lines},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/1739230.1739240}
}

@Article{Fasquel2011847,
	Title = {{A design pattern coupling role and component concepts: Application to medical software}},
	Author = {Fasquel, Jean-Baptiste and Moreau, Johan},
	Journal = {Journal of Systems and Software},
	Year = {2011},
	Number = {5},
	Pages = {847--863},
	Volume = {84},
	Abstract = {One of the challenges in software development regards the appropriate coupling of separated code elements in order to correctly build initially expected high-level software functionalities. In this context, we address issues related to the dynamic composition of such code elements (i.e. how they are dynamically plugged together) as well as their collaboration (i.e. how they work together). We also consider the limitation of build-level dependencies, to avoid the entire re-compilation and re-deployment of a software when modifying it or integrating new functionalities. To solve these issues, we propose a new design pattern coupling role and component concepts and illustrate its relevance for medical software. Compared to most related works focusing on few role concepts while ignoring others, the proposed pattern integrates many role concepts as first-class entities, including in particular a refinement of the notion of collaboration. Another significant contribution of our proposal concerns the coupling of role and component concepts. Roles are related to the functional aspects of a target software program (composition and collaboration of functional units). Components correspond to the physical distribution of code elements with limited build-level dependencies. As illustrated in this paper, such a coupling enables to instantiate a software program using a generic main program together with a description file focusing on software functionalities only. Related code elements are transparently retrieved and composed at run-time before appropriately collaborating, regardless the specificity of their distribution over components. },
	Doi = {https://doi.org/10.1016/j.jss.2011.01.026},
	ISSN = {0164-1212},
	Keywords = {Collaboration,Component,Design pattern,Dynamic composition,Medical software,Role},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121211000185}
}

@Article{SPE:SPE530,
	Title = {{Software product line migration and deployment}},
	Author = {Faust, D and Verhoef, C},
	Journal = {Software: Practice and Experience},
	Year = {2003},
	Number = {10},
	Pages = {933--955},
	Volume = {33},
	Abstract = {We describe a method to migrate multiple instances of a successful single information system to a product line. The deployed product line is able to deal with the variants evolved over time in a cost-effective manner. We proposed and used federated architectures that partition the software into so-called genericity layers. We argue that federated architectures are at the heart of product lines, and we provide compelling arguments as to why federated architectures are a sound weapon in today's corporate strategy: they enable smooth enterprise integration and rapid change. We support our arguments with a real-world case: we successfully migrated a large global transaction and settlement system with many site-specific variations to a product line with a federated architecture. Moreover, we measured the success rate of this architectural modification effort by showing that the annual direct cost savings are of the order of millions of dollars during the deployment of the product line. Copyright {\textcopyright} 2003 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spe.530},
	ISSN = {1097-024X},
	Keywords = {configuration oscillation,federated architecture,genericity layer,grow-and-prune model,proactive/reactive product line,relativistic effects of software engineering,release indigestion,software mitosis},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/spe.530}
}

@Article{Feigenspan2013699,
	Title = {{Do background colors improve program comprehension in the {\#}ifdef hell?}},
	Author = {Feigenspan, J and K{\"{a}}stner, C and Apel, S and Liebig, J and Schulze, M and Dachselt, R and Papendieck, M and Leich, T and Saake, G},
	Journal = {Empirical Software Engineering},
	Year = {2013},
	Number = {4},
	Pages = {699--745},
	Volume = {18},
	Abstract = {Software-product-line engineering aims at the development of variable and reusable software systems. In practice, software product lines are often implemented with preprocessors. Preprocessor directives are easy to use, and many mature tools are available for practitioners. However, preprocessor directives have been heavily criticized in academia and even referred to as "{\#}ifdef hell", because they introduce threats to program comprehension and correctness. There are many voices that suggest to use other implementation techniques instead, but these voices ignore the fact that a transition from preprocessors to other languages and tools is tedious, erroneous, and expensive in practice. Instead, we and others propose to increase the readability of preprocessor directives by using background colors to highlight source code annotated with ifdef directives. In three controlled experiments with over 70 subjects in total, we evaluate whether and how background colors improve program comprehension in preprocessor-based implementations. Our results demonstrate that background colors have the potential to improve program comprehension, independently of size and programming language of the underlying product. Additionally, we found that subjects generally favor background colors. We integrate these and other findings in a tool called FeatureCommander, which facilitates program comprehension in practice and which can serve as a basis for further research. {\textcopyright} 2012 Springer Science+Business Media, LLC.},
	Annote = {cited By 23},
	Doi = {10.1007/s10664-012-9208-x},
	Keywords = {Color; Computer software reusability; Program pro,Computer programming,Empirical Software Engineering; FeatureCommander;},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879846095{\&}doi=10.1007{\%}2Fs10664-012-9208-x{\&}partnerID=40{\&}md5=76165de9ec8580474e8390c79a7ad728}
}

@Article{Feitelson2012859,
	Title = {{Perpetual development: A model of the Linux kernel life cycle}},
	Author = {Feitelson, Dror G},
	Journal = {Journal of Systems and Software},
	Year = {2012},
	Number = {4},
	Pages = {859--875},
	Volume = {85},
	Abstract = {Software evolution is widely recognized as an important and common phenomenon, whereby the system follows an ever-extending development trajectory with intermittent releases. Nevertheless there have been only few lifecycle models that attempt to portray such evolution. We use the evolution of the Linux kernel as the basis for the formulation of such a model, integrating the progress in time with growth of the codebase, and differentiating between development of new functionality and maintenance of production versions. A unique element of the model is the sequence of activities involved in releasing new production versions, and how this has changed with the growth of Linux. In particular, the release follow-up phase before the forking of a new development version, which was prominent in early releases of production versions, has been eliminated in favor of a concurrent merge window in the release of 2.6.x versions. We also show that a piecewise linear model with increasing slopes provides the best description of the growth of Linux. The perpetual development model is used as a framework in which commonly recognized benefits of incremental and evolutionary development may be demonstrated, and to comment on issues such as architecture, conservation of familiarity, and failed projects. We suggest that this model and variants thereof may apply to many other projects in addition to Linux. },
	Doi = {https://doi.org/10.1016/j.jss.2011.10.050},
	ISSN = {0164-1212},
	Keywords = {Linux kernel,Maintenance,Software evolution,Software release},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121211002822}
}

@Article{Ferber2002364,
	Title = {{Reviewing product line architectures: Experience report of ATAM in an automotive context}},
	Author = {Ferber, S and Heidi, P and Lutz, P},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2002},
	Pages = {364--382},
	Volume = {2290},
	Abstract = {Product lines are an important system development paradigm in the automotive industry to amortize costs beyond a single product. The paradigm is well established in the mechanical and electrical engineering practice in automotive companies like Bosch. As software is covering more and more functionality in cars, software product lines are getting more attention. The architecture of a software-intensive system is a key asset in developing a software product line. The Architecture Trade-off Analysis Method (ATAM) developed by the SEI assesses the quality of software architecture early in the development process. ATAM is therefore a useful review technique to guarantee important quality attributes of every single product created with the product line architecture later on. This article reports about the experience Bosch made in using ATAM in two cases. Benefits in using ATAM are not only the review results itself but a better documented and better understood architecture. We experienced the most important benefit of ATAM is the rising stakeholders' awareness of architectural decisions, tradeoffs, and risks. It illuminates the software architecture better than any written documentation. Bosch employees are trained in the evaluation roles in order to transition ATAM to Bosch. The reports conclude with some suggestions for improving the ATAM itself and the training of ATAM roles. {\textcopyright} Springer-Verlag Berlin Heidelberg 2002.},
	Annote = {cited By 8},
	Keywords = {Architectural decision; Automotive companies; Dev,Automotive industry; Computer software; Economic a,Quality control},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944052218{\&}partnerID=40{\&}md5=34bea22dd9d1e0b027ae7749212fe967}
}

@Article{Fernandez2013161,
	Title = {{Empirical validation of a usability inspection method for model-driven Web development}},
	Author = {Fernandez, Adrian and Abrah{\~{a}}o, Silvia and Insfran, Emilio},
	Journal = {Journal of Systems and Software},
	Year = {2013},
	Number = {1},
	Pages = {161--186},
	Volume = {86},
	Abstract = {Web applications should be usable in order to be accepted by users and to improve their success probability. Despite the fact that this requirement has promoted the emergence of several usability evaluation methods, there is a need for empirically validated methods that provide evidence about their effectiveness and that can be properly integrated into early stages of Web development processes. Model-driven Web development processes have grown in popularity over the last few years, and offer a suitable context in which to perform early usability evaluations due to their intrinsic traceability mechanisms. These issues have motivated us to propose a Web Usability Evaluation Process (WUEP) which can be integrated into model-driven Web development processes. This paper presents a family of experiments that we have carried out to empirically validate WUEP. The family of experiments was carried out by 64 participants, including PhD and Master's computer science students. The objective of the experiments was to evaluate the participants' effectiveness, efficiency, perceived ease of use and perceived satisfaction when using {\{}WUEP{\}} in comparison to an industrial widely used inspection method: Heuristic Evaluation (HE). The statistical analysis and meta-analysis of the data obtained separately from each experiment indicated that {\{}WUEP{\}} is more effective and efficient than {\{}HE{\}} in the detection of usability problems. The evaluators were also more satisfied when applying WUEP, and found it easier to use than HE. Although further experiments must be carried out to strengthen these results, {\{}WUEP{\}} has proved to be a promising usability inspection method for Web applications which have been developed by using model-driven development processes. },
	Doi = {https://doi.org/10.1016/j.jss.2012.07.043},
	ISSN = {0164-1212},
	Keywords = {Family of experiments,Model-driven development,Usability inspection,Web applications},
	Url = {http://www.sciencedirect.com/science/article/pii/S016412121200218X}
}

@Article{Ferrari20131639,
	Title = {{Towards the practical mutation testing of AspectJ programs}},
	Author = {Ferrari, Fabiano Cutigi and Rashid, Awais and Maldonado, Jos{\'{e}} Carlos},
	Journal = {Science of Computer Programming},
	Year = {2013},
	Number = {9},
	Pages = {1639--1662},
	Volume = {78},
	Abstract = {Abstract Mutation testing is a test selection criterion that relies on the assumption that test cases which can reveal artificial faults in the software are also good to reveal the real ones. It helps to expose faults which would go otherwise unnoticed. This criterion has been shown to be a promising means to deal with testing-related specificities of contemporary programming techniques such as Aspect-Oriented Programming. However, to date the few initiatives for customising mutation testing for aspect-oriented (AO) programs show either limited coverage with respect to the range of simulated faults, or a need for both adequate tool support and proper evaluation in regard to properties like application cost and effectiveness. This article tackles these limitations by describing a comprehensive mutation-based testing approach for programs written in AspectJ, which represents the most investigated {\{}AO{\}} programming language to date. The approach encompasses the definition of a set of mutation operators for AspectJ-specific constructs and the implementation of a tool that automates the approach. The results of a preliminary evaluation study show that the mutation operators are able to simulate faults that may not be revealed by pre-existing, non-mutation-based test suites. The results also suggest that the approach seems not to overwhelm the testers and hence represents a step towards the practical fault-based testing of AspectJ-like programs. },
	Doi = {https://doi.org/10.1016/j.scico.2013.02.011},
	ISSN = {0167-6423},
	Keywords = {Aspect-oriented programming,AspectJ,Mutation testing,Test evaluation,Testing AspectJ programs},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642313000713}
}

@Article{Ferreira2014,
	Title = {{On the use of feature-oriented programming for evolving software product lines — A comparative study}},
	Author = {Ferreira, Gabriel Coutinho Sousa and Gaia, Felipe Nunes and Figueiredo, Eduardo and {de Almeida Maia}, Marcelo},
	Journal = {Science of Computer Programming},
	Year = {2014},
	Pages = {65--85},
	Volume = {93, Part A},
	Abstract = {Abstract Feature-oriented programming (FOP) is a programming technique based on composition mechanisms, called refinements. It is often assumed that feature-oriented programming is more suitable than other variability mechanisms for implementing Software Product Lines (SPLs). However, there is no empirical evidence to support this claim. In fact, recent research work found out that some composition mechanisms might degenerate the {\{}SPL{\}} modularity and stability. However, there is no study investigating these properties focusing on the {\{}FOP{\}} composition mechanisms. This paper presents quantitative and qualitative analysis of how feature modularity and change propagation behave in the context of two evolving SPLs, namely WebStore and MobileMedia. Quantitative data have been collected from the {\{}SPLs{\}} developed in three different variability mechanisms: {\{}FOP{\}} refinements, conditional compilation, and object-oriented design patterns. Our results suggest that {\{}FOP{\}} requires few changes in source code and a balanced number of added modules, providing better support than other techniques for non-intrusive insertions. Therefore, it adheres closer to the Open–Closed principle. Additionally, {\{}FOP{\}} seems to be more effective tackling modularity degeneration, by avoiding feature tangling and scattering in source code, than conditional compilation and design patterns. These results are based not only on the variability mechanism itself, but also on careful {\{}SPL{\}} design. However, the aforementioned results are weaker when the design needs to cope with crosscutting and fine-grained features. },
	Annote = {Special Issue with Selected Papers from the Brazilian Symposium on Programming Languages (SBLP 2011)},
	Doi = {https://doi.org/10.1016/j.scico.2013.10.010},
	ISSN = {0167-6423},
	Keywords = {Conditional compilation,Design patterns,Feature-oriented programming,Software product lines,Variability management},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642313002815}
}

@InProceedings{Figueiredo:2011:ICC:1960275.1960287,
	Title = {{On the Impact of Crosscutting Concern Projection on Code Measurement}},
	Author = {Figueiredo, Eduardo and Garcia, Alessandro and Maia, Marcelo and Ferreira, Gabriel and Nunes, Camila and Whittle, Jon},
	Booktitle = {Proceedings of the Tenth International Conference on Aspect-oriented Software Development},
	Year = {2011},
	Address = {New York, NY, USA},
	Pages = {81--92},
	Publisher = {ACM},
	Series = {AOSD '11},
	Doi = {10.1145/1960275.1960287},
	ISBN = {978-1-4503-0605-8},
	Keywords = {concern metrics,concern projection,crosscutting concerns},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/1960275.1960287}
}

@Article{Figueiredo2012227,
	Title = {{Applying and evaluating concern-sensitive design heuristics}},
	Author = {Figueiredo, Eduardo and Sant'Anna, Claudio and Garcia, Alessandro and Lucena, Carlos},
	Journal = {Journal of Systems and Software},
	Year = {2012},
	Number = {2},
	Pages = {227--243},
	Volume = {85},
	Abstract = {Manifestation of crosscutting concerns in software systems is often an indicative of design modularity flaws and further design instabilities as those systems evolve. Without proper design evaluation mechanisms, the identification of harmful crosscutting concerns can become counter-productive and impractical. Nowadays, metrics and heuristics are the basic mechanisms to support their identification and classification either in object-oriented or aspect-oriented programs. However, conventional mechanisms have a number of limitations to support an effective identification and classification of crosscutting concerns in a software system. In this paper, we claim that those limitations are mostly caused by the fact that existing metrics and heuristics are not sensitive to primitive concern properties, such as either their degree of tangling and scattering or their specific structural shapes. This means that modularity assessment is rooted only at conventional attributes of modules, such as module cohesion, coupling and size. This paper proposes a representative suite of concern-sensitive heuristic rules. The proposed heuristics are supported by a prototype tool. The paper also reports an exploratory study to evaluate the accuracy of the proposed heuristics by applying them to seven systems. The results of this exploratory analysis give evidences that the heuristics offer support for: (i) addressing the shortcomings of conventional metrics-based assessments, (ii) reducing the manifestation of false positives and false negatives in modularity assessment, (iii) detecting sources of design instability, and (iv) finding the presence of design modularity flaws in both object-oriented and aspect-oriented programs. Although our results are limited to a number of decisions we made in this study, they indicate a promising research direction. Further analyses are required to confirm or refute our preliminary findings and, so, this study should be seen as a stepping stone on understanding how concerns can be useful assessment abstractions. We conclude this paper by discussing the limitations of this exploratory study focusing on some situations which hinder the accuracy of concern-sensitive heuristics. },
	Annote = {Special issue with selected papers from the 23rd Brazilian Symposium on Software Engineering},
	Doi = {https://doi.org/10.1016/j.jss.2011.09.060},
	ISSN = {0164-1212},
	Keywords = {Aspect-oriented software development,Crosscutting concerns,Design heuristics,Modularity,Software metrics},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121211002585}
}

@Article{Fleischmann201683,
	Title = {{The role of software updates in information systems continuance — An experimental study from a user perspective}},
	Author = {Fleischmann, Marvin and Amirpur, Miglena and Grupp, Tillmann and Benlian, Alexander and Hess, Thomas},
	Journal = {Decision Support Systems},
	Year = {2016},
	Pages = {83--96},
	Volume = {83},
	Abstract = {Abstract Although software updates are a ubiquitous phenomenon in professional and private {\{}IT{\}} usage, they have to date received little attention in the {\{}IS{\}} post-adoption literature. Drawing on expectation–confirmation theory and the {\{}IS{\}} continuance literature, we investigate whether, when and how software updates affect users' continuance intentions (CI). Based on a controlled laboratory experiment, we find a positive effect of feature updates on users' CI. According to this effect, software vendors can increase their users' {\{}CI{\}} by delivering features through updates after a software has been released and is already used by customers. We also find that users prefer frequent feature updates over less frequent update packages that bundle several features in one update. However, the positive effect from updates occurs only with functional feature updates and not with technical non-feature updates, disclosing update frequency and update type as crucial moderators to this effect. Furthermore, we unveil that this beneficial effect of feature updates operates through positive disconfirmation of expectations, resulting in increased perceived usefulness and satisfaction. Implications for research and practice as well as directions for future research are discussed. },
	Doi = {https://doi.org/10.1016/j.dss.2015.12.010},
	ISSN = {0167-9236},
	Keywords = {Expectation–confirmation theory,IS continuance,IS post-adoption,IT features,Software updates},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167923616000026}
}

@Article{SPE:SPE2116,
	Title = {{Playing MUSIC — building context-aware and self-adaptive mobile applications}},
	Author = {Floch, J and Fr{\`{a}}, C and Fricke, R and Geihs, K and Wagner, M and Lorenzo, J and Soladana, E and Mehlhase, S and Paspallis, N and Rahnama, H and Ruiz, P A and Scholz, U},
	Journal = {Software: Practice and Experience},
	Year = {2013},
	Number = {3},
	Pages = {359--388},
	Volume = {43},
	Abstract = {Although the idea of context-awareness was introduced almost two decades ago, few mobile software applications are available today that can sense and adapt to their run-time environment. The development of context-aware and self-adaptive applications is complex and few developers have experience in this area. On the basis of several demonstrators built by the joint European research project MUSIC, this paper describes typical context and adaptation features relevant for the development of context-aware and self-adaptive mobile applications. We explain how the demonstrators were realised using the open-source platform MUSIC and present the feedback of the developers of these demonstrators. The main contribution of this paper is to show how the development complexity of context-aware and self-adaptive mobile applications can be mastered by using an adaptation framework such as MUSIC. Copyright {\textcopyright} 2012 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spe.2116},
	ISSN = {1097-024X},
	Keywords = {context awareness,middleware,model-driven development,self-adaptive software},
	Publisher = {John Wiley {\&} Sons, Ltd},
	Url = {http://dx.doi.org/10.1002/spe.2116}
}

@Article{Folmer200461,
	Title = {{Architecting for usability: a survey}},
	Author = {Folmer, Eelke and Bosch, Jan},
	Journal = {Journal of Systems and Software},
	Year = {2004},
	Number = {1–2},
	Pages = {61--78},
	Volume = {70},
	Abstract = {Over the years the software engineering community has increasingly realized the important role software architecture plays in fulfilling the quality requirements of a system. The quality attributes of a software system are, to a large extent determined by the system's software architecture. In recent years, the software engineering community has developed various tools and techniques that allow for design for quality attributes, such as performance or maintainability, at the software architecture level. We believe this design approach can be applied not only to “traditional? quality attributes such as performance or maintainability but also to usability. This survey explores the feasibility of such a design approach. Current practice is surveyed from the perspective of a software architect. Are there any design methods that allow for design for usability at the architectural level? Are there any evaluation tools that allow assessment of architectures for their support of usability? What is usability? A framework is presented which visualizes these three research questions. Usability should drive design at all stages, but current usability engineering practice fails to fully achieve this goal. Our survey shows that there are no design techniques or assessment tools that allow for design for usability at the architectural level. },
	Doi = {https://doi.org/10.1016/S0164-1212(02)00159-0},
	ISSN = {0164-1212},
	Keywords = {Design for quality attributes,Software architecture,Usability},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121202001590}
}

@Article{SPIP:SPIP171,
	Title = {{A framework for capturing the relationship between usability and software architecture}},
	Author = {Folmer, Eelke and van Gurp, Jilles and Bosch, Jan},
	Journal = {Software Process: Improvement and Practice},
	Year = {2003},
	Number = {2},
	Pages = {67--87},
	Volume = {8},
	Abstract = {Usability is increasingly recognized as an essential factor that determines the success of software systems. Practice shows that for current software systems, most usability issues are detected during testing and deployment. Fixing usability issues during this late stage of the development proves to be very costly. Some usability-improving modifications such as usability patterns may have architectural implications. We believe that the software architecture may restrict usability. The high costs associated with fixing usability issues during late-stage development prevent developers from making the necessary adjustments for meeting all the usability requirements. To improve upon this situation, we have investigated the relationship between usability and software architecture to gain a better understanding of how the architecture restricts the level of usability. Our article makes a number of contributions; a framework is presented that expresses the relationship between usability and software architecture. The framework consists of an integrated set of design solutions such as usability patterns and usability properties that have been identified in various cases in industry, modern day software and literature surveys. These solutions, in most cases, have a positive effect on usability but are difficult to retrofit into applications because they have architectural impact. Our framework may be used to guide and inform the architectural design phase. This may decrease development costs by reducing the amount of usability issues that need to be fixed during the later stages of development. Copyright {\textcopyright} 2004 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spip.171},
	ISSN = {1099-1670},
	Keywords = {software architecture,usability,usability patterns},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/spip.171}
}

@Article{Font201720,
	Title = {{Leveraging variability modeling to address metamodel revisions in Model-based Software Product Lines}},
	Author = {Font, Jaime and Arcega, Lorena and Haugen, ??ystein and Cetina, Carlos and Haugen, {\O} and Cetina, Carlos},
	Journal = {Computer Languages, Systems and Structures},
	Year = {2017},
	Pages = {20--38},
	Volume = {48},
	Abstract = {Metamodels evolve over time, which can break the conformance between the models and the metamodel. Model migration strategies aim to co-evolve models and metamodels together, but their application is currently not fully automatizable and is thus cumbersome and error prone. We introduce the Variable MetaModel (VMM) strategy to address the evolution of the reusable model assets of a model-based Software Product Line. The VMM strategy applies variability modeling ideas to express the evolution of the metamodel in terms of commonalities and variabilities. When the metamodel evolves, changes are automatically formalized into the VMM and models that conform to previous versions of the metamodel continue to conform to the VMM, thus eliminating the need for migration. We have applied both the traditional migration strategy and the VMM strategy to a retrospective case study that includes 13 years of evolution of our industrial partner, an induction hobs manufacturer. The comparison between the two strategies shows better results for the VMM strategy in terms of model indirection, automation, and trust leak. {\textcopyright} 2016 Elsevier Ltd},
	Annote = {From Duplicate 1 (Leveraging variability modeling to address metamodel revisions in Model-based Software Product Lines - Font, J; Arcega, L; Haugen, {\O}; Cetina, C)
		cited By 0},
	Doi = {10.1016/j.cl.2016.08.003},
	File = {:Users/mac/Downloads/Sciencedirect{\_}articles{\_}13Jun2017{\_}15-32-18.445/Leveraging-variability-modeling-to-address-metamodel-revisions-in-Model-based-Software-Product-Lines{\_}2017{\_}Computer-Languages-Systems-Structures.pdf:pdf},
	ISSN = {14778424},
	Keywords = {Computer software,Computer software reusability,Industrial partners,Meta model,Migration strate,Model and metamodel co-evolution,Model-based Software Product Lines,Software design,Variability Modeling},
	Publisher = {Elsevier},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994193695{\&}doi=10.1016{\%}2Fj.cl.2016.08.003{\&}partnerID=40{\&}md5=86f0c11fe9f791dbe01ed189f402cde1 http://dx.doi.org/10.1016/j.cl.2016.08.003}
}

@Article{Fontana2014140,
	Title = {{Processes versus people: How should agile software development maturity be defined?}},
	Author = {Fontana, Rafaela Mantovani and Fontana, Isabela Mantovani and {da Rosa Garbuio}, Paula Andrea and Reinehr, Sheila and Malucelli, Andreia},
	Journal = {Journal of Systems and Software},
	Year = {2014},
	Pages = {140--155},
	Volume = {97},
	Abstract = {Abstract Maturity in software development is currently defined by models such as CMMI-DEV and ISO/IEC 15504, which emphasize the need to manage, establish, measure and optimize processes. Teams that develop software using these models are guided by defined, detailed processes. However, an increasing number of teams have been implementing agile software development methods that focus on people rather than processes. What, then, is maturity for these agile teams that focus less on detailed, defined processes? This is the question we sought to answer in this study. To this end, we asked agile practitioners about their perception of the maturity level of a number of practices and how they defined maturity in agile software development. We used cluster analysis to analyze quantitative data and triangulated the results with content analysis of the qualitative data. We then proposed a new definition for agile software development maturity. The findings show that practitioners do not see maturity in agile software development as process definition or quantitative management capabilities. Rather, agile maturity means fostering more subjective capabilities, such as collaboration, communication, commitment, care, sharing and self-organization. },
	Doi = {https://doi.org/10.1016/j.jss.2014.07.030},
	ISSN = {0164-1212},
	Keywords = {Agile software development,Maturity,Software process improvement},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121214001587}
}

@Article{Fontana201588,
	Title = {{Progressive Outcomes: A framework for maturing in agile software development}},
	Author = {Fontana, Rafaela Mantovani and Jr., Victor Meyer and Reinehr, Sheila and Malucelli, Andreia},
	Journal = {Journal of Systems and Software},
	Year = {2015},
	Pages = {88--108},
	Volume = {102},
	Abstract = {Abstract Maturity models are used to guide improvements in the software engineering field and a number of maturity models for agile methods have been proposed in the last years. These models differ in their underlying structure prescribing different possible paths to maturity in agile software development, neglecting the fact that agile teams struggle to follow prescribed processes and practices. Our objective, therefore, was to empirically investigate how agile teams evolve to maturity, as a means to conceive a theory for agile software development evolvement that considers agile teams nature. The complex adaptive systems theory was used as a lens for analysis and four case studies were conducted to collect qualitative and quantitative data. As a result, we propose the Progressive Outcomes framework to describe the agile software development maturing process. It is a framework in which people have the central role, ambidexterity is a key ability to maturity, and improvement is guided by outcomes agile teams pursue, instead of prescribed practices. },
	Doi = {https://doi.org/10.1016/j.jss.2014.12.032},
	ISSN = {0164-1212},
	Keywords = {Agile software development,Ambidexterity,Complex adaptive systems,Maturity,Software process improvement},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121214002908}
}

@Article{Fortier2010915,
	Title = {{Dealing with variability in context-aware mobile software}},
	Author = {Fortier, Andr{\'{e}}s and Rossi, Gustavo and Gordillo, Silvia E and Challiol, Cecilia},
	Journal = {Journal of Systems and Software},
	Year = {2010},
	Number = {6},
	Pages = {915--936},
	Volume = {83},
	Abstract = {Mobile context-aware software pose a set of challenging requirements to developers as these applications exhibit novel features, such as handling varied sensing devices and dynamically adapting to the user's context (e.g. his or her location), and evolve quickly according to technological advances. In this paper, we discuss how to handle variability both across different domains and during the evolution of a single application. We present a set of design structures for solving different problems related with mobility (such as location sensing, behaviour adaptation, etc.), together with the design rationale underlying them, and show how these sound micro-architectural constructs impact on variability. Our presentation is illustrated with case studies in different domains. },
	Annote = {Software Architecture and Mobility},
	Doi = {https://doi.org/10.1016/j.jss.2009.11.002},
	ISSN = {0164-1212},
	Keywords = {Architecture,Context-awareness,Mobile software,Software variability},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121209002830}
}

@Article{Frantz201689,
	Title = {{On the design of a maintainable software development kit to implement integration solutions}},
	Author = {Frantz, Rafael Z and Corchuelo, Rafael and Roos-Frantz, Fabricia},
	Journal = {Journal of Systems and Software},
	Year = {2016},
	Pages = {89--104},
	Volume = {111},
	Abstract = {Abstract Companies typically rely on applications purchased from third parties or developed at home to support their business activities. It is not uncommon that these applications were not designed taking integration into account. Enterprise Application Integration provides methodologies and tools to design and implement integration solutions. Camel, Spring Integration, and Mule range amongst the most popular open-source tools that provide support to implement integration solutions. The adaptive maintenance of a software tool is very important for companies that need to reuse existing tools to build their own. We have analysed 25 maintainability measures on Camel, Spring Integration, and Mule. We have conducted a statistical analysis to confirm the results obtained with the maintainability measures, and it follows that these tools may have problems regarding maintenance. These problems increase the costs of the adaptation process. This motivated us to work on a new proposal that has been carefully designed in order to reduce maintainability efforts. Guaran{\'{a}} SDK is the software tool that we provide to implement integration solutions. We have also computed the maintainability measures regarding Guaran{\'{a}} SDK and the results suggest that maintaining it is easier than maintaining the others. Furthermore, we have conducted an industrial experience to demonstrate the application of our proposal in industry. },
	Doi = {https://doi.org/10.1016/j.jss.2015.08.044},
	ISSN = {0164-1212},
	Keywords = {Enterprise Application Integration,Integration framework},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121215001880}
}

@Article{Freeman200816,
	Title = {{Lifting transformational models of product lines: A case study}},
	Author = {Freeman, G and Batory, D and Lavender, G},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2008},
	Pages = {16--30},
	Volume = {5063 LNCS},
	Abstract = {Model driven development (MDD) of software product lines (SPLs) merges two increasing important paradigms that synthesize programs by transformation. MDD creates programs by transforming models, and SPLs elaborate programs by applying transformations called features. In this paper, we present the design and implementation of a transformational model of a product line of scalar vector graphics and JavaScript applications. We explain how we simplified our implementation by lifting selected features and their compositions from our original product line (whose implementations were complex) to features and their compositions of another product line (whose specifications were simple). We used operators to map higher-level features and their compositions to their lower-level counterparts. Doing so exposed commuting relationships among feature compositions in both product lines that helped validate our model and implementation. {\textcopyright} Springer-Verlag Berlin Heidelberg 2008.},
	Annote = {cited By 3},
	Doi = {10.1007/978-3-540-69927-9_2},
	Keywords = {Chemical analysis,Code generation; Features; Highlevel transformati,Software design},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-54249099993{\&}doi=10.1007{\%}2F978-3-540-69927-9{\_}2{\&}partnerID=40{\&}md5=7fbb966fabdf9448fa7bca4939a832d9}
}

@InProceedings{Fruehwirth:2010:QSS:1852786.1852869,
	Title = {{Quantitative Software Security Measurement in an Engineering Service Bus Platform}},
	Author = {Fruehwirth, Christian and Biffl, Stefan and Schatten, Alexander and Winkler, Dietmar and Sunindyo, Wikan D},
	Booktitle = {Proceedings of the 2010 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
	Year = {2010},
	Address = {New York, NY, USA},
	Pages = {67:1----67:1},
	Publisher = {ACM},
	Series = {ESEM '10},
	Doi = {10.1145/1852786.1852869},
	ISBN = {978-1-4503-0039-1},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/1852786.1852869}
}

@Article{FuentesFernandez2012247,
	Title = {{A model-driven process for the modernization of component-based systems}},
	Author = {Fuentes-Fern{\'{a}}ndez, Rub{\'{e}}n and Pav{\'{o}}n, Juan and Garijo, Francisco},
	Journal = {Science of Computer Programming},
	Year = {2012},
	Number = {3},
	Pages = {247--269},
	Volume = {77},
	Abstract = {Software modernization is critical for organizations that need cost-effective solutions to deal with the rapid obsolescence of software and the increasing demand for new functionality. This paper presents the {\{}XIRUP{\}} modernization methodology, which proposes a highly iterative process, structured into four phases: preliminary evaluation, understanding, building and migration. This modernization process is feature-driven, component-based, focused on the early elicitation of key information, and relies on a model-driven approach with extensive use of experience from the previous projects. {\{}XIRUP{\}} has been defined in the European {\{}IST{\}} project MOMOCS, which has also built a suite of support tools. This paper introduces the process using a case study that illustrates its activities, related tools and results. The discussion highlights the specific characteristics of modernization projects and how a customized methodology can take advantage of them. },
	Annote = {Feature-Oriented Software Development (FOSD 2009)},
	Doi = {https://doi.org/10.1016/j.scico.2011.04.003},
	ISSN = {0167-6423},
	Keywords = {Agile process,Component,Model-driven engineering,Modernization of software systems,Software engineering,Software methodology},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642311001110}
}

@InProceedings{Fukuda:2011:AET:2019136.2019173,
	Title = {{An Approach to Evaluate Time-dependent Changes in Feature Constraints}},
	Author = {Fukuda, Takeshi and Atarashi, Yoshitaka and Yoshimura, Kentaro},
	Booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
	Year = {2011},
	Address = {New York, NY, USA},
	Pages = {33:1----33:5},
	Publisher = {ACM},
	Series = {SPLC '11},
	Doi = {10.1145/2019136.2019173},
	ISBN = {978-1-4503-0789-5},
	Keywords = {embedded systems,feature modeling,industry case study,software product line engineering},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2019136.2019173}
}

@Article{Furtado2010316,
	Title = {{Streamlining domain analysis for digital games product lines}},
	Author = {Furtado, A W B and Santos, A L M and Ramalho, G L},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2010},
	Pages = {316--330},
	Volume = {6287 LNCS},
	Abstract = {Digital games and their development process are quite peculiar when compared to other software in general. However, current domain engineering processes do not addresses such peculiarities and, not surprisingly, successful cases of software product lines (SPLs) for digital games cannot be found in the literature nor the industry. With such a motivation, this paper focuses on streamlining and enriching the Domain Analysis process for SPLs targeted at digital games. Guidelines are provided for making Domain Analysis tasks aware of digital games peculiarities, in order to tackle the challenges of and benefit from the unique characteristics of such a macro-domain. A case study for an SPL aimed at arcade-based games is also presented to illustrate and evaluate the proposed guidelines. {\textcopyright} 2010 Springer-Verlag Berlin Heidelberg.},
	Annote = {cited By 3},
	Doi = {10.1007/978-3-642-15579-6_22},
	Keywords = {Computer software reusability,Concentration (process),Development process; Digital games; Domain analysi},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049405732{\&}doi=10.1007{\%}2F978-3-642-15579-6{\_}22{\&}partnerID=40{\&}md5=36dc531dd5107ba35baaa27c46fa5a4a}
}

@Article{Gomez20141101,
	Title = {{A framework for variable content document generation with multiple actors}},
	Author = {G{\'{o}}mez, Abel and Penad{\'{e}}s, M Carmen and Can{\'{o}}s, Jos{\'{e}} H and Borges, Marcos R S and Llavador, Manuel},
	Journal = {Information and Software Technology},
	Year = {2014},
	Number = {9},
	Pages = {1101--1121},
	Volume = {56},
	Abstract = {AbstractContext Advances in customization have highlighted the need for tools supporting variable content document management and generation in many domains. Current tools allow the generation of highly customized documents that are variable in both content and layout. However, most frameworks are technology-oriented, and their use requires advanced skills in implementation-related tools, which means their use by end users (i.e. document designers) is severely limited. Objective Starting from past and current trends for customized document authoring, our goal is to provide a document generation alternative in which variants are specified at a high level of abstraction and content reuse can be maximized in high variability scenarios. Method Based on our experience in Document Engineering, we identified areas in the variable content document management and generation field open to further improvement. We first classified the primary sources of variability in document composition processes and then developed a methodology, which we called {\{}DPL{\}} – based on Software Product Lines principles – to support document generation in high variability scenarios. Results In order to validate the applicability of our methodology we implemented a tool – {\{}DPLfw{\}} – to carry out {\{}DPL{\}} processes. After using this in different scenarios, we compared our proposal with other state-of-the-art tools for variable content document management and generation. Conclusion The {\{}DPLfw{\}} showed a good capacity for the automatic generation of variable content documents equal to or in some cases surpassing other currently available approaches. To the best of our knowledge, {\{}DPLfw{\}} is the only framework that combines variable content and document workflow facilities, easing the generation of variable content documents in which multiple actors play different roles. },
	Annote = {Special Sections from “Asia-Pacific Software Engineering Conference (APSEC), 2012? and “ Software Product Line conference (SPLC), 2012?},
	Doi = {https://doi.org/10.1016/j.infsof.2013.12.006},
	ISSN = {0950-5849},
	Keywords = {Document generation,Document product line,Document workflow,Feature modeling,Model driven engineering,Variable data printing},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584913002358}
}

@Article{GomezAbajo2017152,
	Title = {{A domain-specific language for model mutation and its application to the automated generation of exercises}},
	Author = {G{\'{o}}mez-Abajo, Pablo and Guerra, Esther and de Lara, Juan},
	Journal = {Computer Languages, Systems {\&} Structures},
	Year = {2017},
	Pages = {152--173},
	Volume = {49},
	Abstract = {Abstract Model-Driven Engineering (MDE) is a software engineering paradigm that uses models as main assets in all development phases. While many languages for model manipulation exist (e.g., for model transformation or code generation), there is a lack of frameworks to define and apply model mutations. A model mutant is a variation of an original model, created by the application of specific model mutation operations. Model mutation has many applications, for instance, in the areas of model transformation testing, model-based testing or education. In this paper, we present a domain-specific language called Wodel for the specification and generation of model mutants. Wodel is domain-independent, as it can be used to generate mutants of models conformant to arbitrary meta-models. Its development environment is extensible, permitting the incorporation of post-processors for different applications. In particular, we describe Wodel-Edu, a post-processing extension directed to the automated generation of exercises for particular domains and their automated correction. We show the application of Wodel-Edu to the generation of exercises for deterministic automata, and report on an evaluation of the quality of the generated exercises, obtaining overall good results. },
	Doi = {https://doi.org/10.1016/j.cl.2016.11.001},
	ISSN = {1477-8424},
	Keywords = {Automatic exercise generation and correction,Domain-Specific Languages,Education,Model mutation,Model-Driven Engineering},
	Url = {http://www.sciencedirect.com/science/article/pii/S147784241630094X}
}

@Article{Gunther2012152,
	Title = {{rbFeatures: Feature-oriented programming with Ruby}},
	Author = {G{\"{u}}nther, Sebastian and Sunkle, Sagar},
	Journal = {Science of Computer Programming},
	Year = {2012},
	Number = {3},
	Pages = {152--173},
	Volume = {77},
	Abstract = {Features are pieces of core functionality of a program that is relevant to particular stakeholders. Features pose dependencies and constraints among each other. These dependencies and constraints describe the possible number of variants of the program: A valid feature configuration generates a specific variant with unique behavior. Feature-Oriented Programming is used to implement features as program units. This paper introduces rbFeatures, a feature-oriented programming language implemented on top of the dynamic programming language Ruby. With rbFeatures, programmers use software product lines, variants, and features as first-class entities. This allows several runtime reflection and modification capabilities, including the extension of the product line with new features and the provision of multiple variants. The paper gives a broad overview to the implementation and application of rbFeatures. We explain how features as first-class entities are designed and implemented, and discuss how the semantics of features are carefully added to Ruby programs. We show two case studies: The expression product line, a common example in feature-oriented programming, and a web application. },
	Annote = {Feature-Oriented Software Development (FOSD 2009)},
	Doi = {https://doi.org/10.1016/j.scico.2010.12.007},
	ISSN = {0167-6423},
	Keywords = {Domain-specific languages,Dynamic programming languages,Feature-oriented programming},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642311000025}
}

@Article{Gaia2014230,
	Title = {{A quantitative and qualitative assessment of aspectual feature modules for evolving software product lines}},
	Author = {Gaia, Felipe Nunes and Ferreira, Gabriel Coutinho Sousa and Figueiredo, Eduardo and {de Almeida Maia}, Marcelo},
	Journal = {Science of Computer Programming},
	Year = {2014},
	Pages = {230--253},
	Volume = {96, Part 2},
	Abstract = {Abstract Feature-Oriented Programming (FOP) and Aspect-Oriented Programming (AOP) are programming techniques based on composition mechanisms, called refinements and aspects, respectively. These techniques are assumed to be good variability mechanisms for implementing Software Product Lines (SPLs). Aspectual Feature Modules (AFM) is an approach that combines advantages of feature modules and aspects to increase concern modularity. Some guidelines on how to integrate these techniques have been established in some studies, but these studies do not focus the analysis on how effectively {\{}AFM{\}} can preserve the modularity and stability facilitating {\{}SPL{\}} evolution. The main purpose of this paper is to investigate whether the simultaneous use of aspects and features through the {\{}AFM{\}} approach facilitates the evolution of SPLs. The quantitative data were collected from two {\{}SPLs{\}} developed using four different variability mechanisms: (1) feature modules, aspects and aspects refinements of AFM, (2) aspects of aspect-oriented programming (AOP), (3) feature modules of feature-oriented programming (FOP), and (4) conditional compilation (CC) with object-oriented programming. Metrics for change propagation and modularity were calculated and the results support the benefits of the {\{}AFM{\}} option in a context where the product line has been evolved with addition or modification of crosscutting concerns. However a drawback of this approach is that refactoring components' design requires a higher degree of modifications to the {\{}SPL{\}} structure. },
	Annote = {Selected and extended papers of the Brazilian Symposium on Programming Languages 2012 (SBLP 2012)},
	Doi = {https://doi.org/10.1016/j.scico.2014.03.006},
	ISSN = {0167-6423},
	Keywords = {Aspect-oriented programming,Aspectual feature modules,Feature-oriented programming,Software product lines,Variability mechanisms},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642314001336}
}

@Article{Galindo201578,
	Title = {{Supporting distributed product configuration by integrating heterogeneous variability modeling approaches}},
	Author = {Galindo, Jos{\'{e}} A and Dhungana, Deepak and Rabiser, Rick and Benavides, David and Botterweck, Goetz and Gr{\"{u}}nbacher, Paul},
	Journal = {Information and Software Technology},
	Year = {2015},
	Pages = {78--100},
	Volume = {62},
	Abstract = {AbstractContext In industrial settings products are developed by more than one organization. Software vendors and suppliers commonly typically maintain their own product lines, which contribute to a larger (multi) product line or software ecosystem. It is unrealistic to assume that the participating organizations will agree on using a specific variability modeling technique—they will rather use different approaches and tools to manage the variability of their systems. Objective We aim to support product configuration in software ecosystems based on several variability models with different semantics that have been created using different notations. Method We present an integrative approach that provides a unified perspective to users configuring products in multi product line environments, regardless of the different modeling methods and tools used internally. We also present a technical infrastructure and a prototype implementation based on web services. Results We show the feasibility of the approach and its implementation by using it with the three most widespread types of variability modeling approaches in the product line community, i.e., feature-based, OVM-style, and decision-oriented modeling. To demonstrate the feasibility and flexibility of our approach, we present an example derived from industrial experience in enterprise resource planning. We further applied the approach to support the configuration of privacy settings in the Android ecosystem based on multiple variability models. We also evaluated the performance of different model enactment strategies used in our approach. Conclusions Tools and techniques allowing stakeholders to handle variability in a uniform manner can considerably foster the initiation and growth of software ecosystems from the perspective of software reuse and configuration. },
	Doi = {https://doi.org/10.1016/j.infsof.2015.02.002},
	ISSN = {0950-5849},
	Keywords = {Automated analysis,Product configuration,Software product lines},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584915000312}
}

@Article{Galster201516,
	Title = {{An industrial case study on variability handling in large enterprise software systems}},
	Author = {Galster, Matthias and Avgeriou, Paris},
	Journal = {Information and Software Technology},
	Year = {2015},
	Pages = {16--31},
	Volume = {60},
	Abstract = {AbstractContext Enterprise software systems (e.g., enterprise resource planning software) are often deployed in different contexts (e.g., different organizations or different business units or branches of one organization). However, even though organizations, business units or branches have the same or similar business goals, they may differ in how they achieve these goals. Thus, many enterprise software systems are subject to variability and adapted depending on the context in which they are used. Objective Our goal is to provide a snapshot of variability in large scale enterprise software systems. We aim at understanding the types of variability that occur in large industrial enterprise software systems. Furthermore, we aim at identifying how variability is handled in such systems. Method We performed an exploratory case study in two large software organizations, involving two large enterprise software systems. Data were collected through interviews and document analysis. Data were analyzed following a grounded theory approach. Results We identified seven types of variability (e.g., functionality, infrastructure) and eight mechanisms to handle variability (e.g., add-ons, code switches). Conclusions We provide generic types for classifying variability in enterprise software systems, and reusable mechanisms for handling such variability. Some variability types and handling mechanisms for enterprise software systems found in the real world extend existing concepts and theories. Others confirm findings from previous research literature on variability in software in general and are therefore not specific to enterprise software systems. Our findings also offer a theoretical foundation for describing variability handling in practice. Future work needs to provide more evaluations of the theoretical foundations, and refine variability handling mechanisms into more detailed practices. },
	Doi = {https://doi.org/10.1016/j.infsof.2014.12.003},
	ISSN = {0950-5849},
	Keywords = {Case study,Enterprise software systems,Grounded theory,Variability},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584914002572}
}

@Article{Galster2013428,
	Title = {{Constraints for the design of variability-intensive service-oriented reference architectures – An industrial case study}},
	Author = {Galster, Matthias and Avgeriou, Paris and Tofan, Dan},
	Journal = {Information and Software Technology},
	Year = {2013},
	Number = {2},
	Pages = {428--441},
	Volume = {55},
	Abstract = {Context Service-oriented architecture has become a widely used concept in software industry. However, we currently lack support for designing variability-intensive service-oriented systems. Such systems could be used in different environments, without the need to design them from scratch. To support the design of variability-intensive service-oriented systems, reference architectures that facilitate variability in instantiated service-oriented architectures can help. Objective The design of variability-intensive service-oriented reference architectures is subject to specific constraints. Architects need to know these constraints when designing such reference architectures. Our objective is to identify these constraints. Method An exploratory case study was performed in the context of local e-government in the Netherlands to study constraints from the perspective of (a) the users of a variability-intensive service-oriented system (municipalities that implement national laws), and (b) the implementing organizations (software vendors). We collected data through interviews with representatives from five organizations, document analyses and expert meetings. Results We identified ten constraints (e.g., organizational constraints, integration-related constraints) which affect the process of designing reference architectures for variability-intensive service-oriented systems. Also, we identified how stakeholders are affected by these constraints, and how constraints are specific to the case study domain. Conclusions Our results help design variability-intensive service-oriented reference architectures. Furthermore, our results can be used to define processes to design such reference architectures. },
	Annote = {Special Section: Component-Based Software Engineering (CBSE), 2011},
	Doi = {https://doi.org/10.1016/j.infsof.2012.09.011},
	ISSN = {0950-5849},
	Keywords = {Case study,Reference architectures,SOA,Service-oriented architecture,Variability,e-Government},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584912002054}
}

@Article{Gamez2013563,
	Title = {{Architectural evolution of FamiWare using cardinality-based feature models}},
	Author = {Gamez, Nadia and Fuentes, Lidia},
	Journal = {Information and Software Technology},
	Year = {2013},
	Number = {3},
	Pages = {563--580},
	Volume = {55},
	Abstract = {Context Ambient Intelligence systems domain is an outstanding example of modern systems that are in permanent evolution, as new devices, technologies or facilities are continuously appearing. This means it would be desirable to have a mechanism that helps with the propagation of evolution changes in deployed systems. Objective We present a software product line engineering process to manage the evolution of FamiWare, a family of middleware for ambient intelligence environments. This process drives the evolution of FamiWare middleware configurations using cardinality-based feature models, which are especially well suited to express the structural variability of ambient intelligence systems. Method FamiWare uses cardinality-based feature models and clonable features to model the structural variability present in ambient intelligence systems, composed of a large variety of heterogeneous devices. Since the management evolution of configurations with clonable features is manually untreatable due to the high number of features, our process automates it and propagates changes made at feature level to the architectural components of the FamiWare middleware. This is a model driven development process as the evolution management, the propagation of evolution changes and the code generation are performed using some kind of model mappings and transformations. Concretely we present a variability modelling language to map the selection of features to the corresponding FamiWare middleware architectural components. Results Our process is able to manage the evolution of cardinality-based feature models with thousands of features, something which is not possible to tackle manually. Thanks to the use of the variability language and the automatic code generation it is possible to propagate and maintain a correspondence between the FamiWare architectural model and the code. The process is then able to calculate the architectural differences between the evolved configuration and the previous one. Checking these differences, our process helps to calculate the effort needed to perform the evolution changes in the customized products. To perform those tasks we have defined two operators, one to calculate the differences between two feature model configurations and another to create a new configuration from a previous one. Conclusion Our process automatically propagates the evolution changes of the middleware family into the existing configurations where the middleware is already deployed and also helps us to calculate the effort in performing the changes in every configuration. Finally, we validated our approach, demonstrating the functioning of the defined operators and showing that by using our tool we can generate evolved configurations for FamiWare with thousands of cloned features, for several case studies. },
	Annote = {Special Issue on Software Reuse and Product LinesSpecial Issue on Software Reuse and Product Lines},
	Doi = {https://doi.org/10.1016/j.infsof.2012.06.012},
	ISSN = {0950-5849},
	Keywords = {Evolution,Feature Models,Middleware family,Software Product Lines},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584912001152}
}

@Article{Ganesan20132360,
	Title = {{An analysis of unit tests of a flight software product line}},
	Author = {Ganesan, Dharmalingam and Lindvall, Mikael and McComas, David and Bartholomew, Maureen and Slegel, Steve and Medina, Barbara and Krikhaar, Rene and Verhoef, Chris and Montgomery, Lisa P},
	Journal = {Science of Computer Programming},
	Year = {2013},
	Number = {12},
	Pages = {2360--2380},
	Volume = {78},
	Abstract = {This paper presents an analysis of the unit testing approach developed and used by the Core Flight Software System (CFS) product line team at the {\{}NASA{\}} Goddard Space Flight Center (GSFC). The goal of the analysis is to understand, review, and recommend strategies for improving the CFS' existing unit testing infrastructure as well as to capture lessons learned and best practices that can be used by other software product line (SPL) teams for their unit testing. The results of the analysis show that the core and application modules of the {\{}CFS{\}} are unit tested in isolation using a stub framework developed by the {\{}CFS{\}} team. The application developers can unit test their code without waiting for the core modules to be completed, and vice versa. The analysis found that this unit testing approach incorporates many practical and useful solutions such as allowing for unit testing without requiring hardware and special {\{}OS{\}} features in-the-loop by defining stub implementations of dependent modules. These solutions are worth considering when deciding how to design the testing architecture for a SPL. },
	Annote = {Special Section on International Software Product Line Conference 2010 and Fundamentals of Software Engineering (selected papers of {\{}FSEN{\}} 2011)},
	Doi = {https://doi.org/10.1016/j.scico.2012.02.006},
	ISSN = {0167-6423},
	Keywords = {Flight software,Metrics,Self-testable components,Software architecture,Stub,Unit testing},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642312000317}
}

@Article{Gao2014405,
	Title = {{Process model fragmentization, clustering and merging: An empirical study}},
	Author = {Gao, X and Chen, Y and Ding, Z and Wang, M and Zhang, X and Yan, Z and Wen, L and Guo, Q and Chen, R},
	Journal = {Lecture Notes in Business Information Processing},
	Year = {2014},
	Pages = {405--416},
	Volume = {171 LNBIP},
	Abstract = {Nowadays, it is common for an organization tomaintain thousands of business processes. Technologies that provide automatic management for such amount of models are required. The objective of this paper is to deal with the problem of process model fragmentization, clustering and merging for the consolidation of Office Automation (OA) systems in ChinaMobile Communications Corporation (CMCC). After investigating the structural statistics of real-life process model samples, we propose an approach, based on the refined process structure tree (RPST) and software product line (SPL), to automatically identify reusable process fragments and merge similar ones into master fragments. These fragments can, for example, be used to facilitate the (re)design of numerous process models. Special attention is paid to the empirical study and statistics from the experiment on a sample set of 37 real-life OA processes. Lesson learned and problems to be further considered are also proposed. {\textcopyright} Springer International Publishing Switzerland 2014.},
	Annote = {cited By 1},
	Doi = {10.1007/978-3-319-06257-0},
	Keywords = {Automatic management; Clustering; Empirical studi,Enterprise resource management; Office automation,Merging},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904538233{\&}doi=10.1007{\%}2F978-3-319-06257-0{\&}partnerID=40{\&}md5=c3ba390ac1f0cde60cc030b31e040df4}
}

@Article{GarciaBorgonon2014103,
	Title = {{Software process modeling languages: A systematic literature review}},
	Author = {Garc{\'{i}}a-Borgo{\~{n}}{\'{o}}n, L and Barcelona, M A and Garc{\'{i}}a-Garc{\'{i}}a, J A and Alba, M and Escalona, M J},
	Journal = {Information and Software Technology},
	Year = {2014},
	Number = {2},
	Pages = {103--116},
	Volume = {56},
	Abstract = {AbstractContext Organizations working in software development are aware that processes are very important assets as well as they are very conscious of the need to deploy well-defined processes with the goal of improving software product development and, particularly, quality. Software process modeling languages are an important support for describing and managing software processes in software-intensive organizations. Objective This paper seeks to identify what software process modeling languages have been defined in last decade, the relationships and dependencies among them and, starting from the current state, to define directions for future research. Method A systematic literature review was developed. 1929 papers were retrieved by a manual search in 9 databases and 46 primary studies were finally included. Results Since 2000 more than 40 languages have been first reported, each of which with a concrete purpose. We show that different base technologies have been used to define software process modeling languages. We provide a scheme where each language is registered together with the year it was created, the base technology used to define it and whether it is considered a starting point for later languages. This scheme is used to illustrate the trend in software process modeling languages. Finally, we present directions for future research. Conclusion This review presents the different software process modeling languages that have been developed in the last ten years, showing the relevant fact that model-based {\{}SPMLs{\}} (Software Process Modeling Languages) are being considered as a current trend. Each one of these languages has been designed with a particular motivation, to solve problems which had been detected. However, there are still several problems to face, which have become evident in this review. This let us provide researchers with some guidelines for future research on this topic. },
	Doi = {https://doi.org/10.1016/j.infsof.2013.10.001},
	ISSN = {0950-5849},
	Keywords = {Software process language,Software process modeling,Systematic literature review},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584913001894}
}

@Article{GarciaDiaz20101179,
	Title = {{{\{}TALISMAN{\}} MDE: Mixing {\{}MDE{\}} principles}},
	Author = {Garc{\'{i}}a-D{\'{i}}az, Vicente and Fern{\'{a}}ndez-Fern{\'{a}}ndez, H{\'{e}}ctor and Palacios-Gonz{\'{a}}lez, El{\'{i}}as and G-Bustelo, B Cristina Pelayo and Sanju{\'{a}}n-Mart{\'{i}}nez, Oscar and Lovelle, Juan Manuel Cueva},
	Journal = {Journal of Systems and Software},
	Year = {2010},
	Number = {7},
	Pages = {1179--1191},
	Volume = {83},
	Abstract = {The Model-Driven Engineering approach is progressively gaining popularity in the software engineering community as it raises the level of abstraction in software development. In {\{}TALISMAN{\}} {\{}MDE{\}} framework, we combine the principles of the two most important initiatives, Model-Driven Architecture and Software Factories. Both have their pros and cons, and we select the best from each in {\{}TALISMAN{\}} MDE. To show the advantages of {\{}TALISMAN{\}} MDE, we have developed a systems generator and used it to create applications for controlling food traceability. The applications are being used in dairies with different manufacturing processes, using software developed specifically for each dairy by working only with models, without additional programming. },
	Annote = {{\{}SPLC{\}} 2008},
	Doi = {https://doi.org/10.1016/j.jss.2010.01.010},
	ISSN = {0164-1212},
	Keywords = {MDA,MDE,Model-Driven,Software factory,TALISMAN,TMDE},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121210000075}
}

@Article{GarciaGalan2016200,
	Title = {{Automated configuration support for infrastructure migration to the cloud}},
	Author = {Garc{\'{i}}a-Gal{\'{a}}n, Jes{\'{u}}s and Trinidad, Pablo and Rana, Omer F and Ruiz-Cort{\'{e}}s, Antonio},
	Journal = {Future Generation Computer Systems},
	Year = {2016},
	Pages = {200--212},
	Volume = {55},
	Abstract = {Abstract With an increasing number of cloud computing offerings in the market, migrating an existing computational infrastructure to the cloud requires comparison of different offers in order to find the most suitable configuration. Cloud providers offer many configuration options, such as location, purchasing mode, redundancy, and extra storage. Often, the information about such options is not well organised. This leads to large and unstructured configuration spaces, and turns the comparison into a tedious, error-prone search problem for the customers. In this work we focus on supporting customer decision making for selecting the most suitable cloud configuration—in terms of infrastructural requirements and cost. We achieve this by means of variability modelling and analysis techniques. Firstly, we structure the configuration space of an IaaS using feature models, usually employed for the modelling of variability-intensive systems, and present the case study of the Amazon EC2. Secondly, we assist the configuration search process. Feature models enable the use of different analysis operations that, among others, automate the search of optimal configurations. Results of our analysis show how our approach, with a negligible analysis time, outperforms commercial approaches in terms of expressiveness and accuracy. },
	Doi = {https://doi.org/10.1016/j.future.2015.03.006},
	ISSN = {0167-739X},
	Keywords = {Automated analysis,Cloud migration,EC2,Feature model,IaaS},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167739X15000618}
}

@Article{GarciaMagarino2015161,
	Title = {{FTS-SOCI: An agent-based framework for simulating teaching strategies with evolutions of sociograms}},
	Author = {Garc{\'{i}}a-Magari{\~{n}}o, Iv{\'{a}}n and Plaza, Inmaculada},
	Journal = {Simulation Modelling Practice and Theory},
	Year = {2015},
	Pages = {161--178},
	Volume = {57},
	Abstract = {Abstract Teaching strategies have been proven to influence the academic performance of students, as well as group sociometrics have been proven to be directly related to group performance. Although in the literature there are examples of teaching strategies and the resulting sociograms, there is not any detailed technique, tool or simulator that predicts the influence of a new teaching strategy on group sociometrics. The current work is aimed at covering this gap in the literature, by providing a framework for programming teaching strategies to simulate their influence on group sociometrics. In particular, this framework is called FTS-SOCI (Framework for simulating Teaching Strategies with evolutions of SOCIograms). This framework includes an agent-based simulator that simulates the evolution of sociograms taking five models of the literature into account. In this framework, students and teacher are modelled as agents, and the teacher agent can have any teaching strategy defined by the user. In order to test the current approach, this work simulates existing teaching strategies in (1) nursing education and (2) sport lessons. The resulting sociograms of FTS-SOCI for these strategies have been compared with the corresponding real sociograms reported in the literature. This works shows that the group sociometric values provided by FTS-SOCI are quite similar to the real cases. For instance, the mean squared error of the group cohesion sociometric (i.e. {\{}IAg{\}} metric) is only 0.00024 and 0.00068 respectively for the teaching strategies of both fields. },
	Doi = {https://doi.org/10.1016/j.simpat.2015.07.003},
	ISSN = {1569-190X},
	Keywords = {Agent-based framework,Agent-based simulator,Multi-agent system,Sociogram,Teaching strategy},
	Url = {http://www.sciencedirect.com/science/article/pii/S1569190X15001070}
}

@Article{Garousi201716,
	Title = {{Software test maturity assessment and test process improvement: A multivocal literature review}},
	Author = {Garousi, Vahid and Felderer, Michael and Hacaloğlu, Tuna},
	Journal = {Information and Software Technology},
	Year = {2017},
	Pages = {16--42},
	Volume = {85},
	Abstract = {AbstractContext Software testing practices and processes in many companies are far from being mature and are usually conducted in ad-hoc fashions. Such immature practices lead to various negative outcomes, e.g., ineffectiveness of testing practices in detecting all the defects, and cost and schedule overruns of testing activities. To conduct test maturity assessment (TMA) and test process improvement (TPI) in a systematic manner, various TMA/TPI models and approaches have been proposed. Objective It is important to identify the state-of-the-art and the –practice in this area to consolidate the list of all various test maturity models proposed by practitioners and researchers, the drivers of TMA/TPI, the associated challenges and the benefits and results of TMA/TPI. Our article aims to benefit the readers (both practitioners and researchers) by providing the most comprehensive survey of the area, to this date, in assessing and improving the maturity of test processes. Method To achieve the above objective, we have performed a Multivocal Literature Review (MLR) study to find out what we know about TMA/TPI. A {\{}MLR{\}} is a form of a Systematic Literature Review (SLR) which includes the grey literature (e.g., blog posts and white papers) in addition to the published (formal) literature (e.g., journal and conference papers). We searched the academic literature using the Google Scholar and the grey literature using the regular Google search engine. Results Our {\{}MLR{\}} and its results are based on 181 sources, 51 (29{\%}) of which were grey literature and 130 (71{\%}) were formally published sources. By summarizing what we know about TMA/TPI, our review identified 58 different test maturity models and a large number of sources with varying degrees of empirical evidence on this topic. We also conducted qualitative analysis (coding) to synthesize the drivers, challenges and benefits of TMA/TPI from the primary sources. Conclusion We show that current maturity models and techniques in TMA/TPI provides reasonable advice for industry and the research community. We suggest directions for follow-up work, e.g., using the findings of this {\{}MLR{\}} in industry-academia collaborative projects and empirical evaluation of models and techniques in the area of TMA/TPI as reported in this article. },
	Doi = {https://doi.org/10.1016/j.infsof.2017.01.001},
	ISSN = {0950-5849},
	Keywords = {Multivocal literature review,Software testing,Systematic literature review,Test management,Test maturity,Test process,Test process assessment,Test process improvement},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584917300162}
}

@Article{Garousi2016195,
	Title = {{A systematic literature review of literature reviews in software testing}},
	Author = {Garousi, Vahid and M{\"{a}}ntyl{\"{a}}, Mika V},
	Journal = {Information and Software Technology},
	Year = {2016},
	Pages = {195--216},
	Volume = {80},
	Abstract = {AbstractContext Any newcomer or industrial practitioner is likely to experience difficulties in digesting large volumes of knowledge in software testing. In an ideal world, all knowledge used in industry, education and research should be based on high-quality evidence. Since no decision should be made based on a single study, secondary studies become essential in presenting the evidence. According to our search, over 101 secondary studies have been published in the area of software testing since 1994. With this high number of secondary studies, it is important to conduct a review in this area to provide an overview of the research landscape in this area. Objective The goal of this study is to systematically map (classify) the secondary studies in software testing. We propose that tertiary studies can serve as summarizing indexes which facilitate finding the most relevant information from secondary studies and thus supporting evidence-based decision making in any given area of software engineering. Our research questions (RQs) investigate: (1) Software-testing-specific areas, (2) Types of {\{}RQs{\}} investigated, (3) Numbers and Trends, and (4) Citations of the secondary studies. Method To conduct the tertiary study, we use the systematic-mapping approach. Additionally, we contrast the testing topics to the number of Google hits to address a general popularity of a testing topic and study the most popular papers in terms of citations. We furthermore demonstrate the practicality and usefulness of our results by mapping them to {\{}ISTQB{\}} foundation syllabus and to {\{}SWEBOK{\}} to provide implications for practitioners, testing educators, and researchers. Results After a systematic search and voting process, our study pool included 101 secondary studies in the area of software testing between 1994 and 2015. Among our results are the following: (1) In terms of number of secondary studies, model-based approach is the most popular testing method, web services are the most popular system under test (SUT), while regression testing is the most popular testing phase; (2) The quality of secondary studies, as measured by a criteria set established in the community, is slowly increasing as the years go by; and (3) Analysis of research questions, raised and studied in the pool of secondary studies, showed that there is a lack of ‘causality' and ‘relationship' type of research questions, a situation which needs to be improved if we, as a community, want to advance as a scientific field. (4) Among secondary studies, we found that regular surveys receive significantly more citations than {\{}SMs{\}} (p = 0.009) and {\{}SLRs{\}} (p = 0.014). Conclusion Despite the large number of secondary studies, we found that many important areas of software testing currently lack secondary studies, e.g., test management, role of product risk in testing, human factors in software testing, beta-testing (A/B-testing), exploratory testing, testability, test stopping criteria, and test-environment development. Having secondary studies in those areas is important for satisfying industrial and educational needs in software testing. On the other hand, education material of {\{}ISTQB{\}} foundation syllabus and {\{}SWEBOK{\}} could benefit from the inclusion of the latest research topics, namely search-based testing, use of cloud-computing for testing and symbolic execution. },
	Doi = {https://doi.org/10.1016/j.infsof.2016.09.002},
	ISSN = {0950-5849},
	Keywords = {Secondary studies,Software testing,Surveys,Systematic literature reviews,Systematic mapping,Tertiary study},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584916301446}
}

@Article{Garousi201692,
	Title = {{When and what to automate in software testing? A multi-vocal literature review}},
	Author = {Garousi, Vahid and M{\"{a}}ntyl{\"{a}}, Mika V},
	Journal = {Information and Software Technology},
	Year = {2016},
	Pages = {92--117},
	Volume = {76},
	Abstract = {AbstractContext Many organizations see software test automation as a solution to decrease testing costs and to reduce cycle time in software development. However, establishment of automated testing may fail if test automation is not applied in the right time, right context and with the appropriate approach. Objective The decisions on when and what to automate is important since wrong decisions can lead to disappointments and major wrong expenditures (resources and efforts). To support decision making on when and what to automate, researchers and practitioners have proposed various guidelines, heuristics and factors since the early days of test automation technologies. As the number of such sources has increased, it is important to systematically categorize the current state-of-the-art and -practice, and to provide a synthesized overview. Method To achieve the above objective, we have performed a Multivocal Literature Review (MLR) study on when and what to automate in software testing. A {\{}MLR{\}} is a form of a Systematic Literature Review (SLR) which includes the grey literature (e.g., blog posts and white papers) in addition to the published (formal) literature (e.g., journal and conference papers). We searched the academic literature using the Google Scholar and the grey literature using the regular Google search engine. Results Our {\{}MLR{\}} and its results are based on 78 sources, 52 of which were grey literature and 26 were formally published sources. We used the qualitative analysis (coding) to classify the factors affecting the when- and what-to-automate questions to five groups: (1) Software Under Test (SUT)-related factors, (2) test-related factors, (3) test-tool-related factors, (4) human and organizational factors, and (5) cross-cutting and other factors. The most frequent individual factors were: need for regression testing (44 sources), economic factors (43), and maturity of {\{}SUT{\}} (39). Conclusion We show that current decision-support in software test automation provides reasonable advice for industry, and as a practical outcome of this research we have summarized it as a checklist that can be used by practitioners. However, we recommend developing systematic empirically-validated decision-support approaches as the existing advice is often unsystematic and based on weak empirical evidence. },
	Doi = {https://doi.org/10.1016/j.infsof.2016.04.015},
	ISSN = {0950-5849},
	Keywords = {Decision support,Multivocal literature review,Software test automation,Systematic Mapping study,Systematic literature review,What to automate,When to automate},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584916300702}
}

@Article{Garousi2016106,
	Title = {{Challenges and best practices in industry-academia collaborations in software engineering: A systematic literature review}},
	Author = {Garousi, Vahid and Petersen, Kai and Ozkan, Baris},
	Journal = {Information and Software Technology},
	Year = {2016},
	Pages = {106--127},
	Volume = {79},
	Abstract = {Abstract Context: The global software industry and the software engineering (SE) academia are two large communities. However, unfortunately, the level of joint industry-academia collaborations in {\{}SE{\}} is still relatively very low, compared to the amount of activity in each of the two communities. It seems that the two 'camps' show only limited interest/motivation to collaborate with one other. Many researchers and practitioners have written about the challenges, success patterns (what to do, i.e., how to collaborate) and anti-patterns (what not do do) for industry-academia collaborations. Objective: To identify (a) the challenges to avoid risks to the collaboration by being aware of the challenges, (b) the best practices to provide an inventory of practices (patterns) allowing for an informed choice of practices to use when planning and conducting collaborative projects. Method: A systematic review has been conducted. Synthesis has been done using grounded-theory based coding procedures. Results: Through thematic analysis we identified 10 challenge themes and 17 best practice themes. A key outcome was the inventory of best practices, the most common ones recommended in different contexts were to hold regular workshops and seminars with industry, assure continuous learning from industry and academic sides, ensure management engagement, the need for a champion, basing research on real-world problems, showing explicit benefits to the industry partner, be agile during the collaboration, and the co-location of the researcher on the industry side. Conclusion: Given the importance of industry-academia collaboration to conduct research of high practical relevance we provide a synthesis of challenges and best practices, which can be used by researchers and practitioners to make informed decisions on how to structure their collaborations. },
	Doi = {https://doi.org/10.1016/j.infsof.2016.07.006},
	ISSN = {0950-5849},
	Keywords = {Best practices,Challenges,Industry,Industry-academia collaborations,Software engineering,Success patterns,Systematic literature review,Universities},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584916301203}
}

@Article{SMR:SMR1603,
	Title = {{Embracing the C preprocessor during refactoring}},
	Author = {Garrido, Alejandra and Johnson, Ralph},
	Journal = {Journal of Software: Evolution and Process},
	Year = {2013},
	Number = {12},
	Pages = {1285--1304},
	Volume = {25},
	Abstract = {C preprocessor directives are heavily used in C programs because they provide useful and even necessary additions to the C language. However, they are usually executed and discarded before any analysis is applied on C programs. In refactoring, preprocessor directives must be preserved through the whole process of parsing, analysis and transformation to retain editable yet correct source code. We propose a new preprocessing approach and special program representations that allow a program to be analyzed and transformed without losing its preprocessor directives, but treating them as first-class program entities. These representations are essential for a correct refactoring tool. We also describe the challenges that preprocessor directives bring to refactoring and how the program representations that we propose solve those challenges. Finally, we give details of two refactorings and present some case studies with our successfully applied solution. Copyright {\textcopyright} 2013 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/smr.1603},
	ISSN = {2047-7481},
	Keywords = {code analysis,preprocessor directives,refactoring,software maintenance},
	Url = {http://dx.doi.org/10.1002/smr.1603}
}

@Article{SMR:SMR1873,
	Title = {{Automated support for reuse-based requirements engineering in global software engineering}},
	Author = {de Gea, Juan Manuel and Nicol{\'{a}}s, Joaqu{\'{i}}n and Fern{\'{a}}ndez-Alem{\'{a}}n, Jos{\'{e}} L and Toval, Ambrosio},
	Journal = {Journal of Software: Evolution and Process},
	Year = {2017},
	Pages = {n/a----n/a},
	Abstract = {GlobalSoftware Engineering implies a paradigm shift towards globally-distributed development that can be advantageous, but at the cost of having to address the specific challenges that arise when the stakeholders are not colocated. Reusing assets during the initial processes of the software development life cycle could be beneficial, but automated support is essential if the expected benefits of requirements reuse are to be actually obtained. The main contribution of this paper is the specification of a collection of software features for a tool support for distributed, catalogue-based natural-language requirements reuse. Two additional contributions are also made: (1) an implementation of the requirements specifications previously mentioned using Drupal, a Content Management System; and (2) an empirical assessment of this tool support using distributed university students as subjects (n=57). According to our findings, the tool helps in making requirements reuse better than requirements specification from scratch and in managing traceability, is easy to use, useful, and easy to learn. In contrast, the tool is not particularly suitable for managing users and user roles.},
	Doi = {10.1002/smr.1873},
	ISSN = {2047-7481},
	Keywords = {computer-aided requirements engineering tool,global software engineering,requirements engineering,reuse},
	Url = {http://dx.doi.org/10.1002/smr.1873}
}

@Article{Gencel:2008:FSM:1363102.1363106,
	Title = {{Functional Size Measurement Revisited}},
	Author = {Gencel, Cigdem and Demirors, Onur},
	Journal = {ACM Trans. Softw. Eng. Methodol.},
	Year = {2008},
	Number = {3},
	Pages = {15:1----15:36},
	Volume = {17},
	Address = {New York, NY, USA},
	Doi = {10.1145/1363102.1363106},
	ISSN = {1049-331X},
	Keywords = {COSMIC FFP,Functional size measurement,MkII FPA,software benchmarking,software estimation},
	Publisher = {ACM},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/1363102.1363106}
}

@Article{Geppert2001627,
	Title = {{The {\{}SDL{\}} pattern approach – a reuse-driven {\{}SDL{\}} design methodology}},
	Author = {Geppert, Birgit and R{\"{o}}{\ss}ler, Frank},
	Journal = {Computer Networks},
	Year = {2001},
	Number = {6},
	Pages = {627--645},
	Volume = {35},
	Abstract = {There are several {\{}SDL{\}} methodologies that offer full system life-cycle support. Only few of them consider software reuse, not to mention high-level reuse of architecture and design. However, software reuse is a proven software engineering paradigm leading to high quality and reduced development effort. Experience made it apparent that – beyond the more traditional reuse of code – especially high-level reuse of architecture and design (as in the case of design patterns or frameworks) has the potential of achieving more systematic and widespread reuse. This paper presents the {\{}SDL{\}} pattern approach, a design methodology for distributed systems which integrates SDL-based system development with the pattern paradigm. It supports reuse of design knowledge modeled as {\{}SDL{\}} patterns and concentrates on the design phase of SDL-based system development. In order to get full life-cycle support, the pattern-based design process can be integrated within existing {\{}SDL{\}} methodologies. },
	Doi = {https://doi.org/10.1016/S1389-1286(00)00202-4},
	ISSN = {1389-1286},
	Keywords = {Design methodology,Distributed systems,Patterns,Process model,SDL,Software reuse},
	Url = {http://www.sciencedirect.com/science/article/pii/S1389128600002024}
}

@Article{Gerostathopoulos2016378,
	Title = {{Self-adaptation in software-intensive cyber–physical systems: From system goals to architecture configurations}},
	Author = {Gerostathopoulos, Ilias and Bures, Tomas and Hnetynka, Petr and Keznikl, Jaroslav and Kit, Michal and Plasil, Frantisek and Plouzeau, No{\"{e}}l},
	Journal = {Journal of Systems and Software},
	Year = {2016},
	Pages = {378--397},
	Volume = {122},
	Abstract = {Abstract Design of self-adaptive software-intensive cyber–physical systems (siCPS) operating in dynamic environments is a significant challenge when a sufficient level of dependability is required. This stems partly from the fact that the concerns of self-adaptivity and dependability are to an extent contradictory. In this paper, we introduce IRM-SA (Invariant Refinement Method for Self-Adaptation)—a design method and associated formally grounded model targeting siCPS—that addresses self-adaptivity and supports dependability by providing traceability between system requirements, distinct situations in the environment, and predefined configurations of system architecture. Additionally, IRM-SA allows for architecture self-adaptation at runtime and integrates the mechanism of predictive monitoring that deals with operational uncertainty. As a proof of concept, it was implemented in DEECo, a component framework that is based on dynamic ensembles of components. Furthermore, its feasibility was evaluated in experimental settings assuming decentralized system operation. },
	Doi = {https://doi.org/10.1016/j.jss.2016.02.028},
	ISSN = {0164-1212},
	Keywords = {Cyber–physical systems,Dependability,Self-adaptivity},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121216000601}
}

@Article{IIS2:IIS2239,
	Title = {{Functional Analysis and Design Approach for an Optimal Virtual IP Multimedia Subsystem (IMS) Architecture}},
	Author = {Gevorgyan, Arevik and Krob, Daniel and Spencer, Peter},
	Journal = {INCOSE International Symposium},
	Year = {2016},
	Number = {1},
	Pages = {1463--1476},
	Volume = {26},
	Abstract = {Network Functions Virtualization (NFV) is the greatest transformation in Telecommunications industry. It reveals the challenge of transformation from traditional monolithic to optimal virtual architectures, while meeting the standards driven functional, performance constraints and stakeholders conflicting objectives: maximizing flexibility, capacity, autonomy, minimizing expense in terms of financial and human effort (Capex, Opex), etc.Our analysis approach, specifying a customizable policy baseline to arbitrate and optimize for the NFV strategic objectives and enablers, defines an optimal functional architecture adapted to the context at any levels: from orchestration of network functions resources (virtual machines scaling/growth, placement) till an infrastructural equilibrium (integration of multiple virtual IMS systems).To define the optimal functional sets, we explore the multi-objective optimization for decisions trade-offs, incorporating it with the system views and environment, modeled through an adapted model-based Architectural Framework combined with PESTEL and FURPSE analyses.We case study the IP Multimedia Subsystem, as essential for communication services across networks and the only for Voice over 4G/5G.We further discuss our vision: self-regulating system and the related constraints.},
	Doi = {10.1002/j.2334-5837.2016.00239.x},
	ISSN = {2334-5837},
	Keywords = {Decisions Trade-Off,IP Multimedia Subsystem,Model-Based Architectural Framework,Multi-Objective Optimization,Network Functions Virtualization,Optimal Functional Architecture},
	Url = {http://dx.doi.org/10.1002/j.2334-5837.2016.00239.x}
}

@Article{Ghanam201043,
	Title = {{Extreme product line engineering - Refactoring for variability: A test-driven approach}},
	Author = {Ghanam, Y and Maurer, F},
	Journal = {Lecture Notes in Business Information Processing},
	Year = {2010},
	Pages = {43--57},
	Volume = {48 LNBIP},
	Abstract = {Software product lines - families of similar but not identical software products - need to address the issue of feature variability. That is, a single feature might require various implementations for different customers. Also, features might need optional extensions that are needed by some but not all products. Software product line engineering manages variability by conducting a thorough domain analysis upfront during the planning phases. However, upfront, heavyweight planning approaches are not well-aligned with the values of minimalistic practices like XP where bottom-up, incremental development is common. In this paper, we introduce a bottom-up, test-driven approach to introduce variability to systems by reactively refactoring existing code. We support our approach with an eclipse plug-in to automate the refactoring process. We evaluate our approach by a case study to determine the feasibility and practicality of the approach. {\textcopyright} Springer-Verlag Berlin Heidelberg 2010.},
	Annote = {cited By 8},
	Doi = {10.1007/978-3-642-13054-0_4},
	Keywords = {Agile methods; Feature variability; Incremental d,Computer programming,Computer software; Production engineering; Softwar},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876252634{\&}doi=10.1007{\%}2F978-3-642-13054-0{\_}4{\&}partnerID=40{\&}md5=322d60a770930761dd517f417e6d38cf}
}

@Article{Ghanam2012968,
	Title = {{Making the leap to a software platform strategy: Issues and challenges}},
	Author = {Ghanam, Yaser and Maurer, Frank and Abrahamsson, Pekka},
	Journal = {Information and Software Technology},
	Year = {2012},
	Number = {9},
	Pages = {968--984},
	Volume = {54},
	Abstract = {Context While there are many success stories of achieving high reuse and improved quality using software platforms, there is a need to investigate the issues and challenges organizations face when transitioning to a software platform strategy. Objective This case study provides a comprehensive taxonomy of the challenges faced when a medium-scale organization decided to adopt software platforms. The study also reveals how new trends in software engineering (i.e. agile methods, distributed development, and flat management structures) interplayed with the chosen platform strategy. Method We used an ethnographic approach to collect data by spending time at a medium-scale company in Scandinavia. We conducted 16 in-depth interviews with representatives of eight different teams, three of which were working on three separate platforms. The collected data was analyzed using Grounded Theory. Results The findings identify four classes of challenges, namely: business challenges, organizational challenges, technical challenges, and people challenges. The article explains how these findings can be used to help researchers and practitioners identify practical solutions and required tool support. Conclusion The organization's decision to adopt a software platform strategy introduced a number of challenges. These challenges need to be understood and addressed in order to reap the benefits of reuse. Researchers need to further investigate issues such as supportive organizational structures for platform development, the role of agile methods in software platforms, tool support for testing and continuous integration in the platform context, and reuse recommendation systems. },
	Doi = {https://doi.org/10.1016/j.infsof.2012.03.005},
	ISSN = {0950-5849},
	Keywords = {Ethnographic study,Grounded Theory,Platform challenges,Software platform,Software reuse},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584912000547}
}

@Article{ISJ:ISJ406,
	Title = {{Balancing platform control and external contribution in third-party development: the boundary resources model}},
	Author = {Ghazawneh, Ahmad and Henfridsson, Ola},
	Journal = {Information Systems Journal},
	Year = {2013},
	Number = {2},
	Pages = {173--192},
	Volume = {23},
	Abstract = {Prior research documents the significance of using platform boundary resources (e.g. application programming interfaces) for cultivating platform ecosystems through third-party development. However, there are few, if any, theoretical accounts of this relationship. To this end, this paper proposes a theoretical model that centres on two drivers behind boundary resources design and use – resourcing and securing – and how these drivers interact in third-party development. We apply the model to a detailed case study of Apple's iPhone platform. Our application of the model not only serves as an illustration of its plausibility but also generates insights about the conflicting goals of third-party development: the maintenance of platform control and the transfer of design capability to third-party developers. We generate four specialised constructs for understanding the actions taken by stakeholders in third-party development: self-resourcing, regulation-based securing, diversity resourcing and sovereignty securing. Our research extends and complements existing platform literature and contributes new knowledge about an alternative form of system development.},
	Doi = {10.1111/j.1365-2575.2012.00406.x},
	ISSN = {1365-2575},
	Keywords = {application programming interfaces (APIs),boundary resources model,platform,resourcing,securing,third-party development},
	Publisher = {Blackwell Publishing Ltd},
	Url = {http://dx.doi.org/10.1111/j.1365-2575.2012.00406.x}
}

@Article{Ghazouani201761,
	Title = {{A survey on cloud service description}},
	Author = {Ghazouani, Souad and Slimani, Yahya},
	Journal = {Journal of Network and Computer Applications},
	Year = {2017},
	Pages = {61--74},
	Volume = {91},
	Abstract = {Abstract Cloud service description (CSD) becomes an active area which attracts the attention of many research organizations, while no standard {\{}CSD{\}} exists. This lack of standardization is caused by the vendor lock-in problem, where cloud providers use various techniques (languages, standards, ontologies, models, etc.) to describe cloud services. Furthermore, there are few studies which dealt with the four aspects of service especially the technical, operational, business, and semantic aspects. Whereas, many other studies dealt only with some of these four aspects. This is the case of {\{}WSDL{\}} (Web Service Description Language) language, which focuses in the technical aspect, but it does not cover business and semantic ones. Our objective in this paper is to identify the foundations of a standardized {\{}CSD{\}} covering the technical, operational, business, and semantic aspects. We present a comparative study of the different approaches that address {\{}CSD{\}} issues. The result of this comparative study shows that {\{}USDL{\}} (Unified Service Description Language) is the appropriate language supporting a service description by covering three aspects (technical, operational, and business), despite its inability to cover the semantic aspect. },
	Doi = {https://doi.org/10.1016/j.jnca.2017.04.013},
	ISSN = {1084-8045},
	Keywords = {Cloud computing,Cloud service,Cloud service description,IaaS service,PaaS service,SaaS service,USDL},
	Url = {http://www.sciencedirect.com/science/article/pii/S1084804517301613}
}

@Article{Ghezzi2013508,
	Title = {{Model-based verification of quantitative non-functional properties for software product lines}},
	Author = {Ghezzi, Carlo and Sharifloo, Amir Molzam},
	Journal = {Information and Software Technology},
	Year = {2013},
	Number = {3},
	Pages = {508--524},
	Volume = {55},
	Abstract = {Evaluating quality attributes of a design model in the early stages of development can significantly reduce the cost and risks of developing a low quality product. To make this possible, software designers should be able to predict quality attributes by reasoning on a model of the system under development. Although there exists a variety of quality-driven analysis techniques for software systems, only a few work address software product lines. This paper describes how probabilistic model checking techniques and tools can be used to verify non-functional properties of different configurations of a software product line. We propose a model-based approach that enables software engineers to assess their design solutions for software product lines in the early stages of development. Furthermore, we discuss how the analysis time can be surprisingly reduced by applying parametric model checking instead of classic model checking. The results show that the parametric approach is able to substantially alleviate the verification time and effort required to analyze non-functional properties of software product lines. },
	Annote = {Special Issue on Software Reuse and Product LinesSpecial Issue on Software Reuse and Product Lines},
	Doi = {https://doi.org/10.1016/j.infsof.2012.07.017},
	ISSN = {0950-5849},
	Keywords = {Non-functional requirements,Parametric verification,Probabilistic model checking,Quality analysis,Software product lines},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584912001516}
}

@Article{Gholami201631,
	Title = {{Cloud migration process—A survey, evaluation framework, and open challenges}},
	Author = {Gholami, Mahdi Fahmideh and Daneshgar, Farhad and Low, Graham and Beydoun, Ghassan},
	Journal = {Journal of Systems and Software},
	Year = {2016},
	Pages = {31--69},
	Volume = {120},
	Abstract = {Abstract Moving mission-oriented enterprise software applications to cloud environments is a crucial {\{}IT{\}} task and requires a systematic approach. The foci of this paper is to provide a detailed review of extant cloud migration approaches from the perspective of the process model. To this aim, an evaluation framework is proposed and used to appraise and compare existing approaches for highlighting their features, similarities, and key differences. The survey distills the status quo and makes a rich inventory of important activities, recommendations, techniques, and concerns that are common in a typical cloud migration process in one place. This enables both academia and practitioners in the cloud computing community to get an overarching view of the process of the legacy application migration to the cloud. Furthermore, the survey identifies a number challenges that have not been yet addressed by existing approaches, developing opportunities for further research endeavours. },
	Doi = {https://doi.org/10.1016/j.jss.2016.06.068},
	ISSN = {0164-1212},
	Keywords = {Cloud computing,Cloud migration,Evaluation framework,Legacy application,Migration methodology,Process model},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121216300966}
}

@Article{SPE:SPE2295,
	Title = {{Pairwise testing for systems with data derived from real-valued variable inputs}},
	Author = {Go, Kyungmin and Kang, Sungwon and Baik, Jongmoon and Kim, Myungchul},
	Journal = {Software: Practice and Experience},
	Year = {2016},
	Number = {3},
	Pages = {381--403},
	Volume = {46},
	Abstract = {Pairwise testing is an effective combinatorial test case generation approach in which test cases are developed to execute all possible pairwise combinations of system inputs. It can help reduce the number of test cases and save testing time yet still effective in finding defects. However, it is very difficult for practitioners to effectively apply pairwise testing in the real world because of the lack of suitable techniques and guidelines. To redress this situation, this paper conducts a case study of applying pairwise testing to system data derived from real-valued variable inputs. In order to apply pairwise testing to this case study, this paper develops a test procedure and a novel partitioning method to test derived data as a na{\"{i}}ve application of the conventional pairwise testing that would produce a huge number of test cases. A comparative evaluation shows that the pairwise testing of the proposed approach is more effective than the random testing with a 12–20{\%} higher fault detection ratio. Based on our experience, guidelines for applying pairwise testing in practice are also presented. Copyright {\textcopyright} 2014 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spe.2295},
	ISSN = {1097-024X},
	Keywords = {INS/GPS system,combinatorial testing,pairwise testing,real-valued variable inputs},
	Url = {http://dx.doi.org/10.1002/spe.2295}
}

@Article{Goedicke2002384,
	Title = {{Domain-specific runtime variability in product line architectures}},
	Author = {Goedicke, M and Pohl, K and Zdun, U},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2002},
	Pages = {384--396},
	Volume = {2425},
	Abstract = {A software product line primarily structures the software architecture around the commonalities of a set of products within a specific organization.Common alities can be implemented in prefabricated components, and product differences are typically treated by well-defined variation points that are actualized later on.Dyna mic, domain-specific aspects, such as ad hoc customization by domian experts, are hard to model with static extension techniques.In this paper, we will discuss open issues for dynamic and domain-specific customizations of product line architectures.W e will also present an indirection architecture based on Component Wrapper objects and message redirection for dynamically composing and customizing generic components for the use in concrete products.As a case study, we will discuss two designs from a Multimedia Home Platform product line: end-user personalization across different new media platforms and customization of interactive applications by content editors. {\textcopyright} Springer-Verlag Berlin Heidelberg 2002},
	Annote = {cited By 5},
	Keywords = {Application programs; Concrete products; Informati,Architecture-based; Generic components; Interacti,Object oriented programming},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944033745{\&}partnerID=40{\&}md5=01db694c08604b3248980fd35eddc119}
}

@InProceedings{Gomaa:2011:DSA:2019136.2019176,
	Title = {{Dynamic Software Adaptation for Service-oriented Product Lines}},
	Author = {Gomaa, Hassan and Hashimoto, Koji},
	Booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
	Year = {2011},
	Address = {New York, NY, USA},
	Pages = {35:1----35:8},
	Publisher = {ACM},
	Series = {SPLC '11},
	Doi = {10.1145/2019136.2019176},
	ISBN = {978-1-4503-0789-5},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2019136.2019176}
}

@Article{Gonzalez-Huerta2013388,
	Title = {{Defining and validating a multimodel approach for product architecture derivation and improvement}},
	Author = {Gonz{\'{a}}lez-Huerta, J and Insfr{\'{a}}n, E and Abrah{\~{a}}o, S},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2013},
	Pages = {388--404},
	Volume = {8107 LNCS},
	Abstract = {Software architectures are the key to achieving the non-functional requirements (NFRs) in any software project. In software product line (SPL) development, it is crucial to identify whether the NFRs for a specific product can be attained with the built-in architectural variation mechanisms of the product line architecture, or whether additional architectural transformations are required. This paper presents a multimodel approach for quality-driven product architecture derivation and improvement (QuaDAI). A controlled experiment is also presented with the objective of comparing the effectiveness, efficiency, perceived ease of use, intention to use and perceived usefulness with regard to participants using QuaDAI as opposed to the Architecture Tradeoff Analysis Method (ATAM). The results show that QuaDAI is more efficient and perceived as easier to use than ATAM, from the perspective of novice software architecture evaluators. However, the other variables were not found to be statistically significant. Further replications are needed to obtain more conclusive results. {\textcopyright} 2013 Springer-Verlag.},
	Annote = {cited By 5},
	Doi = {10.1007/978-3-642-41533-3_24},
	Keywords = {Architectural pattern; Controlled experiment; Mode,Computer software; Experiments; Models; Software,Quality control},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886848508{\&}doi=10.1007{\%}2F978-3-642-41533-3{\_}24{\&}partnerID=40{\&}md5=470a1e83060cea2a038d5adeb4f1f478}
}

@Article{GonzalezHerrera2016398,
	Title = {{ScapeGoat: Spotting abnormal resource usage in component-based reconfigurable software systems}},
	Author = {Gonzalez-Herrera, I and Bourcier, J and Daubert, E and Rudametkin, W and Barais, O and Fouquet, F and J{\'{e}}z{\'{e}}quel, J M and Baudry, B},
	Journal = {Journal of Systems and Software},
	Year = {2016},
	Pages = {398--415},
	Volume = {122},
	Abstract = {Abstract Modern component frameworks support continuous deployment and simultaneous execution of multiple software components on top of the same virtual machine. However, isolation between the various components is limited. A faulty version of any one of the software components can compromise the whole system by consuming all available resources. In this paper, we address the problem of efficiently identifying faulty software components running simultaneously in a single virtual machine. Current solutions that perform permanent and extensive monitoring to detect anomalies induce high overhead on the system, and can, by themselves, make the system unstable. In this paper we present an optimistic adaptive monitoring system to determine the faulty components of an application. Suspected components are finely analyzed by the monitoring system, but only when required. Unsuspected components are left untouched and execute normally. Thus, we perform localized just-in-time monitoring that decreases the accumulated overhead of the monitoring system. We evaluate our approach on two case studies against a state-of-the-art monitoring system and show that our technique correctly detects faulty components, while reducing overhead by an average of 93{\%}. },
	Doi = {https://doi.org/10.1016/j.jss.2016.02.027},
	ISSN = {0164-1212},
	Keywords = {Component,Models@Run.Time,Resource monitoring},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121216000595}
}

@Article{GonzalezHuerta2015405,
	Title = {{Validating a model-driven software architecture evaluation and improvement method: A family of experiments}},
	Author = {Gonzalez-Huerta, Javier and Insfran, Emilio and Abrah{\~{a}}o, Silvia and Scanniello, Giuseppe},
	Journal = {Information and Software Technology},
	Year = {2015},
	Pages = {405--429},
	Volume = {57},
	Abstract = {AbstractContext Software architectures should be evaluated during the early stages of software development in order to verify whether the non-functional requirements (NFRs) of the product can be fulfilled. This activity is even more crucial in software product line (SPL) development, since it is also necessary to identify whether the {\{}NFRs{\}} of a particular product can be achieved by exercising the variation mechanisms provided by the product line architecture or whether additional transformations are required. These issues have motivated us to propose QuaDAI, a method for the derivation, evaluation and improvement of software architectures in model-driven {\{}SPL{\}} development. Objective We present in this paper the results of a family of four experiments carried out to empirically validate the evaluation and improvement strategy of QuaDAI. Method The family of experiments was carried out by 92 participants: Computer Science Master's and undergraduate students from Spain and Italy. The goal was to compare the effectiveness, efficiency, perceived ease of use, perceived usefulness and intention to use with regard to participants using the evaluation and improvement strategy of QuaDAI as opposed to the Architecture Tradeoff Analysis Method (ATAM). Results The main result was that the participants produced their best results when applying QuaDAI, signifying that the participants obtained architectures with better values for the {\{}NFRs{\}} faster, and that they found the method easier to use, more useful and more likely to be used. The results of the meta-analysis carried out to aggregate the results obtained in the individual experiments also confirmed these results. Conclusions The results support the hypothesis that QuaDAI would achieve better results than {\{}ATAM{\}} in the experiments and that QuaDAI can be considered as a promising approach with which to perform architectural evaluations that occur after the product architecture derivation in model-driven {\{}SPL{\}} development processes when carried out by novice software evaluators. },
	Doi = {https://doi.org/10.1016/j.infsof.2014.05.018},
	ISSN = {0950-5849},
	Keywords = {ATAM,Family of experiments,Meta-analysis,Quality attributes,Software architecture evaluation methods,Software architectures},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584914001359}
}

@Article{GonzalezPerez20081288,
	Title = {{A work product pool approach to methodology specification and enactment}},
	Author = {Gonzalez-Perez, Cesar and Henderson-Sellers, Brian},
	Journal = {Journal of Systems and Software},
	Year = {2008},
	Number = {8},
	Pages = {1288--1305},
	Volume = {81},
	Abstract = {Software development methodologies advocated and used today, whether traditional and plan-based or contemporary and agile, usually focus on process steps i.e. they start with requirements and iteratively describe what steps are necessary to move to the next stage or phase, until the software application is delivered to the end user. Such a process-oriented view of methodologies, based on the metaphor that human organizations are “machines? that “execute? processes, often results in methodologies that are too rigid and hard to follow, and most often than not end up being ignored or bypassed. Our proposal here is that, since the ultimate aim of software development is to provide a software product, software development methodologies should be described in terms of the intermediate products that are necessary to reach such a final product, plus the needed micro-processes that, as necessary evils, will be required to produce the appropriate work products from other, previously created ones. Using this product-oriented approach, software development methodologies can be specified that are, at least, as flexible as lightweight, agile approaches and, at the same time, as powerful and scalable as plan-oriented ones. },
	Doi = {https://doi.org/10.1016/j.jss.2007.10.001},
	ISSN = {0164-1212},
	Keywords = {Enactment,ISO/IEC 24744,Metamodelling,Software development methodologies},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121207002439}
}

@Article{SPIP:SPIP413,
	Title = {{Quantitative defects management in iterative development with BiDefect}},
	Author = {Gou, Lang and Wang, Qing and Yuan, Jun and Yang, Ye and Li, Mingshu and Jiang, Nan},
	Journal = {Software Process: Improvement and Practice},
	Year = {2009},
	Number = {4},
	Pages = {227--241},
	Volume = {14},
	Abstract = {Iterative development methodology has been widely adopted in recent years since it is flexible and capable of dealing with requirement volatility. However, how to quantitatively manage iterative projects, and in particular, how to quantitatively manage defects across multiple iterations, remains a challenging issue. In this article, we identify three main challenges of quantitative defects management in iterative development in a leading Chinese telecommunications company (named ZZNode). The three challenges are: identifying appropriate ‘control points' in each iteration, selecting appropriate measures and corresponding measurement methods, and determining the ‘sweet spot' amount of effort for performing testing and defect-fixing activities. We propose a process performance Baselines based iteration Defects management (BiDefect) method to address the three challenges. We also report an industrial experience where several iterative development projects of ZZNode successfully applied the BiDefect method in initial estimating, analyzing, re-estimating, and controlling number of defects and defect detecting/fixing effort. In addition, we provide the evaluation of using BiDefect method, and discuss the benefits and lessons learned from BiDefect and its application. Copyright {\textcopyright} 2009 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spip.413},
	ISSN = {1099-1670},
	Keywords = {defect management,measurement,quantitative process management,software process improvement},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/spip.413}
}

@Article{Groner201483,
	Title = {{Validation of user intentions in process orchestration and choreography}},
	Author = {Gr{\"{o}}ner, Gerd and Asadi, Mohsen and Mohabbati, Bardia and Ga{\v{s}}evi{\'{c}}, Dragan and Bo{\v{s}}kovi{\'{c}}, Marko and Parreiras, Fernando Silva},
	Journal = {Information Systems},
	Year = {2014},
	Pages = {83--99},
	Volume = {43},
	Abstract = {Abstract Goal models and business process models are complementary artifacts for capturing the requirements and their execution flow in software engineering. In this case, goal models serve as input for designing business process models. This requires mappings between both types of models in order to describe which user goals are implemented by which activities in a business process. Due to the large number of possible relationships among goals in the goal model and possible control flows of activities, developers struggle with the challenge of maintaining consistent configurations of both models and their mappings. Managing these mappings manually is error-prone. In our work, we propose an automated solution that relies on Description Logics and automated reasoners for validating mappings that describe the realization of goals by activities in business process models. The results are the identification of two inconsistency patterns – orchestration inconsistency and choreography inconsistency – and the development of the corresponding algorithms for detecting these inconsistencies. },
	Doi = {https://doi.org/10.1016/j.is.2013.05.006},
	ISSN = {0306-4379},
	Keywords = {Goal-oriented process design,Goal-oriented process engineering,Inconsistency detection,Requirement modeling},
	Url = {http://www.sciencedirect.com/science/article/pii/S0306437913000756}
}

@Article{Groner2013709,
	Title = {{Modeling and validation of business process families}},
	Author = {Gr{\"{o}}ner, Gerd and Bo{\v{s}}kovi{\'{c}}, Marko and Parreiras, Fernando Silva and Ga{\v{s}}evi{\'{c}}, Dragan},
	Journal = {Information Systems},
	Year = {2013},
	Number = {5},
	Pages = {709--726},
	Volume = {38},
	Abstract = {Process modeling is an expensive task that needs to encompass requirements of different stakeholders, assure compliance with different standards, and enable the flexible adaptivity to newly emerging requirements in today's dynamic global market. Identifying reusability of process models is a promising direction towards reducing the costs of process modeling. Recent research has offered several solutions. Such solutions promote effective and formally sound methods for variability modeling and configuration management. However, ensuring behavioral validity of reused process models with respect to the original process models (often referred to as reference process models) is still an open research challenge. To address this challenge, in this paper, we propose the notion of business process families by building upon the well-known software engineering discipline—software product line engineering. Business process families comprise (i) a variability modeling perspective, (ii) a process model template (or reference model), and (iii) mappings between (i) and (ii). For business process families, we propose a correct validation algorithm ensuring that each member of a business process family adheres to the core intended behavior that is specified in the process model template. The proposed validation approach is based on the use of Description Logics, variability is represented by using the well-known Feature Models and behavior of process models is considered in terms of control flow patterns. The paper also reports on the experience gained in two external trial cases and results obtained by measuring the tractability of the implementation of the proposed validation approach. },
	Doi = {https://doi.org/10.1016/j.is.2012.11.010},
	ISSN = {0306-4379},
	Keywords = {Business process families,Control flow relations,Process model configuration,Process model variability,Validation},
	Url = {http://www.sciencedirect.com/science/article/pii/S0306437912001524}
}

@Article{deGraaf20141053,
	Title = {{An exploratory study on ontology engineering for software architecture documentation}},
	Author = {de Graaf, K A and Liang, P and Tang, A and van Hage, W R and van Vliet, H},
	Journal = {Computers in Industry},
	Year = {2014},
	Number = {7},
	Pages = {1053--1064},
	Volume = {65},
	Abstract = {Abstract The usefulness of Software Architecture (SA) documentation depends on how well its Architectural Knowledge (AK) can be retrieved by the stakeholders in a software project. Recent findings show that the use of ontology-based {\{}SA{\}} documentation is promising. However, different roles in software development have different needs for AK, and building an ontology to suit these needs is challenging. In this paper we describe an approach to build an ontology for {\{}SA{\}} documentation. This approach involves the use of typical questions for eliciting and constructing an ontology. We outline eight contextual factors, which influence the successful construction of an ontology, especially in complex software projects with diverse {\{}AK{\}} users. We tested our ‘typical question' approach in a case study and report how it can be used for acquiring and modeling {\{}AK{\}} needs. },
	Doi = {https://doi.org/10.1016/j.compind.2014.04.006},
	ISSN = {0166-3615},
	Keywords = {Knowledge acquisition,Knowledge management,Ontology engineering,Ontology-based documentation,Software architecture,Software ontology},
	Url = {http://www.sciencedirect.com/science/article/pii/S0166361514000840}
}

@Article{deGraaf201675,
	Title = {{How organisation of architecture documentation affects architectural knowledge retrieval}},
	Author = {de Graaf, K A and Liang, P and Tang, A and van Vliet, H},
	Journal = {Science of Computer Programming},
	Year = {2016},
	Pages = {75--99},
	Volume = {121},
	Abstract = {Abstract A common approach to software architecture documentation in industry projects is the use of file-based documents. This approach offers a single-dimensional arrangement of the architectural knowledge. Knowledge retrieval from file-based architecture documentation is efficient if the organisation of knowledge supports the needs of the readers; otherwise it can be difficult. In this paper, we compare the organisation and retrieval of architectural knowledge in a file-based documentation approach and an ontology-based documentation approach. The ontology-based approach offers a multi-dimensional organisation of architectural knowledge by means of a software ontology and semantic wiki, whereas file-based documentation typically uses hierarchical organisation by directory structure and table of content. We conducted case studies in two companies to study the efficiency and effectiveness of retrieving architectural knowledge from the different organisations of knowledge. We found that the use of better knowledge organisation correlates with the efficiency and effectiveness of {\{}AK{\}} retrieval. Professionals who used the knowledge organisation found this beneficial. },
	Annote = {Special Issue on Knowledge-based Software Engineering},
	Doi = {https://doi.org/10.1016/j.scico.2015.10.014},
	ISSN = {0167-6423},
	Keywords = {Architectural knowledge retrieval,Ontology-based documentation,Semantic wiki,Software architecture documentation,Software ontologies},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642315003172}
}

@Article{Graves20122633,
	Title = {{Clustering with proximity knowledge and relational knowledge}},
	Author = {Graves, Daniel and Noppen, Joost and Pedrycz, Witold},
	Journal = {Pattern Recognition},
	Year = {2012},
	Number = {7},
	Pages = {2633--2644},
	Volume = {45},
	Abstract = {In this article, a proximity fuzzy framework for clustering relational data is presented, where the relationships between the entities of the data are given in terms of proximity values. We offer a comprehensive and in-depth comparison of our clustering framework with proximity relational knowledge to clustering with distance relational knowledge, such as the well known relational Fuzzy C-Means (FCM). We conclude that proximity can provide a richer description of the relationships among the data and this offers a significant advantage when realizing clustering. We further motivate clustering relational proximity data and provide both synthetic and real-world experiments to demonstrate both the usefulness and advantage offered by clustering proximity data. Finally, a case study of relational clustering is introduced where we apply proximity fuzzy clustering to the problem of clustering a set of trees derived from software requirements engineering. The relationships between trees are based on the degree of closeness in both the location of the nodes in the trees and the semantics associated with the type of connections between the nodes. },
	Doi = {https://doi.org/10.1016/j.patcog.2011.12.019},
	ISSN = {0031-3203},
	Keywords = {Fuzzy clustering,Knowledge representation,Proximity,Relational clustering,Software requirements},
	Url = {http://www.sciencedirect.com/science/article/pii/S0031320311005206}
}

@Article{Grbac2016967,
	Title = {{A quantitative analysis of the unit verification perspective on fault distributions in complex software systems: an operational replication}},
	Author = {Grbac, T G and Runeson, P and Huljeni{\'{c}}, D},
	Journal = {Software Quality Journal},
	Year = {2016},
	Number = {4},
	Pages = {967--995},
	Volume = {24},
	Abstract = {Unit verification, including software inspections and unit tests, is usually the first code verification phase in the software development process. However, principles of unit verification are weakly explored, mostly due to the lack of data, since unit verification data are rarely systematically collected and only a few studies have been published with such data from industry. Therefore, we explore the theory of fault distributions, originating in the quantitative analysis by Fenton and Ohlsson, in the weakly explored context of unit verification in large-scale software development. We conduct a quantitative case study on a sequence of four development projects on consecutive releases of the same complex software product line system for telecommunication exchanges. We replicate the operationalization from earlier studies, analyzed hypotheses related to the Pareto principle of fault distribution, persistence of faults, effects of module size, and quality in terms of fault densities, however, now from the perspective of unit verification. The patterns in unit verification results resemble those of later verification phases, e.g., regarding the Pareto principle, and may thus be used for prediction and planning purposes. Using unit verification results as predictors may improve the quality and efficiency of software verification. {\textcopyright} 2015, Springer Science+Business Media New York.},
	Annote = {cited By 0},
	Doi = {10.1007/s11219-015-9273-7},
	Keywords = {Computer software; Computer software selection and,Empirical research; Replication; Software fault;,Verification},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925610743{\&}doi=10.1007{\%}2Fs11219-015-9273-7{\&}partnerID=40{\&}md5=06a3c6fc1f779d20074176c121d7df1a}
}

@InProceedings{Grechanik:2010:EIL:1852786.1852801,
	Title = {{An Empirical Investigation into a Large-scale Java Open Source Code Repository}},
	Author = {Grechanik, Mark and McMillan, Collin and DeFerrari, Luca and Comi, Marco and Crespi, Stefano and Poshyvanyk, Denys and Fu, Chen and Xie, Qing and Ghezzi, Carlo},
	Booktitle = {Proceedings of the 2010 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
	Year = {2010},
	Address = {New York, NY, USA},
	Pages = {11:1----11:10},
	Publisher = {ACM},
	Series = {ESEM '10},
	Doi = {10.1145/1852786.1852801},
	ISBN = {978-1-4503-0039-1},
	Keywords = {empirical study,large-scale software,mining software repositories,open source,patterns,practice,software repository},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/1852786.1852801}
}

@Article{IIS2:IIS2204,
	Title = {{The Best of Both Worlds: Agile Development Meets Product Line Engineering at Lockheed Martin}},
	Author = {Gregg, Susan P and Scharadin, Rick and Clements, Paul C},
	Journal = {INCOSE International Symposium},
	Year = {2016},
	Number = {1},
	Pages = {951--965},
	Volume = {26},
	Abstract = {Agile development has long been touted as way to optimize software development team efficiency and improve project success. Product line engineering (PLE) brings large-scale improvements in cost, time to market, product quality, and more. Can these two paradigms work in concert with each other? This paper details the experience of Lockheed Martin as it introduced large-scale agile development practices on one of its largest and most successful product line engineering efforts.},
	Doi = {10.1002/j.2334-5837.2016.00204.x},
	ISSN = {2334-5837},
	Url = {http://dx.doi.org/10.1002/j.2334-5837.2016.00204.x}
}

@Article{Groher2009111,
	Title = {{Aspect-oriented model-driven software product line engineering}},
	Author = {Groher, I and Voelter, M},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2009},
	Pages = {111--152},
	Volume = {5560 LNCS},
	Abstract = {Software product line engineering aims to reduce development time, effort, cost, and complexity by taking advantage of the commonality within a portfolio of similar products. The effectiveness of a software product line approach directly depends on how well feature variability within the portfolio is implemented and managed throughout the development lifecycle, from early analysis through maintenance and evolution. This article presents an approach that facilitates variability implementation, management, and tracing by integrating model-driven and aspect-oriented software development. Features are separated in models and composed of aspect-oriented composition techniques on model level. Model transformations support the transition from problem to solution space models. Aspect-oriented techniques enable the explicit expression and modularization of variability on model, template, and code level. The presented concepts are illustrated with a case study of a home automation system. {\textcopyright} 2009 Springer-Verlag Berlin Heidelberg.},
	Annote = {cited By 15},
	Doi = {10.1007/978-3-642-03764-1_4},
	Keywords = {Aspect oriented software development; Aspect-orien,Computer software maintenance,Computer software; Computer systems programming;},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-71549123364{\&}doi=10.1007{\%}2F978-3-642-03764-1{\_}4{\&}partnerID=40{\&}md5=c78bc11a8131a3e7e439b007880e9f1f}
}

@Article{Gu20141086,
	Title = {{Low-disruptive dynamic updating of Java applications}},
	Author = {Gu, Tianxiao and Cao, Chun and Xu, Chang and Ma, Xiaoxing and Zhang, Linghao and L{\"{u}}, Jian},
	Journal = {Information and Software Technology},
	Year = {2014},
	Number = {9},
	Pages = {1086--1098},
	Volume = {56},
	Abstract = {Context In-use software systems are destined to change in order to fix bugs or add new features. Shutting down a running system before updating it is a normal practice, but the service unavailability can be annoying and sometimes unacceptable. Dynamic software updating (DSU) migrates a running software system to a new version without stopping it. State-of-the-art Java {\{}DSU{\}} systems are unsatisfactory as they may cause a non-negligible system pause during updating. Objective In this paper we present Javelus, a Java HotSpot VM-based Java {\{}DSU{\}} system with very short pausing time. Method Instead of updating everything at once when the running application is suspended, Javelus only updates the changed code during the suspension, and migrates stale objects on-demand after the application is resumed. With a careful design this lazy approach neither sacrifices the update flexibility nor introduces unnecessary object validity checks or access indirections. Results Evaluation experiments show that Javelus can reduce the updating pausing time by one to two orders of magnitude without introducing observable overheads before and after the dynamic updating. Conclusion Our experience with Javelus indicates that low-disruptive and type-safe dynamic updating of Java applications can be practically achieved with a lazy updating approach. },
	Annote = {Special Sections from “Asia-Pacific Software Engineering Conference (APSEC), 2012? and “ Software Product Line conference (SPLC), 2012?},
	Doi = {https://doi.org/10.1016/j.infsof.2014.04.003},
	ISSN = {0950-5849},
	Keywords = {Dynamic software updating,JVM,Lazy updating,Low disruption},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584914000846}
}

@Article{Guana2013541,
	Title = {{Improving software product line configuration: A quality attribute-driven approach}},
	Author = {Guana, V and Correal, D},
	Journal = {Information and Software Technology},
	Year = {2013},
	Number = {3},
	Pages = {541--562},
	Volume = {55},
	Abstract = {Context: During the definition of software product lines (SPLs) it is necessary to choose the components that appropriately fulfil a product's intended functionalities, including its quality requirements (i.e., security, performance, scalability). The selection of the appropriate set of assets from many possible combinations is usually done manually, turning this process into a complex, time-consuming, and error-prone task. Objective: Our main objective is to determine whether, with the use of modeling tools, we can simplify and automate the definition process of a SPL, improving the selection process of reusable assets. Method: We developed a model-driven strategy based on the identification of critical points (sensitivity points) inside the SPL architecture. This strategy automatically selects the components that appropriately match the product's functional and quality requirements. We validated our approach experimenting with different real configuration and derivation scenarios in a mobile healthcare SPL where we have worked during the last three years. Results: Through our SPL experiment, we established that our approach improved in nearly 98{\%} the selection of reusable assets when compared with the unassisted analysis selection. However, using our approach there is an increment in the time required for the configuration corresponding to the learning curve of the proposed tools. Conclusion: We can conclude that our domain-specific modeling approach significantly improves the software architect's decision making when selecting the most suitable combinations of reusable components in the context of a SPL. {\textcopyright} 2012 Elsevier B.V. All rights reserved.},
	Annote = {cited By 1},
	Doi = {10.1016/j.infsof.2012.09.007},
	Keywords = {Domain specific modeling; Product-lines; Quality e,Health care; Network architecture; Software archi,Quality control},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872930912{\&}doi=10.1016{\%}2Fj.infsof.2012.09.007{\&}partnerID=40{\&}md5=aabb6af7e8b142cc656943bcfcc95fc2}
}

@InProceedings{Guana:2011:VQE:2019136.2019158,
	Title = {{Variability Quality Evaluation on Component-based Software Product Lines}},
	Author = {Guana, Victor and Correal, Dario},
	Booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
	Year = {2011},
	Address = {New York, NY, USA},
	Pages = {19:1----19:8},
	Publisher = {ACM},
	Series = {SPLC '11},
	Doi = {10.1145/2019136.2019158},
	ISBN = {978-1-4503-0789-5},
	Keywords = {domain specific modeling,model composition,model-driven software product line,quality attribute,sensitivity point},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2019136.2019158}
}

@Article{Guerra20131239,
	Title = {{A reference architecture for organizing the internal structure of metadata-based frameworks}},
	Author = {Guerra, Eduardo and Alves, Felipe and Kulesza, Uir{\'{a}} and Fernandes, Clovis},
	Journal = {Journal of Systems and Software},
	Year = {2013},
	Number = {5},
	Pages = {1239--1256},
	Volume = {86},
	Abstract = {Metadata-based frameworks enable behavior adaptation through the configuration of custom metadata in application classes. Most of the current frameworks used in the industry for building enterprise applications adopt this approach. However, there is a lack of proven techniques for building such kind of framework, allowing for a better organization of its internal structure. In this paper we propose a pattern language and a reference architecture for better organizing the internal structure of metadata-based frameworks, which were defined as a result of a pattern mining process applied to a set of existing open source frameworks. To evaluate the resulting structure generated by the reference architecture application, a case study examined three frameworks developed according to the proposed reference architecture, each one referring to a distinct application domain. The assessment was conducted by using a metrics suite, metrics thresholds derived from a large set of open source metadata-based frameworks, a process for automatic detection of design disharmonies and manual source code analysis. As a result of this study, framework developers can understand and use the proposed reference architecture to develop new frameworks and refactor existing ones. The assessment revealed that the organization provided by the reference architecture is suitable for metadata-based frameworks, helping in the division of responsibility and functionality among their classes. },
	Doi = {https://doi.org/10.1016/j.jss.2012.12.024},
	ISSN = {0164-1212},
	Keywords = {Framework,Metadata,Metadata-based framework,Pattern language,Reference architecture,Software architecture},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121212003366}
}

@Article{Guillen20132294,
	Title = {{A service-oriented framework for developing cross cloud migratable software}},
	Author = {Guill{\'{e}}n, Joaqu{\'{i}}n and Miranda, Javier and Murillo, Juan Manuel and Canal, Carlos},
	Journal = {Journal of Systems and Software},
	Year = {2013},
	Number = {9},
	Pages = {2294--2308},
	Volume = {86},
	Abstract = {Whilst cloud computing has burst into the current scene as a technology that allows companies to access high computing rates at limited costs, cloud vendors have rushed to provide tools that allow developers to build software for their cloud platforms. The software developed with these tools is often tightly coupled to their services and restrictions. Consequently vendor lock in becomes a common problem which multiple cloud users have to tackle in order to exploit the full potential of cloud computing. A scenario where component-based applications are developed for being deployed across several clouds, and each component can independently be deployed in one cloud or another, remains fictitious due to the complexity and the cost of their development. This paper presents a cloud development framework for developing cloud agnostic applications that may be deployed indifferently across multiple cloud platforms. Information about cloud deployment and cloud integration is separated from the source code and managed by the framework. Interoperability between interdependent components deployed in different clouds is achieved by automatically generating services and service clients. This allows software developers to segment their applications into different modules that can easily be deployed and redistributed across heterogeneous cloud platforms. },
	Doi = {https://doi.org/10.1016/j.jss.2012.12.033},
	ISSN = {0164-1212},
	Keywords = {Adaptation,Cloud framework,Cross-cloud applications},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121212003421}
}

@Article{SanchezGuinea2016251,
	Title = {{A systematic review on the engineering of software for ubiquitous systems}},
	Author = {Guinea, Alejandro S{\'{a}}nchez and Nain, Gr{\'{e}}gory and Traon, Yves Le},
	Journal = {Journal of Systems and Software},
	Year = {2016},
	Pages = {251--276},
	Volume = {118},
	Abstract = {Abstract Context: Software engineering for ubiquitous systems has experienced an important and rapid growth, however the vast research corpus makes it difficult to obtain valuable information from it. Objective: To identify, evaluate, and synthesize research about the most relevant approaches addressing the different phases of the software development life cycle for ubiquitous systems. Method: We conducted a systematic literature review of papers presenting and evaluating approaches for the different phases of the software development life cycle for ubiquitous systems. Approaches were classified according to the phase of the development cycle they addressed, identifying their main concerns and limitations. Results: We identified 128 papers reporting 132 approaches addressing issues related to different phases of the software development cycle for ubiquitous systems. Most approaches have been aimed at addressing the implementation, evolution/maintenance, and feedback phases, while others phases such as testing need more attention from researchers. Conclusion: We recommend to follow existing guidelines when conducting case studies to make the studies more reproducible and closer to real life cases. While some phases of the development cycle have been extensively explored, there is still room for research in other phases, toward a more agile and integrated cycle, from requirements to testing and feedback. },
	Doi = {https://doi.org/10.1016/j.jss.2016.05.024},
	ISSN = {0164-1212},
	Keywords = {Development methods,Empirical software engineering,Evidence-based software engineering,Pervasive systems,Research synthesis,Software development cycle,Systematic review,Ubiquitous systems},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121216300553}
}

@Article{Guizzo201477,
	Title = {{A pattern-driven mutation operator for search-based product line architecture design}},
	Author = {Guizzo, G and Colanzi, T E and Vergilio, S R},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2014},
	Pages = {77--91},
	Volume = {8636 LNCS},
	Abstract = {The application of design patterns through mutation operators in search-based design may improve the quality of the architectures produced in the evolution process. However, we did not find, in the literature, works applying such patterns in the optimization of Product Line Architecture (PLA). Existing works offer manual approaches, which are not search-based, and only apply specific patterns in particular domains. Considering this fact, this paper introduces a meta-model and a mutation operator to allow the design patterns application in the search-based PLA design. The model represents suitable scopes, that is, set of architectural elements that are suitable to receive a pattern. The mutation operator is used with a multi-objective and evolutionary approach to obtain PLA alternatives. Quantitative and qualitative analysis of empirical results show an improvement in the quality of the obtained solutions. {\textcopyright} 2014 Springer International Publishing Switzerland.},
	Annote = {cited By 4},
	Doi = {10.1007/978-3-319-09940-8_6},
	Keywords = {Architectural element; Design Patterns; Evolution,Architecture,Design; Software engineering},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958553590{\&}doi=10.1007{\%}2F978-3-319-09940-8{\_}6{\&}partnerID=40{\&}md5=2d80cf1f16b4ae50ca0b8c782e82e48e}
}

@Article{Guizzo2017331,
	Title = {{A multi-objective and evolutionary hyper-heuristic applied to the Integration and Test Order Problem}},
	Author = {Guizzo, Giovani and Vergilio, Silvia R and Pozo, Aurora T R and Fritsche, Gian M},
	Journal = {Applied Soft Computing},
	Year = {2017},
	Pages = {331--344},
	Volume = {56},
	Abstract = {Abstract The field of Search-Based Software Engineering (SBSE) has widely utilized Multi-Objective Evolutionary Algorithms (MOEAs) to solve complex software engineering problems. However, the use of such algorithms can be a hard task for the software engineer, mainly due to the significant range of parameter and algorithm choices. To help in this task, the use of Hyper-heuristics is recommended. Hyper-heuristics can select or generate low-level heuristics while optimization algorithms are executed, and thus can be generically applied. Despite their benefits, we find only a few works using hyper-heuristics in the {\{}SBSE{\}} field. Considering this fact, we describe HITO, a Hyper-heuristic for the Integration and Test Order Problem, to adaptively select search operators while {\{}MOEAs{\}} are executed using one of the selection methods: Choice Function and Multi-Armed Bandit. The experimental results show that {\{}HITO{\}} can outperform the traditional {\{}MOEAs{\}} NSGA-II and MOEA/DD. {\{}HITO{\}} is also a generic algorithm, since the user does not need to select crossover and mutation operators, nor adjust their parameters. },
	Doi = {https://doi.org/10.1016/j.asoc.2017.03.012},
	ISSN = {1568-4946},
	Keywords = {Hyper-heuristic,Metaheuristic,Multi-objective algorithm,Search-Based Software Engineering,Software testing},
	Url = {http://www.sciencedirect.com/science/article/pii/S1568494617301357}
}

@Article{Guo2016204,
	Title = {{A semantic approach for automated test oracle generation}},
	Author = {Guo, Hai-Feng},
	Journal = {Computer Languages, Systems {\&} Structures},
	Year = {2016},
	Pages = {204--219},
	Volume = {45},
	Abstract = {Abstract This paper presents the design, implementation, and applications of a software testing tool, TAO, which allows users to specify and generate test cases and oracles in a declarative way. Extended from its previous grammar-based test generation tool, {\{}TAO{\}} provides a declarative notation for defining denotational semantics on each productive grammar rule, such that when a test case is generated, its expected semantics will be evaluated automatically as well, serving as its test oracle. {\{}TAO{\}} further provides a simple tagging mechanism to embed oracles into test cases for bridging the automation between test case generation and software testing. Two practical case studies are used to illustrate how automated oracle generation can be effectively integrated with grammar-based test generation in different testing scenarios: locating fault-inducing input patterns on Java applications; and Selenium-based automated web testing. },
	Doi = {https://doi.org/10.1016/j.cl.2016.01.006},
	ISSN = {1477-8424},
	Keywords = {Denotational semantics,Software testing,Test case generation,Test oracle},
	Url = {http://www.sciencedirect.com/science/article/pii/S147784241530021X}
}

@Article{SPE:SPE2278,
	Title = {{A dynamic stochastic model for automatic grammar-based test generation}},
	Author = {Guo, Hai-Feng and Qiu, Zongyan},
	Journal = {Software: Practice and Experience},
	Year = {2015},
	Number = {11},
	Pages = {1519--1547},
	Volume = {45},
	Abstract = {Grammar-based test generation provides a systematic approach to producing test cases from a given context-free grammar. Unfortunately, naive grammar-based test generation is problematic because of the fact that exhaustive random test case production is often explosive, and grammar-based test generation with explicit annotation controls often causes unbalanced testing coverage. In this paper, we present an automatic grammar-based test generation approach, which takes a symbolic grammar as input, requires zero control input from users, and produces well-distributed test cases. Our approach utilizes a novel dynamic stochastic model where each variable is associated with a tuple of probability distributions, which are dynamically adjusted along the derivation. We further present a coverage tree illustrating the distribution of generated test cases and their detailed derivations. More importantly, the coverage tree supports various implicit derivation control mechanisms. We implemented this approach in a Java-based system, named Gena. Each test case generated by Gena automatically comes with a set of structural features, which can play an important and effective role on automated failure causes localization. Experimental results demonstrate the effectiveness of our approach, the well-balanced distribution of generated test cases over grammatical structures, and a case study on grammar-based failure causes localization. Copyright {\textcopyright} 2014 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spe.2278},
	ISSN = {1097-024X},
	Keywords = {fault localization,grammar-based test generation,software testing},
	Url = {http://dx.doi.org/10.1002/spe.2278}
}

@Article{Guo20124987,
	Title = {{Consistency maintenance for evolving feature models}},
	Author = {Guo, Jianmei and Wang, Yinglin and Trinidad, Pablo and Benavides, David},
	Journal = {Expert Systems with Applications},
	Year = {2012},
	Number = {5},
	Pages = {4987--4998},
	Volume = {39},
	Abstract = {Software product line (SPL) techniques handle the construction of customized systems. One of the most common representations of the decisions a customer can make in {\{}SPLs{\}} is feature models (FMs). An {\{}FM{\}} represents the relationships among common and variable features in an SPL. Features are a representation of the characteristics in a system that are relevant to customers. {\{}FMs{\}} are subject to change since the set of features and their relationships can change along an {\{}SPL{\}} lifecycle. Due to this evolution, the consistency of {\{}FMs{\}} may be compromised. There exist some approaches to detect and explain inconsistencies in FMs, however this process can take a long time for large FMs. In this paper we present a complementary approach to dealing with inconsistencies in {\{}FM{\}} evolution scenarios that improves the performance for existing approaches reducing the impact of change to the smallest part of an {\{}FM{\}} that changes. To achieve our goal, we formalize {\{}FMs{\}} from an ontological perspective and define constraints that must be satisfied in {\{}FMs{\}} to be consistent. We define a set of primitive operations that modify {\{}FMs{\}} and which are responsible for the {\{}FM{\}} evolution, analyzing their impact on the {\{}FM{\}} consistency. We propose a set of predefined strategies to keep the consistency for error-prone operations. As a proof-of-concept we present the results of our experiments, where we check for the effectiveness and efficiency of our approach in {\{}FMs{\}} with thousands of features. Although our approach is limited by the kinds of consistency constraints and the primitive operations we define, the experiments present a significant improvement in performance results in those cases where they are applicable. },
	Doi = {https://doi.org/10.1016/j.eswa.2011.10.014},
	ISSN = {0957-4174},
	Keywords = {Consistency maintenance,Evolution,Feature models,Ontology,Semantics,Software product lines},
	Url = {http://www.sciencedirect.com/science/article/pii/S0957417411014990}
}

@Article{Guo20112208,
	Title = {{A genetic algorithm for optimized feature selection with resource constraints in software product lines}},
	Author = {Guo, J and White, J and Wang, G and Li, J and Wang, Y},
	Journal = {Journal of Systems and Software},
	Year = {2011},
	Number = {12},
	Pages = {2208--2221},
	Volume = {84},
	Abstract = {Software product line (SPL) engineering is a software engineering approach to building configurable software systems. SPLs commonly use a feature model to capture and document the commonalities and variabilities of the underlying software system. A key challenge when using a feature model to derive a new SPL configuration is determining how to find an optimized feature selection that minimizes or maximizes an objective function, such as total cost, subject to resource constraints. To help address the challenges of optimizing feature selection in the face of resource constraints, this paper presents an approach that uses G enetic A lgorithms for optimized FE ature S election (GAFES) in SPLs. Our empirical results show that GAFES can produce solutions with 86-97{\%} of the optimality of other automated feature selection algorithms and in 45-99{\%} less time than existing exact and heuristic feature selection techniques. {\textcopyright} 2011 Elsevier Inc. All rights reserved.},
	Annote = {cited By 69},
	Doi = {10.1016/j.jss.2011.06.026},
	Keywords = {Automated features; Configurable; Configuration; E,Computer software selection and evaluation; Flow,Feature extraction},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053576823{\&}doi=10.1016{\%}2Fj.jss.2011.06.026{\&}partnerID=40{\&}md5=30f35b5b93f00b26fa9f0a96ec2e89a2}
}

@Article{SMR:SMR459,
	Title = {{An examination of change profiles in reusable and non-reusable software systems}},
	Author = {Gupta, Anita and Cruzes, Daniela and Shull, Forrest and Conradi, Reidar and R{\o}nneberg, Harald and Landre, Einar},
	Journal = {Journal of Software Maintenance and Evolution: Research and Practice},
	Year = {2010},
	Number = {5},
	Pages = {359--380},
	Volume = {22},
	Abstract = {This paper reports on an industrial case study in a large Norwegian Oil and Gas company (StatoilHydro ASA) involving a reusable Java-class framework and two applications that use that framework. We analyzed software changes from three releases of the reusable framework, called Java Enterprise Framework (JEF), and two applications reusing the framework, called Digital Cargo File (DCF) and Shipment and Allocation (S{\&}A). On the basis of our analysis, we found the following: (1) Profiles of change types for the reused framework and the applications are similar, specifically, perfective changes dominate significantly. (2) Although on observing the mean value adaptive changes are more frequent and are active longer in JEF and S{\&}A, these systems went through less refactoring than DCF. For DCF, we saw that preventive changes were more frequent and were active longer. (3) Finally, we found that designing for reuse seems to lead to a long-term payoff in relation to non-reusable software systems. Copyright {\textcopyright} 2010 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/smr.459},
	ISSN = {1532-0618},
	Keywords = {case study,software changes,software quality,software reuse},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/smr.459}
}

@Article{Gurov201369,
	Title = {{Reducing behavioural to structural properties of programs with procedures}},
	Author = {Gurov, Dilian and Huisman, Marieke},
	Journal = {Theoretical Computer Science},
	Year = {2013},
	Pages = {69--103},
	Volume = {480},
	Abstract = {There is an intimate link between program structure and behaviour. Exploiting this link to phrase program correctness problems in terms of the structural properties of a program graph rather than in terms of its unfoldings is a useful strategy for making analyses more tractable. The present paper presents a characterisation of behavioural program properties through sets of structural properties by means of a translation. The characterisation is given in the context of a program model based on control flow graphs of sequential programs with procedures, abstracting away completely from program data, and properties expressed in a fragment of the modal $\mu$ -calculus with boxes and greatest fixed-points only. The property translation is based on a tableau construction that conceptually amounts to symbolic execution of the behavioural formula, collecting structural constraints along the way. By keeping track of the subformulae that have been examined, recursion in the structural constraints can be identified and captured by fixed-point formulae. The tableau construction terminates, and the characterisation is exact, i.e., the translation is sound and complete. A prototype implementation has been developed. In addition, we show how the translation can be extended beyond the basic flow graph model and safety logic to richer behavioural models (such as open programs) and richer program models (including Boolean programs), and discuss possible extensions for more complex logics. We present several applications of the characterisation, in particular sound and complete compositional verification for behavioural properties based on maximal models. },
	Doi = {https://doi.org/10.1016/j.tcs.2013.02.006},
	ISSN = {0304-3975},
	Keywords = {Compositional reasoning,Control-flow behaviour,Control-flow structure,Modal $\mu$ -calculus,Program verification,Safety properties},
	Url = {http://www.sciencedirect.com/science/article/pii/S0304397513001151}
}

@Article{vanGurp2002105,
	Title = {{Design erosion: problems and causes}},
	Author = {van Gurp, Jilles and Bosch, Jan},
	Journal = {Journal of Systems and Software},
	Year = {2002},
	Number = {2},
	Pages = {105--119},
	Volume = {61},
	Abstract = {Design erosion is a common problem in software engineering. We have found that invariably, no matter how ambitious the intentions of the designers were, software designs tend to erode over time to the point that redesigning from scratch becomes a viable alternative compared to prolonging the life of the existing design. In this paper, we illustrate how design erosion works by presenting the evolution of the design of a small software system. In our analysis of this example, we show how design decisions accumulate and become invalid because of new requirements. Also it is argued that even an optimal strategy for designing the system (i.e. no compromises with respect to e.g. cost are made) does not lead to an optimal design because of unforeseen requirement changes that invalidate design decisions that were once optimal. },
	Doi = {https://doi.org/10.1016/S0164-1212(01)00152-2},
	ISSN = {0164-1212},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121201001522}
}

@Article{SPE:SPE366,
	Title = {{Design, implementation and evolution of object oriented frameworks: concepts and guidelines}},
	Author = {van Gurp, J and Bosch, J},
	Journal = {Software: Practice and Experience},
	Year = {2001},
	Number = {3},
	Pages = {277--300},
	Volume = {31},
	Abstract = {Object-oriented frameworks provide software developers with the means to build an infrastructure for their applications. Unfortunately, frameworks do not always deliver on their promises of reusability and flexibility. To address this, we have developed a conceptual model for frameworks and a set of guidelines to build object oriented frameworks that adhere to this model. Our guidelines focus on improving the flexibility, reusability and usability (i.e. making it easy to use a framework) of frameworks. Copyright {\textcopyright} 2001 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spe.366},
	ISSN = {1097-024X},
	Keywords = {design guidelines,object oriented frameworks,role-based modeling},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/spe.366}
}

@Article{SMR:SMR313,
	Title = {{Design preservation over subsequent releases of a software product: a case study of Baan ERP}},
	Author = {van Gurp, Jilles and Brinkkemper, Sjaak and Bosch, Jan},
	Journal = {Journal of Software Maintenance and Evolution: Research and Practice},
	Year = {2005},
	Number = {4},
	Pages = {277--306},
	Volume = {17},
	Abstract = {We present the results of two case studies we conducted at Baan in the Netherlands. At the time of conducting the case studies, Baan was part of Invensys plc. (Baan is now owned by SSA Global Technologies.) In these case studies we investigated how companies identify design erosion and address this in their software, a practice we call ‘design preservation'. In this study, we selected two sub-systems in Baan products that had recently been subjected to extensive maintenance activities because they were eroded. In this paper, we analyze the problems these systems had, how Baan identified that these systems were problematic, and the remedies that were used to address the problems. In addition to confirming some of our earlier conclusions, we have been able to extract some common causes for design erosion problems as well as a number of recommended design preservation practices, which, at least for Baan, have proven to be very effective in strengthening design preservation. Copyright {\textcopyright} 2005 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/smr.313},
	ISSN = {1532-0618},
	Keywords = {adaptive maintenance,architecture erosion,design erosion,enhancive maintenance,software aging,software evolution,software quality},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/smr.313}
}

@Article{SPE:SPE955,
	Title = {{Comparing practices for reuse in integration-oriented software product lines and large open source software projects}},
	Author = {van Gurp, Jilles and Prehofer, Christian and Bosch, Jan},
	Journal = {Software: Practice and Experience},
	Year = {2010},
	Number = {4},
	Pages = {285--312},
	Volume = {40},
	Doi = {10.1002/spe.955},
	ISSN = {1097-024X},
	Keywords = {open source,software development practice,software product lines},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/spe.955}
}

@Article{Hahnle2013109,
	Title = {{HATS abstract behavioral specification: The architectural view}},
	Author = {H{\"{a}}hnle, R and Helvensteijn, M and Johnsen, E B and Lienhardt, M and Sangiorgi, D and Schaefer, I and Wong, P Y H},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2013},
	Pages = {109--132},
	Volume = {7542 LNCS},
	Abstract = {The Abstract Behavioral Specification (ABS) language is a formal, executable, object-oriented, concurrent modeling language intended for behavioral modeling of complex software systems that exhibit a high degree of variation, such as software product lines. We give an overview of the architectural aspects of ABS: a feature-driven development workflow, a formal notion of deployment components for specifying environmental constraints, and a dynamic component model that is integrated into the language. We employ an industrial case study to demonstrate how the various aspects work together in practice. {\textcopyright} 2013 Springer-Verlag Berlin Heidelberg.},
	Annote = {cited By 5},
	Doi = {10.1007/978-3-642-35887-6-6},
	Keywords = {Architectural views; Behavioral specification; Com,Computer software; Industrial applications,Specifications},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883303962{\&}doi=10.1007{\%}2F978-3-642-35887-6-6{\&}partnerID=40{\&}md5=4e652fb7c5f96ab147171a824e04af6b}
}

@Article{Haber2015601,
	Title = {{Systematic synthesis of delta modeling languages}},
	Author = {Haber, A and H{\"{o}}lldobler, K and Kolassa, C and Look, M and M{\"{u}}ller, K and Rumpe, B and Schaefer, I and Schulze, C},
	Journal = {International Journal on Software Tools for Technology Transfer},
	Year = {2015},
	Number = {5},
	Pages = {601--626},
	Volume = {17},
	Abstract = {Delta modeling is a modular, yet flexible approach to capture variability by explicitly representing differences between system variants or versions. The conceptual idea of delta modeling is language-independent. But, to apply delta modeling to a concrete language, either a generic transformation language has to be used or the corresponding delta language has to be manually developed for each considered base language. Generic languages and their tool support often lack readability and specific context condition checking, since they are unrelated to the base language. In this paper, we present a process that allows synthesizing a delta language from the grammar of a given base language. Our method relies on an automatically generated language extension that can be manually adapted to meet domain-specific needs. We illustrate our method using delta modeling on a textual variant of architecture diagrams. Furthermore, we evaluate our method using a comparative case study. This case study covers an architectural, a structural, and a behavioral language and compares the preexisting handwritten grammars to the generated grammars as well as the manually tailored grammars. This paper is an extension of Haber et al. (Proceedings of the 17th international software product line conference (SPLC'13), pp 22–31, 2013). {\textcopyright} 2015, Springer-Verlag Berlin Heidelberg.},
	Annote = {cited By 1},
	Doi = {10.1007/s10009-015-0387-9},
	Keywords = {Computational linguistics; Computer programming la,Delta model; Domain specific languages; Generatio,Modeling languages},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941426977{\&}doi=10.1007{\%}2Fs10009-015-0387-9{\&}partnerID=40{\&}md5=b20a162f1709303e9a6be78314750243}
}

@Article{Haber2012183,
	Title = {{Evolving delta-oriented software product line architectures}},
	Author = {Haber, A and Rendel, H and Rumpe, B and Schaefer, I},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2012},
	Pages = {183--208},
	Volume = {7539 LNCS},
	Abstract = {Diversity is prevalent in modern software systems. Several system variants exist at the same time in order to adapt to changing user requirements. Additionally, software systems evolve over time in order to adjust to unanticipated changes in their application environment. In modern software development, software architecture modeling is an important means to deal with system complexity by architectural decomposition. This leads to the need of architectural description languages that can represent spatial and temporal variability. In this paper, we present delta modeling of software architectures as a uniform modeling formalism for architectural variability in space and in time. In order to avoid degeneration of the product line model under system evolution, we present refactoring techniques to maintain and improve the quality of the variability model. Using a running example from the automotive domain, we evaluate our approach by carrying out a case study that compares delta modeling with annotative variability modeling. {\textcopyright} 2012 Springer-Verlag.},
	Annote = {cited By 9},
	Doi = {10.1007/978-3-642-34059-8_10},
	Keywords = {Application environment; Architectural description,Computer software; Software architecture,Information technology},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868357096{\&}doi=10.1007{\%}2F978-3-642-34059-8{\_}10{\&}partnerID=40{\&}md5=572ca2236365029622667b4844f59dc6}
}

@Article{HafemannFragal2017210,
	Title = {{Validated test models for software product lines: Featured finite state machines}},
	Author = {{Hafemann Fragal}, V and Simao, A and Mousavi, M R},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2017},
	Pages = {210--227},
	Volume = {10231 LNCS},
	Abstract = {Variants of the finite state machine (FSM) model have been extensively used to describe the behaviour of reactive systems. In particular, several model-based testing techniques have been developed to support test case generation and test case executions from FSMs. Most such techniques require several validation properties to hold for the underlying test models. In this paper, we propose an extension of the FSM test model for software product lines (SPLs), named featured finite state machine (FFSM). As the first step towards using FFSMs as test models, we define feature-oriented variants of basic test model validation criteria. We show how the high-level validation properties coincide with the necessary properties on the product FSMs. Moreover, we provide a mechanised tool prototype for checking the feature-oriented properties using satisfiability modulo theory (SMT) solver tools. We investigate the applicability of our approach by applying it to both randomly generated FFSMs as well as those from a realistic case study (the Body Comfort System). The results of our study show that for random FFSMs over 16 independent non-mandatory features, our technique provides substantial efficiency gains for the set of proposed validity checks. {\textcopyright} Springer International Publishing AG 2017.},
	Annote = {cited By 0},
	Doi = {10.1007/978-3-319-57666-4_13},
	Keywords = {Computer software; Finite automata; Model checking,Feature-oriented; Formal modelling; Model based t,Software testing},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018251728{\&}doi=10.1007{\%}2F978-3-319-57666-4{\_}13{\&}partnerID=40{\&}md5=8f239750161fe96551f1877d8040b11a}
}

@Article{Haghighatkhah201725,
	Title = {{Automotive software engineering: A systematic mapping study}},
	Author = {Haghighatkhah, Alireza and Banijamali, Ahmad and Pakanen, Olli-Pekka and Oivo, Markku and Kuvaja, Pasi},
	Journal = {Journal of Systems and Software},
	Year = {2017},
	Pages = {25--55},
	Volume = {128},
	Abstract = {Abstract The automotive industry is going through a fundamental change by moving from a mechanical to a software-intensive industry in which most innovation and competition rely on software engineering competence. Over the last few decades, the importance of software engineering in the automotive industry has increased significantly and has attracted much attention from both scholars and practitioners. A large body-of-knowledge on automotive software engineering has accumulated in several scientific publications, yet there is no systematic analysis of that knowledge. This systematic mapping study aims to classify and analyze the literature related to automotive software engineering in order to provide a structured body-of-knowledge, identify well-established topics and potential research gaps. The review includes 679 articles from multiple research sub-area, published between 1990 and 2015. The primary studies were analyzed and classified with respect to five different dimensions. Furthermore, potential research gaps and recommendations for future research are presented. Three areas, namely system/software architecture and design, qualification testing, and reuse were the most frequently addressed topics in the literature. There were fewer comparative and validation studies, and the literature lacks practitioner-oriented guidelines. Overall, research activity on automotive software engineering seems to have high industrial relevance but is relatively lower in its scientific rigor. },
	Doi = {https://doi.org/10.1016/j.jss.2017.03.005},
	ISSN = {0164-1212},
	Keywords = {Automotive software engineering,Automotive systems,Embedded systems,Literature survey,Software-intensive systems,Systematic mapping study},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121217300560}
}

@Article{Haitzer2014135,
	Title = {{Semi-automated architectural abstraction specifications for supporting software evolution}},
	Author = {Haitzer, Thomas and Zdun, Uwe},
	Journal = {Science of Computer Programming},
	Year = {2014},
	Pages = {135--160},
	Volume = {90, Part B},
	Abstract = {Abstract In this paper we present an approach for supporting the semi-automated architectural abstraction of architectural models throughout the software life-cycle. It addresses the problem that the design and implementation of a software system often drift apart as software systems evolve, leading to architectural knowledge evaporation. Our approach provides concepts and tool support for the semi-automatic abstraction of architecture component and connector views from implemented systems and keeping the abstracted architecture models up-to-date during software evolution. In particular, we propose architecture abstraction concepts that are supported through a domain-specific language (DSL). Our main focus is on providing architectural abstraction specifications in the {\{}DSL{\}} that only need to be changed, if the architecture changes, but can tolerate non-architectural changes in the underlying source code. Once the software architect has defined an architectural abstraction in the DSL, we can automatically generate architectural component views from the source code using model-driven development (MDD) techniques and check whether architectural design constraints are fulfilled by these models. Our approach supports the automatic generation of traceability links between source code elements and architectural abstractions using {\{}MDD{\}} techniques to enable software architects to easily link between components and the source code elements that realize them. It enables software architects to compare different versions of the generated architectural component view with each other. We evaluate our research results by studying the evolution of architectural abstractions in different consecutive versions of five open source systems and by analyzing the performance of our approach in these cases. },
	Annote = {Special Issue on Component-Based Software Engineering and Software Architecture},
	Doi = {https://doi.org/10.1016/j.scico.2013.10.004},
	ISSN = {0167-6423},
	Keywords = {Architectural abstraction,Architectural component and connector views,Model transformation,Software evolution,UML},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642313002542}
}

@Article{Hall2000449,
	Title = {{Feature combination and interaction detection via foreground/background models}},
	Author = {Hall, Robert J},
	Journal = {Computer Networks},
	Year = {2000},
	Number = {4},
	Pages = {449--469},
	Volume = {32},
	Abstract = {One approach to building complex software product families is to partition the possible functions of the system into conceptual chunks called features. Ideally, system instances are rapidly assembled by combining features desired by the particular customer. Unfortunately, features often interact, meaning their combination causes unintended undesirable behavior even though in isolation the features work fine. This paper describes an approach to feature combination and interaction detection via foreground/background models, which allows expressing features as augmentations to the behavior of a base model. It also classifies interactions into three categories, based on how they can be detected, and describes implemented tools which can detect interactions from the three categories. I show why this approach avoids falsely detecting the spurious Type I interactions to which many existing approaches are prone. The tools and methodology, as well as the prevalence of spurious interactions in existing approaches, are illustrated through application to telephony features from the feature interaction contest associated with FIW'98. This data provides evidence that the foreground/background approach catches more nonspurious interactions, with less human effort, than competing approaches. },
	Doi = {https://doi.org/10.1016/S1389-1286(00)00010-4},
	ISSN = {1389-1286},
	Keywords = {Feature interaction,Formal methods,Software validation},
	Url = {http://www.sciencedirect.com/science/article/pii/S1389128600000104}
}

@Article{SMR:SMR491,
	Title = {{Capturing variability in business process models: the Provop approach}},
	Author = {Hallerbach, Alena and Bauer, Thomas and Reichert, Manfred},
	Journal = {Journal of Software Maintenance and Evolution: Research and Practice},
	Year = {2010},
	Number = {6-7},
	Pages = {519--546},
	Volume = {22},
	Abstract = {Usually, for a particular business process different variants exist. Each of them constitutes an adjustment of a reference process model to specific requirements building the process context. Contemporary process management tools do not adequately support the modeling of such process variants. Either the variants have to be specified as separate process models or they are expressed in terms of conditional branches within the same process model. Both methods often lead to redundancies making model adaptations a time-consuming and error-prone task. In this article, we discuss selected concepts of the Provop approach for modeling and managing process variants. A particular process variant can be configured at a high level of abstraction by applying a set of well-defined change operations to a reference process model. In particular, this article discusses advanced concepts for the design and modeling of such a reference process model as well as for the adjustments required to configure the different process variants. Altogether, Provop provides a flexible and powerful solution for process variant management. Copyright {\textcopyright} 2010 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/smr.491},
	ISSN = {1532-0618},
	Keywords = {process adaptation,process configuration,process design,process reference model,process variant},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/smr.491}
}

@Article{Hallsteinsen20122840,
	Title = {{A development framework and methodology for self-adapting applications in ubiquitous computing environments}},
	Author = {Hallsteinsen, S and Geihs, K and Paspallis, N and Eliassen, F and Horn, G and Lorenzo, J and Mamelli, A and Papadopoulos, G A},
	Journal = {Journal of Systems and Software},
	Year = {2012},
	Number = {12},
	Pages = {2840--2859},
	Volume = {85},
	Abstract = {Today software is the main enabler of many of the appliances and devices omnipresent in our daily life and important for our well being and work satisfaction. It is expected that the software works as intended, and that the software always and everywhere provides us with the best possible utility. This paper discusses the motivation, technical approach, and innovative results of the {\{}MUSIC{\}} project. {\{}MUSIC{\}} provides a comprehensive software development framework for applications that operate in ubiquitous and dynamic computing environments and adapt to context changes. Context is understood as any information about the user needs and operating environment which vary dynamically and have an impact on design choices. {\{}MUSIC{\}} supports several adaptation mechanisms and offers a model-driven application development approach supported by a sophisticated middleware that facilitates the dynamic and automatic adaptation of applications and services based on a clear separation of business logic, context awareness and adaptation concerns. The main contribution of this paper is a holistic, coherent presentation of the motivation, design, implementation, and evaluation of the {\{}MUSIC{\}} development framework and methodology. },
	Annote = {Self-Adaptive Systems},
	Doi = {https://doi.org/10.1016/j.jss.2012.07.052},
	ISSN = {0164-1212},
	Keywords = {Adaptive software,Middleware,Mobile computing,Model-driven development,Ubiquitous computing},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121212002245}
}

@Article{Han2015133,
	Title = {{Fast Directional Handoff and lightweight retransmission protocol for enhancing multimedia quality in indoor {\{}WLANs{\}}}},
	Author = {Han, Sangyup and Kim, Myungchul and Lee, Ben and Kang, Sungwon},
	Journal = {Computer Networks},
	Year = {2015},
	Pages = {133--147},
	Volume = {79},
	Abstract = {Abstract More and more mobile devices such as smartphones are being used with {\{}IEEE{\}} 802.11 wireless {\{}LANs{\}} (WLANs or Wi-Fi). However, mobile users are still experiencing poor service quality on the move due to the large handoff delay and packet loss problem. In order to reduce the delay, a new handoff scheme using the geomagnetic sensor embedded in mobile devices is proposed in this paper. The proposed scheme predicts the movement direction of a Mobile Station (MS) from the currently associated Access Point (AP) and performs active scanning with a reduced number of channels. In terms of the packet loss, a lightweight retransmission protocol is also proposed to minimize lost packets on Wi-Fi without producing a lot of acknowledgement packets. The proposed approaches are implemented on Android smartphones, and their performance is evaluated in a real indoor {\{}WLAN{\}} environment. The evaluation results demonstrate that the proposed schemes maintain seamless quality for real-time video even in an environment with frequent handoffs. Note that the proposed schemes are a client-only solution and do not require modification of the existing APs, which renders them very practical. },
	Doi = {https://doi.org/10.1016/j.comnet.2014.12.019},
	ISSN = {1389-1286},
	Keywords = {Digital compass,Fast Directional Handoff,Geomagnetic sensor,Lightweight retransmission protocol,{\{}IEEE{\}} 802.11},
	Url = {http://www.sciencedirect.com/science/article/pii/S1389128615000031}
}

@Article{Hansen20132511,
	Title = {{Reachability analysis of complex planar hybrid systems}},
	Author = {Hansen, Hallstein A and Schneider, Gerardo and Steffen, Martin},
	Journal = {Science of Computer Programming},
	Year = {2013},
	Number = {12},
	Pages = {2511--2536},
	Volume = {78},
	Abstract = {Abstract Hybrid systems are systems that exhibit both discrete and continuous behavior. Reachability, the question of whether a system in one state can reach some other state, is undecidable for hybrid systems in general. In this paper we are concerned with GSPDIs, 2-dimensional systems generalizing {\{}SPDIs{\}} (planar hybrid systems based on “simple polygonal differential inclusions?), for which reachability have been shown to be decidable. {\{}GSPDIs{\}} are useful to approximate 2-dimensional control systems, allowing the verification of safety properties of such systems. In this paper we present the following two contributions: (i) an optimized algorithm that answers reachability questions for GSPDIs, where all cycles in the reachability graph are accelerated. (ii) An algorithm by which more complex planar hybrid automata are over-approximated by {\{}GSPDIs{\}} subject to two measures of precision. We prove soundness, completeness, and termination of both algorithms, and discuss their implementation. },
	Annote = {Special Section on International Software Product Line Conference 2010 and Fundamentals of Software Engineering (selected papers of {\{}FSEN{\}} 2011)},
	Doi = {https://doi.org/10.1016/j.scico.2013.02.007},
	ISSN = {0167-6423},
	Keywords = {Differential inclusions,Hybrid systems,Non-linear systems,Reachability checking,Safety verification},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642313000427}
}

@Article{Hanssen20121455,
	Title = {{A longitudinal case study of an emerging software ecosystem: Implications for practice and theory}},
	Author = {Hanssen, G K},
	Journal = {Journal of Systems and Software},
	Year = {2012},
	Number = {7},
	Pages = {1455--1466},
	Volume = {85},
	Abstract = {Software ecosystems is an emerging trend within the software industry, implying a shift from closed organizations and processes towards open structures, where actors external to the software development organization are becoming increasingly involved in development. This forms an ecosystem of organizations that are related through the shared interest in a software product, leading to new opportunities and new challenges to the industry and its organizational environment. To understand why and how this change occurs, we have followed the development of a software product line organization for a period of approximately five years. We have studied their change from a waterfall-like approach, via agile software product line engineering, towards an emerging software ecosystem. We discuss implications for practice, and propose a nascent theory on software ecosystems. We conclude that the observed change has led to an increase in collaboration across (previously closed) organizational borders, and to the development of a shared value consisting of two components: the technology (the product line, as an extensible platform), and the business domain it supports. Opening up both the technical interface of the product and the organizational interfaces are key enablers of such a change. {\textcopyright} 2012 Elsevier Inc. All rights reserved.},
	Annote = {cited By 39},
	Doi = {10.1016/j.jss.2011.04.020},
	Keywords = {Agile software development; Agile softwares; Busin,Ecosystems,Software design},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79958283622{\&}doi=10.1016{\%}2Fj.jss.2011.04.020{\&}partnerID=40{\&}md5=8fb17817169ad361e9012b12f779eaba}
}

@Article{Hanssen2011883,
	Title = {{Agile software product line engineering: Enabling factors}},
	Author = {Hanssen, G K},
	Journal = {Software - Practice and Experience},
	Year = {2011},
	Number = {8},
	Pages = {883--897},
	Volume = {41},
	Abstract = {This paper reports on a study of a software product line organization that has adopted agile software development to address process rigidity and slowing performance. Experience has showed that despite some impediments, this has become a valuable change to both the organization and its development process. The aim of this study is to identify and understand enabling factors of a combined process, and to understand their subsequent effects. Qualitative data are summarized and analyzed, giving insight into the actions taken, their effects that have emerged over time, and the enabling and contextual factors. The study concludes that a combined process is feasible, that the simplified approach makes the organization more flexible and thus capable of serving a volatile market with fast-changing technologies. This has also enabled the organization to collaborate better with external actors. Copyright {\textcopyright} 2011 John Wiley {\&} Sons, Ltd.},
	Annote = {cited By 1},
	Doi = {10.1002/spe.1064},
	Keywords = {Agile software development; Agile softwares; Combi,Industrial applications; Production engineering,Software design},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79958281206{\&}doi=10.1002{\%}2Fspe.1064{\&}partnerID=40{\&}md5=c50cda5a08c3f2e8eb05f37a2cdc5aa3}
}

@Article{Hanssen2008843,
	Title = {{Process fusion: An industrial case study on agile software product line engineering}},
	Author = {Hanssen, G K and F{\ae}gri, T E},
	Journal = {Journal of Systems and Software},
	Year = {2008},
	Number = {6},
	Pages = {843--854},
	Volume = {81},
	Abstract = {This paper presents a case study of a software product company that has successfully integrated practices from software product line engineering and agile software development. We show how practices from the two fields support the company's strategic and tactical ambitions, respectively. We also discuss how the company integrates strategic, tactical and operational processes to optimize collaboration and consequently improve its ability to meet market needs, opportunities and challenges. The findings from this study are relevant to software product companies seeking ways to balance agility and product management. The findings also contribute to research on industrializing software engineering. {\textcopyright} 2007 Elsevier Inc. All rights reserved.},
	Annote = {cited By 45},
	Doi = {10.1016/j.jss.2007.10.025},
	Keywords = {Decision theory; Optimization; Product design; Str,Operational processes; Product management,Software engineering},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-42049088680{\&}doi=10.1016{\%}2Fj.jss.2007.10.025{\&}partnerID=40{\&}md5=70bc5b3b44f009b164aebff9608f2e82}
}

@InProceedings{Harman:2014:SBS:2648511.2648513,
	Title = {{Search Based Software Engineering for Software Product Line Engineering: A Survey and Directions for Future Work}},
	Author = {Harman, M and Jia, Y and Krinke, J and Langdon, W B and Petke, J and Zhang, Y},
	Booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
	Year = {2014},
	Address = {New York, NY, USA},
	Pages = {5--18},
	Publisher = {ACM},
	Series = {SPLC '14},
	Doi = {10.1145/2648511.2648513},
	ISBN = {978-1-4503-2740-4},
	Keywords = {SBSE,SPL,genetic programming,program synthesis},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2648511.2648513}
}

@Article{SMR:SMR1785,
	Title = {{Towards a multi-criteria decision support method for consumer electronics software ecosystems}},
	Author = {Hartmann, Herman and Bosch, Jan},
	Journal = {Journal of Software: Evolution and Process},
	Year = {2016},
	Number = {6},
	Pages = {460--482},
	Volume = {28},
	Abstract = {Many consumer electronics firms are adopting an ecosystem-centric approach for supporting third-party applications. In an emerging market, a consumer electronics firm may need to create a new ecosystem or adopt a newly developed platform, both which has significant commercial and technical implications.In this paper we identify three types of ecosystems that are used today: vertically integrated hardware/software platforms, closed-source software platforms, and open-source software platforms. We introduce a first step towards a multi-criteria decision support method, which determines what type of ecosystem is most suitable for a specific product category from a software engineering perspective.We use this method to analyze a wide range of consumer electronics products. The analysis shows that the vertically integrated platform type is most suitable for product with a high degree of innovation, and open-source software platforms are more suitable when a large amount of variants are needed. The closed-source software platform type is less suitable for most types of consumer electronics devices.This first step towards a full-fledged decision support method can be used by platform owners to decide whether their platform can also be successfully transferred to another product type.},
	Doi = {10.1002/smr.1785},
	ISSN = {2047-7481},
	Keywords = {consumer electronics,ecosystems,embedded systems,platform leadership,software architectures},
	Url = {http://dx.doi.org/10.1002/smr.1785}
}

@Article{Hartmann20132313,
	Title = {{Using MDA for integration of heterogeneous components in software supply chains}},
	Author = {Hartmann, H and Keren, M and Matsinger, A and Rubin, J and Trew, T and Yatzkar-Haham, T},
	Journal = {Science of Computer Programming},
	Year = {2013},
	Number = {12},
	Pages = {2313--2330},
	Volume = {78},
	Abstract = {Software product lines are increasingly built using components from specialized suppliers. A company that is in the middle of a supply chain has to integrate components from its suppliers and offer (partially configured) products to its customers. To satisfy both the variability required by each customer and the variability required to satisfy different customers' needs, it may be necessary for such a company to use components from different suppliers, partly offering the same feature set. This leads to a product line with alternative components, possibly using different mechanisms for interfacing, binding and variability, which commonly occurs in embedded software development. In this paper, we describe the limitations of the current practice of combining heterogeneous components in a product line and describe the challenges that arise from software supply chains. We introduce a model-driven approach for automating the integration between components that can generate a partially or fully configured variant, including glue between mismatched components. We analyze the consequences of using this approach in an industrial context, using a case study derived from an existing supply chain and describe the process and roles associated with this approach. {\textcopyright} 2012 Elsevier B.V. All rights reserved.},
	Annote = {cited By 5},
	Doi = {10.1016/j.scico.2012.04.004},
	File = {:Users/mac/Downloads/Sciencedirect{\_}articles{\_}13Jun2017{\_}15-32-18.445/Using-MDA-for-integration-of-heterogeneous-components-in-software-supply-chains{\_}2013{\_}Science-of-Computer-Programming.pdf:pdf},
	Keywords = {Component technologies; Model-driven Engineering;,Computer software; Customer satisfaction; Industr,Supply chains},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884673619{\&}doi=10.1016{\%}2Fj.scico.2012.04.004{\&}partnerID=40{\&}md5=84e11f2c4726419a91a5e5660508d0e6}
}

@Article{Hartmann2012178,
	Title = {{The changing industry structure of software development for consumer electronics and its consequences for software architectures}},
	Author = {Hartmann, Herman and Trew, Tim and Bosch, Jan},
	Journal = {Journal of Systems and Software},
	Year = {2012},
	Number = {1},
	Pages = {178--192},
	Volume = {85},
	Abstract = {During the last decade the structure of the consumer electronics industry has been changing profoundly. Current consumer electronics products are built using components from a large variety of specialized firms, whereas previously each product was developed by a single, vertically integrated company. Taking a software development perspective, we analyze the transition in the consumer electronics industry using case studies from digital televisions and mobile phones. We introduce a model consisting of five industry structure types and describe the forces that govern the transition between types and we describe the consequences for software architectures. We conclude that, at this point in time, software supply chains are the dominant industry structure for developing consumer electronics products. This is because the modularization of the architecture is limited, due to the lack of industry-wide standards and because resource constrained devices require variants of supplied software that are optimized for different hardware configurations. Due to these characteristics open ecosystems have not been widely adopted. The model and forces can serve the decision making process for individual companies that consider the transition to a different type of industry structure as well as provide a framework for researchers studying the software-intensive industries. },
	Annote = {Dynamic Analysis and Testing of Embedded Software},
	Doi = {https://doi.org/10.1016/j.jss.2011.08.007},
	ISSN = {0164-1212},
	Keywords = {Case study,Consumer electronics,Ecosystems,Embedded systems,Industry structures,Mobile phones,Software architecture,Software evolution,Software management,Software supply chains},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121211002081}
}

@Article{Hauksdottir2014952,
	Title = {{Identified adjustability dimensions when generating a product specific requirements specification by requirements reuse}},
	Author = {Hauksd{\'{o}}ttir, Dagn{\'{y}} and Mortensen, Niels Henrik and Nielsen, Poul Erik},
	Journal = {Computers in Industry},
	Year = {2014},
	Number = {6},
	Pages = {952--966},
	Volume = {65},
	Abstract = {Abstract A requirements reuse setups typically includes reusable requirement set(s) containing a collection of reusable requirements and a number of product specific requirements sets which are drawn from the reusable set(s). The ideal scenario when reusing requirements is that all the product requirements can be drawn directly from the reusable set. However, this is rarely the case in product development as new requirements are likely to surface. A critical issue in requirements reuse therefore becomes how to enable products to efficiently reuse requirements as well incorporating changes to the product set. In this paper the objective is not to present a specific method for requirements reuse but to introduce and discuss the possible dimensions of adjustability when generating a product requirement set by reusing requirements from a reusable set. Six adjustability dimensions have been identified. An extensive state of the art is included to introduce the presented methods related to each adjustability dimensions. The options for implementing each adjustability dimensions in a requirement reuse approach are illustrated along with a discussion regarding the benefits and issues resulting from each option. This discussion should help practitioners to better understand the possible methods that can be implemented and to design a user friendly and sustainable approach. A case study, describing how the dimensions are incorporated in two requirements reuse approaches, for Danfoss Solar Inverters (SI) and Danfoss Frequency Drives is provided. As a result an overview of how each adjustability dimensions is implemented in each case is presented. The case study demonstrates that all the identified adjustability dimensions were important elements in requirements reuse implementation. The case study furthermore highlights the need, not only to understand the effects of each adjustability dimension but also of the dependencies to case specific criterions. The classification of adjustability dimensions in requirements reuse and the options for their implementation has not been outlined by previous research and should be a useful contribution both to researchers and practitioners working in the field of requirements reuse. },
	Doi = {https://doi.org/10.1016/j.compind.2014.02.011},
	ISSN = {0166-3615},
	Keywords = {Adjustability dimensions,Product development,Requirements reuse,Requirements specification},
	Url = {http://www.sciencedirect.com/science/article/pii/S0166361514000451}
}

@Article{IIS2:IIS2132,
	Title = {{Model-based Product Line Engineering -- Enabling Product Families with Variants}},
	Author = {Hause, Matthew and Hummell, James},
	Journal = {INCOSE International Symposium},
	Year = {2015},
	Number = {1},
	Pages = {1320--1332},
	Volume = {25},
	Abstract = {Product Lines are a group of related products manufactured or produced within or between collaborating organizations. To effectively manage a product line, one needs to understand both the similarities and differences between the different products and optimize the development lifecycle to leverage the similarities, and concentrate development on the differences. ISO 26550:2013 Software {\&} Systems Engineering – Reference Model for Product Line Management {\&} Engineering provides a standard for defining these similarities and differences as well as the choices between them. Model-Based Systems and Software Engineering (MBSE) using the Systems Modeling Language (SysML) and the Unified Modeling Language (UML) provide a means of modeling systems and software. Bringing the two together allows users to model product lines in industry standard formats. These standards provide Model-Based Product Line Engineering (MB-PLE). Combining these with an execution engine means that product models can be created for specific products, whilst maintaining the original product line model. This provides significant ROI for aerospace companies.},
	Doi = {10.1002/j.2334-5837.2015.00132.x},
	ISSN = {2334-5837},
	Url = {http://dx.doi.org/10.1002/j.2334-5837.2015.00132.x}
}

@Article{IIS2:IIS2305,
	Title = {{Decision-Driven Product Development}},
	Author = {Hause, Matthew and Korff, Andreas},
	Journal = {INCOSE International Symposium},
	Year = {2016},
	Number = {1},
	Pages = {2446--2461},
	Volume = {26},
	Abstract = {Product Line Engineering (PLE) is the engineering and management of a group of related products using a shared set of assets and a means of design and manufacturing. PLE can include system and software, assets and involves all aspects of engineering including electrical, electronic, mechanical, chemical, etc. PLE is normally considered after the product has evolved and complexity becomes too much to manage. Leveraging PLE from the very beginning will identify cost savings and commonality and provide a natural means for product evolution. Orthogonal Variability Modeling (OVM) provides a natural decision set allowing engineers to perform trade-offs for specific customers and guide system development along the most effective route. Using automotive examples, this paper will describe Model-based Product Line Engineering, the process for creating product lines, and the benefits of this approach as applicable to the military ground vehicle domain. Finally, it will show how the adoption of MB-PLE early on in the development lifecycle provides more benefits without the potential disruption and re-engineering that can be involved when it is adopted later on in the lifecycle.},
	Doi = {10.1002/j.2334-5837.2016.00305.x},
	ISSN = {2334-5837},
	Url = {http://dx.doi.org/10.1002/j.2334-5837.2016.00305.x}
}

@Article{SPE:SPE728,
	Title = {{Finding and documenting the specialization interface of an application framework}},
	Author = {Hautam{\"{a}}ki, Juha and Koskimies, Kai},
	Journal = {Software: Practice and Experience},
	Year = {2006},
	Number = {13},
	Pages = {1443--1465},
	Volume = {36},
	Abstract = {This paper presents an approach to find, specify and use the specialization interface of an object-oriented framework as a set of framework-specific patterns. The approach is based on the assumption that the user tries to reuse a framework by setting meaningful goals in the context of their application and then achieves the goals by performing a sequence of programming tasks. The goals can be refined as informal specialization patterns, which are framework-specific descriptions on how to reach a particular specialization goal. Furthermore, the obtained specialization patterns can be transformed into more precise specifications to enable tool support. As a result, the framework user can use both a cookbook-like informal documentation and supporting tools to specialize the framework. Copyright {\textcopyright} 2006 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spe.728},
	ISSN = {1097-024X},
	Keywords = {framework,framework specialization,pattern,product-line architecture,variability management,variation point},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/spe.728}
}

@Article{Heidenreich201069,
	Title = {{Relating feature models to other models of a software product line: A comparative study of FeatureMapper and VML}},
	Author = {Heidenreich, F and S{\'{a}}nchez, P and Santos, J and Zschaler, S and Alf{\'{e}}rez, M and Ara{\'{u}}jo, J and Fuentes, L and Kulesza, U and Moreira, A and Rashid, A},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2010},
	Pages = {69--114},
	Volume = {6210 LNCS},
	Abstract = {Software product lines using feature models often require the relation between feature models in problem space and the models used to describe the details of the product line to be expressed explicitly. This is particularly important, where automatic product derivation is required. Different approaches for modelling this mapping have been proposed in the literature. However, a discussion of their relative benefits and drawbacks is currently missing. As a first step towards a better understanding of this field, this paper applies two of these approaches-FeatureMapper as a representative of declarative approaches and VML* as a representative of operational approaches-to the case study. We show in detail how the case study can be expressed using these approaches and discuss strengths and weaknesses of the two approaches with regard to the case study. {\textcopyright} 2010 Springer-Verlag.},
	Annote = {cited By 20},
	Doi = {10.1007/978-3-642-16086-8_3},
	Keywords = {Comparative studies; Feature models; Problem space,Computer systems programming; Network architectur,Software design},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649878322{\&}doi=10.1007{\%}2F978-3-642-16086-8{\_}3{\&}partnerID=40{\&}md5=a4aa098925031225dfa6e2c3468ffc6b}
}

@Article{Heider2010758,
	Title = {{Simulating evolution in model-based product line engineering}},
	Author = {Heider, Wolfgang and Froschauer, Roman and Gr{\"{u}}nbacher, Paul and Rabiser, Rick and Dhungana, Deepak},
	Journal = {Information and Software Technology},
	Year = {2010},
	Number = {7},
	Pages = {758--769},
	Volume = {52},
	Abstract = {Context Numerous approaches are available for modeling product lines and their variability. However, the long-term impacts of model-based development on maintenance effort and model complexity can hardly be investigated due to a lack of empirical data. Conducting empirical research in product line engineering is difficult as companies are typically reluctant to provide access to data from their product lines. Also, many benefits of product lines can be measured only in longitudinal studies, which are difficult to perform in most environments. Objective In this paper, we thus aim to explore the benefit of simulation to investigate the evolution of model-based product lines. Method We present a simulation approach for exploring the effects of product line evolution on model complexity and maintenance effort. Our simulation considers characteristics of product lines (e.g., size, dependencies in models) and we experiment with different evolution profiles (e.g., technical refactoring vs. placement of new products). Results We apply the approach in a simulation experiment that uses data from real-world product lines from the domain of industrial automation systems to demonstrate its feasibility. Conclusion Our results demonstrate that simulation contributes to understanding the effects of maintenance and evolution in model-based product lines. },
	Doi = {https://doi.org/10.1016/j.infsof.2010.03.007},
	ISSN = {0950-5849},
	Keywords = {Industrial automation systems,Maintenance and evolution,Model-based development,Product line engineering,Simulation},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584910000479}
}

@Article{SMR:SMR560,
	Title = {{A review of methods for evaluation of maturity models for process improvement}},
	Author = {Helgesson, Yeni Yuqin Li and H{\"{o}}st, Martin and Weyns, Kim},
	Journal = {Journal of Software: Evolution and Process},
	Year = {2012},
	Number = {4},
	Pages = {436--454},
	Volume = {24},
	Abstract = {Maturity models are widely used in process improvement. The users of a maturity model should be confident that the weak points of the assessed processes can be found, and that the most valuable changes are introduced. Therefore, the evaluation of maturity models is an important activity. In this paper, a mapping study of the literature on the evaluation of maturity models is presented. Two databases are searched resulting in a set of relevant papers. The identified papers can be classified according to six categories, namely the maturity model under evaluation, type of evaluation, relation of the evaluators/authors to the maturity model, level of objectivity, main purpose of the paper, and size of study. Further, a framework of different evaluations of maturity models is developed, and the relevant papers are mapped to the framework. Finally, the relevant research on the evaluation of the maturity models in the Capability Maturity Model family is discussed in more detail. The result of this mapping study is a clear overview of how the evaluation of maturity models has been done, and some discussions are provided for further research on the evaluation of commonly used or newly developed maturity models. Copyright {\textcopyright} 2011 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/smr.560},
	ISSN = {2047-7481},
	Keywords = {evaluation,mapping study,maturity models,software process improvement},
	Publisher = {John Wiley {\&} Sons, Ltd},
	Url = {http://dx.doi.org/10.1002/smr.560}
}

@Article{Henard2014,
	Title = {{Bypassing the Combinatorial Explosion: Using Similarity to Generate and Prioritize T-Wise Test Configurations for Software Product Lines}},
	Author = {Henard, Christopher and Papadakis, Mike and Perrouin, Gilles and Klein, Jacques and Heymans, Patrick and {Le Traon}, Yves},
	Journal = {IEEE Transactions on Software Engineering},
	Year = {2014},
	Month = {jul},
	Number = {7},
	Pages = {650--670},
	Volume = {40},
	Doi = {10.1109/TSE.2014.2327020},
	File = {:Users/mac/Downloads/bulk-download (7)/Bypassing the Combinatorial Explosion Using Similarity to Generate and Prioritize T-Wise Test Configurations for Software Product Lines.pdf:pdf},
	ISSN = {0098-5589},
	Url = {http://ieeexplore.ieee.org/document/6823132/}
}

@Article{Her2007740,
	Title = {{A framework for evaluating reusability of core asset in product line engineering}},
	Author = {Her, Jin Sun and Kim, Ji Hyeok and Oh, Sang Hun and Rhew, Sung Yul and Kim, Soo Dong},
	Journal = {Information and Software Technology},
	Year = {2007},
	Number = {7},
	Pages = {740--760},
	Volume = {49},
	Abstract = {Product line engineering (PLE) is a new effective approach to software reuse, where applications are generated by instantiating a core asset which is a large-grained reuse unit. Hence, a core asset is a key element of PLE, and therefore the reusability of the core asset largely determines the success of {\{}PLE{\}} projects. However, current quality models to evaluate reusability do not adequately address the unique characteristics of core assets in PLE. This paper proposes a comprehensive framework for evaluating the reusability of core assets. We first identify the key characteristics of core assets, and derive a set of quality attributes that characterizes the reusability of core assets. Then, we define metrics for each quality attribute and finally present practical guidelines for applying the evaluation framework in {\{}PLE{\}} projects. Using the proposed framework, the reusability of core assets can be more effectively and precisely evaluated. },
	Doi = {https://doi.org/10.1016/j.infsof.2006.08.008},
	ISSN = {0950-5849},
	Keywords = {Core asset,Metric,Product line engineering,Quality model,Reusability},
	Url = {http://www.sciencedirect.com/science/article/pii/S095058490600111X}
}

@Article{Heradio20161066,
	Title = {{Augmenting measure sensitivity to detect essential, dispensable and highly incompatible features in mass customization}},
	Author = {Heradio, Ruben and Perez-Morago, Hector and Alf{\'{e}}rez, Mauricio and Fernandez-Amoros, David and Alf{\'{e}}rez, Germ{\'{a}}n H},
	Journal = {European Journal of Operational Research},
	Year = {2016},
	Number = {3},
	Pages = {1066--1077},
	Volume = {248},
	Abstract = {Abstract Mass customization is the new frontier in business competition for both manufacturing and service industries. To improve customer satisfaction, reduce lead-times and shorten costs, families of similar products are built jointly by combining reusable parts that implement the features demanded by the customers. To guarantee the validity of the products derived from mass customization processes, feature dependencies and incompatibilities are usually specified with a variability model. As market demand grows and evolves, variability models become increasingly complex. In such entangled models it is hard to identify which features are essential, dispensable, highly required by other features, or highly incompatible with the remaining features. This paper exposes the limitations of existing approaches to gather such knowledge and provides efficient algorithms to retrieve that information from variability models. },
	Doi = {https://doi.org/10.1016/j.ejor.2015.08.005},
	ISSN = {0377-2217},
	Keywords = {Binary decision diagram,Mass customization,Product platform,Variability modeling},
	Url = {http://www.sciencedirect.com/science/article/pii/S0377221715007225}
}

@Article{Heradio20161,
	Title = {{A bibliometric analysis of 20 years of research on software product lines}},
	Author = {Heradio, Ruben and Perez-Morago, Hector and Fernandez-Amoros, David and Cabrerizo, Francisco Javier and Herrera-Viedma, Enrique},
	Journal = {Information and Software Technology},
	Year = {2016},
	Pages = {1--15},
	Volume = {72},
	Abstract = {Abstract Context: Software product line engineering has proven to be an efficient paradigm to developing families of similar software systems at lower costs, in shorter time, and with higher quality. Objective: This paper analyzes the literature on product lines from 1995 to 2014, identifying the most influential publications, the most researched topics, and how the interest in those topics has evolved along the way. Method: Bibliographic data have been gathered from {\{}ISI{\}} Web of Science and Scopus. The data have been examined using two prominent bibliometric approaches: science mapping and performance analysis. Results: According to the study carried out, (i) software architecture was the initial motor of research in SPL; (ii) work on systematic software reuse has been essential for the development of the area; and (iii) feature modeling has been the most important topic for the last fifteen years, having the best evolution behavior in terms of number of published papers and received citations. Conclusion: Science mapping has been used to identify the main researched topics, the evolution of the interest in those topics and the relationships among topics. Performance analysis has been used to recognize the most influential papers, the journals and conferences that have published most papers, how numerous is the literature on product lines and what is its distribution over time. },
	Doi = {https://doi.org/10.1016/j.infsof.2015.11.004},
	File = {:Users/mac/Downloads/Sciencedirect{\_}articles{\_}13Jun2017{\_}14-36-55.006/A-bibliometric-analysis-of-20-years-of-research-on-software-product-lines{\_}2016{\_}Information-and-Software-Technology.pdf:pdf},
	ISSN = {0950-5849},
	Keywords = {Bibliometrics,Performance analysis,Science mapping,Software product lines},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584915001883}
}

@Article{Hervieu2016129,
	Title = {{Practical minimization of pairwise-covering test configurations using constraint programming}},
	Author = {Hervieu, Aymeric and Marijan, Dusica and Gotlieb, Arnaud and Baudry, Benoit},
	Journal = {Information and Software Technology},
	Year = {2016},
	Pages = {129--146},
	Volume = {71},
	Abstract = {Abstract Context: Testing highly-configurable software systems is challenging due to a large number of test configurations that have to be carefully selected in order to reduce the testing effort as much as possible, while maintaining high software quality. Finding the smallest set of valid test configurations that ensure sufficient coverage of the system's feature interactions is thus the objective of validation engineers, especially when the execution of test configurations is costly or time-consuming. However, this problem is NP-hard in general and approximation algorithms have often been used to address it in practice. Objective: In this paper, we explore an alternative exact approach based on constraint programming that will allow engineers to increase the effectiveness of configuration testing while keeping the number of configurations as low as possible. Method: Our approach consists in using a (time-aware) minimization algorithm based on constraint programming. Given the amount of time, our solution generates a minimized set of valid test configurations that ensure coverage of all pairs of feature values (a.k.a. pairwise coverage). The approach has been implemented in a tool called PACOGEN. Results: {\{}PACOGEN{\}} was evaluated on 224 feature models in comparison with the two existing tools that are based on a greedy algorithm. For 79{\%} of 224 feature models, {\{}PACOGEN{\}} generated up to 60{\%} fewer test configurations than the competitor tools. We further evaluated {\{}PACOGEN{\}} in the case study of an industrial video conferencing product line with a feature model of 169 features, and found 60{\%} fewer configurations compared with the manual approach followed by test engineers. The set of test configurations generated by {\{}PACOGEN{\}} decreased the time required by test engineers in manual test configuration by 85{\%}, increasing the feature-pairs coverage at the same time. Conclusion: Our experimental evaluation concluded that optimal time-aware minimization of pairwise-covering test configurations is efficiently addressed using constraint programming techniques. },
	Doi = {https://doi.org/10.1016/j.infsof.2015.11.007},
	ISSN = {0950-5849},
	Keywords = {Constraint programming,Highly-configurable software systems,Variability testing},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584915002013}
}

@Article{Hierons:2016:SOP:2913009.2897760,
	Title = {{SIP: Optimal Product Selection from Feature Models Using Many-Objective Evolutionary Optimization}},
	Author = {Hierons, Robert M and Li, Miqing and Liu, Xiaohui and Segura, Sergio and Zheng, Wei},
	Journal = {ACM Trans. Softw. Eng. Methodol.},
	Year = {2016},
	Number = {2},
	Pages = {17:1----17:39},
	Volume = {25},
	Address = {New York, NY, USA},
	Doi = {10.1145/2897760},
	ISSN = {1049-331X},
	Keywords = {Product selection},
	Publisher = {ACM},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2897760}
}

@Article{SPE:SPE1036,
	Title = {{Using meta-modeling in design and implementation of component-based systems: the SOFA case study}},
	Author = {Hn{\v{e}}tynka, Petr and Pl{\'{a}}{\v{s}}il, Franti{\v{s}}ek},
	Journal = {Software: Practice and Experience},
	Year = {2011},
	Number = {11},
	Pages = {1185--1201},
	Volume = {41},
	Abstract = {To allow efficient and user-friendly development of a component-based application, component systems have to provide a rather complex development infrastructure, including a tool for component composition, component repository, and a run-time infrastructure. In this paper, we present and evaluate benefits of using meta-modeling during the process of defining a component system and also during creation of the development and run-time infrastructures. Most of the presented arguments are based on a broad practical experience with designing the component systems SOFA and SOFA 2; the former designed in a classical ad hoc ‘manual' way, whereas the latter with the help of meta-modeling. Copyright {\textcopyright} 2010 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spe.1036},
	ISSN = {1097-024X},
	Keywords = {meta-models,model transformation,model-driven development,software architectures,software components},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/spe.1036}
}

@Article{Hoda20171339,
	Title = {{Systematic literature reviews in agile software development: A tertiary study}},
	Author = {Hoda, R and Salleh, N and Grundy, J and Tee, H M},
	Journal = {Information and Software Technology},
	Year = {2017},
	Pages = {1339--1351},
	Volume = {85},
	Abstract = {Context A number of systematic literature reviews and mapping studies (SLRs) covering numerous primary research studies on various aspects of agile software development (ASD) exist. Objective The aim of this paper is to provide an overview of the SLRs on ASD research topics for software engineering researchers and practitioners. Method We followed the tertiary study guidelines by Kitchenham et al. to find SLRs published between late 1990s to December 2015. Results We found 28 SLRs focusing on ten different ASD research areas: adoption, methods, practices, human and social aspects, CMMI, usability, global software engineering (GSE), organizational agility, embedded systems, and software product line engineering. The number of SLRs on ASD topics, similar to those on software engineering (SE) topics in general, is on the rise. A majority of the SLRs applied standardized guidelines and the quality of these SLRs on ASD topics was found to be slightly higher for journal publications than for conferences. While some individuals and institutions seem to lead this area, the spread of authors and institutions is wide. With respect to prior review recommendations, significant progress was noticed in the area of connecting agile to established domains such as usability, CMMI, and GSE; and considerable progress was observed in focusing on management-oriented approaches as Scrum and sustaining ASD in different contexts such as embedded systems. Conclusion SLRs of ASD studies are on the rise and cover a variety of ASD aspects, ranging from early adoption issues to newer applications of ASD such as in product line engineering. ASD research can benefit from further primary and secondary studies on evaluating benefits and challenges of ASD methods, agile hybrids in large-scale setups, sustainability, motivation, teamwork, and project management; as well as a fresh review of empirical studies in ASD to cover the period post 2008. {\textcopyright} 2017 Elsevier B.V.},
	Annote = {cited By 0},
	Doi = {10.1016/j.infsof.2017.01.007},
	Keywords = {Agile manufacturing systems; Embedded systems; Hum,Agile software development; Global software engin,Software design},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009961943{\&}doi=10.1016{\%}2Fj.infsof.2017.01.007{\&}partnerID=40{\&}md5=6a85ea7243f2d03424972d48f898f921}
}

@Article{Hoffman2003143,
	Title = {{{\{}API{\}} documentation with executable examples}},
	Author = {Hoffman, Daniel and Strooper, Paul},
	Journal = {Journal of Systems and Software},
	Year = {2003},
	Number = {2},
	Pages = {143--156},
	Volume = {66},
	Abstract = {The rise of component-based software development has created an urgent need for effective application program interface (API) documentation. Experience has shown that it is hard to create precise and readable documentation. Prose documentation can provide a good overview but lacks precision. Formal methods offer precision but the resulting documentation is expensive to develop. Worse, few developers have the skill or inclination to read formal documentation. We present a pragmatic solution to the problem of {\{}API{\}} documentation. We augment the prose documentation with executable test cases, including expected outputs, and use the prose plus the test cases as the documentation. With appropriate tool support, the test cases are easy to develop and read. Such test cases constitute a completely formal, albeit partial, specification of input/output behavior. Equally important, consistency between code and documentation is demonstrated by running the test cases. This approach provides an attractive bridge between formal and informal documentation. We also present a tool that supports compact and readable test cases, and generation of test drivers and documentation, and illustrate the approach with detailed case studies. },
	Doi = {https://doi.org/10.1016/S0164-1212(02)00055-9},
	ISSN = {0164-1212},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121202000559}
}

@Article{Hofmeister2007106,
	Title = {{A general model of software architecture design derived from five industrial approaches}},
	Author = {Hofmeister, Christine and Kruchten, Philippe and Nord, Robert L and Obbink, Henk and Ran, Alexander and America, Pierre},
	Journal = {Journal of Systems and Software},
	Year = {2007},
	Number = {1},
	Pages = {106--126},
	Volume = {80},
	Abstract = {We compare five industrial software architecture design methods and we extract from their commonalities a general software architecture design approach. Using this general approach, we compare across the five methods the artifacts and activities they use or recommend, and we pinpoint similarities and differences. Once we get beyond the great variance in terminology and description, we find that the five approaches have a lot in common and match more or less the “ideal? pattern we introduced. From the ideal pattern we derive an evaluation grid that can be used for further method comparisons. },
	Doi = {https://doi.org/10.1016/j.jss.2006.05.024},
	ISSN = {0164-1212},
	Keywords = {Architectural method,Software architecture,Software architecture analysis,Software architecture design},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121206001634}
}

@InProceedings{Holl:2011:PLB:2019136.2019184,
	Title = {{Product Line Bundles to Support Product Derivation in Multi Product Lines}},
	Author = {Holl, Gerald},
	Booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
	Year = {2011},
	Address = {New York, NY, USA},
	Pages = {41:1----41:6},
	Publisher = {ACM},
	Series = {SPLC '11},
	Doi = {10.1145/2019136.2019184},
	ISBN = {978-1-4503-0789-5},
	Keywords = {multi product lines,product derivation,tool support},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2019136.2019184}
}

@Article{Holl2012828,
	Title = {{A systematic review and an expert survey on capabilities supporting multi product lines}},
	Author = {Holl, Gerald and Gr{\"{u}}nbacher, Paul and Rabiser, Rick},
	Journal = {Information and Software Technology},
	Year = {2012},
	Number = {8},
	Pages = {828--852},
	Volume = {54},
	Abstract = {Context Complex software-intensive systems comprise many subsystems that are often based on heterogeneous technological platforms and managed by different organizational units. Multi product lines (MPLs) are an emerging area of research addressing variability management for such large-scale or ultra-large-scale systems. Despite the increasing number of publications addressing {\{}MPLs{\}} the research area is still quite fragmented. Objective The aims of this paper are thus to identify, describe, and classify existing approaches supporting {\{}MPLs{\}} and to increase the understanding of the underlying research issues. Furthermore, the paper aims at defining success-critical capabilities of infrastructures supporting MPLs. Method Using a systematic literature review we identify and analyze existing approaches and research issues regarding MPLs. Approaches described in the literature support capabilities needed to define and operate MPLs. We derive capabilities supporting {\{}MPLs{\}} from the results of the systematic literature review. We validate and refine these capabilities based on a survey among experts from academia and industry. Results The paper discusses key research issues in {\{}MPLs{\}} and presents basic and advanced capabilities supporting MPLs. We also show examples from research approaches that demonstrate how these capabilities can be realized. Conclusions We conclude that approaches supporting {\{}MPLs{\}} need to consider both technical aspects like structuring large models and defining dependencies between product lines as well as organizational aspects such as distributed modeling and product derivation by multiple stakeholders. The identified capabilities can help to build, enhance, and evaluate {\{}MPL{\}} approaches. },
	Annote = {Special Issue: Voice of the Editorial BoardSpecial Issue: Voice of the Editorial Board},
	Doi = {https://doi.org/10.1016/j.infsof.2012.02.002},
	ISSN = {0950-5849},
	Keywords = {Large-scale systems,Multi product lines,Product line engineering,Systematic literature review},
	Url = {http://www.sciencedirect.com/science/article/pii/S095058491200033X}
}

@Article{SMR:SMR1876,
	Title = {{From ad hoc to strategic ecosystem management: the “Three-Layer Ecosystem Strategy Model? (TeLESM)}},
	Author = {{Holmstr{\"{o}}m Olsson}, Helena and Bosch, Jan},
	Journal = {Journal of Software: Evolution and Process},
	Year = {2017},
	Pages = {n/a----n/a},
	Abstract = {Recently, business ecosystems have been recognized as one of the most interesting phenomenon in software engineering research. Companies experience a paradigm shift where product development and innovation is moving outside the boundaries of the firm and where networks of stakeholders join forces to co-create value. While there is prominent research focusing on the managerial perspective of business ecosystems, few studies provide strategic guidance for how to intentionally manage the different ecosystems that companies operate in. Therefore, and on the basis of multicase study research, we provide empirical evidence on the challenges that software-intensive companies experience in relation to the different types of business ecosystems they operate in. We conduct a “state-of-the-art? literature review to identify strategies that are used to manage ecosystem engagements, and we develop a conceptual model in which we identify strategies for managing the innovation ecosystem, the differentiating ecosystem, and the commoditizing ecosystem. By categorising the different strategies in relation to the different types of ecosystems for which they are valid, the “three-layer ecosystem strategy model? provides comprehensive support for strategy selection. We validate the use of the identified strategies in 6 software-intensive case companies, and we provide empirical insights on the “relevance? and the “desired use? of these strategies as experienced by the case companies.},
	Doi = {10.1002/smr.1876},
	ISSN = {2047-7481},
	Keywords = {business ecosystems,commoditizing ecosystem,differentiating ecosystem,ecosystem challenges,ecosystem strategies,innovation ecosystem},
	Url = {http://dx.doi.org/10.1002/smr.1876}
}

@InProceedings{Hoole:2016:IVD:2915970.2915994,
	Title = {{Improving Vulnerability Detection Measurement: [Test Suites and Software Security Assurance]}},
	Author = {Hoole, Alexander M and Traore, Issa and Delaitre, Aurelien and de Oliveira, Charles},
	Booktitle = {Proceedings of the 20th International Conference on Evaluation and Assessment in Software Engineering},
	Year = {2016},
	Address = {New York, NY, USA},
	Pages = {27:1----27:10},
	Publisher = {ACM},
	Series = {EASE '16},
	Doi = {10.1145/2915970.2915994},
	ISBN = {978-1-4503-3691-8},
	Keywords = {dynamic analysis,security metrics,static analysis,test suites,vulnerability,weakness},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2915970.2915994}
}

@Article{Horcas2014106,
	Title = {{Closing the gap between the specification and enforcement of security policies}},
	Author = {Horcas, J.-M. and Pinto, M and Fuentes, L},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2014},
	Pages = {106--118},
	Volume = {8647 LNCS},
	Abstract = {Security policies are enforced through the deployment of certain security functionalities within the applications. Applications can have different levels of security and thus each security policy is enforced by different security functionalities. Thus, the secure deployment of an application is not an easy task, being more complicated due to the existing gap between the specification of a security policy and the deployment, inside the application, of the security functionalities that are required to enforce that security policy. The main goal of this paper is to close this gap. This is done by using the paradigms of Software Product Lines and Aspect-Oriented Programming in order to: (1) link the security policies with the security functionalities, (2) generate a configuration of the security functionalities that fit a security policy, and (3) weave the selected security functionalities into an application. We qualitatively evaluate our approach, and discuss its benefits using a case study. {\textcopyright} 2014 Springer International Publishing.},
	Annote = {cited By 2},
	Doi = {10.1007/978-3-319-09770-1_10},
	Keywords = {Aspect oriented programming; Computer software; Sp,Security enforcement; Security policy; Software P,Security systems},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906739535{\&}doi=10.1007{\%}2F978-3-319-09770-1{\_}10{\&}partnerID=40{\&}md5=ce8a5ec191a26dc73244892638bde52e}
}

@Article{Horcas201678,
	Title = {{An automatic process for weaving functional quality attributes using a software product line approach}},
	Author = {Horcas, Jose-Miguel and Pinto, M{\'{o}}nica and Fuentes, Lidia},
	Journal = {Journal of Systems and Software},
	Year = {2016},
	Pages = {78--95},
	Volume = {112},
	Abstract = {Abstract Some quality attributes can be modelled using software components, and are normally known as Functional Quality Attributes (FQAs). Applications may require different FQAs, and each {\{}FQA{\}} (e.g., security) can be composed of many concerns (e.g., access control or authentication). They normally have dependencies between them and crosscut the system architecture. The goal of the work presented here is to provide the means for software architects to focus only on application functionality, without having to worry about FQAs. The idea is to model {\{}FQAs{\}} separately from application functionality following a Software Product Line (SPL) approach. By combining {\{}SPL{\}} and aspect-oriented mechanisms, we will define a generic process to model and automatically inject {\{}FQAs{\}} into the application without breaking the base architecture. We will provide and compare two implementations of our generic approach using different variability and architecture description languages: (i) feature models and an aspect-oriented architecture description language; and (ii) the Common Variability Language (CVL) and a MOF-compliant language (e.g., UML). We also discuss the benefits and limitations of our approach. Modelling {\{}FQAs{\}} separately from the base application has many advantages (e.g., reusability, less coupled components, high cohesive architectures). },
	Doi = {https://doi.org/10.1016/j.jss.2015.11.005},
	ISSN = {0164-1212},
	Keywords = {Quality attributes,Software product lines,Weaving},
	Url = {http://www.sciencedirect.com/science/article/pii/S016412121500240X}
}

@Article{Horcas201620,
	Title = {{An approach for deploying and monitoring dynamic security policies}},
	Author = {Horcas, Jose-Miguel and Pinto, M{\'{o}}nica and Fuentes, Lidia and Mallouli, Wissam and de Oca, Edgardo Montes},
	Journal = {Computers {\&} Security},
	Year = {2016},
	Pages = {20--38},
	Volume = {58},
	Abstract = {Abstract Security policies are enforced through the deployment of certain security functionalities within the applications. When the security policies dynamically change, the associated security functionalities currently deployed within the applications must be adapted at runtime in order to enforce the new security policies. INTER-TRUST is a framework for the specification, negotiation, deployment and dynamic adaptation of interoperable security policies, in the context of pervasive systems where devices are constantly exchanging critical information through the network. The dynamic adaptation of the security policies at runtime is addressed using Aspect-Oriented Programming (AOP) that allows enforcing security requirements by dynamically weaving security aspects into the applications. However, a mechanism to guarantee the correct adaptation of the functionality that enforces the changing security policies is needed. In this paper, we present an approach based on the combination of monitoring and detection techniques in order to maintain the correlation between the security policies and the associated functionality deployed using AOP, allowing the INTER-TRUST framework to automatically react when needed. },
	Doi = {https://doi.org/10.1016/j.cose.2015.11.007},
	ISSN = {0167-4048},
	Keywords = {Aspect-oriented programming,Dynamic deployment,Monitoring,Security framework,Security policies},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167404815001832}
}

@Article{Hosseini201543,
	Title = {{Crowdsourcing: A taxonomy and systematic mapping study}},
	Author = {Hosseini, Mahmood and Shahri, Alimohammad and Phalp, Keith and Taylor, Jacqui and Ali, Raian},
	Journal = {Computer Science Review},
	Year = {2015},
	Pages = {43--69},
	Volume = {17},
	Abstract = {Abstract Context: Crowdsourcing, or tapping into the power of the crowd for problem solving, has gained ever-increasing attraction since it was first introduced. Crowdsourcing has been used in different disciplines, and it is becoming well-accepted in the marketplace as a new business model which utilizes Human Intelligence Tasks (HITs). Objective: While both academia and industry have extensively delved into different aspects of crowdsourcing, there seems to be no common understanding of what crowdsourcing really means and what core and optional features it has. Also, we still lack information on the kinds and disciplines of studies conducted on crowdsourcing and how they defined it in the context of their application area. This paper will clarify this ambiguity by analysing the distribution and demographics of research in crowdsourcing and extracting taxonomy of the variability and commonality in the constructs defining the concept in the literature. Method: We conduct a systematic mapping study and analyse 113 papers, selected via a formal process, and report and discuss the results. The study is combined by a content analysis process to extract a taxonomy of features describing crowdsourcing. Results: We extract and describe the taxonomy of features which characterize crowdsourcing in its four constituents; the crowd, the crowdsourcer, the crowdsourced task and the crowdsourcing platform. In addition, we report on different mappings between these features and the characteristics of the studied papers. We also analyse the distribution of the research using multiple criteria and draw conclusions. For example, our results show a constantly increasing interest in the area, especially in North America and a significant interest from industry. Also, we illustrate that although crowdsourcing is shown to be useful in a variety of disciplines, the research in the field of computer science still seems to be dominant in investigating it. Conclusions: This study allows forming a clear picture of the research in crowdsourcing and understanding the different features of crowdsourcing and their popularity, what type of research was conducted, where and how and by whom. The study enables researchers and practitioners to estimate the current status of the research in this new field. Our taxonomy of extracted features provides a reference model which could be used to configure crowdsourcing and also define it precisely and make design decisions on which of its variation to adopt. },
	Doi = {https://doi.org/10.1016/j.cosrev.2015.05.001},
	ISSN = {1574-0137},
	Keywords = {Crowdsourcing,Crowdsourcing features,Systematic mapping,Taxonomy},
	Url = {http://www.sciencedirect.com/science/article/pii/S1574013715000052}
}

@Article{Huang20121332,
	Title = {{An accurate on-demand time synchronization protocol for wireless sensor networks}},
	Author = {Huang, Ge and Zomaya, Albert Y and Delicato, Fl{\'{a}}via C and Pires, Paulo F},
	Journal = {Journal of Parallel and Distributed Computing},
	Year = {2012},
	Number = {10},
	Pages = {1332--1346},
	Volume = {72},
	Abstract = {Time synchronization is a critical component in any wireless sensor network (WSN). In terms of energy consumption, on-demand time synchronization is better than continuous synchronization. However, currently existing on-demand time synchronization protocols have a very low accuracy and very strong spatial accumulative effect. These features are not suitable for several types of {\{}WSN{\}} applications, such as applications with stringent temporal requirements, or applications that have a large spatial region of interest. In this paper, we propose an on-demand time synchronization protocol, named {\{}AOTSP{\}} (Accurate On-demand Time Synchronization Protocol), which differs from other protocols of the same category by having the following advantages, as shown in our theoretical analysis and simulation results: (1) weak spatial accumulative effect; (2) fairly low communication cost; (3) low computational complexity; (4) high accuracy; (5) high scalability. Such features make {\{}AOTSP{\}} a suitable time synchronization protocol for a broad range of {\{}WSN{\}} applications. },
	Doi = {https://doi.org/10.1016/j.jpdc.2012.06.003},
	ISSN = {0743-7315},
	Keywords = {On-demand synchronization,Taylor expansion,Wireless sensor networks},
	Url = {http://www.sciencedirect.com/science/article/pii/S0743731512001426}
}

@Article{Hubaux2011337,
	Title = {{Evaluating a textual feature modelling language: Four industrial case studies}},
	Author = {Hubaux, A and Boucher, Q and Hartmann, H and Michel, R and Heymans, P},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2011},
	Pages = {337--356},
	Volume = {6563 LNCS},
	Abstract = {Feature models are commonly used in software product line engineering as a means to document variability. Since their introduction, feature models have been extended and formalised in various ways. The majority of these extensions are variants of the original tree-based graphical notation. But over time, textual dialects have also been proposed. The textual variability language (TVL) was proposed to combine the advantages of both graphical and textual notations. However, its benefits and limitations have not been empirically evaluated up to now. In this paper, we evaluate TVL with four cases from companies of different sizes and application domains. The study shows that practitioners can benefit from TVL. The participants appreciated the notation, the advantages of a textual language and considered the learning curve to be gentle. The study also reveals some limitations of the current version of TVL. {\textcopyright} 2011 Springer-Verlag Berlin Heidelberg.},
	Annote = {cited By 7},
	Doi = {10.1007/978-3-642-19440-5_23},
	Keywords = {Application domains; Different sizes; Document var,Models,Production engineering; Software design},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952272925{\&}doi=10.1007{\%}2F978-3-642-19440-5{\_}23{\&}partnerID=40{\&}md5=0157326f6bab790b5a139316da08a7c1}
}

@Article{Hubaux:2013:SCF:2501654.2501665,
	Title = {{Separation of Concerns in Feature Diagram Languages: A Systematic Survey}},
	Author = {Hubaux, Arnaud and Tun, Thein Than and Heymans, Patrick},
	Journal = {ACM Comput. Surv.},
	Year = {2013},
	Number = {4},
	Pages = {51:1----51:23},
	Volume = {45},
	Address = {New York, NY, USA},
	Doi = {10.1145/2501654.2501665},
	ISSN = {0360-0300},
	Keywords = {Software product line,feature diagram,separation of concerns,variability},
	Publisher = {ACM},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2501654.2501665}
}

@Article{Hunsen2016449,
	Title = {{Preprocessor-based variability in open-source and industrial software systems: An empirical study}},
	Author = {Hunsen, C and Zhang, B and Siegmund, J and K{\"{a}}stner, C and Le{\ss}enich, O and Becker, M and Apel, S},
	Journal = {Empirical Software Engineering},
	Year = {2016},
	Number = {2},
	Pages = {449--482},
	Volume = {21},
	Abstract = {Almost every sufficiently complex software system today is configurable. Conditional compilation is a simple variability-implementation mechanism that is widely used in open-source projects and industry. Especially, the C preprocessor (CPP) is very popular in practice, but it is also gaining (again) interest in academia. Although there have been several attempts to understand and improve CPP, there is a lack of understanding of how it is used in open-source and industrial systems and whether different usage patterns have emerged. The background is that much research on configurable systems and product lines concentrates on open-source systems, simply because they are available for study in the first place. This leads to the potentially problematic situation that it is unclear whether the results obtained from these studies are transferable to industrial systems. We aim at lowering this gap by comparing the use of CPP in open-source projects and industry—especially from the embedded-systems domain—based on a substantial set of subject systems and well-known variability metrics, including size, scattering, and tangling metrics. A key result of our empirical study is that, regarding almost all aspects we studied, the analyzed open-source systems and the considered embedded systems from industry are similar regarding most metrics, including systems that have been developed in industry and made open source at some point. So, our study indicates that, regarding CPP as variability-implementation mechanism, insights, methods, and tools developed based on studies of open-source systems are transferable to industrial systems—at least, with respect to the metrics we considered. {\textcopyright} 2015, Springer Science+Business Media New York.},
	Annote = {cited By 1},
	Doi = {10.1007/s10664-015-9360-1},
	Keywords = {C (programming language); Computer software; Embed,C preprocessor; Configurable systems; cppstats; I,Open systems},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927927173{\&}doi=10.1007{\%}2Fs10664-015-9360-1{\&}partnerID=40{\&}md5=ccef9e8933e9d1d78ec680cab9cf1b1c}
}

@Article{SMR:SMR1578,
	Title = {{Avispa: a tool for analyzing software process models}},
	Author = {{Hurtado Alegr{\'{i}}a}, Julio A and Bastarrica, Mar{\'{i}}a Cecilia and Bergel, Alexandre},
	Journal = {Journal of Software: Evolution and Process},
	Year = {2014},
	Number = {4},
	Pages = {434--450},
	Volume = {26},
	Abstract = {Defining and formalizing the software development process is a common means for improving it. Software process modeling is often a challenging and expensive endeavor, because a well specified process may still include inefficiencies that are hardly detected before enacting it. Thus, assessing process quality is a relevant concern to improve several aspects such as conceptual integrity, correctness, usability, maintainability, and performance, among others. This paper describes Avispa, a graphical tool that allows analyzing the quality of SPEM 2.0 software processes models. Avispa identifies a series of error patterns and highlights them in different blueprints. A detailed description of the internals of Avispa is provided to show both its structure and its extensibility mechanisms. We also present an interactive mechanism to define new analysis scripts and to implement new patterns and blueprints. This paper illustrates the application of Avispa in an industrial case study where process engineers are assisted to analyze the quality of their process. Copyright {\textcopyright} 2013 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/smr.1578},
	ISSN = {2047-7481},
	Keywords = {software process analysis,software process verification,software visualization},
	Url = {http://dx.doi.org/10.1002/smr.1578}
}

@Article{SMR:SMR1576,
	Title = {{MDE-based process tailoring strategy}},
	Author = {{Hurtado Alegr{\'{i}}a}, Julio A and Bastarrica, Mar{\'{i}}a Cecilia and Quispe, Alcides and Ochoa, Sergio F},
	Journal = {Journal of Software: Evolution and Process},
	Year = {2014},
	Number = {4},
	Pages = {386--403},
	Volume = {26},
	Abstract = {Defining organizational software processes is essential for enhancing maturity because they cannot be improved if they are not specified. However, software process definition is hard and still not good for assuring productivity because the best process depends on the project's particularities. The process engineer can define a specific process for each kind of project, but this is expensive, unrepeatable, and error prone. Moreover, it is difficult to foresee all project scenarios and therefore the appropriate processes. The most usual situation is to apply always the same software process, although it is known to be suboptimal. To deal with this challenge, we propose a model-based approach to software process tailoring that automatically generates project-specific processes on the basis of the organizational process and project contexts. We still require competent process engineers to define the company's process, but once done, our approach is systematic, repeatable, and easy to use. The proposal is applied for tailoring the requirements engineering process of a medium-size Chilean company. Processes obtained matched those used in the company for planned project contexts, and they were also reasonable for nonexpected situations. The company's process and project engineers agreed that the approach was highly valuable. Copyright {\textcopyright} 2013 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/smr.1576},
	ISSN = {2047-7481},
	Keywords = {model-driven engineering,process tailoring,software processes},
	Url = {http://dx.doi.org/10.1002/smr.1576}
}

@Article{Hurtado20131153,
	Title = {{MDE software process lines in small companies}},
	Author = {Hurtado, J A and Bastarrica, M C and Ochoa, S F and Simmonds, J},
	Journal = {Journal of Systems and Software},
	Year = {2013},
	Number = {5},
	Pages = {1153--1171},
	Volume = {86},
	Abstract = {Software organizations specify their software processes so that process knowledge can be systematically reused across projects. However, different projects may require different processes. Defining a separate process for each potential project context is expensive and error-prone, since these processes must simultaneously evolve in a consistent manner. Moreover, an organization cannot envision all possible project contexts in advance because several variables may be involved, and these may also be combined in different ways. This problem is even worse in small companies since they usually cannot afford to define more than one process. Software process lines are a specific type of software product lines, in the software process domain. A benefit of software process lines is that they allow software process customization with respect to a context. In this article we propose a model-driven approach for software process lines specification and configuration. The article also presents two industrial case studies carried out at two small Chilean software development companies. Both companies have benefited from applying our approach to their processes: new projects are now developed using custom processes, process knowledge is systematically reused, and the total time required to customize a process is much shorter than before.{\textcopyright} 2012 Elsevier Inc. All rights reserved.},
	Annote = {cited By 8},
	Doi = {10.1016/j.jss.2012.09.033},
	Keywords = {Industrial case study; Model driven approach; Mode,Industry,Software design},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875270026{\&}doi=10.1016{\%}2Fj.jss.2012.09.033{\&}partnerID=40{\&}md5=b4bd0bb9378eaba09a42ff8306d01824}
}

@Article{Hutchinson2014144,
	Title = {{Model-driven engineering practices in industry: Social, organizational and managerial factors that lead to success or failure}},
	Author = {Hutchinson, John and Whittle, Jon and Rouncefield, Mark},
	Journal = {Science of Computer Programming},
	Year = {2014},
	Pages = {144--161},
	Volume = {89, Part B},
	Abstract = {Abstract In this article, we attempt to address the relative absence of empirical studies of model driven engineering (MDE) in two different but complementary ways. First, we present an analysis of a large online survey of {\{}MDE{\}} deployment and experience that provides some rough quantitative measures of {\{}MDE{\}} practices in industry. Second, we supplement these figures with qualitative data obtained from some semi-structured, in-depth interviews with {\{}MDE{\}} practitioners, and, in particular, through describing the practices of four commercial organizations as they adopted a model driven engineering approach to their software development practices. Using in-depth semi-structured interviewing, we invited practitioners to reflect on their experiences and selected four to use as exemplars or case studies. In documenting some details of their attempts to deploy model driven practices, we identify a number of factors, in particular the importance of complex organizational, managerial and social factors–as opposed to simple technical factors–that appear to influence the relative success, or failure, of the endeavor. Three of the case study companies describe genuine success in their use of model driven development, but explain that as examples of organizational change management, the successful deployment of model driven engineering appears to require: a progressive and iterative approach; transparent organizational commitment and motivation; integration with existing organizational processes and a clear business focus. },
	Annote = {Special issue on Success Stories in Model Driven Engineering},
	Doi = {https://doi.org/10.1016/j.scico.2013.03.017},
	ISSN = {0167-6423},
	Keywords = {Empirical software engineering,Industry practice,Model driven engineering},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642313000786}
}

@Article{Huysegoms2013189,
	Title = {{Visualizing Variability Management in Requirements Engineering through Formal Concept Analysis}},
	Author = {Huysegoms, Tom and Snoeck, Monique and Dedene, Guido and Goderis, Antoon and Stumpe, Frank},
	Journal = {Procedia Technology},
	Year = {2013},
	Pages = {189--199},
	Volume = {9},
	Abstract = {Abstract While research on the visualization and documentation of variability in software artefacts by means of e.g. feature diagrams is well established, most of these documentation methods in the field of variability management assume the presence of variability as a given fact. The decision whether variability within the requirements should actually give rise to variability in the envisaged software artefact is often taken unconsciously and as a result techniques to visualize and document the amount, the structure and the impact of requirements evolution on variability are scarce. This paper provides a real life proof of concept that formal concept analysis (FCA) can be used for the visualization and documentation of variability re- lated decisions during (early) requirements engineering. {\{}FCA{\}} is used in a real-life case study to check the usability of {\{}FCA{\}} as a visualization method to support variability management during requirements engineering. The real-life case study also provides initial proof that useful documentation can be obtained by representing the requirements in a {\{}FCA{\}} concept lattice. },
	Annote = {{\{}CENTERIS{\}} 2013 - Conference on {\{}ENTERprise{\}} Information Systems / ProjMAN 2013 - International Conference on Project MANagement/ {\{}HCIST{\}} 2013 - International Conference on Health and Social Care Information Systems and Technologies},
	Doi = {https://doi.org/10.1016/j.protcy.2013.12.021},
	ISSN = {2212-0173},
	Keywords = {Requirements management,formal concept analysis,harmonization,variability management,variabilization},
	Url = {http://www.sciencedirect.com/science/article/pii/S2212017313001758}
}

@Article{Hwong20132435,
	Title = {{Formalising and analysing the control software of the Compact Muon Solenoid Experiment at the Large Hadron Collider}},
	Author = {Hwong, Yi Ling and Keiren, Jeroen J A and Kusters, Vincent J J and Leemans, Sander and Willemse, Tim A C},
	Journal = {Science of Computer Programming},
	Year = {2013},
	Number = {12},
	Pages = {2435--2452},
	Volume = {78},
	Abstract = {The control software of the {\{}CERN{\}} Compact Muon Solenoid experiment contains over 27 500 finite state machines. These state machines are organised hierarchically: commands are sent down the hierarchy and state changes are sent upwards. The sheer size of the system makes it virtually impossible to fully understand the details of its behaviour at the macro level. This is fuelled by unclarities that already exist at the micro level. We have solved the latter problem by formally describing the finite state machines in the mCRL2 process algebra. The translation has been implemented using the ASF+SDF meta-environment, and its correctness was assessed by means of simulations and visualisations of individual finite state machines and through formal verification of subsystems of the control software. Based on the formalised semantics of the finite state machines, we have developed dedicated tooling for checking properties that can be verified on finite state machines in isolation. },
	Annote = {Special Section on International Software Product Line Conference 2010 and Fundamentals of Software Engineering (selected papers of {\{}FSEN{\}} 2011)},
	Doi = {https://doi.org/10.1016/j.scico.2012.11.009},
	ISSN = {0167-6423},
	Keywords = {Bounded model checking,Case study,Model transformations,Process algebra,SML},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642312002365}
}

@Article{Ianzen201354,
	Title = {{Software process improvement in a financial organization: An action research approach}},
	Author = {Ianzen, Andressa and Mauda, Everson Carlos and Paludo, Marco Ant{\^{o}}nio and Reinehr, Sheila and Malucelli, Andreia},
	Journal = {Computer Standards {\&} Interfaces},
	Year = {2013},
	Number = {1},
	Pages = {54--65},
	Volume = {36},
	Abstract = {Abstract In order to increase the quality of systems of a financial company, the process of a software development team has changed some times to get stabilized. This paper presents the action research steps that were conducted, the perceptions of the team about the process evolution and the solved problems. Also, a software process improvement assessment has been conducted in order to identify the success factors on this implementation and the result is analyzed and discussed through the Servqual method. Among other conclusions, the involvement of the team during the improvement process and future perspectives are crucial to achieve success. },
	Doi = {https://doi.org/10.1016/j.csi.2013.07.002},
	ISSN = {0920-5489},
	Keywords = {Software engineering,Software process evaluation,Software process improvement,Software quality improvement,Systems development},
	Url = {http://www.sciencedirect.com/science/article/pii/S0920548913000676}
}

@Article{SMR:SMR1875,
	Title = {{Software integration in global software development: Challenges for GSD vendors}},
	Author = {Ilyas, Muhammad and Khan, Siffat Ullah},
	Journal = {Journal of Software: Evolution and Process},
	Year = {2017},
	Pages = {n/a----n/a},
	Abstract = {Context The advances in information and communication technologies have revolutionized the software development environment from local to global software development (GSD). This revolution also created challenges for vendor organizations. Vendors face challenges in integrating the components developed independently by GSD teams into a final working product.
		Objective The objectives of the current study is to find out those critical barriers/challenges that hinder the integration process at any stage in the GSD environment for different types and size of projects.
		Methodology For achieving the objectives we initially conducted a comprehensive systematic literature review (SLR). We searched 6 digital libraries and also followed the snowballing technique. The data was extracted from a total of 88 finally selected papers. Findings of the SLR study were then empirically validated through a questionnaire survey in GSD industry. A total of 96 experts from 22 different countries participated in the survey.
		Results We have found a total of 16 barriers/challenges among which ten barriers are ranked as critical barriers/challenges. Some of the top ranked barriers are “lack of communication,? “lack of proper documentation,? “lack of compatibility,? and “architecture mismatch.? The findings of our industrial survey are mostly coherent with the SLR findings. However, there is a difference in ranks of the various barriers/challenges across the 2 data sets (SLR and industrial survey). The identified challenges need to be properly addressed by software vendors to reduce the complexity of the integration process in GSD projects.
		Conclusion We found that the severity of these barriers increases in large size projects. On the other hand, bespoke products are more affected by “lack of communication,? while off-the-shelf–based projects face integration problems due to “lack of compatibility,? “architecture mismatch,? and “wrong off the shelf product selection and customization?.
		},
	Doi = {10.1002/smr.1875},
	ISSN = {2047-7481},
	Keywords = {barriers/challenges,empirical study,global software development,software integration,systematic literature review},
	Url = {http://dx.doi.org/10.1002/smr.1875}
}

@Article{EEJ:EEJ21166,
	Title = {{A model-based method to design an application common platform for enterprise information systems}},
	Author = {Ishihara, Akira and Furuta, Hirohisa and Yamaoka, Takayuki and Seo, Kazuo and Nishida, Shogo},
	Journal = {Electrical Engineering in Japan},
	Year = {2011},
	Number = {3},
	Pages = {37--51},
	Volume = {176},
	Abstract = {This paper presents a model-based method to design a software development platform for enterprise information systems. We call it an application common platform, ACP. ACP wraps existing reusable software assets to hide their details from application developers and provide domain level API (Application Programming Interface), so that reusability of software assets and productivity of applications are improved. In this paper, we present a software architecture which organizes applications, ACP, and software assets and illustrate the development of ACP. In particular, we present transformation rules used to derive ACP design models from both application design models and software assets design models. We have evaluated our proposed method through case studies of the development of enterprise information systems. Our proposed method was found to reduce development costs by 20{\%} compared to the estimated costs. {\textcopyright} 2011 Wiley Periodicals, Inc. Electr Eng Jpn, 176(3): 37–51, 2011; Published online in Wiley Online Library (wileyonlinelibrary.com). DOI 10.1002/eej.21166},
	Doi = {10.1002/eej.21166},
	ISSN = {1520-6416},
	Keywords = {enterprise information systems,model-based development,software assets,software product line},
	Publisher = {Wiley Subscription Services, Inc., A Wiley Company},
	Url = {http://dx.doi.org/10.1002/eej.21166}
}

@Article{Itzik2016,
	Title = {{Variability Analysis of Requirements: Considering Behavioral Differences and Reflecting Stakeholders? Perspectives}},
	Author = {Itzik, Nili and Reinhartz-Berger, Iris and Wand, Yair},
	Journal = {IEEE Transactions on Software Engineering},
	Year = {2016},
	Month = {jul},
	Number = {7},
	Pages = {687--706},
	Volume = {42},
	Doi = {10.1109/TSE.2015.2512599},
	File = {:Users/mac/Downloads/bulk-download (2)/Variability Analysis of Requirements Considering Behavioral Differences and Reflecting Stakeholders- Perspectives.pdf:pdf},
	ISSN = {0098-5589},
	Url = {http://ieeexplore.ieee.org/document/7366597/}
}

@Article{Jorges2012511,
	Title = {{A constraint-based variability modeling framework}},
	Author = {J{\"{o}}rges, S and Lamprecht, A.-L. and Margaria, T and Schaefer, I and Steffen, B},
	Journal = {International Journal on Software Tools for Technology Transfer},
	Year = {2012},
	Number = {5},
	Pages = {511--530},
	Volume = {14},
	Abstract = {Constraint-based variability modeling is a flexible, declarative approach to managing solution-space variability. Product variants are defined in a top-down manner by successively restricting the admissible combinations of product artifacts until a specific product variant is determined. In this paper, we illustrate the range of constraint-based variability modeling by discussing two of its extreme flavors: constraint-guarded variability modeling and constraint-driven variability modeling. The former applies model checking to establish the global consistency of product variants which are built by manual specification of variations points, whereas the latter uses synthesis technology to fully automatically generate product variants that satisfy all given constraints. Each flavor is illustrated by means of a concrete case study. {\textcopyright} 2012 Springer-Verlag.},
	Annote = {cited By 21},
	Doi = {10.1007/s10009-012-0254-x},
	Keywords = {Constraint-based; Global consistency; Product vari,Information systems; Software engineering,Model checking},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866284416{\&}doi=10.1007{\%}2Fs10009-012-0254-x{\&}partnerID=40{\&}md5=63ea47588f2d468bea6a411704f58378}
}

@Article{Jaksic2014122,
	Title = {{Evaluating the usability of a visual feature modeling notation}},
	Author = {Jaksic, A and France, R B and Collet, P and Ghosh, S},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2014},
	Pages = {122--140},
	Volume = {8706},
	Abstract = {Feature modeling is a popular Software Product Line Engineering (SPLE) technique used to describe variability in a product family. A usable feature modeling tool environment should enable SPLE practitioners to produce good quality models, in particular, models that effectively communicate modeled information. FAMILIAR is a text-based environment for manipulating and composing Feature Models (FMs). In this paper we present extensions we made to FAMILIAR to enhance its usability. The extensions include a visualization of FMs, or more precisely, a feature diagram rendering mechanism that supports the use of a combination of text and graphics to describe FMs, their configurations, and the results of FM analyses. We also present the results of a preliminary evaluation of the environment's usability. The evaluation involves comparing the use of the extended environment with the previous text-based console-driven version. The preliminary experiment provides some evidence that use of the new environment results in increased cognitive effectiveness of novice users and improved quality of new FMs. {\textcopyright} Springer International Publishing Switzerland 2014.},
	Annote = {cited By 1},
	Keywords = {Computer graphics; Computer software; Visualizatio,FAMILIAR; Feature modeling; Model driven developm,Software design},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921862446{\&}partnerID=40{\&}md5=2ad230a54434045936f51004b1c8b0e6}
}

@Article{Jalote:2008:PRG:13487689.13487690,
	Title = {{Post-release Reliability Growth in Software Products}},
	Author = {Jalote, Pankaj and Murphy, Brendan and Sharma, Vibhu Saujanya},
	Journal = {ACM Trans. Softw. Eng. Methodol.},
	Year = {2008},
	Number = {4},
	Pages = {17:1----17:20},
	Volume = {17},
	Address = {New York, NY, USA},
	Doi = {10.1145/13487689.13487690},
	ISSN = {1049-331X},
	Keywords = {Post-release reliability growth,product stabilization time},
	Publisher = {ACM},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/13487689.13487690}
}

@Article{Jamshidi2015249,
	Title = {{Orthogonal to support multi-cloud application configuration}},
	Author = {Jamshidi, P and Pahl, C},
	Journal = {Communications in Computer and Information Science},
	Year = {2015},
	Pages = {249--261},
	Volume = {508},
	Abstract = {Cloud service providers benefit from a vast majority of customers due to variability and making profit from commonalities between the cloud services that they provide. Recently, application configuration dimensions has been increased dramatically due to multi-tenant, multi-device and multi-cloud paradigm. This challenges the configuration and customization of cloud-based software that are typically offered as a service due to the intrinsic variability. In this paper, we present a model-driven approach based on variability models originating from the software product line community to handle such multi-dimensional variability in the cloud. We exploit orthogonal variability models to systematically manage and create tenant-specific configuration and customizations. We also demonstrate how such variability models can be utilized to take into account the already deployed application parts to enable harmonized deployments for new tenants in a multi-cloud setting. The approach considers application functional and non-functional requirements to provide a set of valid multi-cloud configurations. We illustrate our approach through a case study. {\textcopyright} Springer International Publishing Switzerland 2015.},
	Annote = {cited By 0},
	Doi = {10.1007/978-3-319-14886-1_23},
	Keywords = {Cloud architectures; Cloud service providers; Int,Cloud computing,Distributed database systems},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924168003{\&}doi=10.1007{\%}2F978-3-319-14886-1{\_}23{\&}partnerID=40{\&}md5=a1e8c513d863386eefdc8b6c17e84678}
}

@Article{SPE:SPE2442,
	Title = {{Pattern-based multi-cloud architecture migration}},
	Author = {Jamshidi, Pooyan and Pahl, Claus and Mendon{\c{c}}a, Nabor C},
	Journal = {Software: Practice and Experience},
	Year = {2016},
	Pages = {n/a----n/a},
	Abstract = {Many organizations migrate on-premise software applications to the cloud. However, current coarse-grained cloud migration solutions have made such migrations a non transparent task, an endeavor based on trial-and-error. This paper presents Variability-based, Pattern-driven Architecture Migration (V-PAM), a migration method based on (i) a catalogue of fine-grained service-based cloud architecture migration patterns that target multi-cloud, (ii) a situational migration process framework to guide pattern selection and composition, and (iii) a variability model to structure system migration into a coherent framework. The proposed migration patterns are based on empirical evidence from several migration projects, best practice for cloud architectures and a systematic literature review of existing research. Variability-based, Pattern-driven Architecture Migration allows an organization to (i) select appropriate migration patterns, (ii) compose them to define a migration plan, and (iii) extend them based on the identification of new patterns in new contexts. The patterns are at the core of our solution, embedded into a process model, with their selection governed by a variability model. Copyright {\textcopyright} 2016 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spe.2442},
	ISSN = {1097-024X},
	Keywords = {cloud architecture,cloud migration,microservice architecture,migration pattern,multi-cloud,situational method engineering,variability model},
	Publisher = {John Wiley {\&} Sons, Ltd},
	Url = {http://dx.doi.org/10.1002/spe.2442}
}

@Article{SPE:SPE1143,
	Title = {{Flex-eWare: a flexible model driven solution for designing and implementing embedded distributed systems}},
	Author = {Jan, Mathieu and Jouvray, Christophe and Kordon, Fabrice and Kung, Antonio and Lalande, Jimmy and Loiret, Fr{\'{e}}d{\'{e}}ric and Navas, Juan and Pautet, Laurent and Pulou, Jacques and Radermacher, Ansgar and Seinturier, Lionel},
	Journal = {Software: Practice and Experience},
	Year = {2012},
	Number = {12},
	Pages = {1467--1494},
	Volume = {42},
	Abstract = {The complexity of modern embedded systems increases as they incorporate new concerns such as distribution and mobility. These new features need to be considered as early as possible in the software development life cycle. Model driven engineering promotes an intensive use of models and is now widely seen as a solution to master the development of complex systems such as embedded ones. Component-based software engineering is another major trend that gains acceptance in the embedded world because of its properties such as reuse, modularity, and flexibility.This article proposes the Flex-eWare component model (FCM) for designing and implementing modern embedded systems. The FCM unifies model driven engineering and component-based software engineering and has been evaluated in several application domains with different requirements: wireless sensor networks, distributed client/server applications, and control systems for electrical devices. This approach highlights a new concept: flexibility points that arise at several stages of the development process, that is, in the model (design phase), in the execution platform, and during the execution itself. This flexibility points are captured with model libraries that can extend the FCM. Copyright {\textcopyright} 2011 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spe.1143},
	ISSN = {1097-024X},
	Keywords = {embedded system,flexibility,model driven engineering,software component},
	Publisher = {John Wiley {\&} Sons, Ltd},
	Url = {http://dx.doi.org/10.1002/spe.1143}
}

@Article{SMR:SMR1602,
	Title = {{Managing changes in requirements: an empirical investigation}},
	Author = {Janes, Andrea and Remencius, Tadas and Sillitti, Alberto and Succi, Giancarlo},
	Journal = {Journal of Software: Evolution and Process},
	Year = {2013},
	Number = {12},
	Pages = {1273--1283},
	Volume = {25},
	Abstract = {This paper describes the challenges of handling changing requirements in software companies. This empirical investigation deals with the different sources of changes and with the different approaches to requirements evolution. We have considered the point of view of 35 managers of software companies interviewed through a questionnaire. The final results highlight some areas to improve requirements engineering and requirements management. Copyright {\textcopyright} 2013 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/smr.1602},
	ISSN = {2047-7481},
	Keywords = {empirical investigation,management,requirements},
	Url = {http://dx.doi.org/10.1002/smr.1602}
}

@Article{Jansen2008536,
	Title = {{Documenting after the fact: Recovering architectural design decisions}},
	Author = {Jansen, Anton and Bosch, Jan and Avgeriou, Paris},
	Journal = {Journal of Systems and Software},
	Year = {2008},
	Number = {4},
	Pages = {536--557},
	Volume = {81},
	Abstract = {Software architecture documentation helps people in understanding the software architecture of a system. In practice, software architectures are often documented after the fact, i.e. they are maintained or created after most of the design decisions have been made and implemented. To keep the architecture documentation up-to-date an architect needs to recover and describe these decisions. This paper presents ADDRA, an approach an architect can use for recovering architectural design decisions after the fact. {\{}ADDRA{\}} uses architectural deltas to provide the architect with clues about these design decisions. This allows the architect to systematically recover and document relevant architectural design decisions. The recovered architectural design decisions improve the documentation of the architecture, which increases traceability, communication, and general understanding of a system. },
	Annote = {Selected papers from the 10th Conference on Software Maintenance and Reengineering (CSMR 2006)},
	Doi = {https://doi.org/10.1016/j.jss.2007.08.025},
	ISSN = {0164-1212},
	Keywords = {Architectural design decisions,Software architecture recovery},
	Url = {http://www.sciencedirect.com/science/article/pii/S016412120700194X}
}

@Article{SMR:SMR330,
	Title = {{Integrated development and maintenance for the release, delivery, deployment, and customization of product software: a case study in mass-market ERP software}},
	Author = {Jansen, Slinger and Ballintijn, Gerco and Brinkkemper, Sjaak and van Nieuwland, Arco},
	Journal = {Journal of Software Maintenance and Evolution: Research and Practice},
	Year = {2006},
	Number = {2},
	Pages = {133--151},
	Volume = {18},
	Abstract = {The maintenance of enterprise application software at a customer site is a complex task for software vendors. This complexity results in a significant amount of work and risk. This article presents a case study of a product software vendor that tries to reduce this complexity by integrating product data management (PDM), software configuration management (SCM), and customer relationship management (CRM) into one system. The case study shows that by combining these management areas in a single software knowledge base, software maintenance processes can be automated and improved, thereby enabling a software vendor of enterprise resource planning software to serve a large number of customers with many different product configurations. Copyright {\textcopyright} 2006 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/smr.330},
	ISSN = {1532-0618},
	Keywords = {customer relationship management,customizations,deployment,product data management,product release,product software,software configuration management,software delivery},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/smr.330}
}

@Article{Jaring2004449,
	Title = {{Expressing product diversification - Categorizing and classifying variability in software product family engineering}},
	Author = {Jaring, M and Bosch, J},
	Journal = {International Journal of Software Engineering and Knowledge Engineering},
	Year = {2004},
	Number = {5},
	Pages = {449--470},
	Volume = {14},
	Abstract = {In a software product family context, software architects design architectures that support product diversification in both space (multiple contexts) and time (changing contexts). Product diversification is based on the concept of variability: a single architecture and a set of components support a family of products. Software product families have to support increasing amounts of variability, thereby making variability engineering a primary concern in software product family development. The first part of this paper (1) suggests a two-dimensional, orthogonal categorization of variability realization techniques and classifies these variability categories into system maturity levels. The second part (2) discusses a case study of an industrial software product family of mobile communication infrastructure for professional markets such as the military. The study categorizes and classifies the variability in this product family according to criteria common to virtually all software development projects.},
	Annote = {cited By 3},
	Doi = {10.1142/S0218194004001804},
	Keywords = {Computer aided design; Computer architecture; Mark,Product diversification; Software product familie,Software engineering},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-10044283207{\&}doi=10.1142{\%}2FS0218194004001804{\&}partnerID=40{\&}md5=d4be64864295692ddc6446ec441911ea}
}

@Article{Jaring200481,
	Title = {{Variability dependencies in product family engineering}},
	Author = {Jaring, M and Bosch, J},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2004},
	Pages = {81--97},
	Volume = {3014},
	Abstract = {In a product family context, software architects anticipate product diversification and design architectures that support variants in both space (multiple contexts) and time (changing contexts). Product diversification is based on the concept of variability: a single architecture and a set of components support a family of products. Software product families need to support increasing amounts of variability, leading to a situation where variability dependencies become of primary concern. This paper discusses (1) a taxonomy of variability dependencies and (2) a case study in designing a program monitor and exception handler for a legacy system. The study shows that the types of variability dependencies in a system depend on how the system is designed and architected. {\textcopyright} Springer-Verlag Berlin Heidelberg 2004.},
	Annote = {cited By 9},
	Keywords = {Design architecture; Exception handlers; Multiple,Legacy systems; Software architecture,Product design},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-35048866917{\&}partnerID=40{\&}md5=ccfba4d47481eb567d6b15890b866eec}
}

@Article{Jaring200215,
	Title = {{Representing variability in software product lines: A case study}},
	Author = {Jaring, M and Bosch, J},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2002},
	Pages = {15--36},
	Volume = {2379},
	Abstract = {Variability is the ability to change or customize a software system (i.e., software architects anticipate change and design architectures that support those changes). If the architecture is used for different product versions (e.g., in a software product line context, it becomes important to understand where change has to be planned and the possible options in particular situations. Three variability issues have been identified in a case study involving a software company specializing in product and system development for a professional mobile communication infrastructure. These issues are discussed and analyzed and illustrate the need for handling variability in a more explicit manner. To address this need, this paper suggests a method to represent and normalize variability in industrial software systems. The method is exemplified by applying it to the software product line of the aforementioned company. {\textcopyright} Springer-Verlag Berlin Heidelberg 2002.},
	Annote = {cited By 39},
	Keywords = {Computer software,Design architecture; Industrial software; Mobile,Software architecture; Software design},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974661208{\&}partnerID=40{\&}md5=860864deb3816bcf9b81bf59258e5a5e}
}

@Article{SPE:SPE558,
	Title = {{Representing variability in a family of MRI scanners}},
	Author = {Jaring, M and Krikhaar, R L and Bosch, J},
	Journal = {Software: Practice and Experience},
	Year = {2004},
	Number = {1},
	Pages = {69--100},
	Volume = {34},
	Doi = {10.1002/spe.558},
	File = {:Users/mac/Downloads/Jaring{\_}et{\_}al-2004-Software-{\_}Practice{\_}and{\_}Experience.pdf:pdf},
	ISSN = {1097-024X},
	Keywords = {dependencies,product derivation,software product families,variability modeling},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/spe.558}
}

@Article{Jarke2011992,
	Title = {{The brave new world of design requirements}},
	Author = {Jarke, Matthias and Loucopoulos, Pericles and Lyytinen, Kalle and Mylopoulos, John and Robinson, William},
	Journal = {Information Systems},
	Year = {2011},
	Number = {7},
	Pages = {992--1008},
	Volume = {36},
	Abstract = {Despite its success over the last 30 years, the field of Requirements Engineering (RE) is still experiencing fundamental problems that indicate a need for a change of focus to better ground its research on issues underpinning current practices. We posit that these practices have changed significantly in recent years. To this end we explore changes in software system operational environments, targets, and the process of RE. Our explorations include a field study, as well as two workshops that brought together experts from academia and industry. We recognize that these changes influence the nature of central {\{}RE{\}} research questions. We identify four new principles that underlie contemporary requirements processes, namely: (1) intertwining of requirements with implementation and organizational contexts, (2) dynamic evolution of requirements, (3) emergence of architectures as a critical stabilizing force, and (4) need to recognize unprecedented levels of design complexity. We recommend a re-focus of {\{}RE{\}} research based on a review and analysis of these four principles, and identify several theoretical and practical implications that flow from this analysis. },
	Annote = {Special Issue: Advanced Information Systems Engineering (CAiSE'10)},
	Doi = {https://doi.org/10.1016/j.is.2011.04.003},
	ISSN = {0306-4379},
	Keywords = {Architectures,Complexity,Evolution,Future of requirements engineering,Requirements,Requirements engineering,Requirements principles},
	Url = {http://www.sciencedirect.com/science/article/pii/S0306437911000548}
}

@Article{Jezek2015129,
	Title = {{How Java {\{}APIs{\}} break – An empirical study}},
	Author = {Jezek, Kamil and Dietrich, Jens and Brada, Premek},
	Journal = {Information and Software Technology},
	Year = {2015},
	Pages = {129--146},
	Volume = {65},
	Abstract = {AbstractContext It has become common practice to build programs by using libraries. While the benefits of reuse are well known, an often overlooked risk are system runtime failures due to {\{}API{\}} changes in libraries that evolve independently. Traditionally, the consistency between a program and the libraries it uses is checked at build time when the entire system is compiled and tested. However, the trend towards partially upgrading systems by redeploying only evolved library versions results in situations where these crucial verification steps are skipped. For Java programs, partial upgrades create additional interesting problems as the compiler and the virtual machine use different rule sets to enforce contracts between the providers and the consumers of APIs. Objective We have studied the extent of the problem in real world programs. We were interested in two aspects: the compatibility of {\{}API{\}} changes as libraries evolve, and the impact this has on programs using these libraries. Method This study is based on the qualitas corpus version 20120401. A data set consisting of 109 Java open-source programs and 564 program versions was used from this corpus. We have investigated two types of library dependencies: explicit dependencies to embedded libraries, and dependencies defined by symbolic references in Maven build files that are resolved at build time. We have used JaCC for {\{}API{\}} analysis, this tool is based on the popular {\{}ASM{\}} byte code analysis library. Results We found that for most of the programs we investigated, {\{}APIs{\}} are unstable as incompatible changes are common. Surprisingly, there are more compatibility problems in projects that use automated dependency resolution. However, we found only a few cases where this has an actual impact on other programs using such an API. Conclusion It is concluded that {\{}API{\}} instability is common and causes problems for programs using these APIs. Therefore, better tools and methods are needed to safeguard library evolution. },
	Doi = {https://doi.org/10.1016/j.infsof.2015.02.014},
	ISSN = {0950-5849},
	Keywords = {API evolution,Backward compatibility,Binary compatibility,Byte-code,Java},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584915000506}
}

@Article{Johnsen201567,
	Title = {{Integrating deployment architectures and resource consumption in timed object-oriented models}},
	Author = {Johnsen, Einar Broch and Schlatte, Rudolf and Tarifa, S Lizeth Tapia},
	Journal = {Journal of Logical and Algebraic Methods in Programming},
	Year = {2015},
	Number = {1},
	Pages = {67--91},
	Volume = {84},
	Abstract = {Abstract Software today is often developed for many deployment scenarios; the software may be adapted to sequential, concurrent, distributed, and even virtualized architectures. Since software performance can vary significantly depending on the target architecture, design decisions need to address which features to include and what performance to expect for different architectures. To make use of formal methods for these design decisions, system models need to range over deployment scenarios. For this purpose, it is desirable to lift aspects of low-level deployment to the abstraction level of the modeling language. This paper proposes an integration of deployment architectures in the Real-Time {\{}ABS{\}} language, with restrictions on processing resources. Real-Time {\{}ABS{\}} is a timed, abstract and behavioral specification language with a formal semantics and a Java-like syntax, that targets concurrent, distributed and object-oriented systems. A separation of concerns between execution cost at the object level and execution capacity at the deployment level makes it easy to compare the timing and performance of different deployment scenarios already during modeling. The language and associated simulation tool is demonstrated on examples and its semantics is formalized. },
	Annote = {Special Issue: The 23rd Nordic Workshop on Programming Theory (NWPT 2011)Special Issue: Domains X, International workshop on Domain Theory and applications, Swansea, 5-7 September, 2011},
	Doi = {https://doi.org/10.1016/j.jlamp.2014.07.001},
	ISSN = {2352-2208},
	Keywords = {Deployment architecture,Formal methods,Object orientation,Performance,Real-Time {\{}ABS{\}},Resource management},
	Url = {http://www.sciencedirect.com/science/article/pii/S2352220814000479}
}

@Article{deJonge200963,
	Title = {{Developing Product Lines with Third-Party Components}},
	Author = {de Jonge, Merijn},
	Journal = {Electronic Notes in Theoretical Computer Science},
	Year = {2009},
	Number = {5},
	Pages = {63--80},
	Volume = {238},
	Abstract = {The trends toward product line development and toward adopting more third-party software are hard to combine. The reason is that product lines demand fine control over the software (e.g., for diversity management), while third-party software (almost by definition) provides only little or no control. A growing use of third-party software may therefore lead to less control over the product development process or, vice-versa, requiring large control over the software may limit the ability to use third-party components. Since both are means to reduce costs and to shorten time to market, the question is whether they can be combined effectively. In this paper, we describe our solution to this problem which combines the Koala component model developed within Philips with the concept of build-level components. We show that by lifting component granularity of Koala components from individual C files to build-level components, both trends can be united. The Koala architectural description language is used to orchestrate product composition and to manage diversity, while build-level components form the unit of third-party component composition. },
	Annote = {Proceedings of the 8th Workshop on Language Descriptions, Tools and Applications (LDTA 2008)},
	Doi = {https://doi.org/10.1016/j.entcs.2009.09.041},
	ISSN = {1571-0661},
	Keywords = {Koala,build-level components,software composition,software product lines,third-party sofware},
	Url = {http://www.sciencedirect.com/science/article/pii/S1571066109003958}
}

@Article{Jordan2015120,
	Title = {{A feature model of actor, agent, functional, object, and procedural programming languages}},
	Author = {Jordan, Howell and Botterweck, Goetz and Noll, John and Butterfield, Andrew and Collier, Rem},
	Journal = {Science of Computer Programming},
	Year = {2015},
	Pages = {120--139},
	Volume = {98, Part 2},
	Abstract = {Abstract The number of programming languages is large and steadily increasing. However, little structured information and empirical evidence is available to help software engineers assess the suitability of a language for a particular development project or software architecture. We argue that these shortages are partly due to a lack of high-level, objective programming language feature assessment criteria: existing advice to practitioners is often based on ill-defined notions of ‘paradigms' [3, p. xiii] and ‘orientation', while researchers lack a shared common basis for generalisation and synthesis of empirical results. This paper presents a feature model constructed from the programmer's perspective, which can be used to precisely compare general-purpose programming languages in the actor-oriented, agent-oriented, functional, object-oriented, and procedural categories. The feature model is derived from the existing literature on general concepts of programming, and validated with concrete mappings of well-known languages in each of these categories. The model is intended to act as a tool for both practitioners and researchers, to facilitate both further high-level comparative studies of programming languages, and detailed investigations of feature usage and efficacy in specific development contexts. },
	Annote = {Special Issue on Programming Based on Actors, Agents and Decentralized Control},
	Doi = {https://doi.org/10.1016/j.scico.2014.02.009},
	ISSN = {0167-6423},
	Keywords = {Agent-oriented programming,Functional programming,Object-oriented programming,Programming language constructs,Programming languages},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642314000501}
}

@InProceedings{Jureczko:2010:TIS:1868328.1868342,
	Title = {{Towards Identifying Software Project Clusters with Regard to Defect Prediction}},
	Author = {Jureczko, Marian and Madeyski, Lech},
	Booktitle = {Proceedings of the 6th International Conference on Predictive Models in Software Engineering},
	Year = {2010},
	Address = {New York, NY, USA},
	Pages = {9:1----9:10},
	Publisher = {ACM},
	Series = {PROMISE '10},
	Doi = {10.1145/1868328.1868342},
	ISBN = {978-1-4503-0404-7},
	Keywords = {clustering,defect prediction,design metrics,size metrics},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/1868328.1868342}
}

@Article{SMR:SMR492,
	Title = {{An information systems design product theory for the class of integrated requirements and release management systems}},
	Author = {K{\"{a}}k{\"{o}}l{\"{a}}, Timo and Koivulahti-Ojala, Mervi and Liimatainen, Jani},
	Journal = {Journal of Software Maintenance and Evolution: Research and Practice},
	Year = {2011},
	Number = {6},
	Pages = {443--463},
	Volume = {23},
	Abstract = {High-tech companies conducting product development need to collect and analyze requirements effectively, plan and implement releases, and allocate requirements to appropriate releases. Requirements and release management are complicated because development activities typically are scattered across multiple sites, involve multiple partners in different countries, leverage various development methods and tools, and are realized through various organizational arrangements such as release projects in organizations structured around products and permanent release teams in organizations responsible for the long-term development and maintenance of strategic software and hardware assets. Flexible, scalable, and secure groupware-based support for the activities provides substantial payoffs. Yet, the extant literature provides little theoretical guidance for designing and using requirements and release management systems (RRMS) in multi-site, multi-partner environments. This article develops the meta-requirements and a meta-design of an Information Systems Design Product Theory for the class of RRMS based on a case study in a global company and a literature review. The theory is scalable to meet the needs of global companies but simple enough so that small and medium-sized companies can also leverage it to implement requirements and release management solutions. Copyright {\textcopyright} 2010 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/smr.492},
	ISSN = {1532-0618},
	Keywords = {,global software product development,information systems design theory,knowledge management,release management,requirements management,software process improvement},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/smr.492 http://dx.doi.org/10.1002/smr.448}
}

@Article{Kastner:2012:TCA:2211616.2211617,
	Title = {{Type Checking Annotation-based Product Lines}},
	Author = {K{\"{a}}stner, Christian and Apel, Sven and Th{\"{u}}m, Thomas and Saake, Gunter},
	Journal = {ACM Trans. Softw. Eng. Methodol.},
	Year = {2012},
	Number = {3},
	Pages = {14:1----14:39},
	Volume = {21},
	Address = {New York, NY, USA},
	Doi = {10.1145/2211616.2211617},
	ISSN = {1049-331X},
	Keywords = {{\#}ifdef,CFJ,CIDE,Featherweight Java,conditional compilation,software product lines,type system},
	Publisher = {ACM},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2211616.2211617}
}

@Article{Koksal2017191,
	Title = {{Obstacles in Data Distribution Service Middleware: A Systematic Review}},
	Author = {K{\"{o}}ksal, {\"{O}}mer and Tekinerdogan, Bedir},
	Journal = {Future Generation Computer Systems},
	Year = {2017},
	Pages = {191--210},
	Volume = {68},
	Abstract = {Abstract Context: Data Distribution Service (DDS) is a standard data-centric publish–subscribe programming model and specification for distributed systems. {\{}DDS{\}} has been applied for the development of high performance distributed systems such as in the defense, finance, automotive, and simulation domains. Various papers have been written on the application of DDS, however, there has been no attempt to systematically review and categorize the identified obstacles. Objective: The overall objective of this paper is to identify the state of the art of DDS, and describe the main lessons learned and obstacles in applying DDS. In addition, we aim to identify the important open research issues. Method: A systematic literature review (SLR) is conducted by a multiphase study selection process using the published literature since the introduction of {\{}DDS{\}} in 2003. Results: We reviewed 468 papers that are discovered using a well-planned review protocol, and 34 of them were assessed as primary studies related to our research questions. Conclusions: We have identified 11 basic categories for describing the identified obstacles and the corresponding research challenges that can be used to depict the state-of-the-art in {\{}DDS{\}} and provide a vision for further research. },
	Doi = {https://doi.org/10.1016/j.future.2016.09.020},
	ISSN = {0167-739X},
	Keywords = {Data Distribution Service (DDS),Middleware,Systematic literature review},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167739X1630351X}
}

@Article{JPIM:JPIM257,
	Title = {{Abstracts}},
	Author = {Kahn, Kenneth B},
	Journal = {Journal of Product Innovation Management},
	Year = {2007},
	Number = {4},
	Pages = {392--403},
	Volume = {24},
	Doi = {10.1111/j.1540-5885.2007.00257.x},
	ISSN = {1540-5885},
	Publisher = {Blackwell Publishing Inc},
	Url = {http://dx.doi.org/10.1111/j.1540-5885.2007.00257.x}
}

@Article{Kakarontzas2011999,
	Title = {{Agents, clusters and components: A synergistic approach to the {\{}GSP{\}}}},
	Author = {Kakarontzas, G and Savvas, I K and Stamelos, I},
	Journal = {Future Generation Computer Systems},
	Year = {2011},
	Number = {8},
	Pages = {999--1010},
	Volume = {27},
	Abstract = {Grids provide access to a vast amount of computational resources for the execution of demanding computations. These resources are geographically distributed, owned by different organizations and are vastly heterogeneous. The aforementioned factors introduce uncertainty in all phases of a Grid Scheduling Process (GSP). This work describes a synergistic multidisciplinary approach which aims at addressing this uncertainty. It proposes a network of resource representatives (RRs), which maintain the more or less static characteristics of available workers they represent. Clustering techniques are used for the efficient searching in the network of {\{}RRs{\}} by client agents. After the discovery of possibly suitable resources, client agents and resource agents negotiate directly for the selection of the best available resource set. Finally, according to the characteristics of the selected resource set and its current state, we propose a component-based application configuration approach based on component variants, that adjusts the application for the forthcoming execution phase in the selected resource set. We evaluate our approach using simulation and we show that it outperforms centralized index approaches for large computational grids. },
	Doi = {https://doi.org/10.1016/j.future.2011.05.002},
	ISSN = {0167-739X},
	Keywords = {Clustering,Grid scheduling process,Software agents,Software components},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167739X11000835}
}

@Article{Kanewala20141219,
	Title = {{Testing scientific software: A systematic literature review}},
	Author = {Kanewala, Upulee and Bieman, James M},
	Journal = {Information and Software Technology},
	Year = {2014},
	Number = {10},
	Pages = {1219--1232},
	Volume = {56},
	Abstract = {AbstractContext Scientific software plays an important role in critical decision making, for example making weather predictions based on climate models, and computation of evidence for research publications. Recently, scientists have had to retract publications due to errors caused by software faults. Systematic testing can identify such faults in code. Objective This study aims to identify specific challenges, proposed solutions, and unsolved problems faced when testing scientific software. Method We conducted a systematic literature survey to identify and analyze relevant literature. We identified 62 studies that provided relevant information about testing scientific software. Results We found that challenges faced when testing scientific software fall into two main categories: (1) testing challenges that occur due to characteristics of scientific software such as oracle problems and (2) testing challenges that occur due to cultural differences between scientists and the software engineering community such as viewing the code and the model that it implements as inseparable entities. In addition, we identified methods to potentially overcome these challenges and their limitations. Finally we describe unsolved challenges and how software engineering researchers and practitioners can help to overcome them. Conclusions Scientific software presents special challenges for testing. Specifically, cultural differences between scientist developers and software engineers, along with the characteristics of the scientific software make testing more difficult. Existing techniques such as code clone detection can help to improve the testing process. Software engineers should consider special challenges posed by scientific software such as oracle problems when developing testing techniques. },
	Doi = {https://doi.org/10.1016/j.infsof.2014.05.006},
	ISSN = {0950-5849},
	Keywords = {Scientific software,Software quality,Software testing,Systematic literature review},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584914001232}
}

@Article{Kang200545,
	Title = {{Feature-oriented re-engineering of legacy systems into product line assets - A case study}},
	Author = {Kang, K C and Kim, M and Lee, J and Kim, B},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2005},
	Pages = {45--56},
	Volume = {3714 LNCS},
	Abstract = {Home service robots have a wide range of potential applications, such as home security, patient caring, cleaning, etc. The services provided by the robots in each application area are being defined as markets are formed and, therefore, they change constantly. Thus, robot applications need to evolve both quickly and flexibly adopting frequently changing requirements. This makes software product line framework ideal for the domain of home service robots. Unfortunately, however, robot manufacturers often focus on developing technical components (e.g., vision recognizer and speech processor) and then attempt to develop robots by integrating these components in an ad-hoc way. This practice produces robot applications that are hard to re-use and evolve when requirements change. We believe that re-engineering legacy robot applications into product line assets can significantly enhance reusability and evolvability. In this paper, we present our experience of re-engineering legacy home service robot applications into product line assets through feature modeling and analysis. First, through reverse engineering, we recovered architectures and components of the legacy applications. Second, based on the recovered information and domain knowledge, we reconstructed a feature model for the legacy applications. Anticipating changes in business opportunities or technologies, we restructured and refined the feature model to produce a feature model for the product line. Finally, based on the refined feature model and engineering principles we adopted for asset development, we designed a new architecture and components for robot applications. {\textcopyright} Springer-Verlag Berlin Heidelberg 2005.},
	Annote = {cited By 19},
	Doi = {10.1007/11554844_6},
	Keywords = {Artificial intelligence; Computer architecture; Co,Asset development; Business opportunities; Evolva,Reverse engineering},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646181380{\&}doi=10.1007{\%}2F11554844{\_}6{\&}partnerID=40{\&}md5=9c88e06d6a7ef9f72602c858dceb5f92}
}

@Article{Karataş20132295,
	Title = {{From extended feature models to constraint logic programming}},
	Author = {Karataş, Ahmet Serkan and Oğuzt{\"{u}}z{\"{u}}n, Halit and Doğru, Ali},
	Journal = {Science of Computer Programming},
	Year = {2013},
	Number = {12},
	Pages = {2295--2312},
	Volume = {78},
	Abstract = {Since feature models for realistic product families may be quite complicated, the automated analysis of feature models is desirable. Although several approaches reported in the literature address this issue, complex cross-tree relationships involving attributes in extended feature models have not been handled. In this article, we introduce a mapping from extended feature models to constraint logic programming over finite domains. This mapping is used to translate into constraint logic programs; basic, cardinality-based and extended feature models, which can include complex cross-tree relationships involving attributes. This translation enables the use of off-the-shelf constraint solvers for the automated analysis of extended feature models involving such complex relationships. We also present the performance results of some well-known analysis operations on an example translated model. },
	Annote = {Special Section on International Software Product Line Conference 2010 and Fundamentals of Software Engineering (selected papers of {\{}FSEN{\}} 2011)},
	Doi = {https://doi.org/10.1016/j.scico.2012.06.004},
	ISSN = {0167-6423},
	Keywords = {Constraint logic programming,Extended feature model,Feature attribute,Variability modeling},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642312001153}
}

@Article{Karhu2009663,
	Title = {{Investigating the relationship between schedules and knowledge transfer in software testing}},
	Author = {Karhu, Katja and Taipale, Ossi and Smolander, Kari},
	Journal = {Information and Software Technology},
	Year = {2009},
	Number = {3},
	Pages = {663--677},
	Volume = {51},
	Abstract = {This empirical study investigates the relationship between schedules and knowledge transfer in software testing. In our exploratory survey, statistical analysis indicated that increased knowledge transfer between testing and earlier phases of software development was associated with testing schedule over-runs. A qualitative case study was conducted to interpret this result. We found that this relationship can be explained with the size and complexity of software, knowledge management issues, and customer involvement. We also found that the primary strategies for avoiding testing schedule over-runs were reducing the scope of testing, leaving out features from the software, and allocating more resources to testing. },
	Doi = {https://doi.org/10.1016/j.infsof.2008.09.001},
	ISSN = {0950-5849},
	Keywords = {Case study,Knowledge transfer,Schedule over-runs,Software testing,Survey},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584908001249}
}

@Article{Karimpour2017189,
	Title = {{Evolutionary robust optimization for software product line scoping: An explorative study}},
	Author = {Karimpour, Reza and Ruhe, Guenther},
	Journal = {Computer Languages, Systems {\&} Structures},
	Year = {2017},
	Pages = {189--210},
	Volume = {47, Part 2},
	Abstract = {Abstract Background: Software product line (SPL) scoping is an important phase when planning for product line adoption. An {\{}SPL{\}} scope specifies: (1) the extent of the domain supported by the product line, (2) portfolio of products in the product line and (3) list of assets to be developed for reuse across the family of products. Issue: {\{}SPL{\}} scope planning is usually based on estimates about the state of the market and the engineering capabilities of the development team. One challenge with these estimates is that there are inaccuracies due to uncertainty in the environment or accuracy of measurement. This may result in issues ranging from suboptimal plans to infeasible plans. Objective: To address the above, we propose to include uncertainty as part of the {\{}SPL{\}} scoping model. Plans developed in consideration of uncertainty would be more robust against possible fluctuations in estimates. Approach: In this paper, a method to incorporate uncertainty in scoping optimization and its application to generate robust solutions is proposed. We capture uncertainty as part of the formulation and model scoping optimization as a multi-objective problem with profit and stability as fitness functions. Profit stability and feasibility stability are considered to represent stability concerns. Results: Results show that, compared to other scope optimization approaches, both performance stability and feasibility stability are improved while maintaining near optimal performance for profit objective. Also, generated results consist of solutions with trade-offs between profit and stability, providing the decision maker with enhanced decision support. Conclusion: Multi-objective optimization with stability consideration for {\{}SPL{\}} scoping provides project managers with a robust and flexible way to address uncertainty in the process of {\{}SPL{\}} scoping. },
	Doi = {https://doi.org/10.1016/j.cl.2016.07.007},
	ISSN = {1477-8424},
	Keywords = {Evolutionary optimization,Robust optimization,Search-based software engineering,Software product line scoping},
	Url = {http://www.sciencedirect.com/science/article/pii/S1477842416301063}
}

@Article{Karus20111161,
	Title = {{Predicting the maintainability of {\{}XSL{\}} transformations}},
	Author = {Karus, Siim and Dumas, Marlon},
	Journal = {Science of Computer Programming},
	Year = {2011},
	Number = {12},
	Pages = {1161--1176},
	Volume = {76},
	Abstract = {{\{}XSLT{\}} is a popular language for implementing both presentation templates in Web applications as well as document and message converters in enterprise applications. The widespread adoption and popularity of {\{}XSLT{\}} raises the challenge of efficiently managing the evolution of significant amounts of {\{}XSLT{\}} code. This challenge calls for guidelines and tool support for developing maintainable {\{}XSLT{\}} code. In this setting, this paper addresses the following question: Can the maintainability of {\{}XSL{\}} transformations, measured in terms of code churn in the next revision of a transformation, be predicted using a combination of simple metrics? This question is studied using a dataset extracted from open-source software project repositories. An outcome of this empirical study is a set of statistical models for predicting the maintainability of {\{}XSL{\}} transformations with relatively high accuracy. In addition, by analyzing the major influencers of code churn in these models, the paper identifies guidelines for designing {\{}XSL{\}} transformations with reduced future churn. },
	Annote = {Special Issue on Software Evolution, Adaptability and Variability},
	Doi = {https://doi.org/10.1016/j.scico.2010.12.006},
	ISSN = {0167-6423},
	Keywords = {Software maintenance,Software metrics,XML,XSLT},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642310002315}
}

@Article{Kaur2017152,
	Title = {{Software component and the semantic Web: An in-depth content analysis and integration history}},
	Author = {Kaur, Loveleen and Mishra, Ashutosh},
	Journal = {Journal of Systems and Software},
	Year = {2017},
	Pages = {152--169},
	Volume = {125},
	Abstract = {Abstract With the advent of Component-based software engineering (CBSE), large software systems are being built by integrating pre-built software components. The Semantic Web in association with {\{}CBSE{\}} has shown to offer powerful representation facilities and reasoning techniques to enhance and support querying, reasoning, discovery, etc. of software components. The goal of this paper is to research the applicability of Semantic Web technologies in performing the various tasks of {\{}CBSE{\}} and review the experimental results of the same in an easy and effective manner. To the best of our knowledge, this is the first study which provides an extensive review of the application of Semantic Web in {\{}CBSE{\}} from different perspectives. A systematic literature review of the Semantic Web approaches, employed for use in CBSE, reported from 2001 until 2015, is conducted in this research article. Empirical results have been drawn through the question-answer based analysis of the research, which clearly tells the year wise trend of the research articles, with the possible justification of the usage of Semantic Web technology and tools for a particular phase of CBSE. To conclude, gaps in the current research and potential future prospects have been discussed. },
	Doi = {https://doi.org/10.1016/j.jss.2016.11.028},
	ISSN = {0164-1212},
	Keywords = {Component-based software engineering,Linked Data,Ontology,Reasoners,Semantic Web,Web services},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121216302308}
}

@InProceedings{Kelly:2001:CSU:782096.782103,
	Title = {{A Case Study in the Use of Defect Classification in Inspections}},
	Author = {Kelly, Diane and Shepard, Terry},
	Booktitle = {Proceedings of the 2001 Conference of the Centre for Advanced Studies on Collaborative Research},
	Year = {2001},
	Pages = {7----},
	Publisher = {IBM Press},
	Series = {CASCON '01},
	Keywords = {orthogonal defect classification,software engineering,software maintenance,software metrics,software testing,software validation},
	Url = {http://0-dl.acm.org.fama.us.es/citation.cfm?id=782096.782103}
}

@Article{CPE:CPE1844,
	Title = {{Optimized composition of performance-aware parallel components}},
	Author = {Kessler, C and L{\"{o}}we, W},
	Journal = {Concurrency and Computation: Practice and Experience},
	Year = {2012},
	Number = {5},
	Pages = {481--498},
	Volume = {24},
	Abstract = {We describe the principles of a novel framework for performance-aware composition of sequential and explicitly parallel software components with implementation variants. Automatic composition results in a table-driven implementation that, for each parallel call of a performance-aware component, looks up the expected best implementation variant, processor allocation and schedule given the current problem, and processor group sizes. The dispatch tables are computed off-line at component deployment time by an interleaved dynamic programming algorithm from time-prediction meta-code provided by the component supplier. Copyright {\textcopyright} 2011 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/cpe.1844},
	ISSN = {1532-0634},
	Keywords = {auto-tuning,parallel computing,program optimization,software component,software composition},
	Publisher = {John Wiley {\&} Sons, Ltd},
	Url = {http://dx.doi.org/10.1002/cpe.1844}
}

@Conference{Keunecke2014,
	Title = {{The feature pack approach: Systematically managing implementations in software ecosystems}},
	Author = {Keunecke, M and Brummermann, H and Schmid, K},
	Booktitle = {ACM International Conference Proceeding Series},
	Year = {2014},
	Abstract = {In an information system ecosystem customers integrate features, which are independently developed and evolved by multiple organizations. These features need to work together although there is little to no coordination among developer organizations. The handling of such ecosystems becomes the more challenging, the more the solutions provided by the different parties are intertwined. In this paper, we propose to handle implementations on a per-feature basis, and introduce an approach towards this goal, which we call feature packs. We discuss the requirements on such an approach and emphasize in particular the kind of analysis relevant to ensure that the system resulting from a corresponding aggregation of feature packs works reliably. We also illustrate a realization of the approach using a real-world ecosystem case study. {\textcopyright} 2014 ACM.},
	Annote = {cited By 0},
	Doi = {10.1145/2556624.2556639},
	Keywords = {Architecture; Computer applications; Computer pro,Ecosystems,Multiple organizations; Real-world; Software ecosy},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897630974{\&}doi=10.1145{\%}2F2556624.2556639{\&}partnerID=40{\&}md5=9f31d5616acda8f75350ab24c4f6e746}
}

@Article{Khalil201494,
	Title = {{Dependable wireless sensor networks for reliable and secure humanitarian relief applications}},
	Author = {Khalil, Issa M and Khreishah, Abdallah and Ahmed, Faheem and Shuaib, Khaled},
	Journal = {Ad Hoc Networks},
	Year = {2014},
	Pages = {94--106},
	Volume = {13, Part A},
	Abstract = {Disasters such as flooding, earthquake, famine and terrorist attacks might occur any time anywhere without prior warnings. In most cases it is difficult to predict when a disaster might occur however, well-planned disaster recovery procedures will reduce the intensity of expected consequences. When a disaster occurs, infrastructure based communications are most likely to be crippled, worsening the critical situation on hand. Wireless ad hoc and sensor network (WASN) technologies are proven to be valuable in coordinating and managing rescue operations during disasters. However, the increasing reliance on {\{}WASNs{\}} make them attractive to malicious attackers, especially terrorist groups, in a bid to hamper rescue operations amplifying the damage and increasing the number of casualties. Therefore, it is necessary to ensure the fidelity of data traffic through {\{}WASN{\}} against malicious traffic disruption attacks. In this paper, we first demonstrate how {\{}WASN{\}} can be used in a well-planned disaster recovery effort. Then, we introduce and analyze one of the most severe traffic disruption attacks against WASNs, called Identity Delegation, and its countermeasures. Its severity lies in its capability to evade detection by even state-of-the-art intrusion detection techniques such as the neighbor monitoring based mechanisms. Through identity delegation, an adversary can drop packets, evade detection, and frame innocent nodes for dropping the traffic. We introduce a technique to mitigate identity delegation attack, dubbed Sadec, and compare it with the state-of-the-art mitigation technique namely Basic Local Monitoring (BLM) under a wide range of network scenarios. Our analysis which is validated by extensive ns-2 simulation scenarios show that {\{}BLM{\}} fails to efficiently mitigate packet drop through identity delegation attacks while Sadec successfully mitigates them. The results also show that Sadec achieves higher delivery ratios of data packets compared to BLM. On the other hand, the results show similar behavior in framing probabilities between Sadec and BLM. However, the desirable features of Sadec come at the expense of higher false isolation probabilities in networks with heavy traffic load and poor communication links. },
	Annote = {(1)Special Issue : Wireless Technologies for Humanitarian Relief {\&} (2)Special Issue: Models And Algorithms For Wireless Mesh Networks},
	Doi = {https://doi.org/10.1016/j.adhoc.2012.06.002},
	ISSN = {1570-8705},
	Keywords = {Identity delegation,Local monitoring,Multi-hop wireless networks,Packet dropping,Security attacks},
	Url = {http://www.sciencedirect.com/science/article/pii/S1570870512001102}
}

@Article{Khosravi201274,
	Title = {{Using coordinated actors to model families of distributed systems}},
	Author = {Khosravi, R and Sabouri, H},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2012},
	Pages = {74--88},
	Volume = {7274 LNCS},
	Abstract = {Software product line engineering enables strategic reuse in development of families of related products. In a component-based approach to product line development, components capture functionalities appearing in one or more products in the family and different assemblies of components yield to various products or configurations. In this approach, an interaction model which effectively factors out the logic handling variability from the functionality of the system greatly enhances the reusability of components. We study the problem of variability modeling for a family of distributed systems expressed in actor model. We define a special type of actors called coordinators whose behavior is described as Reo circuits with the aim of encapsulating the variability logic. We have the benefits of Reo language for expressing coordination logic, while modeling the entire system as an actor-based distributed model. We have applied this model to a case study extracted from an industrial software family in the domain of interactive TV. {\textcopyright} 2012 IFIP International Federation for Information Processing.},
	Annote = {cited By 1},
	Doi = {10.1007/978-3-642-30829-1_6},
	Keywords = {Artificial intelligence,Component based approach; Distributed models; Dist,Reusability},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862733409{\&}doi=10.1007{\%}2F978-3-642-30829-1{\_}6{\&}partnerID=40{\&}md5=7af824eb3f66a995eea78f1c10a7643d}
}

@Article{Khurum20091982,
	Title = {{A systematic review of domain analysis solutions for product lines}},
	Author = {Khurum, M and Gorschek, T},
	Journal = {Journal of Systems and Software},
	Year = {2009},
	Number = {12},
	Pages = {1982--2003},
	Volume = {82},
	Abstract = {Domain analysis is crucial and central to software product line engineering (SPLE) as it is one of the main instruments to decide what to include in a product and how it should fit in to the overall software product line. For this reason many domain analysis solutions have been proposed both by researchers and industry practitioners. Domain analysis comprises various modeling and scoping activities. This paper presents a systematic review of all the domain analysis solutions presented until 2007. The goal of the review is to analyze the level of industrial application and/or empirical validation of the proposed solutions with the purpose of mapping maturity in terms of industrial application, as well as to what extent proposed solutions might have been evaluated in terms of usability and usefulness. The finding of this review indicates that, although many new domain analysis solutions for software product lines have been proposed over the years, the absence of qualitative and quantitative results from empirical application and/or validation makes it hard to evaluate the potential of proposed solutions with respect to their usability and/or usefulness for industry adoption. The detailed results of the systematic review can be used by individual researchers to see large gaps in research that give opportunities for future work, and from a general research perspective lessons can be learned from the absence of validation as well as from good examples presented. From an industry practitioner view, the results can be used to gauge as to what extent solutions have been applied and/or validated and in what manner, both valuable as input prior to industry adoption of a domain analysis solution. {\textcopyright} 2009 Elsevier Inc. All rights reserved.},
	Annote = {cited By 33},
	Doi = {10.1016/j.jss.2009.06.048},
	Keywords = {Computer software; Industrial applications; Indus,Domain analysis; Domain modeling; Empirical eviden,Research},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-71749101571{\&}doi=10.1016{\%}2Fj.jss.2009.06.048{\&}partnerID=40{\&}md5=918b8e050edb9174e0efeaeb1f2290ee}
}

@Article{SMR:SMR1560,
	Title = {{The software value map — an exhaustive collection of value aspects for the development of software intensive products}},
	Author = {Khurum, Mahvish and Gorschek, Tony and Wilson, Magnus},
	Journal = {Journal of Software: Evolution and Process},
	Year = {2013},
	Number = {7},
	Pages = {711--741},
	Volume = {25},
	Abstract = {In software intensive products such as cars or telecom systems, software has traditionally been associated with cost, and there has been no real perception of its value in relation to the entire product offering. However, because software is becoming a larger part of the main competitive advantage, driving innovation and product differentiation, hardware is becoming more standardized, thus the valuation of software is becoming critical. In existing literature, several value constructs and corresponding valuation/measurement solutions needed for making decisions about software product development are presented. However, the contributions are often isolated with respect to a certain perspective such as focusing on product's internal or external quality aspects only. Consequently, a complete view of value constructs relevant from different perspectives required for making decisions about software product development is missing. This paper presents a consolidated view of the software value concept utilizing the major perspectives and introduces a software value map. The created value map was evaluated through an industry case study through the development of impact evaluation patterns, which were subsequently used by professionals in industry, and experiences gathered. During industry evaluation, practitioners found substantial benefits of having a consolidated, vastly improved, and extended value aspect's view of software. Copyright {\textcopyright} 2012 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/smr.1560},
	ISSN = {2047-7481},
	Keywords = {decision-support,product customization,requirements engineering,software engineering management,software value,software value analysis,software value map,technology and software product management,value taxonomy,value-based software engineering},
	Url = {http://dx.doi.org/10.1002/smr.1560}
}

@Article{Kienzle2016122,
	Title = {{VCU: The three dimensions of reuse}},
	Author = {Kienzle, J and Mussbacher, G and Alam, O and Sch{\"{o}}ttle, M and Belloir, N and Collet, P and Combemale, B and DeAntoni, J and Klein, J and Rumpe, B},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2016},
	Pages = {122--137},
	Volume = {9679},
	Abstract = {Reuse, enabled by modularity and interfaces, is one of the most important concepts in software engineering. This is evidenced by an increasingly large number of reusable artifacts, ranging from small units such as classes to larger, more sophisticated units such as components, services, frameworks, software product lines, and concerns. This paper presents evidence that a canonical set of reuse interfaces has emerged over time: the variation, customization, and usage interfaces (VCU). A reusable artifact that provides all three interfaces reaches the highest potential of reuse, as it explicitly exposes how the artifact can be manipulated during the reuse process along these three dimensions. We demonstrate the wide applicability of the VCU interfaces along two axes: across abstraction layers of a system specification and across existing reuse techniques. The former is shown with the help of a comprehensive case study including reusable requirements, software, and hardware models for the authorization domain. The latter is shown with a discussion on how the VCU interfaces relate to existing reuse techniques. {\textcopyright} Springer International Publishing Switzerland 2016.},
	Annote = {cited By 1},
	Doi = {10.1007/978-3-319-35122-3_9},
	Keywords = {Abstracting; Interfaces (materials); Software engi,Computer software reusability,Concern-oriented reuse; Configuration; Customizat},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977507448{\&}doi=10.1007{\%}2F978-3-319-35122-3{\_}9{\&}partnerID=40{\&}md5=196c3a40799350d9ef9bf7971326ec02}
}

@Article{Kilamo20121467,
	Title = {{From proprietary to open source—Growing an open source ecosystem}},
	Author = {Kilamo, Terhi and Hammouda, Imed and Mikkonen, Tommi and Aaltonen, Timo},
	Journal = {Journal of Systems and Software},
	Year = {2012},
	Number = {7},
	Pages = {1467--1478},
	Volume = {85},
	Abstract = {In today's business and software arena, Free/Libre/Open Source Software has emerged as a promising platform for software ecosystems. Following this trend, more and more companies are releasing their proprietary software as open source, forming a software ecosystem of related development projects complemented with a social ecosystem of community members. Since the trend is relatively recent, there are few guidelines on how to create and maintain a sustainable open source ecosystem for a proprietary software. This paper studies the problem of building open source communities for industrial software that was originally developed as closed source. Supporting processes, guidelines and best practices are discussed and illustrated through an industrial case study. The research is paving the road for new directions in growing a thriving open source ecosystem. },
	Annote = {Software Ecosystems},
	Doi = {https://doi.org/10.1016/j.jss.2011.06.071},
	ISSN = {0164-1212},
	Keywords = {Open source,Open source engineering,Opening proprietary software,Software ecosystem},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121211001683}
}

@Article{SPE:SPE738,
	Title = {{An approach to feature-based software construction for enhancing maintainability}},
	Author = {Kim, Jungyoon and Bae, Doo Hwan},
	Journal = {Software: Practice and Experience},
	Year = {2006},
	Number = {9},
	Pages = {923--948},
	Volume = {36},
	Abstract = {While the way we build software affects significantly its maintenance in terms of the effort and cost, the experience level of the maintainer in a software acquirers' organization is also one of concern. In this context, often the maintainer is the user of the system. Unfortunately, it is quite possible to lose the trustworthiness of the software due to the inexperience of the maintainer, especially when the maintainer is without the help of the original developers. One remedy for providing security against the effects of the maintainer's software modifications is to restrict the access to software parts (modules) relative to the experience level of the maintainers. For such a remedy to be successful, the software should be constructed in such a way that its parts under maintenance affect others as little as possible. We propose an approach to software construction aligning the dependencies among software parts in one direction so that they are allocated to maintainers based on their experience level. Our approach decomposes the software into parts based on functionality and orders the parts by essentiality, which indicates how difficult it is to change each part. Then, we align the dependencies in such a way that the less essential functionality is dependent on the more essential functionality. Consequently, any modification on less essential functionality does not affect the essential functionalities. To demonstrate the feasibility of our proposed approach, we applied it to a military application and found that the constructed software enables us to confine maintainers' activity within a limited working area, and thus the software is safer against maintainers' modification. Copyright {\textcopyright} 2006 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spe.738},
	ISSN = {1097-024X},
	Keywords = {dependency alignment,essentiality,feature,maintainer,unidirectional dependency},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/spe.738}
}

@Article{Kim2014677,
	Title = {{A comparison of software product line traceability approaches from end-to-end traceability perspectives}},
	Author = {Kim, J and Kang, S and Lee, J},
	Journal = {International Journal of Software Engineering and Knowledge Engineering},
	Year = {2014},
	Number = {4},
	Pages = {677--714},
	Volume = {24},
	Abstract = {Software traceability is the ability to provide trace information on requirements, design, and implementation of a system. It helps stakeholders understand the many associations of software artifacts created during a software development project. End-to-end traceability refers to linkage of all artifacts in the entire lifecycle of a software development project. Its goal is to provide stakeholders of the software development with trace information in order to analyze impacts due to changes in a software system. Compared to that of a single product, the end-to-end traceability of software product line is more complicated because Software Product Line Development (SPLD) requires two separate but intimately related phases of domain engineering and application engineering. Various SPLD traceability approaches have been proposed in the past. However, thus far no research work on SPLD traceability has focused on SPLD end-to-end traceability. This paper defines SPLD end-to-end traceability and evaluates the existing SPLD traceability approaches from SPLD end-to-end traceability perspectives. We surveyed studies on SPLD traceability methods, traceability mechanisms used in major SPLD approaches, and software traceability survey papers. We compared the existing SPLD traceability approaches based on Systematic Literature Review (SLR). Through the survey, we found that none of the SPLD traceability studies fully supports SPLD end-to-end traceability, and there are unexplored research areas of SPLD end-to-end traceability in the existing SPLD traceability studies. The contribution of this paper is that it presents future research directions that give research guidelines for each unexplored research area in SPLD end-to-end traceability. Finally, based on the research directions, this paper suggests future research opportunities for SPLD end-to-end traceability. {\textcopyright} 2014 World Scientific Publishing Company.},
	Annote = {cited By 4},
	Doi = {10.1142/S0218194014500260},
	Keywords = {Application engineering; Future research directio,Application programs; Computer software; Engineeri,Software design},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929319373{\&}doi=10.1142{\%}2FS0218194014500260{\&}partnerID=40{\&}md5=1954a03cd457a16de3963c2bd0040fe0}
}

@Article{Kim2006926,
	Title = {{Goal and scenario based domain requirements analysis environment}},
	Author = {Kim, Jintae and Kim, Minseong and Park, Sooyong},
	Journal = {Journal of Systems and Software},
	Year = {2006},
	Number = {7},
	Pages = {926--938},
	Volume = {79},
	Abstract = {Identifying and representing domain requirements among products in a product family are crucial activities for a successful software reuse. The domain requirements should be not only identified based on the business goal, which drives marketing plan, product plan, and differences among products, but also represented as familiar notations in order to support developing a particular product in the product family. Thus, our proposal is to identify the domain requirements through goals and scenarios, and represent them as variable use cases for a product family. Especially, for identification of the domain requirements, we propose four abstraction levels of requirements in a product family, and goal and scenario modeling. For representation of them, variable use case model is suggested, and also the use case transfer rules are proposed so as to bridge the gap between the identification and representation activity. The paper illustrates the application of the approach within a supporting tool using the {\{}HIS{\}} (Home Integration System) example. },
	Annote = {Selected papers from the 11th Asia Pacific Software Engineering Conference (APSEC2004)},
	Doi = {https://doi.org/10.1016/j.jss.2005.06.046},
	ISSN = {0164-1212},
	Keywords = {Domain requirements analysis,Goal,Scenario,Use case},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121205001810}
}

@Article{Kim200837,
	Title = {{DRAMA: A framework for domain requirements analysis and modeling architectures in software product lines}},
	Author = {Kim, Jintae and Park, Sooyong and Sugumaran, Vijayan},
	Journal = {Journal of Systems and Software},
	Year = {2008},
	Number = {1},
	Pages = {37--55},
	Volume = {81},
	Abstract = {One of the benefits of software product line approach is to improve time-to-market. The changes in market needs cause software requirements to be flexible in product lines. Whenever software requirements are changed, software architecture should be evolved to correspond with them. Therefore, domain architecture should be designed based on domain requirements. It is essential that there is traceability between requirements and architecture, and that the structure of architecture is derived from quality requirements. The purpose of this paper is to provide a framework for modeling domain architecture based on domain requirements within product lines. In particular, we focus on the traceable relationship between requirements and architectural structures. Our framework consists of processes, methods, and a supporting tool. It uses four basic concepts, namely, goal based domain requirements analysis, Analytical Hierarchy Process, Matrix technique, and architecture styles. Our approach is illustrated using {\{}HIS{\}} (Home Integration System) product line. Finally, industrial examples are used to validate DRAMA. },
	Doi = {https://doi.org/10.1016/j.jss.2007.04.011},
	ISSN = {0164-1212},
	Keywords = {Domain architecture,Domain requirements,Quality attribute,Traceability},
	Url = {http://www.sciencedirect.com/science/article/pii/S016412120700088X}
}

@Article{Kim2007417,
	Title = {{Managing requirements conflicts in software product lines: A goal and scenario based approach}},
	Author = {Kim, Minseong and Park, Sooyong and Sugumaran, Vijayan and Yang, Hwasil},
	Journal = {Data {\&} Knowledge Engineering},
	Year = {2007},
	Number = {3},
	Pages = {417--432},
	Volume = {61},
	Abstract = {The product line approach is recognized as a successful approach to reuse in software development. However, in many cases, it has resulted in interactions between requirements and/or features. Interaction detection, especially conflict detection between requirements has become more challenging. Thus, detecting conflicts between requirements is essential for successful product line development. Formal methods have been proposed to address this problem, however, they are hard to understand by non-experts and are limited to restricted domains. In addition, there is no overall process that covers all the steps for managing conflicts. We propose an approach for systematically identifying and managing requirements conflicts, which is based on requirements partition in natural language and supported by a tool. To demonstrate its feasibility, the proposed approach has been applied to the home integration system (HIS) domain and the results are discussed. },
	Annote = {Advances on Natural Language ProcessingNLDB 05},
	Doi = {https://doi.org/10.1016/j.datak.2006.06.009},
	ISSN = {0169-023X},
	Keywords = {Goal and scenario authoring,Requirements conflicts,Requirements partitioning,Software product line,Syntactic and semantic requirements conflict detec},
	Url = {http://www.sciencedirect.com/science/article/pii/S0169023X06001121}
}

@Article{Kim20132453,
	Title = {{A distributed logic for Networked Cyber-Physical Systems}},
	Author = {Kim, Minyoung and Stehr, Mark-Oliver and Talcott, Carolyn},
	Journal = {Science of Computer Programming},
	Year = {2013},
	Number = {12},
	Pages = {2453--2467},
	Volume = {78},
	Abstract = {Abstract We describe a distributed logical framework designed to serve as a declarative semantic foundation for Networked Cyber-Physical Systems. The framework provides notions of facts and goals that include interactions with the environment via external goal requests, observations that generate facts, and actions that achieve goals. Reasoning rules are built on a partially ordered knowledge-sharing model for loosely coupled distributed computing. The logic supports reasoning in the context of dynamically changing facts and system goals. It can be used both to program systems and to reason about possible scenarios and emerging properties. The underlying reasoning framework is specified in terms of constraints that must be satisfied, making it very general and flexible. Inference rules for an instantiation to a specific local logic (Horn clause logic) are given as a concrete example. The key novel features are illustrated with snippets from an existing application—a theory for self-organizing robots performing a distributed surveillance task. Traditional properties of logical inference and computation are reformulated in this novel context, and related to features of system design and execution. Proofs are outlined for key properties corresponding to soundness, completeness, and termination. Finally, the framework is compared to other formal systems addressing concurrent/distributed computation. },
	Annote = {Special Section on International Software Product Line Conference 2010 and Fundamentals of Software Engineering (selected papers of {\{}FSEN{\}} 2011)},
	Doi = {https://doi.org/10.1016/j.scico.2013.01.011},
	ISSN = {0167-6423},
	Keywords = {Distributed declarative logic,Networked cyber-physical systems,Partially ordered knowledge},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642313000245}
}

@Article{SPE:SPE2471,
	Title = {{EURECA: End-user requirements engineering with collaborative animation}},
	Author = {Kim, Neunghoe and Park, Soojin and Jeong, Dongwon and Hwang, Mansoo and Park, Sooyong and In, Hoh Peter},
	Journal = {Software: Practice and Experience},
	Year = {2017},
	Number = {7},
	Pages = {1001--1012},
	Volume = {47},
	Abstract = {In recent years, software development environments have changed from being driven by professional developers to being driven by technical end users. One of the key issues in end-user-driven software development environments is how to guarantee the quality of the software artifacts. Measuring the quality of developed software requires the correct specification of a quality range the software is expected to meet. However, technical end users are non-professionals in engineering techniques for software requirements, and training a developer to be an expert in requirements engineering in a short period of time is difficult. This paper proposes a new software development life cycle based on reutilizing previously evaluated software requirements assets from completed projects. End-User Requirements Engineering with Collaborative Animation, a tool that was developed to support the proposed software development life cycle, is described and demonstrated by application to three projects. The results from real cases are used as the basis for a discussion on the efficiency enhancement of requirements work, an increased rate of reusing legacy software requirements assets, and an improvement in the technical end user's individual competency level using the End-User Requirements Engineering with Collaborative Animation. Copyright {\textcopyright} 2017 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spe.2471},
	ISSN = {1097-024X},
	Keywords = {end-user software engineering,requirements engineering,software reuse},
	Url = {http://dx.doi.org/10.1002/spe.2471}
}

@Article{Kim20112035,
	Title = {{A feature-based approach for modeling role-based access control systems}},
	Author = {Kim, Sangsig and Kim, Dae-Kyoo and Lu, Lunjin and Kim, Suntae and Park, Sooyong},
	Journal = {Journal of Systems and Software},
	Year = {2011},
	Number = {12},
	Pages = {2035--2052},
	Volume = {84},
	Abstract = {Role-based access control (RBAC) is a popular access control model for enterprise systems due to its flexibility and scalability. There are many {\{}RBAC{\}} features available, each providing a different function. Not all features are needed for an {\{}RBAC{\}} system. Depending on the requirements, one should be able to configure features on a need basis, which reduces development complexity and thus fosters development. However, there have not been suitable methods that enable systematic configuration of {\{}RBAC{\}} features for system development. This paper presents an approach for configuring {\{}RBAC{\}} features using a combination of feature modeling and {\{}UML{\}} modeling. Feature modeling is used for capturing the structure of features and configuration rules, and {\{}UML{\}} modeling is used for defining the semantics of features. {\{}RBAC{\}} features are defined based on design principles of partial inheritance and compatibility, which facilitates feature composition and verification. We demonstrate the approach using a banking application and present tool support developed for the approach. },
	Doi = {https://doi.org/10.1016/j.jss.2011.03.084},
	ISSN = {0164-1212},
	Keywords = {Feature modeling,Role-based access control,UML},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121211000926}
}

@InProceedings{Kim:2006:MBF:1181775.1181781,
	Title = {{Memories of Bug Fixes}},
	Author = {Kim, Sunghun and Pan, Kai and {Whitehead Jr.}, E E James},
	Booktitle = {Proceedings of the 14th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
	Year = {2006},
	Address = {New York, NY, USA},
	Pages = {35--45},
	Publisher = {ACM},
	Series = {SIGSOFT '06/FSE-14},
	Doi = {10.1145/1181775.1181781},
	ISBN = {1-59593-468-5},
	Keywords = {bug,bug finding tool,fault,fix,patterns,prediction},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/1181775.1181781}
}

@Article{Kitamura2012458,
	Title = {{Test-case design by feature trees}},
	Author = {Kitamura, T and Do, N T B and Ohsaki, H and Fang, L and Yatabe, S},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2012},
	Number = {PART 1},
	Pages = {458--473},
	Volume = {7609 LNCS},
	Abstract = {This paper proposes a test-case design method for black-box testing, called "Feature Oriented Testing (FOT)". The method is realized by applying Feature Models (FMs) developed in software product line engineering to test-case designs. We develop a graphical language for test-case design called "Feature Trees for Testing (FTT)" based on FMs. To firmly underpin the method, we provide a formal semantics of FTT, by means of test-cases derived from test-case designs modelled with FTT. Based on the semantics we develop an automated test-suite generation and correctness checking of test-case designs using SAT, as computer-aided analysis techniques of the method. Feasibility of the method is demonstrated from several viewpoints including its implementation, complexity analysis, experiments, a case study, and an assistant tool. {\textcopyright} 2012 Springer-Verlag.},
	Annote = {cited By 6},
	Doi = {10.1007/978-3-642-34026-0_34},
	Keywords = {Black-box testing; Complexity analysis; Design by,Cad Cam; Computer Programs; Experimentation; Math,Computer aided analysis; Forestry; Semantics; Sof,Testing},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868281704{\&}doi=10.1007{\%}2F978-3-642-34026-0{\_}34{\&}partnerID=40{\&}md5=ca10f8a5ee5220d0da6717a0b2094712}
}

@Article{Knodel200517,
	Title = {{An Efficient Migration to Model-driven Development (MDD)}},
	Author = {Knodel, Jens and Anastasopolous, Michalis and Forster, Thomas and Muthig, Dirk},
	Journal = {Electronic Notes in Theoretical Computer Science},
	Year = {2005},
	Number = {3},
	Pages = {17--27},
	Volume = {137},
	Abstract = {Model-driven development envisions raising the abstraction level of software development. To fully realize this vision, technology-specific aspects must be completely hidden from developers. They produce only platform-independent models (PIM), which are automatically transformed into executable systems. To enable an efficient migration to MDD, we recommend taking advantage of concepts from software architectures, product line engineering and reverse engineering. },
	Annote = {Proceedings of the 2nd International Workshop on Metamodels, Schemas, and Grammars for Reverse Engineering (ateM 2004)Metamodels, Schemas, and Grammars for Reverse Engineering 2004},
	Doi = {https://doi.org/10.1016/j.entcs.2005.07.002},
	ISSN = {1571-0661},
	Keywords = {PuLSE,architecture-driven migration,model-driven development,product lines,reverse engineering},
	Url = {http://www.sciencedirect.com/science/article/pii/S1571066105051029}
}

@Article{Kofroň200931,
	Title = {{Modes in component behavior specification via EBP and their application in product lines}},
	Author = {Kofroň, J and Pl{\'{a}}{\v{s}}il, F and {\v{S}}er{\'{y}}, O},
	Journal = {Information and Software Technology},
	Year = {2009},
	Number = {1},
	Pages = {31--41},
	Volume = {51},
	Abstract = {The concept of software product lines (SPL) is a modern approach to software development simplifying construction of related variants of a product thus lowering development costs and shortening time-to-market. In SPL, software components play an important role. In this paper, we show how the original idea of component mode can be captured and further developed in behavior specification via the formalism of extended behavior protocols (EBP). Moreover, we demonstrate how the modes in behavior specification can be used for modeling behavior of an entire product line. The main benefits include (i) the existence of a single behavior specification capturing the behavior of all product variants, and (ii) automatic verification of absence of communication errors among the cooperating components taking the variability into account. These benefits are demonstrated on a part of a non-trivial case study. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
	Annote = {cited By 5},
	Doi = {10.1016/j.infsof.2008.09.011},
	Keywords = {Automatic verifications; Behavior specification;,Communication,Specifications},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-56649114354{\&}doi=10.1016{\%}2Fj.infsof.2008.09.011{\&}partnerID=40{\&}md5=985fa698b6816ece343555191796cd75}
}

@Article{Kolb2006109,
	Title = {{Refactoring a legacy component for reuse in a software product line: A case study}},
	Author = {Kolb, R and Muthig, D and Patzke, T and Yamauchi, K},
	Journal = {Journal of Software Maintenance and Evolution},
	Year = {2006},
	Number = {2},
	Pages = {109--132},
	Volume = {18},
	Abstract = {Product lines are a promising approach to improve conceptually the productivity of the software development process and thus to reduce both the cost and time of developing and maintaining increasingly complex systems. An important issue in the adoption of the product-line approach is the migration of legacy software components, which have not been designed for reuse, systematically into reusable product-line components. This article describes activities performed to improve systematically the design and implementation of an existing software component in order to reuse it in a software product line. The activities are embedded in the application of Fraunhofer PuLSE™-DSSA - an approach for defining domain-specific software architectures (DSSA) and product-line architectures. The component under investigation is the so-called Image Memory Handler (IMH), which is used in Ricoh's current products of office appliances such as copier machines, printers, and multi-functional peripherals. It is responsible for controlling memory usage and compressing and decompressing image data. Improvement of both the component's design and implementation are based on a systematic analysis and focused on increasing maintainability and reusability and hence suitability for use in a product line. As a result of the analysis and refactoring activities, the documentation and implementation of the component has been considerably improved as shown by quantitative data collected at the end of the activities. Despite a number of changes to the code, the external behavior of the component has been preserved without significantly affecting the performance. Copyright {\textcopyright} 2006 John Wiley {\&} Sons, Ltd.},
	Annote = {cited By 31},
	Doi = {10.1002/smr.329},
	Keywords = {Code analysis; Refactoring; Software product lines,Codes (standards); Components; Computer architect,Computer software maintenance},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646408526{\&}doi=10.1002{\%}2Fsmr.329{\&}partnerID=40{\&}md5=501ee942227e40e04ac86fcd865112dd}
}

@Article{Koning2009258,
	Title = {{VxBPEL: Supporting variability for Web services in {\{}BPEL{\}}}},
	Author = {Koning, Michiel and Sun, Chang-ai and Sinnema, Marco and Avgeriou, Paris},
	Journal = {Information and Software Technology},
	Year = {2009},
	Number = {2},
	Pages = {258--269},
	Volume = {51},
	Abstract = {Web services provide a way to facilitate the business integration over the Internet. Flexibility is an important and desirable property of Web service-based systems due to dynamic business environments. The flexibility can be provided or addressed by incorporating variability into a system. In this study, we investigate how variability can be incorporated into service-based systems. We propose a language, VxBPEL, which is an adaptation of an existing language, BPEL, and able to capture variability in these systems. We develop a prototype to interpret this language. Finally, we illustrate our method by using it to handle variability of an example. },
	Doi = {https://doi.org/10.1016/j.infsof.2007.12.002},
	ISSN = {0950-5849},
	Keywords = {Business Process Execution Language,Service-based system,Variability,Web service},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584908000207}
}

@Article{Kosar201677,
	Title = {{Domain-Specific Languages: A Systematic Mapping Study}},
	Author = {Kosar, Toma{\v{z}} and Bohra, Sudev and Mernik, Marjan},
	Journal = {Information and Software Technology},
	Year = {2016},
	Pages = {77--91},
	Volume = {71},
	Abstract = {Abstract Context: In this study we report on a Systematic Mapping Study (SMS) for Domain-Specific Languages (DSLs), based on an automatic search including primary studies from journals, conferences, and workshops during the period from 2006 until 2012. Objective: The main objective of the described work was to perform an {\{}SMS{\}} on {\{}DSLs{\}} to better understand the {\{}DSL{\}} research field, identify research trends, and any possible open issues. The set of research questions was inspired by a {\{}DSL{\}} survey paper published in 2005. Method: We conducted a {\{}SMS{\}} over 5 stages: defining research questions, conducting the search, screening, classifying, and data extraction. Our {\{}SMS{\}} included 1153 candidate primary studies from the {\{}ISI{\}} Web of Science and {\{}ACM{\}} Digital Library, 390 primary studies were classified after screening. Results: This {\{}SMS{\}} discusses two main research questions: research space and trends/demographics of the literature within the field of DSLs. Both research questions are further subdivided into several research sub-questions. The results from the first research question clearly show that the {\{}DSL{\}} community focuses more on the development of new techniques/methods rather than investigating the integrations of {\{}DSLs{\}} with other software engineering processes or measuring the effectiveness of {\{}DSL{\}} approaches. Furthermore, there is a clear lack of evaluation research. Amongst different {\{}DSL{\}} development phases more attention is needed in regard to domain analysis, validation, and maintenance. The second research question revealed that the number of publications remains stable, and has not increased over the years. Top cited papers and venues are mentioned, as well as identifying the more active institutions carrying {\{}DSL{\}} research. Conclusion: The statistical findings regarding research questions paint an interesting picture about the mainstreams of the {\{}DSL{\}} community, as well as open issues where researchers can improve their research in their future work. },
	Doi = {https://doi.org/10.1016/j.infsof.2015.11.001},
	ISSN = {0950-5849},
	Keywords = {Domain-Specific Languages,Systematic Mapping Study,Systematic Review},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584915001858}
}

@Article{SMR:SMR1592,
	Title = {{Large-scale inter-system clone detection using suffix trees and hashing}},
	Author = {Koschke, Rainer},
	Journal = {Journal of Software: Evolution and Process},
	Year = {2014},
	Number = {8},
	Pages = {747--769},
	Volume = {26},
	Abstract = {Detecting a similar code between two systems has various applications such as comparing two software variants or versions or finding potential license violations. Techniques detecting suspiciously similar code must scale in terms of resources needed to very large code corpora and need to have high precision because a human needs to inspect the results. This paper demonstrates how suffix trees can be used to obtain a scalable comparison. The evaluation is carried out for very large code corpora. Our evaluation shows that our approach is faster than index-based techniques when the analysis is run only once. If the analysis is to be conducted multiple times, creating an index pays off. We report how much code can be filtered out from the analysis using an index-based filter. In addition to that, this paper proposes a method to improve precision through user feedback. A user validates a sample of the found clone candidates. An automated data mining technique learns a decision tree on the basis of the user decisions and different code metrics. We investigate the relevance of several metrics and whether criteria learned from one application domain can be generalized to other domains. Copyright {\textcopyright} 2013 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/smr.1592},
	ISSN = {2047-7481},
	Keywords = {clone detection,code search,license violation detection},
	Url = {http://dx.doi.org/10.1002/smr.1592}
}

@Article{SMR:SMR542,
	Title = {{Incremental reflexion analysis}},
	Author = {Koschke, Rainer},
	Journal = {Journal of Software: Evolution and Process},
	Year = {2013},
	Number = {6},
	Pages = {601--637},
	Volume = {25},
	Abstract = {Architecture conformance checking is implemented in many commercial and research tools. These tools typically implement the reflexion analysis originally proposed by Murphy, Notkin, and Sullivan. This analysis allows for structural validation of an architecture model against a source model connected by a mapping from source entities onto architecture entities. Given this mapping, the reflexion analysis computes the discrepancies between the architecture model and source model automatically. The mapping process is usually highly interactive and the most time-consuming activity in the reflexion analysis. In current tools, the reflexion analysis must be repeated completely whenever the underlying source or architecture models or the mapping changes. In large systems, the recomputation can hinder interactive use as users expect an immediate response to their changes. This paper describes an incremental reflexion analysis that does not require a complete repetition of the reflexion analysis. Instead, it repeats the analysis only for those parts that are actually influenced by a change. The incremental reflexion analysis is evaluated on large real-world systems. Copyright {\textcopyright} 2011 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/smr.542},
	ISSN = {2047-7481},
	Keywords = {architecture conformance checking,architecture reconstruction,incremental reflexion analysis,software maintenance},
	Url = {http://dx.doi.org/10.1002/smr.542}
}

@InProceedings{Koscielny:2014:DDP:2647508.2647512,
	Title = {{DeltaJ 1.5: Delta-oriented Programming for Java 1.5}},
	Author = {Koscielny, Jonathan and Holthusen, S{\"{o}}nke and Schaefer, Ina and Schulze, Sandro and Bettini, Lorenzo and Damiani, Ferruccio},
	Booktitle = {Proceedings of the 2014 International Conference on Principles and Practices of Programming on the Java Platform: Virtual Machines, Languages, and Tools},
	Year = {2014},
	Address = {New York, NY, USA},
	Pages = {63--74},
	Publisher = {ACM},
	Series = {PPPJ '14},
	Doi = {10.1145/2647508.2647512},
	ISBN = {978-1-4503-2926-2},
	Keywords = {delta-oriented programming,program generation,software product line},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2647508.2647512}
}

@InProceedings{Kowal:2016:EAF:2993236.2993248,
	Title = {{Explaining Anomalies in Feature Models}},
	Author = {Kowal, Matthias and Ananieva, Sofia and Th{\"{u}}m, Thomas},
	Booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
	Year = {2016},
	Address = {New York, NY, USA},
	Pages = {132--143},
	Publisher = {ACM},
	Series = {GPCE 2016},
	Doi = {10.1145/2993236.2993248},
	ISBN = {978-1-4503-4446-3},
	Keywords = {Anomalies,Explanations,Feature Models,Software Product Lines},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2993236.2993248}
}

@Article{Koziolek2010634,
	Title = {{Performance evaluation of component-based software systems: A survey}},
	Author = {Koziolek, Heiko},
	Journal = {Performance Evaluation},
	Year = {2010},
	Number = {8},
	Pages = {634--658},
	Volume = {67},
	Abstract = {Performance prediction and measurement approaches for component-based software systems help software architects to evaluate their systems based on component performance specifications created by component developers. Integrating classical performance models such as queueing networks, stochastic Petri nets, or stochastic process algebras, these approaches additionally exploit the benefits of component-based software engineering, such as reuse and division of work. Although researchers have proposed many approaches in this direction during the last decade, none of them has attained widespread industrial use. On this basis, we have conducted a comprehensive state-of-the-art survey of more than 20 of these approaches assessing their applicability. We classified the approaches according to the expressiveness of their component performance modelling languages. Our survey helps practitioners to select an appropriate approach and scientists to identify interesting topics for future research. },
	Annote = {Special Issue on Software and Performance},
	Doi = {https://doi.org/10.1016/j.peva.2009.07.007},
	ISSN = {0166-5316},
	Keywords = {CBSE,Classification,Measurement,Modelling,Performance,Prediction,Software component,Survey},
	Url = {http://www.sciencedirect.com/science/article/pii/S016653160900100X}
}

@Article{Koziolek2016411,
	Title = {{Assessing software product line potential: an exploratory industrial case study}},
	Author = {Koziolek, H and Goldschmidt, T and de Gooijer, T and Domis, D and Sehestedt, S and Gamer, T and Aleksy, M},
	Journal = {Empirical Software Engineering},
	Year = {2016},
	Number = {2},
	Pages = {411--448},
	Volume = {21},
	Abstract = {Corporate organizations sometimes offer similar software products in certain domains due to former company mergers or due to the complexity of the organization. The functional overlap of such products is an opportunity for future systematic reuse to reduce software development and maintenance costs. Therefore, we have tailored existing domain analysis methods to our organization to identify commonalities and variabilities among such products and to assess the potential for software product line (SPL) approaches. As an exploratory case study, we report on our experiences and lessons learned from conducting the domain analysis in four application cases with large-scale software products. We learned that the outcome of a domain analysis was often a smaller integration scenario instead of an SPL and that business case calculations were less relevant for the stakeholders and managers from the business units during this phase. We also learned that architecture reconstruction using a simple block diagram notation aids domain analysis and that large parts of our approach were reusable across application cases. {\textcopyright} 2015, Springer Science+Business Media New York.},
	Annote = {cited By 0},
	Doi = {10.1007/s10664-014-9358-0},
	Keywords = {Application programs; Computer software; Computer,Architecture reconstruction; Business case; Domai,Software design},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922424792{\&}doi=10.1007{\%}2Fs10664-014-9358-0{\&}partnerID=40{\&}md5=d52527a44c67d5bcbcaab488aa82cc99}
}

@Article{Koziolek2009177,
	Title = {{Evolving industrial software architectures into a software product line: A case study}},
	Author = {Koziolek, H and Weiss, R and Doppelhamer, J},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2009},
	Pages = {177--193},
	Volume = {5581 LNCS},
	Abstract = {Industrial software applications have high requirements on performance, availability, and maintainability. Additionally, diverse application landscapes of large corporate companies require systematic planning for reuse, which can be fostered by a software product-line approach. Analyses at the software architecture level can help improving the structure of the systems to account for extra-functional requirements and reuse. This paper reports a case study of product-line development for ABB's robotics PC software. We analysed the software architectures of three existing robotics applications and identified their core assets. As a result, we designed a new product-line architecture, which targets at fulfilling various extra-functional requirements. This paper describes experiences and lessons learned during the project. {\textcopyright} 2009 Springer Berlin Heidelberg.},
	Annote = {cited By 6},
	Doi = {10.1007/978-3-642-02351-4_12},
	Keywords = {Computer software reusability; Computer software,Core asset; Diverse applications; Functional requi,Software architecture},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350640677{\&}doi=10.1007{\%}2F978-3-642-02351-4{\_}12{\&}partnerID=40{\&}md5=96cbc5e5189c1cb1b1be6ac4bffc1aa6}
}

@InProceedings{Kozuka:2011:BPL:2019136.2019152,
	Title = {{Building a Product Line Architecture for Variant-rich Enterprise Applications Using a Data-oriented Approach}},
	Author = {Kozuka, Nobuaki and Ishida, Yuzo},
	Booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
	Year = {2011},
	Address = {New York, NY, USA},
	Pages = {14:1----14:6},
	Publisher = {ACM},
	Series = {SPLC '11},
	Doi = {10.1145/2019136.2019152},
	ISBN = {978-1-4503-0789-5},
	Keywords = {core asset development,data intensiveness,data oriented approach,enterprise applications,product line architecture,quality attributes,relational database management system},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2019136.2019152}
}

@Article{SYS:SYS20122,
	Title = {{Enabling system evolution through configuration management on the hardware/software boundary}},
	Author = {Krikhaar, Ren{\'{e}} and Mosterman, Wim and Veerman, Niels and Verhoef, Chris},
	Journal = {Systems Engineering},
	Year = {2009},
	Number = {3},
	Pages = {233--264},
	Volume = {12},
	Abstract = {As the use of software and electronics in modern products is omnipresent and continuously increasing, companies in the embedded systems industry face increasing complexity in controlling and enabling the evolution of their IT-intensive products. Traditionally, product configurations and their updates were managed separately for the hardware and software discipline. At specified release moments during the development of their products, the hardware and software were released together. But, as the usage, flexibility and complexity of software and electronics increases, and fierce competition requires shorter time-to-market and customizable products, more frequent releases of integrated hardware and software configurations becomes necessary. This evolution requires adequate configuration management both within the hard- and software disciplines and across them. In many organizations, software configuration management is more visibly established than hardware configuration management due to the inherent flexibility and complexity of software. But as the flexibility of hardware has increased through the use of configurable hardware, the need for more intense hardware configuration management increased as well. To properly enable the evolution of integrated hardware/software systems, adequate configuration management is required in both disciplines. Our article deals with just that: configuration management on the hardware/software boundary, and is mainly focused on the product development phase. The hardware/software boundary has a broad scope; in this article we focus on embedded systems containing software and custom and off-the-shelf electronic hardware, thereby concentrating on configurable hardware containing both software and hardware designs. It is important to note that the concepts of our article apply to configuration management issues that go beyond that scope. We investigated the configuration management practices in six large organizations in the embedded systems industry, including medical devices, defense equipment, optics, document processing, and semiconductor equipment. In particular, we looked at programmable/configurable hardware, as the use and complexity of this technology continue to increase. We propose a generic development cycle with real-life examples to illustrate the configuration management concepts. Then, from the sociotechnical design point of view, we raise awareness and argue that configuration management tools, processes, and its organization must be further aligned to support the complexity and interdisciplinary nature of the hardware/software boundary of evolving embedded systems. {\textcopyright} 2009 Wiley Periodicals, Inc. Syst Eng},
	Doi = {10.1002/sys.20122},
	ISSN = {1520-6858},
	Keywords = {configurable hardware,hardware/software boundary,hardware/software integration,information management,multi-disciplinary configuration management,product configuration management,product data management,programmable hardware,software configuration management},
	Publisher = {Wiley Subscription Services, Inc., A Wiley Company},
	Url = {http://dx.doi.org/10.1002/sys.20122}
}

@Article{Krishnan20131479,
	Title = {{Predicting failure-proneness in an evolving software product line}},
	Author = {Krishnan, Sandeep and Strasburg, Chris and Lutz, Robyn R and Goseva-Popstojanova, Katerina and Dorman, Karin S},
	Journal = {Information and Software Technology},
	Year = {2013},
	Number = {8},
	Pages = {1479--1495},
	Volume = {55},
	Abstract = {AbstractContext Previous work by researchers on 3 years of early data for an Eclipse product has identified some predictors of failure-prone files that work well. Eclipse has also been used previously by researchers to study characteristics of product line software. Objective The work reported here investigates whether classification-based prediction of failure-prone files improves as the product line evolves. Method This investigation first repeats, to the extent possible, the previous study and then extends it by including four more recent years of data, comparing the prominent predictors with the previous results. The research then looks at the data for three additional Eclipse products as they evolve over time. The analysis compares results from three different types of datasets with alternative data collection and prediction periods. Results Our experiments with a variety of learners show that the difference between the performance of J48, used in this work, and the other top learners is not statistically significant. Furthermore, new results show that the effectiveness of classification significantly depends on the data collection period and prediction period. The study identifies change metrics that are prominent predictors across all four releases of all four products in the product line for the three different types of datasets. From the product line perspective, prediction of failure-prone files for the four products studied in the Eclipse product line shows statistically significant improvement in accuracy but not in recall across releases. Conclusion As the product line matures, the learner performance improves significantly for two of the three datasets, but not for prediction of post-release failure-prone files using only pre-release change data. This suggests that it may be difficult to detect failure-prone files in the evolving product line. At least in part, this may be due to the continuous change, even for commonalities and high-reuse variation components, which we previously have shown to exist. },
	Doi = {https://doi.org/10.1016/j.infsof.2012.11.008},
	ISSN = {0950-5849},
	Keywords = {Change metrics,Failure-prone files,Post-release defects,Prediction,Reuse,Software product lines},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584912002340}
}

@Article{Krupitzer2015184,
	Title = {{A survey on engineering approaches for self-adaptive systems}},
	Author = {Krupitzer, Christian and Roth, Felix Maximilian and VanSyckel, Sebastian and Schiele, Gregor and Becker, Christian},
	Journal = {Pervasive and Mobile Computing},
	Year = {2015},
	Pages = {184--206},
	Volume = {17, Part B},
	Abstract = {Abstract The complexity of information systems is increasing in recent years, leading to increased effort for maintenance and configuration. Self-adaptive systems (SASs) address this issue. Due to new computing trends, such as pervasive computing, miniaturization of {\{}IT{\}} leads to mobile devices with the emerging need for context adaptation. Therefore, it is beneficial that devices are able to adapt context. Hence, we propose to extend the definition of {\{}SASs{\}} and include context adaptation. This paper presents a taxonomy of self-adaptation and a survey on engineering SASs. Based on the taxonomy and the survey, we motivate a new perspective on {\{}SAS{\}} including context adaptation. },
	Annote = {10 years of Pervasive Computing' In Honor of Chatschik Bisdikian},
	Doi = {https://doi.org/10.1016/j.pmcj.2014.09.009},
	ISSN = {1574-1192},
	Keywords = {Context adaptation,Self-adaptation,Self-adaptive systems,Survey,Taxonomy},
	Url = {http://www.sciencedirect.com/science/article/pii/S157411921400162X}
}

@InProceedings{Kuhn:2012:ESF:2404962.2404994,
	Title = {{An Exploratory Study of Forces and Frictions Affecting Large-scale Model-driven Development}},
	Author = {Kuhn, Adrian and Murphy, Gail C and Thompson, C Albert},
	Booktitle = {Proceedings of the 15th International Conference on Model Driven Engineering Languages and Systems},
	Year = {2012},
	Address = {Berlin, Heidelberg},
	Pages = {352--367},
	Publisher = {Springer-Verlag},
	Series = {MODELS'12},
	Doi = {10.1007/978-3-642-33666-9_23},
	ISBN = {978-3-642-33665-2},
	Url = {http://0-dx.doi.org.fama.us.es/10.1007/978-3-642-33666-9{\_}23}
}

@Article{SMR:SMR1703,
	Title = {{Crafting a software process improvement approach—a retrospective systematization}},
	Author = {Kuhrmann, Marco},
	Journal = {Journal of Software: Evolution and Process},
	Year = {2015},
	Number = {2},
	Pages = {114--145},
	Volume = {27},
	Abstract = {Structured approaches are beneficial for successful software process improvement (SPI). However, process engineers often struggle with standardized SPI methods, such as capability maturity model integration (CMMI) or International Organization for Standardization (ISO) 15504, and complain about too generic or voluminous approaches or methods that are alien to the organizations in which SPI is conducted. Therefore, process engineers need to customize existing SPI models or develop new approaches for company-specific SPI programs. While conducting SPI in the context of the German V-Modell XT, we faced the need to develop a new method for artifact-based SPI. In the process, we found that the construction procedures of SPI models are barely documented, and thus, their successful adaptation solely depends on the process engineers' expertise. With this article, we aim to address this lack of support and provide a structured reflection on our experiences from creating and adopting the Artifact-based Software Process Improvement {\&} Management (ArSPI) model. We present the steps of the construction procedure, the validation, and the dissemination of the model. Furthermore, we detail on the applied methods, the design decisions, and the challenges encountered. By providing a reference procedure and tested methods, we support process engineers with the creation and adoption of SPI approaches. Copyright {\textcopyright} 2015 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/smr.1703},
	ISSN = {2047-7481},
	Keywords = {SPI,V-Modell XT,artifact-orientation,construction procedure,experience report,methodology,software process improvement},
	Url = {http://dx.doi.org/10.1002/smr.1703}
}

@Article{SMR:SMR1751,
	Title = {{On the use of variability operations in the V-Modell XT software process line}},
	Author = {Kuhrmann, Marco and {M{\'{e}}ndez Fern{\'{a}}ndez}, Daniel and Ternit{\'{e}}, Thomas},
	Journal = {Journal of Software: Evolution and Process},
	Year = {2016},
	Number = {4},
	Pages = {241--253},
	Volume = {28},
	Abstract = {Software process lines provide a systematic approach to develop and manage software processes. It defines a reference process containing general process assets, whereas a well-defined customization approach allows process engineers to create new process variants, for example, by extending or modifying process assets. Variability operations are an instrument to realize flexibility by explicitly declaring required modifications, which are applied to create a procedurally generated company-specific process. However, little is known about which variability operations are suitable in practice. In this article, we present a study on the feasibility of variability operations to support the development of software process lines in the context of the V-Modell XT. We analyze which variability operations are defined and practically used. We provide an initial catalog of variability operations as an improvement proposal for other process models. Our findings show that 69 variability operation types are defined across several metamodel versions of which, however, 25 remain unused. The found variability operations allow for systematically modifying the content of process model elements and the process documentation, and they allow for altering the structure of a process model and its description. Furthermore, we also find that variability operations can help process engineers to compensate process metamodel evolution. Copyright {\textcopyright} 2015 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/smr.1751},
	ISSN = {2047-7481},
	Keywords = {metamodel evolution,software process lines,variability operations},
	Url = {http://dx.doi.org/10.1002/smr.1751}
}

@Article{Kuhrmann201649,
	Title = {{Flexible software process lines in practice: A metamodel-based approach to effectively construct and manage families of software process models}},
	Author = {Kuhrmann, Marco and Ternit{\'{e}}, Thomas and Friedrich, Jan and Rausch, Andreas and Broy, Manfred},
	Journal = {Journal of Systems and Software},
	Year = {2016},
	Pages = {49--71},
	Volume = {121},
	Abstract = {Abstract Process flexibility and adaptability is a frequently discussed topic in literature, and several approaches propose techniques to improve and optimize software processes for a given organization- or project context. A software process line (SPrL) is an instrument to systematically construct and manage variable software processes, by combining pre-defined and standardized process assets that can be reused, modified, and extended using a well-defined customization approach. Hence, process engineers can ground context-specific process variants in a standardized or domain-specific reference model that can be adapted to the respective context. In this article, we present an approach to construct flexible software process lines and show its practical application in the German V-Modell XT. The presented approach emerges from a 10-year research endeavor and was used to enhance the metamodel of the V-Modell XT and to allow for improved process variability and lifecycle management. Practical dissemination and complementing empirical research show the suitability of the concept. We therefore contribute a proven approach that is presented as metamodel fragment for reuse and implementation in further process modeling approaches. },
	Doi = {https://doi.org/10.1016/j.jss.2016.07.031},
	ISSN = {0164-1212},
	Keywords = {Process design,Process realisation,Software process,Software process lines,Software process metamodel,V-Modell {\{}XT{\}} metamodel},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121216301236}
}

@Article{Kulesza2013905,
	Title = {{The crosscutting impact of the {\{}AOSD{\}} Brazilian research community}},
	Author = {Kulesza, Uir{\'{a}} and Soares, S{\'{e}}rgio and Chavez, Christina and Castor, Fernando and Borba, Paulo and Lucena, Carlos and Masiero, Paulo and Sant'Anna, Claudio and Ferrari, Fabiano and Alves, Vander and Coelho, Roberta and Figueiredo, Eduardo and Pires, Paulo F and Delicato, Fl{\'{a}}via and Piveta, Eduardo and Silva, Carla and Camargo, Valter and Braga, Rosana and Leite, Julio and Lemos, Ot{\'{a}}vio and Mendon{\c{c}}a, Nabor and Batista, Thais and Bonif{\'{a}}cio, Rodrigo and Cacho, N{\'{e}}lio and Silva, Lyrene and von Staa, Arndt and Silveira, F{\'{a}}bio and Valente, Marco T{\'{u}}lio and Alencar, Fernanda and Castro, Jaelson and Ramos, Ricardo and Penteado, Rosangela and Rubira, Cec{\'{i}}lia},
	Journal = {Journal of Systems and Software},
	Year = {2013},
	Number = {4},
	Pages = {905--933},
	Volume = {86},
	Abstract = {Background Aspect-Oriented Software Development (AOSD) is a paradigm that promotes advanced separation of concerns and modularity throughout the software development lifecycle, with a distinctive emphasis on modular structures that cut across traditional abstraction boundaries. In the last 15 years, research on {\{}AOSD{\}} has boosted around the world. The AOSD-BR research community (AOSD-BR stands for {\{}AOSD{\}} in Brazil) emerged in the last decade, and has provided different contributions in a variety of topics. However, despite some evidence in terms of the number and quality of its outcomes, there is no organized characterization of the AOSD-BR community that positions it against the international {\{}AOSD{\}} Research community and the Software Engineering Research community in Brazil. Aims In this paper, our main goal is to characterize the AOSD-BR community with respect to the research developed in the last decade, confronting it with the {\{}AOSD{\}} international community and the Brazilian Software Engineering community. Method Data collection, validation and analysis were performed in collaboration with several researchers of the AOSD-BR community. The characterization was presented from three different perspectives: (i) a historical timeline of events and main milestones achieved by the community; (ii) an overview of the research developed by the community, in terms of key challenges, open issues and related work; and (iii) an analysis on the impact of the AOSD-BR community outcomes in terms of well-known indicators, such as number of papers and number of citations. Results Our analysis showed that the AOSD-BR community has impacted both the international {\{}AOSD{\}} Research community and the Software Engineering Research community in Brazil. },
	Annote = {{\{}SI{\}} : Software Engineering in Brazil: Retrospective and Prospective Views},
	Doi = {https://doi.org/10.1016/j.jss.2012.08.031},
	ISSN = {0164-1212},
	Keywords = {Aspect-Oriented Software Development,Modularity,Research impact},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121212002427}
}

@Article{Kulk2008136,
	Title = {{Quantifying requirements volatility effects}},
	Author = {Kulk, G P and Verhoef, C},
	Journal = {Science of Computer Programming},
	Year = {2008},
	Number = {3},
	Pages = {136--175},
	Volume = {72},
	Abstract = {In an organization operating in the bancassurance sector we identified a low-risk {\{}IT{\}} subportfolio of 84 {\{}IT{\}} projects comprising together 16,500 function points, each project varying in size and duration, for which we were able to quantify its requirements volatility. This representative portfolio stems from a much larger portfolio of {\{}IT{\}} projects. We calculated the volatility from the function point countings that were available to us. These figures were aggregated into a requirements volatility benchmark. We found that maximum requirements volatility rates depend on size and duration, which refutes currently known industrial averages. For instance, a monthly growth rate of 5{\%} is considered a critical failure factor, but in our low-risk portfolio we found more than 21{\%} of successful projects with a volatility larger than 5{\%}. We proposed a mathematical model taking size and duration into account that provides a maximum healthy volatility rate that is more in line with the reality of low-risk {\{}IT{\}} portfolios. Based on the model, we proposed a tolerance factor expressing the maximal volatility tolerance for a project or portfolio. For a low-risk portfolio its empirically found tolerance is apparently acceptable, and values exceeding this tolerance are used to trigger {\{}IT{\}} decision makers. We derived two volatility ratios from this model, the $\pi$ -ratio and the $\rho$ -ratio. These ratios express how close the volatility of a project has approached the danger zone when requirements volatility reaches a critical failure rate. The volatility data of a governmental {\{}IT{\}} portfolio were juxtaposed to our bancassurance benchmark, immediately exposing a problematic project, which was corroborated by its actual failure. When function points are less common, e.g. in the embedded industry, we used daily source code size measures and illustrated how to govern the volatility of a software product line of a hardware manufacturer. With the three real-world portfolios we illustrated that our results serve the purpose of an early warning system for projects that are bound to fail due to excessive volatility. Moreover, we developed essential requirements volatility metrics that belong on an {\{}IT{\}} governance dashboard and presented such a volatility dashboard. },
	Doi = {https://doi.org/10.1016/j.scico.2008.04.003},
	ISSN = {0167-6423},
	Keywords = {Compound monthly growth rate,IT dashboard,IT portfolio management,Quantitative {\{}IT{\}} portfolio management,Requirements churn,Requirements creep,Requirements metric,Requirements scrap,Requirements volatility,Requirements volatility dashboard,Scope creep,Volatility benchmark,Volatility tolerance factor,$\pi$ -ratio,$\rho$ -ratio},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642308000464}
}

@Article{SPE:SPE2439,
	Title = {{Perils of opportunistically reusing software module}},
	Author = {Kulkarni, Naveen and Varma, Vasudeva},
	Journal = {Software: Practice and Experience},
	Year = {2017},
	Number = {7},
	Pages = {971--984},
	Volume = {47},
	Abstract = {Opportunistic reuse is a need based sourcing of software modules without a prior reuse plan. It is a common tactical approach in software development. Developers often reuse an external software module opportunistically to improve their productivity. But, studies have shown that this results in extensive refactoring and adds maintenance owes. We assert this problem to the mismatches between the software under development and the reused external module; caused because of their different assumptions and constraints. We highlight the problems of such opportunistic reuse practices with the help of a case study. In our study, we found issues such as unanticipated behavior, violated constraints, conflict in assumption, fragile structure, and software bloat. In this paper, we like to draw attention of the research community to the wide spread opportunistic reuse practices and the lack of methods to pro-actively identify and resolve the mismatches. We propose the need for supporting developers in reasoning before reuse from the perspective of identifying and fixing both local and global mismatches. Furthermore, we identify other opportunistic software development practices where similar issues can be observed and also suggest the research areas where further investigation can benefit developers in improving their productivity. Copyright {\textcopyright} 2016 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spe.2439},
	ISSN = {1097-024X},
	Keywords = {design decisions,opportunistic practices,software bloat,software reuse},
	Publisher = {John Wiley {\&} Sons, Ltd},
	Url = {http://dx.doi.org/10.1002/spe.2439}
}

@InProceedings{Kulkarni:2012:TBA:2404962.2404988,
	Title = {{Towards Business Application Product Lines}},
	Author = {Kulkarni, Vinay and Barat, Souvik and Roychoudhury, Suman},
	Booktitle = {Proceedings of the 15th International Conference on Model Driven Engineering Languages and Systems},
	Year = {2012},
	Address = {Berlin, Heidelberg},
	Pages = {285--301},
	Publisher = {Springer-Verlag},
	Series = {MODELS'12},
	Doi = {10.1007/978-3-642-33666-9_19},
	ISBN = {978-3-642-33665-2},
	Keywords = {model driven engineering,software product lines},
	Url = {http://0-dx.doi.org.fama.us.es/10.1007/978-3-642-33666-9{\_}19}
}

@Article{Kumar2008254,
	Title = {{Locking the door but leaving the computer vulnerable: Factors inhibiting home users' adoption of software firewalls}},
	Author = {Kumar, Nanda and Mohan, Kannan and Holowczak, Richard},
	Journal = {Decision Support Systems},
	Year = {2008},
	Number = {1},
	Pages = {254--264},
	Volume = {46},
	Abstract = {In the new era of a ubiquitously networked world, security measures are only as good as their weakest link. Home computers with access to the Internet are one of the weaker links as they are typically not as well protected as computers in the corporate world. Malicious actors can not only target such computers but also use them to launch attacks against other systems connected to the Internet, thus posing severe threats to data and infrastructure as well as disrupting electronic commerce. This paper investigates the factors that affect the use of security protection strategies by home computer users in relation to a specific, but crucial security technology for home – a software firewall. This paper proposes individuals' concern for privacy, awareness of common security measures, attitude towards security and privacy protection technologies, and computer anxiety as important antecedents that have an impact on the users' decision to adopt a software firewall. The results of our study suggest that attitude plays a more important role than perceived usefulness in shaping users' intention to use firewalls. We attribute this interesting finding to the non-functional nature of firewall systems that work best in the background with a complex relationship to users' productivity. Hence, the results add to our current understanding of Technology Acceptance Model vis-{\`{a}}-vis technologies that serve non-functional needs such as security. We then present a set of guidelines to home computer users, Internet Service Providers, e-commerce companies, and the government to increase home users' adoption rate of privacy and security protection technologies. },
	Doi = {https://doi.org/10.1016/j.dss.2008.06.010},
	ISSN = {0167-9236},
	Keywords = {Firewalls,Privacy,Security protection technologies,e-Commerce,{\{}IS{\}} security},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167923608001243}
}

@Article{Kumara2013192,
	Title = {{Runtime evolution of service-based multi-tenant SaaS applications}},
	Author = {Kumara, I and Han, J and Colman, A and Kapuruge, M},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2013},
	Pages = {192--206},
	Volume = {8274 LNCS},
	Abstract = {The Single-Instance Multi-Tenancy (SIMT) model for service delivery enables a SaaS provider to achieve economies of scale via the reuse and runtime sharing of software assets between tenants. However, evolving such an application at runtime to cope with the changing requirements from its different stakeholders is challenging. In this paper, we propose an approach to evolving service-based SIMT SaaS applications that are developed based on Dynamic Software Product Lines (DSPL) with runtime sharing and variation among tenants. We first identify the different kinds of changes to a service-based SaaS application, and the consequential impacts of those changes. We then discuss how to realize and manage each change and its resultant impacts in the DSPL. A software engineer declaratively specifies changes in a script, and realizes the changes to the runtime model of the DSPL using the script. We demonstrate the feasibility of our approach with a case study. {\textcopyright} 2013 Springer-Verlag.},
	Annote = {cited By 2},
	Doi = {10.1007/978-3-642-45005-1_14},
	Keywords = {Compositional; Evolution; Feature; Multi tenancies,Computer software reusability,Software as a service (SaaS)},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892386621{\&}doi=10.1007{\%}2F978-3-642-45005-1{\_}14{\&}partnerID=40{\&}md5=6cebcfa8d05eed97e93b2fa28623bbc5}
}

@Article{Kuvaja2011143,
	Title = {{Software product line adoption - Guidelines from a case study}},
	Author = {Kuvaja, P and Simil{\"{a}}, J and Hanhela, H},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2011},
	Pages = {143--157},
	Volume = {4980 LNCS},
	Abstract = {It is possible to proceed with software product line adoption only once without major reinvestments and loss of time and money. In the literature, reported experiences of using the adoption models are not to be found, and especially the suitability of the models has not been reported. The purpose of this research is to compare known adoption models by formulating general evaluation criteria for the selection of an adoption model. Next an adoption model is selected for empirical research based on the context of a multimedia unit of a global telecommunication company. The empirical part consists of a case study analyzing the present state of adoption and producing plans for proceeding with the adoption. The research results can be utilized when selecting an adoption model for an empirical case and adopting a software product line in a software intensive organization. {\textcopyright} 2011 IFIP International Federation for Information Processing.},
	Annote = {cited By 4},
	Doi = {10.1007/978-3-642-22386-0_11},
	Keywords = {Investments; Research; Computer software; Investm,Software design; Software design,adoption; Adoption model; adoption strategy; guide},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053153703{\&}doi=10.1007{\%}2F978-3-642-22386-0{\_}11{\&}partnerID=40{\&}md5=94cb976461fc93d4a97d1b1adf462476}
}

@Article{Kwiatkowski20131368,
	Title = {{Recovering management information from source code}},
	Author = {Kwiatkowski, {\L} M and Verhoef, C},
	Journal = {Science of Computer Programming},
	Year = {2013},
	Number = {9},
	Pages = {1368--1406},
	Volume = {78},
	Abstract = {{\{}IT{\}} has become a production means for many organizations and an important element of business strategy. Even though its effective management is a must, reality shows that this area still remains in its infancy. {\{}IT{\}} management relies profoundly on relevant information which enables risk mitigation or cost control. However, the needed information is either missing or its gathering boils down to daunting tasks. We propose an approach to recovery of management information from the essence of IT; the software's source code. In this paper we show how to employ source code analysis techniques and recover management information. In our approach we exploit the potential of the concealed data which resides in the source code statements, source comments, and also compiler listings. We show how to depart from the raw sources, extract data, organize it, and eventually utilize so that the bit level data provides {\{}IT{\}} executives with support at the portfolio level. Our approach is pragmatic as we rely on real management questions, best practices in software engineering, and also {\{}IT{\}} market specifics. We enable, for instance, an assessment of the IT-portfolio market value, support for carrying out what-if scenarios, or identification and evaluation of the hidden risks for IT-portfolio maintainability. The study is based on a real-life IT-portfolio which supports business functions of an organization operating in the financial sector. The IT-portfolio comprises Cobol applications run on a mainframe with the total number of lines of code amounting to over 18 million. The approach we propose is suited for facilitation within a large organization. It provides for a fact-based support for strategic decision making at the portfolio level. },
	Doi = {https://doi.org/10.1016/j.scico.2012.07.016},
	ISSN = {0167-6423},
	Keywords = {Automated data extraction,Case study,Cobol,Compilers,Cost control,IT assets,IT metrics,IT-portfolio management,Information retrieval,LSI,Latent Semantic Indexing,Legacy systems,Lexical analysis,Management information,Market value,Obsolete language constructs,Operational risk,Risk mitigation,Scenario analysis,Source code analysis,Source code comments,Technology risk,Vendor locks,Volatility},
	Url = {http://www.sciencedirect.com/science/article/pii/S016764231200144X}
}

@Conference{Lopez-Ruiz2008,
	Title = {{A first generation software product line for data acquisition systems in astronomy}},
	Author = {L{\'{o}}pez-Ruiz, J C and Heradio, R and Somolinos, J A C and Fernandez, J R C and Ramos, P L},
	Booktitle = {Proceedings of SPIE - The International Society for Optical Engineering},
	Year = {2008},
	Volume = {7019},
	Abstract = {This article presents a case study on developing a software product line for data acquisition systems in astronomy based on the Exemplar Driven Development methodology and the Exemplar Flexibilization Language tool. The main strategies to build the software product line are based on the domain commonality and variability, the incremental scope and the use of existing artifacts. It consists on a lean methodology with little impact on the organization, suitable for small projects, which reduces product line start-up time. Software Product Lines focuses on creating a family of products instead of individual products. This approach has spectacular benefits on reducing the time to market, maintaining the know-how, reducing the development costs and increasing the quality of new products. The maintenance of the products is also enhanced since all the data acquisition systems share the same product line architecture.},
	Annote = {cited By 0},
	Doi = {10.1117/12.786834},
	Keywords = {Aerospace vehicles; Astrophysics; Computer softwa,Commonality and variability; Data acquisition syst,Software architecture},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-66949171734{\&}doi=10.1117{\%}2F12.786834{\&}partnerID=40{\&}md5=cbed487881d960fa6a07475ce855378c}
}

@Article{SMR:SMR581,
	Title = {{Increasing software development efficiency and maintainability for complex industrial systems – A case study}},
	Author = {Lagerstr{\"{o}}m, Robert and Sporrong, Ulf and Wall, Anders},
	Journal = {Journal of Software: Evolution and Process},
	Year = {2013},
	Number = {3},
	Pages = {285--301},
	Volume = {25},
	Abstract = {It is difficult to manage complex software systems. Thus, many research initiatives focus on how to improve software development efficiency and maintainability. However, the trend in the industry is still alarming, software development projects fail, and maintenance is becoming more and more expensive. One problem could be that research has been focusing on the wrong things. Most research publications address either process improvements or architectural improvements. There are few known approaches that consider how architectural changes affect processes and vice versa. One method proposed, called the Business–Architecture–Process method, takes these aspects into consideration. In 2007 the method was tested in one case study. Findings in the 2007 case study show that the method is useful, but in need of improvements and further validation. The present paper employs the method in a second case study. The contribution in this paper is thus a second test and validation of the proposed method, and useful method improvements for future use of the method. Copyright {\textcopyright} 2011 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/smr.581},
	ISSN = {2047-7481},
	Keywords = {architectural change,method support,process change,software development efficiency,software maintainability},
	Publisher = {John Wiley {\&} Sons, Ltd},
	Url = {http://dx.doi.org/10.1002/smr.581}
}

@Article{Lago2009168,
	Title = {{A scoped approach to traceability management}},
	Author = {Lago, Patricia and Muccini, Henry and van Vliet, Hans},
	Journal = {Journal of Systems and Software},
	Year = {2009},
	Number = {1},
	Pages = {168--182},
	Volume = {82},
	Abstract = {Traceability is the ability to describe and follow the life of a software artifact and a means for modeling the relations between software artifacts in an explicit way. Traceability has been successfully applied in many software engineering communities and has recently been adopted to document the transition among requirements, architecture and implementation. We present an approach to customize traceability to the situation at hand. Instead of automating tracing, or representing all possible traces, we scope the traces to be maintained to the activities stakeholders must carry out. We define core traceability paths, consisting of essential traceability links required to support the activities. We illustrate the approach through two examples: product derivation in software product lines, and release planning in software process management. By using a running software product line example, we explain why the core traceability paths identified are needed when navigating from feature to structural models and from family to product level and backward between models used in software product derivation. A feasibility study in release planning carried out in an industrial setting further illustrates the use of core traceability paths during production and measures the increase in performance of the development processes supported by our approach. These examples show that our approach can be successfully used to support both product and process traceability in a pragmatic yet efficient way. },
	Annote = {Special Issue: Software Performance - Modeling and Analysis},
	Doi = {https://doi.org/10.1016/j.jss.2008.08.026},
	ISSN = {0164-1212},
	Keywords = {Software process management,Software product line,Traceability issues,Traceability paths},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121208002033}
}

@Article{SMR:SMR1856,
	Title = {{Editorial: Reality check for software engineering for sustainability—pragmatism required}},
	Author = {Lago, Patricia and Penzenstadler, Birgit},
	Journal = {Journal of Software: Evolution and Process},
	Year = {2017},
	Number = {2},
	Pages = {n/a----n/a},
	Volume = {29},
	Doi = {10.1002/smr.1856},
	ISSN = {2047-7481},
	Keywords = {editorial,software engineering,sustainability},
	Url = {http://dx.doi.org/10.1002/smr.1856}
}

@Article{Lago2004214,
	Title = {{Observations from the recovery of a software product family}},
	Author = {Lago, P and {Van Vliet}, H},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2004},
	Pages = {214--227},
	Volume = {3154},
	Abstract = {The problem of managing the evolution of complex and large software systems is well known. Evolution implies the reuse and modification of existing software artifacts, and this means that the related knowledge must be documented and maintained. This paper focuses on the evolution of software product families, although the same principles apply in other software development environments as well. We describe our experience gained in a case study recovering a family of six software products. We give an overview of the case study, and provide lessons learned, implicit assumptions reconstructed during the case study, and some rules we think are generally applicable. Our experience indicates that organizing architectural knowledge is a difficult task. To properly serve the various uses of this knowledge, it needs to be organized along different dimensions and tools are required. Our experience also indicates that, next to variability explicitly designed into the product family, a "variation creep" is caused by different, and evolving, technical and organizational environments of the products. We propose explicitly modeling invariabilities, next to variabilities, in software product lines to get a better grip on this variation creep. {\textcopyright} Springer-Verlag 2004.},
	Annote = {cited By 4},
	Keywords = {Architectural knowledge; Large software systems;,Computer software; Computer software reusability;,Software design},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-35048869103{\&}partnerID=40{\&}md5=269509eb0f763140ade98854c78a3159}
}

@Article{Laguna20131010,
	Title = {{A systematic mapping study on software product line evolution: From legacy system reengineering to product line refactoring}},
	Author = {Laguna, M A and Crespo, Y},
	Journal = {Science of Computer Programming},
	Year = {2013},
	Number = {8},
	Pages = {1010--1034},
	Volume = {78},
	Abstract = {Software product lines (SPLs) are used in industry to develop families of similar software systems. Legacy systems, either highly configurable or with a story of versions and local variations, are potential candidates for reconfiguration as SPLs using reengineering techniques. Existing SPLs can also be restructured using specific refactorings to improve their internal quality. Although many contributions (including industrial experiences) can be found in the literature, we lack a global vision covering the whole life cycle of an evolving product line. This study aims to survey existing research on the reengineering of legacy systems into SPLs and the refactoring of existing SPLs in order to identify proven approaches and pending challenges for future research in both subfields. We launched a systematic mapping study to find as much literature as possible, covering the diverse terms involved in the search string (restructuring, refactoring, reengineering, etc. always connected with SPLs) and filtering the papers using relevance criteria. The 74 papers selected were classified with respect to several dimensions: main focus, research and contribution type, academic or industrial validation if included, etc. We classified the research approaches and analyzed their feasibility for use in industry. The results of the study indicate that the initial works focused on the adaptation of generic reengineering processes to SPL extraction. Starting from that foundation, several trends have been detected in recent research: the integrated or guided reengineering of (typically object-oriented) legacy code and requirements; specific aspect-oriented or feature-oriented refactoring into SPLs, and more recently, refactoring for the evolution of existing product lines. A majority of papers include academic or industrial case studies, though only a few are based on quantitative data. The degree of maturity of both subfields is different: Industry examples for the reengineering of the legacy system subfield are abundant, although more evaluation research is needed to provide better evidence for adoption in industry. Product line evolution through refactoring is an emerging topic with some pending challenges. Although it has recently received some attention, the theoretical foundation is rather limited in this subfield and should be addressed in the near future. To sum up, the main contributions of this work are the classification of research approaches as well as the analysis of remaining challenges, open issues, and research opportunities. {\textcopyright} 2012 Elsevier B.V. All rights reserved.},
	Annote = {cited By 24},
	Doi = {10.1016/j.scico.2012.05.003},
	File = {:Users/mac/Downloads/Sciencedirect{\_}articles{\_}13Jun2017{\_}15-33-20.965/A-systematic-mapping-study-on-software-product-line-evolution-From-legacy-system-reengineering-to-product-line-refactoring{\_}2013{\_}Science-of-Computer-Pr.pdf:pdf},
	Keywords = {Computer software; Industry; Legacy systems; Rese,Evolution; Legacy system reengineering; Product li,Reengineering},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878225914{\&}doi=10.1016{\%}2Fj.scico.2012.05.003{\&}partnerID=40{\&}md5=b8fba00f0623f75c93648c14304019ce}
}

@Article{Laguna20102313,
	Title = {{UML support for designing software product lines: The package merge mechanism}},
	Author = {Laguna, M A and Marqu{\'{e}}s, J M},
	Journal = {Journal of Universal Computer Science},
	Year = {2010},
	Number = {17},
	Pages = {2313--2332},
	Volume = {16},
	Abstract = {Software product lines have become a successful but challenging approach to software reuse. Some of the problems that hinder the adoption of this development paradigm are the conceptual gap between the variability and design models, as well as the complexity of the traceability management between them. Most current development methods use UML stereotypes or modify UML to face variability and traceability issues. Commercial tools focus mainly on code management, at a fine-grained level. However, the use of specialized techniques and tools represent additional barriers for the widespread introduction of product lines in software companies. In this paper, we propose an alternative based on the UML package merge mechanisms to reflect the structure of the variability models in product line package architecture, thus making the traceability of the configuration decisions straightforward. This package architecture and the configuration of the concrete products are automatically generated (using Model Driven Engineering techniques) from the variability models. As an additional advantage, the package merge mechanism can be directly implemented at code level using partial classes (present in languages such as C{\#}). To support the proposal, we have developed a tool incorporated into MS Visual Studio. This tool permits the product line variability to be modeled and the required transformations to be automated, including the final compilation of concrete products. A case study of a successful experience is described in the article as an example of applying these techniques and tools. The proposed approach, a combination of UML techniques and conventional IDE tools, can make the development of product lines easier for an organization as it removes the need for specialized tools and personnel. {\textcopyright} J.UCS.},
	Annote = {cited By 6},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650294973{\&}partnerID=40{\&}md5=10049679947c53bf4832416b3646fedf}
}

@Article{Laitenberger20005,
	Title = {{An encompassing life cycle centric survey of software inspection}},
	Author = {Laitenberger, Oliver and DeBaud, Jean-Marc},
	Journal = {Journal of Systems and Software},
	Year = {2000},
	Number = {1},
	Pages = {5--31},
	Volume = {50},
	Abstract = {This paper contributes an integrated survey of the work in the area of software inspection. It consists of two main sections. The first one introduces a detailed description of the core concepts and relationships that together define the field of software inspection. The second one elaborates a taxonomy that uses a generic development life-cycle to contextualize software inspection in detail. After Fagan's seminal work presented in 1976, the body of work in software inspection has greatly increased and reached measured maturity. Yet, there is still no encompassing and systematic view of this research body driven from a life-cycle perspective. This perspective is important since inspection methods and refinements are most often aligned to particular life-cycle artifacts. It also provides practitioners with a roadmap available in their terms. To provide a systematic and encompassing view of the research and practice body in software inspection, the contribution of this survey is, in a first step, to introduce in detail the core concepts and relationships that together embody the field of software inspection. This lays out the field key ideas and benefits and elicits a common vocabulary. There, we make a strong effort to unify the relevant vocabulary used in available literature sources. In a second step, we use this vocabulary to build a contextual map of the field in the form of a taxonomy indexed by the different development stages of a generic process. This contextual map can guide practitioners and focus their attention on the inspection work most relevant to the introduction or development of inspections at the level of their particular development stage; or to help motivate the use of software inspection earlier in their development cycle. Our work provides three distinct, practical benefits: First, the index taxonomy can help practitioners identify inspection experience directly related to a particular life-cycle stage. Second, our work allows structuring of the large amount of published inspection work. Third, such taxonomy can help researchers compare and assess existing inspection methods and refinements to identify fruitful areas of future work. },
	Doi = {https://doi.org/10.1016/S0164-1212(99)00073-4},
	ISSN = {0164-1212},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121299000734}
}

@Article{Lamancha201158,
	Title = {{Systematic review on Software Product Line Testing}},
	Author = {Lamancha, B P and Polo, M and Piattini, M},
	Journal = {Communications in Computer and Information Science},
	Year = {2011},
	Pages = {58--71},
	Volume = {170},
	Abstract = {This article presents a systematic review of the literature about Testing in Software Product Lines. The objective is to analyze the existing approaches to testing in software product lines, discussing the significant issues related to this area of knowledge and providing an up-to-date state of the art, which can serve as a basis for innovative research activities. The paper includes an analysis on how SPL research can contribute to dynamize the research in software testing. {\textcopyright} Springer-Verlag Berlin Heidelberg 2011.},
	Annote = {cited By 2},
	Doi = {10.1007/978-3-642-29578-2},
	Keywords = {Innovative research; Software Product Line; Softwa,Research; Software testing; Surveying; Testing,Software design},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879472096{\&}doi=10.1007{\%}2F978-3-642-29578-2{\&}partnerID=40{\&}md5=c62c91ce78008def01df926a79a396c5}
}

@Article{PerezLamancha20151,
	Title = {{PROW: A Pairwise algorithm with constRaints, Order and Weight}},
	Author = {Lamancha, Beatriz P{\'{e}}rez and Polo, Macario and Piattini, Mario},
	Journal = {Journal of Systems and Software},
	Year = {2015},
	Pages = {1--19},
	Volume = {99},
	Abstract = {Abstract Testing systems with many variables and/or values is often quite expensive due to the huge number of possible combinations to be tested. There are several criteria available to combine test data and produce scalable test suites. One of them is pairwise. With the pairwise criterion, each pair of values of any two parameters is included in at least one test case. Although this is a widely-used coverage criterion, two main characteristics improve considerably pairwise: constraints handling and prioritisation. This paper presents an algorithm and a tool. The algorithm (called PROW: Pairwise with constRaints, Order and Weight) handles constraints and prioritisation for pairwise coverage. The tool called {\{}CTWeb{\}} adds functionalities to execute {\{}PROW{\}} in different contexts, one of them is product sampling in Software Product Lines via importing feature models. Software Product Line (SPL) development is a recent paradigm, where a family of software systems is constructed by means of the reuse of a set of common functionalities and some variable functionalities. An essential artefact of a {\{}SPL{\}} is the feature model, which shows the features offered by the product line, jointly with the relationships (includes and excludes) among them. Pairwise testing could be used to obtain the product sampling to test in a SPL, using features as pairwise parameters. In this context, the constraint handling becomes essential. As a difference with respect to other tools, {\{}CTWeb{\}} does not require {\{}SAT{\}} solvers. This paper describes the {\{}PROW{\}} algorithm, also analysing its complexity and efficiency. The {\{}CTWeb{\}} tool is presented, including two examples of the {\{}PROW{\}} application to two real environments: the first corresponds to the migration of the subsystem of transactions processing of a credit card management system from {\{}AS400{\}} to Oracle with .NET; the second applies both the algorithm and the tool to a {\{}SPL{\}} that monitors and controls some parameters of the load in trucks. },
	Doi = {https://doi.org/10.1016/j.jss.2014.08.005},
	ISSN = {0164-1212},
	Keywords = {Combinatorial testing,Software testing},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121214001733}
}

@Article{IIS2:IIS203094,
	Title = {{4.6.3 Affordable Systems: Balancing the Capability, Schedule, Flexibility, and Technical Debt Tradespace}},
	Author = {Lane, Jo Ann and Koolmanojwong, Supannika and Boehm, Barry},
	Journal = {INCOSE International Symposium},
	Year = {2013},
	Number = {1},
	Pages = {1385--1399},
	Volume = {23},
	Abstract = {With the increasing demands for affordable system capabilities that can be provided quickly to the user community, developers must explore a variety of options for identifying “satisficing? solutions. The system capability affordability tradespace must balance expedited systems engineering to reduce schedule and cost, encourage flexibility in architecture decisions to support future evolution of the system, and minimize technical debt that either results in later rework or adversely impacts future options. This paper shows how the University of Southern California (USC) Center for Systems and Software Engineering (CSSE) software and systems engineering cost models can be used in the analysis of this tradespace to show the range of options and the resulting consequences.},
	Doi = {10.1002/j.2334-5837.2013.tb03094.x},
	ISSN = {2334-5837},
	Url = {http://dx.doi.org/10.1002/j.2334-5837.2013.tb03094.x}
}

@Article{Lane2011424,
	Title = {{Process models for service-based applications: A systematic literature review}},
	Author = {Lane, Stephen and Richardson, Ita},
	Journal = {Information and Software Technology},
	Year = {2011},
	Number = {5},
	Pages = {424--439},
	Volume = {53},
	Abstract = {Context Service-Oriented Computing (SOC) is a promising computing paradigm which facilitates the development of adaptive and loosely coupled service-based applications (SBAs). Many of the technical challenges pertaining to the development of {\{}SBAs{\}} have been addressed, however, there are still outstanding questions relating to the processes required to develop them. Objective The objective of this study is to systematically identify process models for developing service-based applications (SBAs) and review the processes within them. This will provide a useful starting point for any further research in the area. A secondary objective of the study is to identify process models which facilitate the adaptation of SBAs. Method In order to achieve this objective a systematic literature review (SLR) of the existing software engineering literature is conducted. Results During this research 722 studies were identified using a predefined search strategy, this number was narrowed down to 57 studies based on a set of strict inclusion and exclusion criteria. The results are reported both quantitatively in the form of a mapping study, as well as qualitatively in the form of a narrative summary of the key processes identified. Conclusion There are many process models reported for the development of {\{}SBAs{\}} varying in detail and maturity, this review has identified and categorised the processes within those process models. The review has also identified and evaluated process models which facilitate the adaptation of SBAs. },
	Annote = {Special Section on Best Papers from {\{}XP2010{\}}},
	Doi = {https://doi.org/10.1016/j.infsof.2010.12.005},
	ISSN = {0950-5849},
	Keywords = {SOA,Service-based application,Software process,Systematic literature review},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584910002211}
}

@Article{Lassing200247,
	Title = {{Experiences with ALMA: Architecture-Level Modifiability Analysis}},
	Author = {Lassing, Nico and Bengtsson, PerOlof and van Vliet, Hans and Bosch, Jan},
	Journal = {Journal of Systems and Software},
	Year = {2002},
	Number = {1},
	Pages = {47--57},
	Volume = {61},
	Abstract = {Modifiability is an important quality for software systems, because a large part of the costs associated with these systems is spent on modifications. The effort, and therefore cost, that is required for these modifications is largely determined by a system's software architecture. Analysis of software architectures is therefore an important technique to achieve modifiability and reduce maintenance costs. However, few techniques for software architecture analysis currently exist. Based on our experiences with software architecture analysis of modifiability, we have developed ALMA, an architecture-level modifiability analysis method consisting of five steps. In this paper we report on our experiences with ALMA. We illustrate our experiences with examples from two case studies of software architecture analysis of modifiability. These case studies concern a system for mobile positioning at Ericsson Software Technology {\{}AB{\}} and a system for freight handling at {\{}DFDS{\}} Fraktarna. Our experiences are related to each step of the analysis process. In addition, we made some observations on software architecture analysis of modifiability in general. },
	Doi = {https://doi.org/10.1016/S0164-1212(01)00113-3},
	ISSN = {0164-1212},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121201001133}
}

@InProceedings{Lavazza:2009:CFS:1540438.1540451,
	Title = {{Convertibility of Functional Size Measurements: New Insights and Methodological Issues}},
	Author = {Lavazza, Luigi},
	Booktitle = {Proceedings of the 5th International Conference on Predictor Models in Software Engineering},
	Year = {2009},
	Address = {New York, NY, USA},
	Pages = {9:1----9:12},
	Publisher = {ACM},
	Series = {PROMISE '09},
	Doi = {10.1145/1540438.1540451},
	ISBN = {978-1-60558-634-2},
	Keywords = {COSMIC function points,function point analysis,functional size measure convertibility,functional size measurement},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/1540438.1540451}
}

@InProceedings{Lavazza:2010:POT:1852786.1852834,
	Title = {{Predicting OSS Trustworthiness on the Basis of Elementary Code Assessment}},
	Author = {Lavazza, Luigi and Morasca, Sandro and Taibi, Davide and Tosi, Davide},
	Booktitle = {Proceedings of the 2010 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
	Year = {2010},
	Address = {New York, NY, USA},
	Pages = {36:1----36:4},
	Publisher = {ACM},
	Series = {ESEM '10},
	Doi = {10.1145/1852786.1852834},
	ISBN = {978-1-4503-0039-1},
	Keywords = {elementary code assessment (ECA) rules,source code analysis,static analysis,trustworthiness of open-source software},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/1852786.1852834}
}

@InProceedings{Lavazza:2010:IEC:1852786.1852820,
	Title = {{Introducing the Evaluation of Complexity in Functional Size Measurement: A UML-based Approach}},
	Author = {Lavazza, Luigi and Robiolo, Gabriela},
	Booktitle = {Proceedings of the 2010 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
	Year = {2010},
	Address = {New York, NY, USA},
	Pages = {25:1----25:9},
	Publisher = {ACM},
	Series = {ESEM '10},
	Doi = {10.1145/1852786.1852820},
	ISBN = {978-1-4503-0039-1},
	Keywords = {COSMIC function points,UML-based measurement,effort estimation,function points,functional complexity measurement,functional size measurement,use case-based measurement},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/1852786.1852820}
}

@InProceedings{Lavazza:2010:RMF:1868328.1868338,
	Title = {{The Role of the Measure of Functional Complexity in Effort Estimation}},
	Author = {Lavazza, Luigi and Robiolo, Gabriela},
	Booktitle = {Proceedings of the 6th International Conference on Predictive Models in Software Engineering},
	Year = {2010},
	Address = {New York, NY, USA},
	Pages = {6:1----6:10},
	Publisher = {ACM},
	Series = {PROMISE '10},
	Doi = {10.1145/1868328.1868338},
	ISBN = {978-1-4503-0404-7},
	Keywords = {COSMIC function points,effort estimation,function points,functional complexity measurement,functional size measurement,use case points,use case-based measurement},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/1868328.1868338}
}

@Article{Lavoie201732,
	Title = {{A case study of TTCN-3 test scripts clone analysis in an industrial telecommunication setting}},
	Author = {Lavoie, Thierry and M{\'{e}}rineau, Mathieu and Merlo, Ettore and Potvin, Pascal},
	Journal = {Information and Software Technology},
	Year = {2017},
	Pages = {32--45},
	Volume = {87},
	Abstract = {Abstract Context: This paper presents a novel experiment focused on detecting and analyzing clones in test suites written in TTCN-3, a standard telecommunication test script language, for different industrial projects. Objective: This paper investigates frequencies, types, and similarity distributions of TTCN-3 clones in test scripts from three industrial projects in telecommunication. We also compare the distribution of clones in TTCN-3 test scripts with the distribution of clones in C/C++ and Java projects from the telecommunication domain. We then perform a statistical analysis to validate the significance of differences between these distributions. Method: Similarity is computed using CLAN, which compares metrics syntactically derived from script fragments. Metrics are computed from the Abstract Syntax Trees produced by a TTCN-3 parser called Titan developed by Ericsson as an Eclipse plugin. Finally, clone classification of similar script pairs is computed using the Longest Common Subsequence algorithm on token types and token images. Results: This paper presents figures and diagrams reporting TTCN-3 clone frequencies, types, and similarity distributions. We show that the differences between the distribution of clones in test scripts and the distribution of clones in applications are statistically significant. We also present and discuss some lessons that can be learned about the transferability of technology from this study. Conclusion: About 24{\%} of fragments in the test suites are cloned, which is a very high proportion of clones compared to what is generally found in source code. The difference in proportion of Type-1 and Type-2 clones is statistically significant and remarkably higher in TTCN-3 than in source code. Type-1 and Type-2 clones represent 82.9{\%} and 15.3{\%} of clone fragments for a total of 98.2{\%}. Within the projects this study investigated, this represents more and easier potential re-factoring opportunities for test scripts than for code. },
	Doi = {https://doi.org/10.1016/j.infsof.2017.01.008},
	ISSN = {0950-5849},
	Keywords = {Clone detection,Telecommunications software,Test},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584917300605}
}

@Article{Lee2009137,
	Title = {{Experience report on using a domain model-based extractive approach to software product line asset development}},
	Author = {Lee, H and Choi, H and Kang, K C and Kim, D and Lee, Z},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2009},
	Pages = {137--149},
	Volume = {5791 LNCS},
	Abstract = {When we attempted to introduce an extractive approach to a company, we were faced with a challenging project situation where legacy applications did not have many commonalities among their implementations as they were developed independently by different teams without sharing a common code base. Although there were not many structural similarities, we expected to find similarities if we view them from the domain model perspective as they were in the same domain and were developed with the object-oriented paradigm. Therefore, we decided to place the domain model at the center of extraction and reengineering, thus developing a domain model-based extractive method. The method has been successfully applied to introduce software product line to a set-top box manufacturing company. {\textcopyright} 2009 Springer Berlin Heidelberg.},
	Annote = {cited By 9},
	Doi = {10.1007/978-3-642-04211-9_14},
	Keywords = {Component extraction; Domain model; Experience rep,Computer software reusability,Feature extraction; Production engineering},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350417614{\&}doi=10.1007{\%}2F978-3-642-04211-9{\_}14{\&}partnerID=40{\&}md5=76fc90edc472d461632563b61a170d17}
}

@Article{Lee20101123,
	Title = {{A feature-oriented approach for developing reusable product line assets of service-based systems}},
	Author = {Lee, Jaejoon and Muthig, Dirk and Naab, Matthias},
	Journal = {Journal of Systems and Software},
	Year = {2010},
	Number = {7},
	Pages = {1123--1136},
	Volume = {83},
	Abstract = {Service orientation (SO) is a relevant promising candidate for accommodating rapidly changing user needs and expectations. One of the goals of adopting {\{}SO{\}} is the improvement of reusability, however, the development of service-based system in practice has uncovered several challenging issues, such as how to identify reusable services, how to determine configurations of services that are relevant to users' current product configuration and context, and how to maintain service validity after configuration changes. In this paper, we propose a method that addresses these issues by adapting a feature-oriented product line engineering approach. The method is notable in that it guides developers to identify reusable services at the right level of granularity and to map users' context to relevant service configuration, and it also provides a means to check the validity of services at runtime in terms of invariants and pre/post-conditions of services. Moreover, we propose a heterogeneous style based architecture model for developing such systems. },
	Annote = {{\{}SPLC{\}} 2008},
	Doi = {https://doi.org/10.1016/j.jss.2010.01.048},
	ISSN = {0164-1212},
	Keywords = {Feature-oriented,Service-based systems,Software architecture,Software architecture styles,Software product line engineering},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121210000324}
}

@Article{Lee2016199,
	Title = {{What situational information would help developers when using a graphical code recommender?}},
	Author = {Lee, Seonah and Kang, Sungwon},
	Journal = {Journal of Systems and Software},
	Year = {2016},
	Pages = {199--217},
	Volume = {117},
	Abstract = {Abstract Developers spend a significant amount of time trying to understand code bases. To aid developers' comprehension of code, researchers have developed software visualization tools. However, the uses of these tools in situ have rarely been investigated. To make matters worse, as studies have revealed, developers seldom use diagramming tools, making such investigations a challenge. To determine the possible uses of such tools in real practice, we conduct a diary study in which eleven developers in real-world developments use a novel visualization tool (a graphical code recommender) for one month. In the study, we ask what information and features the visualization and diagramming tools should provide to aid developers' work according to their situations. The study reveals the situations in which developers would use such visualization and diagramming tools and also the concrete requirements for such tools that would make them useful. },
	Doi = {https://doi.org/10.1016/j.jss.2016.02.050},
	ISSN = {0164-1212},
	Keywords = {Code navigation,Design requirement,Diagramming tool,Diary study,Software visualization},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121216000819}
}

@Article{Lee20132154,
	Title = {{Clustering navigation sequences to create contexts for guiding code navigation}},
	Author = {Lee, Seonah and Kang, Sungwon},
	Journal = {Journal of Systems and Software},
	Year = {2013},
	Number = {8},
	Pages = {2154--2165},
	Volume = {86},
	Abstract = {Abstract To guide programmer code navigation, previous approaches such as TeamTracks recommend pieces of code to visit by mining the associations between pieces of code in programmer interaction histories. However, these result in low recommendation accuracy. To create more accurate recommendations, we propose NavClus an approach that clusters navigation sequences from programmer interaction histories. NavClus automatically forms collections of code that are relevant to the tasks performed by programmers, and then retrieves the collections best matched to a programmer's current navigation path. This makes it possible to recommend the collections of code that are relevant to the programmer's given task. We compare NavClus' recommendation accuracy with TeamTracks' by simulating recommendations using 4397 interaction histories. The comparative experiment shows that the recommendation accuracy of NavClus is twice as high as that of TeamTracks. },
	Doi = {https://doi.org/10.1016/j.jss.2013.03.103},
	ISSN = {0164-1212},
	Keywords = {Code navigation,Context aware code recommender,Data clustering techniques,Data stream mining,Programmer interaction histories},
	Url = {http://www.sciencedirect.com/science/article/pii/S016412121300085X}
}

@Article{Lee20061,
	Title = {{Modeling Variant User Interfaces for Web-Based Software Product Lines}},
	Author = {Lee, S C},
	Journal = {International Journal of Information Technology and Web Engineering (IJITWE)},
	Year = {2006},
	Number = {1},
	Pages = {1--34},
	Volume = {1},
	Abstract = {Software product line (SPL) is a software engineering paradigm for software development. SPL is important in promoting software reuse, leading to higher productivity and quality. A software product within a product line often has specific functionalities that are not common to all other products within the product line. Those specific functionalities are termed “variant features? in a product line. SPL paradigm involves the modeling of variant features. However, little work in SPL investigates and addresses the modeling of variant features specific to UI. UML is the de facto modeling language for object-oriented software systems. It is known that UML needs better support in modeling UIs. Thus, much research developed UML extensions to improve UML support in modeling UIs. Yet little of this work is related to developing such extensions for modeling UIs for SPLs in which variant features specific to user interfaces (UI) modeling must be addressed. This research develops a UML extension, WUIML, to address these problems. WUIML defines elements for modeling variant features specific to UIs for Web-based SPLs. The model elements in WUIML extend from the metaclass and of the UML2.0 metamodel. WUIML integrates the modeling of variant features specific to UIs to UML. For example, in a Web-based patient registration SPL, member products targeting British users may use British date format in the user interface, while member products targeting United States users may use United States date format in the user interface. Thus, this is a variant feature for this product line. WUIML defines a model element, XOR, to represent such exclusive or conditions in a product line user interface model. WUIML would reduce SPL engineers' efforts needed in UI development. To validate the WUIML research outcome, a case study was conducted. The results of this empirical study indicate that modeling UIs for Web-based SPLs using WUIML is more effective and efficient than using standard UML. {\textcopyright} 2006, IGI Global. All rights reserved.},
	Annote = {cited By 0},
	Doi = {10.4018/jitwe.2006010101},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85001610191{\&}doi=10.4018{\%}2Fjitwe.2006010101{\&}partnerID=40{\&}md5=48768aae9dbeec09b817e0f8045f0510}
}

@Article{Lee2006127,
	Title = {{An approach to managing feature dependencies for product releasing in software product lines}},
	Author = {Lee, Y and Yang, C and Zhu, C and Zhao, W},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2006},
	Pages = {127--141},
	Volume = {4039 LNCS},
	Abstract = {Product line software engineering is a systematic approach to realize large scale software reuse. Software product lines deal with reusable assets across a domain by exploring requirements commonality and variability. Requirements dependencies have very strong influence on all development phases of member products in a product line. There are many feature oriented approaches on requirement dependencies. However, most of them are limited to the problem domain. Among those few focusing on the solution domain, they are limited to modeling requirement dependencies. This paper presents a feature oriented approach to managing domain requirements dependencies. Not only is a requirement dependencies model presented, but a directed graph-based approach is also developed to analyze domain requirement dependencies for effective release of member products in a product line. This approach returns a simple directed graph, and uses an effective algorithm to get a set of requirements to be released in a member product. A case study for spot and futures transaction domain is described to illustrate the approach. {\textcopyright} Springer-Verlag Berlin Heidelberg 2006.},
	Annote = {cited By 9},
	Keywords = {Computer software reusability; Database systems; G,Feature dependencies; Graph-based approach; Softw,Software engineering},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746191953{\&}partnerID=40{\&}md5=cf0236cda43ad4ae7c11c7b928c64e65}
}

@Article{Lemos2013951,
	Title = {{Evaluation studies of software testing research in Brazil and in the world: A survey of two premier software engineering conferences}},
	Author = {Lemos, Ot{\'{a}}vio Augusto Lazzarini and Ferrari, Fabiano Cutigi and Eler, Marcelo Medeiros and Maldonado, Jos{\'{e}} Carlos and Masiero, Paulo Cesar},
	Journal = {Journal of Systems and Software},
	Year = {2013},
	Number = {4},
	Pages = {951--969},
	Volume = {86},
	Abstract = {This paper reports on a historical perspective of the evaluation studies present in software testing research published in the Brazilian Symposium on Software Engineering (SBES) in comparison to the International Conference on Software Engineering (ICSE). The survey characterizes the software testing-related papers published in the 25-year history of SBES, investigates the types of evaluation presented in these publications, and how the rate of evaluations has evolved over the years. A similar analysis within the same period is made for ICSE, allowing for a comparison between the national and international scenario. Results show that the rate of papers that present evaluation studies in {\{}SBES{\}} has significantly increased over the years. However, among the papers that described some kind of evaluation, only around 20{\%} performed more rigorous evaluations (i.e. case studies, quasi experiments, or controlled experiments). Such percentage is low when compared to ICSE, which presented 40{\%} of papers with more rigorous evaluations within the same period. Nevertheless, we noticed that both venues still lack the publication of research reporting controlled experiments: only a single paper in each conference presented this type of evaluation. },
	Annote = {{\{}SI{\}} : Software Engineering in Brazil: Retrospective and Prospective Views},
	Doi = {https://doi.org/10.1016/j.jss.2012.11.040},
	ISSN = {0164-1212},
	Keywords = {Evaluation studies,Software testing,Software testing research in Brazil},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121212003238}
}

@Conference{Lena2009,
	Title = {{Digital differentiation, software product lines, and the challenge of isomorphism in innovation: A case study}},
	Author = {Lena, A and Ola, H},
	Booktitle = {17th European Conference on Information Systems, ECIS 2009},
	Year = {2009},
	Abstract = {This paper examines the adoption of software product line engineering to implement digital differentiation of physical products. The introduction of such software-based variety can typically be challenging for firms innovating within the realm of a manufacturing paradigm. In particular, the mutual dependency between the organization design and product design of new product developing firms may counteract attempts to induce change through software product line engineering. On the basis of innovation theory and the notion of isomorphism, the paper presents a case study of digital differentiation at one of the world's largest automakers, GlobalCarCorp. Relating to the literatures of software product lines and product families, the contribution of the paper is a lens through which to understand the role of isomorphism in implementing digital differentiation in new product development. In addition, practical implications are derived from this in-depth study.},
	Annote = {cited By 1},
	Keywords = {Digital differentiation; Innovation management; IS,Industry; Information systems; Innovation; Produc,Software design},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870699353{\&}partnerID=40{\&}md5=4838f1913ec0fe1d8c629e21d308a728}
}

@Article{CPE:CPE4142,
	Title = {{Special issue: Advanced stencil-code engineering}},
	Author = {Lengauer, Christian and Bolten, Matthias and Falgout, Robert and Schenk, Olaf},
	Journal = {Concurrency and Computation: Practice and Experience},
	Year = {2017},
	Pages = {n/a----n/a},
	Doi = {10.1002/cpe.4142},
	ISSN = {1532-0634},
	Url = {http://dx.doi.org/10.1002/cpe.4142}
}

@Conference{Lengauer2014,
	Title = {{Where has all my memory gone? Determining memory characteristics of product variants using virtual-machine-level monitoring}},
	Author = {Lengauer, P and Bitto, V and Angerer, F and Gr{\"{u}}nbacher, P and M{\"{o}}ssenb{\"{o}}ck, H},
	Booktitle = {ACM International Conference Proceeding Series},
	Year = {2014},
	Abstract = {Non-functional properties such as memory footprint have recently gained importance in software product line research. However, determining the memory characteristics of individual features and product variants is extremely challenging. We present an approach that supports the monitoring of memory characteristics of individual features at the level of Java virtual machines. Our approach provides extensions to Java virtual machines to track memory allocations and deal-locations of individual features based on a feature-to-code mapping. The approach enables continuous monitoring at the level of features to detect anomalies such as memory leaks, excessive memory consumption, or abnormal garbage collection times in product variants. We provide an evaluation of our approach based on different product variants of the DesktopSearcher product line. Our experiment with different program inputs demonstrates the feasibility of our technique. {\textcopyright} 2014 ACM.},
	Annote = {cited By 0},
	Doi = {10.1145/2556624.2556628},
	Keywords = {Computer simulation; Monitoring; Software design,Continuous monitoring; Feature-oriented software d,Java programming language},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897637982{\&}doi=10.1145{\%}2F2556624.2556628{\&}partnerID=40{\&}md5=5ac5c19edd096deefec85dc26781350b}
}

@Article{SMR:SMR512,
	Title = {{Improving software testing process: feature prioritization to make winners of success-critical stakeholders}},
	Author = {Li, Qi and Yang, Ye and Li, Mingshu and Wang, Qing and Boehm, Barry W and Hu, Chenyong},
	Journal = {Journal of Software: Evolution and Process},
	Year = {2012},
	Number = {7},
	Pages = {783--801},
	Volume = {24},
	Abstract = {For a successful software project, acceptable quality must be achieved within an acceptable cost, demonstrating business value to customers and satisfactorily meeting delivery timeliness. Testing serves as the most widely used approaches to determine that the intended functionalities are performed correctly and achieve the desired level of services; however, it is also a labor-intensive and expensive process during the whole software life cycle. Most current testing processes are often technique-centered, rather than organized to maximize business value. In this article, we extend and elaborate the ‘4+1' theoretical lenses of Value-based Software Engineering (VBSE) framework in the software testing process; propose a multi-objective feature prioritization strategy for testing planning and controlling, which aligns the internal testing process with value objectives coming from customers and markets. Our case study in a real-life business project shows that this method allows reasoning about the software testing process in different dimensions: it helps to manage the testing process effectively and efficiently, provides information for continuous internal software process improvement, and increases customer satisfaction, which makes winners of all success-critical stakeholders (SCSs) in the software testing process. Copyright {\textcopyright} 2010 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/smr.512},
	ISSN = {2047-7481},
	Keywords = {business importance,cost,market,quality risk,software testing,value-based},
	Publisher = {John Wiley {\&} Sons, Ltd},
	Url = {http://dx.doi.org/10.1002/smr.512}
}

@Article{Li20141775,
	Title = {{Efficient allocation of resources in multiple heterogeneous Wireless Sensor Networks}},
	Author = {Li, Wei and Delicato, Fl{\'{a}}via C and Pires, Paulo F and Lee, Young Choon and Zomaya, Albert Y and Miceli, Claudio and Pirmez, Luci},
	Journal = {Journal of Parallel and Distributed Computing},
	Year = {2014},
	Number = {1},
	Pages = {1775--1788},
	Volume = {74},
	Abstract = {Abstract Wireless Sensor Networks (WSNs) are useful for a wide range of applications, from different domains. Recently, new features and design trends have emerged in the {\{}WSN{\}} field, making those networks appealing not only to the scientific community but also to the industry. One such trend is the running different applications on heterogeneous sensor nodes deployed in multiple {\{}WSNs{\}} in order to better exploit the expensive physical network infrastructure. Another trend deals with the capability of accessing sensor generated data from the Web, fitting {\{}WSNs{\}} in novel paradigms of Internet of Things (IoT) and Web of Things (WoT). Using well-known and broadly accepted Web standards and protocols enables the interoperation of heterogeneous {\{}WSNs{\}} and the integration of their data with other Web resources, in order to provide the final user with value-added information and applications. Such emergent scenarios where multiple networks and applications interoperate to meet high level requirements of the user will pose several changes in the design and execution of {\{}WSN{\}} systems. One of these challenges regards the fact that applications will probably compete for the resources offered by the underlying sensor nodes through the Web. Thus, it is crucial to design mechanisms that effectively and dynamically coordinate the sharing of the available resources to optimize resource utilization while meeting application requirements. However, it is likely that Quality of Service (QoS) requirements of different applications cannot be simultaneously met, while efficiently sharing the scarce networks resources, thus bringing the need of managing an inherent tradeoff. In this paper, we argue that a middleware platform is required to manage heterogeneous {\{}WSNs{\}} and efficiently share their resources while satisfying user needs in the emergent scenarios of WoT. Such middleware should provide several services to control running application as well as to distribute and coordinate nodes in the execution of submitted sensing tasks in an energy-efficient and QoS-enabled way. As part of the middleware provided services we present the Resource Allocation in Heterogeneous {\{}WSNs{\}} (SACHSEN) algorithm. {\{}SACHSEN{\}} is a new resource allocation heuristic for systems composed of heterogeneous {\{}WSNs{\}} that effectively deals with the tradeoff between possibly conflicting QoS requirements and exploits heterogeneity of multiple WSNs. },
	Doi = {https://doi.org/10.1016/j.jpdc.2013.09.012},
	ISSN = {0743-7315},
	Keywords = {Resource allocation,Task allocation,Wireless sensor network},
	Url = {http://www.sciencedirect.com/science/article/pii/S0743731513002104}
}

@Article{Li2013777,
	Title = {{Application of knowledge-based approaches in software architecture: A systematic mapping study}},
	Author = {Li, Zengyang and Liang, Peng and Avgeriou, Paris},
	Journal = {Information and Software Technology},
	Year = {2013},
	Number = {5},
	Pages = {777--794},
	Volume = {55},
	Abstract = {Context Knowledge management technologies have been employed across software engineering activities for more than two decades. Knowledge-based approaches can be used to facilitate software architecting activities (e.g., architectural evaluation). However, there is no comprehensive understanding on how various knowledge-based approaches (e.g., knowledge reuse) are employed in software architecture. Objective This work aims to collect studies on the application of knowledge-based approaches in software architecture and make a classification and thematic analysis on these studies, in order to identify the gaps in the existing application of knowledge-based approaches to various architecting activities, and promising research directions. Method A systematic mapping study is conducted for identifying and analyzing the application of knowledge-based approaches in software architecture, covering the papers from major databases, journals, conferences, and workshops, published between January 2000 and March 2011. Results Fifty-five studies were selected and classified according to the architecting activities they contribute to and the knowledge-based approaches employed. Knowledge capture and representation (e.g., using an ontology to describe architectural elements and their relationships) is the most popular approach employed in architecting activities. Knowledge recovery (e.g., documenting past architectural design decisions) is an ignored approach that is seldom used in software architecture. Knowledge-based approaches are mostly used in architectural evaluation, while receive the least attention in architecture impact analysis and architectural implementation. Conclusions The study results show an increased interest in the application of knowledge-based approaches in software architecture in recent years. A number of knowledge-based approaches, including knowledge capture and representation, reuse, sharing, recovery, and reasoning, have been employed in a spectrum of architecting activities. Knowledge-based approaches have been applied to a wide range of application domains, among which “Embedded software? has received the most attention. },
	Doi = {https://doi.org/10.1016/j.infsof.2012.11.005},
	ISSN = {0950-5849},
	Keywords = {Architecting activity,Knowledge-based approach,Software architecture,Systematic mapping study},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584912002315}
}

@Article{Li201539,
	Title = {{A complete approach for {\{}CIM{\}} modelling and model formalising}},
	Author = {Li, Zonghua and Zhou, Xiaofeng and Gu, Aihua and Li, Qinfeng},
	Journal = {Information and Software Technology},
	Year = {2015},
	Pages = {39--55},
	Volume = {65},
	Abstract = {AbstractContext Computation Independent Model (CIM) as a business model describes the requirements and environment of a business system and instructs the designing and development; it is a key to influencing software success. Although many studies currently focus on model driven development (MDD); those researches, to a large extent, study the PIM-level and PSM-level model, and few have dealt with CIM-level modelling for case in which the requirements are unclear or incomplete. Objective This paper proposes a CIM-level modelling approach, which applies a stepwise refinement approach to modelling the CIM-level model starting from a high-level goal model to a lower-level business process model. A key advantage of our approach is the combination of the requirement model with the business model, which helps software engineers to define business models exactly for cases in which the requirements are unclear or incomplete. Method This paper, based on the model driven approach, proposes a set of models at the CIM-level and model transformations to connect these models. Accordingly, the formalisation approach of this paper involves formalising the goal model using the category theory and the scenario model and business process model using Petri nets. Results We have defined a set of metamodels and transformation rules making it possible to obtain automatically a scenario model from the goal model and a business process model from the scenario model. At the same time, we have defined a mapping rule to formalise these models. Our proposed {\{}CIM{\}} modelling approach and formalisation approach are implemented with an {\{}MDA{\}} tool, and it has been empirically validated by a travel agency case study. Conclusion This study shows how a {\{}CIM{\}} modelling approach helps to build a complete and consistent model at the {\{}CIM{\}} level for cases in which the requirements are unclear or incomplete in advance. },
	Doi = {https://doi.org/10.1016/j.infsof.2015.04.003},
	ISSN = {0950-5849},
	Keywords = {CIM,Model consistency verification,Model formalisation,Model transformations,Petri nets},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584915000786}
}

@Conference{Lillacka2013185,
	Title = {{Improved prediction of non-functional properties in Software Product Lines with domain context}},
	Author = {Lillacka, M and M{\"{u}}llerb, J and Eiseneckera, U W},
	Booktitle = {Lecture Notes in Informatics (LNI), Proceedings - Series of the Gesellschaft fur Informatik (GI)},
	Year = {2013},
	Pages = {185--198},
	Volume = {P-213},
	Abstract = {Software Product Lines (SPLs) enable software reuse by systematically managing commonalities and variability. Usually, commonalities and variability are expressed by features. Functional requirements of a software product are met by selecting appropriate features. However, selecting features also influences non-functional properties. To satisfy non-functional requirements of a software product, as well, the effect of a feature selection on non-functional properties has to be known. Often an SPL allows a vast number of valid products, which renders a test of non-functional properties on the basis of all valid products impractical. Recent research offers a solution to this problem: the effect of features on non-functional properties of software products is predicted by measuring in advance. A sample of feature configurations is generated, executed with a predefined benchmark, and then non-functional properties are measured. Based on these data a model is created that allows to predict non-functional properties of a software product before actually building it. However, in some domains contextual influences, such as input data, can heavily affect nonfunctional properties. We argue that the measurement of the effect of features on non-functional properties can be drastically improved by considering contextual influences of a domain. We study this assumption on input data as an example for a contextual influence and using an artificial but intuitive case study from the domain of compression algorithms. Our study shows that prediction accuracy of non-functional properties can be significantly improved.},
	Annote = {cited By 1},
	Keywords = {Compression algorithms; Feature configuration; Fu,Computer software reusability,Computer software; Forecasting; Input output progr},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922707665{\&}partnerID=40{\&}md5=54c26ec6127b77b87871663fe02acf05}
}

@Article{Lim2015,
	Title = {{Investigating Country Differences in Mobile App User Behavior and Challenges for Software Engineering}},
	Author = {Lim, Soo Ling and Bentley, Peter J. and Kanakam, Natalie and Ishikawa, Fuyuki and Honiden, Shinichi},
	Journal = {IEEE Transactions on Software Engineering},
	Year = {2015},
	Month = {jan},
	Number = {1},
	Pages = {40--64},
	Volume = {41},
	Doi = {10.1109/TSE.2014.2360674},
	File = {:Users/mac/Downloads/bulk-download (11)/Investigating Country Differences in Mobile App User Behavior and Challenges for Software Engineering.pdf:pdf},
	ISSN = {0098-5589},
	Url = {http://ieeexplore.ieee.org/document/6913003/}
}

@InProceedings{Lincke:2008:CSM:1390630.1390648,
	Title = {{Comparing Software Metrics Tools}},
	Author = {Lincke, R{\"{u}}diger and Lundberg, Jonas and L{\"{o}}we, Welf},
	Booktitle = {Proceedings of the 2008 International Symposium on Software Testing and Analysis},
	Year = {2008},
	Address = {New York, NY, USA},
	Pages = {131--142},
	Publisher = {ACM},
	Series = {ISSTA '08},
	Doi = {10.1145/1390630.1390648},
	ISBN = {978-1-60558-050-0},
	Keywords = {comparing tools,software quality metrics},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/1390630.1390648}
}

@InProceedings{Lind:2010:CRS:1852786.1852821,
	Title = {{Categorization of Real-time Software Components for Code Size Estimation}},
	Author = {Lind, Kenneth and Heldal, Rogardt},
	Booktitle = {Proceedings of the 2010 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
	Year = {2010},
	Address = {New York, NY, USA},
	Pages = {26:1----26:10},
	Publisher = {ACM},
	Series = {ESEM '10},
	Doi = {10.1145/1852786.1852821},
	ISBN = {978-1-4503-0039-1},
	Keywords = {COSMIC function points,UML components,categorization,functional size measurement,software code size,system architecture},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/1852786.1852821}
}

@Article{Lisboa2010,
	Title = {{A systematic review of domain analysis tools}},
	Author = {Lisboa, Liana Barachisio and Garcia, Vinicius Cardoso and Lucr{\'{e}}dio, Daniel and de Almeida, Eduardo Santana and {de Lemos Meira}, Silvio Romero and {de Mattos Fortes}, Renata Pontin},
	Journal = {Information and Software Technology},
	Year = {2010},
	Month = {jan},
	Number = {1},
	Pages = {1--13},
	Volume = {52},
	Doi = {10.1016/j.infsof.2009.05.001},
	File = {:Users/mac/Downloads/1-s2.0-S0950584909000834-main.pdf:pdf},
	ISSN = {09505849},
	Publisher = {Elsevier B.V.},
	Url = {http://linkinghub.elsevier.com/retrieve/pii/S0950584909000834}
}

@Article{Liu20071879,
	Title = {{Safety analysis of software product lines using state-based modeling}},
	Author = {Liu, Jing and Dehlinger, Josh and Lutz, Robyn},
	Journal = {Journal of Systems and Software},
	Year = {2007},
	Number = {11},
	Pages = {1879--1892},
	Volume = {80},
	Abstract = {The difficulty of managing variations and their potential interactions across an entire product line currently hinders safety analysis in safety-critical, software product lines. The work described here contributes to a solution by integrating product-line safety analysis with model-based development. This approach provides a structured way to construct state-based models of a product line having significant, safety-related variations and to systematically explore the relationships between behavioral variations and potential hazardous states through scenario-guided executions of the state model over the variations. The paper uses a product line of safety-critical medical devices to demonstrate and evaluate the technique and results. },
	Doi = {https://doi.org/10.1016/j.jss.2007.01.047},
	ISSN = {0164-1212},
	Keywords = {Model-based development,Product lines,Safety-critical systems,State-based modeling},
	Url = {http://www.sciencedirect.com/science/article/pii/S016412120700057X}
}

@Article{Lobato2013523,
	Title = {{Risk management in software product line engineering: A mapping study}},
	Author = {Lobato, L L and Bittar, T J and Neto, P.A.D.M.S. and MacHado, I D C and {De Almeida}, E S and Meira, S.R.D.L.},
	Journal = {International Journal of Software Engineering and Knowledge Engineering},
	Year = {2013},
	Number = {4},
	Pages = {523--558},
	Volume = {23},
	Abstract = {Software Product Line (SPL) Engineering focuses on systematic software reuse, which has benefits such as reductions in time-to-market and effort, and improvements in the quality of products. However, establishing a SPL is not a simple matter, and can affect all aspects of the organization, since the approach is complex and involves major investment and considerable risk. These risks can have a negative impact on the expected ROI for an organization, if SPL is not sufficiently managed. This paper presents a mapping study of Risk Management (RM) in SPL Engineering. We analyzed a set of thirty studies in the field. The results points out the need for risk management practices in SPL, due to the little research on RM practices in SPL and the importance of identifying insight on RM in SPL. Most studies simply mention the importance of RM, however the steps for managing risk are not clearly specified. Our findings suggest that greater attention should be given, through the use of industrial case studies and experiments, to improve SPL productivity and ensure its success. This research is a first attempt within the SPL community to identify, classify, and manage risks, and establish mitigation strategies. {\textcopyright} 2013 World Scientific Publishing Company.},
	Annote = {cited By 2},
	Doi = {10.1142/S0218194013500150},
	Keywords = {Computer software; Mapping; Research; Software de,Industrial case study; Mapping studies; Mitigation,Risk management},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881014807{\&}doi=10.1142{\%}2FS0218194013500150{\&}partnerID=40{\&}md5=c6c53641edd6d02b759a1afa12f08755}
}

@Article{Lochau2017125,
	Title = {{Specification and automated validation of staged reconfiguration processes for dynamic software product lines}},
	Author = {Lochau, M and B{\"{u}}rdek, J and H{\"{o}}lzle, S and Sch{\"{u}}rr, A},
	Journal = {Software and Systems Modeling},
	Year = {2017},
	Number = {1},
	Pages = {125--152},
	Volume = {16},
	Abstract = {Dynamic software product lines (DSPLs) propose elaborated design and implementation principles for engineering highly configurable runtime-adaptive systems in a sustainable and feature-oriented way. For this, DSPLs add to classical software product lines (SPL) the notions of (1) staged (pre-)configurations with dedicated binding times for each individual feature, and (2) continuous runtime reconfigurations of dynamic features throughout the entire product life cycle. Especially in the context of safety- and mission-critical systems, the design of reliable DSPLs requires capabilities for accurately specifying and validating arbitrary complex constraints among configuration parameters and/or respective reconfiguration options. Compared to classical SPL domain analysis which is usually based on Boolean constraint solving, DSPL validation, therefore, further requires capabilities for checking temporal properties of reconfiguration processes. In this article, we present a comprehensive approach for modeling and automatically verifying essential validity properties of staged reconfiguration processes with complex binding time constraints during DSPL domain engineering. The novel modeling concepts introduced are motivated by (re-)configuration constraints apparent in a real-world industrial case study from the automation engineering domain, which are not properly expressible and analyzable using state-of-the-art SPL domain modeling approaches. We present a prototypical tool implementation based on the model checker SPIN and present evaluation results obtained from our industrial case study, demonstrating the applicability of the approach. {\textcopyright} 2015, Springer-Verlag Berlin Heidelberg.},
	Annote = {cited By 0},
	Doi = {10.1007/s10270-015-0470-4},
	Keywords = {Computer software; Computer software reusability;,Configuration constraints; Configuration paramete,Life cycle},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929120403{\&}doi=10.1007{\%}2Fs10270-015-0470-4{\&}partnerID=40{\&}md5=deb247d3adad24816cc4021911e09fe4}
}

@Article{Lochau201463,
	Title = {{Delta-oriented model-based integration testing of large-scale systems}},
	Author = {Lochau, Malte and Lity, Sascha and Lachmann, Remo and Schaefer, Ina and Goltz, Ursula},
	Journal = {Journal of Systems and Software},
	Year = {2014},
	Pages = {63--84},
	Volume = {91},
	Abstract = {Abstract Software architecture specifications are of growing importance for coping with the complexity of large-scale systems. They provide an abstract view on the high-level structural system entities together with their explicit dependencies and build the basis for ensuring behavioral conformance of component implementations and interactions, e.g., using model-based integration testing. The increasing inherent diversity of such large-scale variant-rich systems further complicates quality assurance. In this article, we present a combination of architecture-driven model-based testing principles and regression-inspired testing strategies for efficient, yet comprehensive variability-aware conformance testing of variant-rich systems. We propose an integrated delta-oriented architectural test modeling and testing approach for component as well as integration testing that allows the generation and reuse of test artifacts among different system variants. Furthermore, an automated derivation of retesting obligations based on accurate delta-oriented architectural change impact analysis is provided. Based on a formal conceptual framework that guarantees stable test coverage for every system variant, we present a sample implementation of our approach and an evaluation of the validity and efficiency by means of a case study from the automotive domain. },
	Doi = {https://doi.org/10.1016/j.jss.2013.11.1096},
	ISSN = {0164-1212},
	Keywords = {Large-scale systems,Model-based testing,Regression testing,Variable software architectures},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121213002781}
}

@Article{Lochau2016245,
	Title = {{Incremental model checking of delta-oriented software product lines}},
	Author = {Lochau, Malte and Mennicke, Stephan and Baller, Hauke and Ribbeck, Lars},
	Journal = {Journal of Logical and Algebraic Methods in Programming},
	Year = {2016},
	Number = {1, Part 2},
	Pages = {245--267},
	Volume = {85},
	Abstract = {Abstract We propose DeltaCCS, a delta-oriented extension to Milner's process calculus {\{}CCS{\}} to formalize behavioral variability in software product line specifications in a modular way. In DeltaCCS, predefined change directives are applied to core process semantics by overriding the {\{}CCS{\}} term rewriting rule in a determined way. On this basis, behavioral properties expressed in the Modal $\mu$-Calculus are verifiable for entire product-line specifications both product-by-product as well as in a family-based manner as usual. To overcome potential scalability limitations of those existing strategies, we propose a novel approach for incremental model checking of product lines. Therefore, variability-aware congruence notions and a respective normal form for DeltaCCS specifications allow for a rigorous local reasoning on the preservation of behavioral properties after varying {\{}CCS{\}} specifications. We present a prototypical DeltaCCS model checker implementation based on Maude and provide evaluation results obtained from various experiments concerning efficiency trade-offs compared to existing approaches. },
	Annote = {Formal Methods for Software Product Line Engineering},
	Doi = {https://doi.org/10.1016/j.jlamp.2015.09.004},
	ISSN = {2352-2208},
	Keywords = {Model checking,Operational semantics,Variability modeling},
	Url = {http://www.sciencedirect.com/science/article/pii/S2352220815000863}
}

@Article{Lochau2012567,
	Title = {{Model-based pairwise testing for feature interaction coverage in software product line engineering}},
	Author = {Lochau, M and Oster, S and Goltz, U and Sch{\"{u}}rr, A},
	Journal = {Software Quality Journal},
	Year = {2012},
	Number = {3-4},
	Pages = {567--604},
	Volume = {20},
	Abstract = {Testing software product lines (SPLs) is very challenging due to a high degree of variability leading to an enormous number of possible products. The vast majority of today's testing approaches for SPLs validate products individually using different kinds of reuse techniques for testing. Because of their reusability and adaptability capabilities, model-based approaches are suitable to describe variability and are therefore frequently used for implementation and testing purposes of SPLs. Due to the enormous number of possible products, individual product testing becomes more and more infeasible. Pairwise testing offers one possibility to test a subset of all possible products. However, according to the best of our knowledge, there is no contribution discussing and rating this approach in the SPL context. In this contribution, we provide a mapping between feature models describing the common and variable parts of an SPL and a reusable test model in the form of statecharts. Thereby, we interrelate feature model-based coverage criteria and test model-based coverage criteria such as control and data flow coverage and are therefore able to discuss the potentials and limitations of pairwise testing. We pay particular attention to test requirements for feature interactions constituting a major challenge in SPL engineering. We give a concise definition of feature dependencies and feature interactions from a testing point of view, and we discuss adequacy criteria for SPL coverage under pairwise feature interaction testing and give a generalization to the T-wise case. The concept and implementation of our approach are evaluated by means of a case study from the automotive domain. {\textcopyright} 2011 Springer Science+Business Media, LLC.},
	Annote = {cited By 18},
	Doi = {10.1007/s11219-011-9165-4},
	Keywords = {Combinatorial testing; Feature interactions; Mode,Computer software; Reusability,Software design},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865660809{\&}doi=10.1007{\%}2Fs11219-011-9165-4{\&}partnerID=40{\&}md5=ba9f3f3ec3827bc0c43273d7364916b1}
}

@Article{Lochau201267,
	Title = {{Incremental model-based testing of delta-oriented software product lines}},
	Author = {Lochau, M and Schaefer, I and Kamischke, J and Lity, S},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2012},
	Pages = {67--82},
	Volume = {7305 LNCS},
	Abstract = {Software product line (SPL) engineering provides a promising approach for developing variant-rich software systems. But, testing of every product variant in isolation to ensure its correctness is in general not feasible due to the large number of product variants. Hence, a systematic approach that applies SPL reuse principles also to testing of SPLs in a safe and efficient way is essential. To address this issue, we propose a novel, model-based SPL testing framework that is based on a delta-oriented SPL test model and regression-based test artifact derivations. Test artifacts are incrementally constructed for every product variant by explicitly considering commonality and variability between two consecutive products under test. The resulting SPL testing process is proven to guarantee stable test coverage for every product variant and allows the derivation of redundancy-reduced, yet reliable retesting obligations. We compare our approach with an alternative SPL testing strategy by means of a case study from the automotive domain. {\textcopyright} 2012 Springer-Verlag.},
	Annote = {cited By 37},
	Doi = {10.1007/978-3-642-30473-6_7},
	Keywords = {Automotive domains; Commonality and variability; M,Network architecture; Software design; Software t,Testing},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862204405{\&}doi=10.1007{\%}2F978-3-642-30473-6{\_}7{\&}partnerID=40{\&}md5=e736952c241a30cc6f92d8e3b797827c}
}

@Article{Lohmann2006227,
	Title = {{Lean and efficient system software product lines: Where aspects beat objects}},
	Author = {Lohmann, D and Spinczyk, O and Schr{\"{o}}der-Preikschat, W},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2006},
	Pages = {227--255},
	Volume = {4242 LNCS},
	Abstract = {Software development in the domain of embedded and deeply embedded systems is dominated by cost pressure and extremely limited hardware resources. As a result, modern concepts for separation of concerns and software reuse are widely ignored, as developers worry about the thereby induced memory and performance overhead. Especially object-oriented programming (OOP) is still little in demand. For the development of highly configurable fine-grained system software product lines, however, separation of concerns (SoC) is a crucial property. As the overhead of object-orientation is not acceptable in this domain, we propose aspect-oriented programming (AOP) as an alternative. Compared to OOP, AOP makes it possible to reach similar or even better separation of concerns with significantly smaller memory footprints. In a case study for an embedded system product line the memory costs for SoC could be reduced from 148-236{\%} to 2-10{\%} by using AOP instead of OOP. {\textcopyright} Springer-Verlag Berlin Heidelberg 2006.},
	Annote = {cited By 8},
	Keywords = {Computer software,Computer software reusability; Embedded systems; O,Fine-grained system; Software product lines},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-38549155482{\&}partnerID=40{\&}md5=f4cc2c748e553e56181f5cfcf4e89349}
}

@Article{Lopez-Herrejon2010217,
	Title = {{Detecting inconsistencies in multi-view models with variability}},
	Author = {Lopez-Herrejon, R E and Egyed, A},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2010},
	Pages = {217--232},
	Volume = {6138 LNCS},
	Abstract = {Multi-View Modeling (MVM) is a common modeling practice that advocates the use of multiple, different and yet related models to represent the needs of diverse stakeholders. Of crucial importance in MVM is consistency checking - the description and verification of semantic relationships amongst the views. Variability is the capacity of software artifacts to vary, and its effective management is a core tenet of the research in Software Product Lines (SPL). MVM has proven useful for developing one-of-a-kind systems; however, to reap the potential benefits of MVM in SPL it is vital to provide consistency checking mechanisms that cope with variability. In this paper we describe how to address this need by applying Safe Composition - the guarantee that all programs of a product line are type safe. We evaluate our approach with a case study. {\textcopyright} 2010 Springer-Verlag.},
	Annote = {cited By 17},
	Doi = {10.1007/978-3-642-13595-8_18},
	Keywords = {Computer software,Consistency checking; Effective management; Multi-},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954628413{\&}doi=10.1007{\%}2F978-3-642-13595-8{\_}18{\&}partnerID=40{\&}md5=7b0566671c141c7b6ac3e8239fb83b26}
}

@Article{LopezHerrejon201533,
	Title = {{A systematic mapping study of search-based software engineering for software product lines}},
	Author = {Lopez-Herrejon, Roberto E and Linsbauer, Lukas and Egyed, Alexander},
	Journal = {Information and Software Technology},
	Year = {2015},
	Pages = {33--51},
	Volume = {61},
	Abstract = {AbstractContext Search-Based Software Engineering (SBSE) is an emerging discipline that focuses on the application of search-based optimization techniques to software engineering problems. Software Product Lines (SPLs) are families of related software systems whose members are distinguished by the set of features each one provides. {\{}SPL{\}} development practices have proven benefits such as improved software reuse, better customization, and faster time to market. A typical {\{}SPL{\}} usually involves a large number of systems and features, a fact that makes them attractive for the application of {\{}SBSE{\}} techniques which are able to tackle problems that involve large search spaces. Objective The main objective of our work is to identify the quantity and the type of research on the application of {\{}SBSE{\}} techniques to {\{}SPL{\}} problems. More concretely, the {\{}SBSE{\}} techniques that have been used and at what stage of the {\{}SPL{\}} life cycle, the type of case studies employed and their empirical analysis, and the fora where the research has been published. Method A systematic mapping study was conducted with five research questions and assessed 77 publications from 2001, when the term {\{}SBSE{\}} was coined, until 2014. Results The most common application of {\{}SBSE{\}} techniques found was testing followed by product configuration, with genetic algorithms and multi-objective evolutionary algorithms being the two most commonly used techniques. Our study identified the need to improve the robustness of the empirical evaluation of existing research, a lack of extensive and robust tool support, and multiple avenues worthy of further investigation. Conclusions Our study attested the great synergy existing between both fields, corroborated the increasing and ongoing interest in research on the subject, and revealed challenging open research questions. },
	Doi = {https://doi.org/10.1016/j.infsof.2015.01.008},
	ISSN = {0950-5849},
	Keywords = {Evolutionary algorithm,Metaheuristics,Search based software engineering,Software product line,Systematic mapping study},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584915000166}
}

@Article{LopezHerrejon2015353,
	Title = {{An assessment of search-based techniques for reverse engineering feature models}},
	Author = {Lopez-Herrejon, Roberto E and Linsbauer, Lukas and Galindo, Jos{\'{e}} A and Parejo, Jos{\'{e}} A and Benavides, David and Segura, Sergio and Egyed, Alexander},
	Journal = {Journal of Systems and Software},
	Year = {2015},
	Pages = {353--369},
	Volume = {103},
	Abstract = {Abstract Successful software evolves from a single system by adding and changing functionality to keep up with users' demands and to cater to their similar and different requirements. Nowadays it is a common practice to offer a system in many variants such as community, professional, or academic editions. Each variant provides different functionality described in terms of features. Software Product Line Engineering (SPLE) is an effective software development paradigm for this scenario. At the core of {\{}SPLE{\}} is variability modelling whose goal is to represent the combinations of features that distinguish the system variants using feature models, the de facto standard for such task. As {\{}SPLE{\}} practices are becoming more pervasive, reverse engineering feature models from the feature descriptions of each individual variant has become an active research subject. In this paper we evaluated, for this reverse engineering task, three standard search based techniques (evolutionary algorithms, hill climbing, and random search) with two objective functions on 74 SPLs. We compared their performance using precision and recall, and found a clear trade-off between these two metrics which we further reified into a third objective function based on F$\beta$, an information retrieval measure, that showed a clear performance improvement. We believe that this work sheds light on the great potential of search-based techniques for {\{}SPLE{\}} tasks. },
	Doi = {https://doi.org/10.1016/j.jss.2014.10.037},
	ISSN = {0164-1212},
	Keywords = {Feature model,Reverse engineering,Search Based Software Engineering},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121214002349}
}

@Article{Lorenz2011167,
	Title = {{Code reuse with language oriented programming}},
	Author = {Lorenz, D H and Rosenan, B},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2011},
	Pages = {167--182},
	Volume = {6727 LNCS},
	Abstract = {There is a gap between our ability to reuse high-level concepts in software design and our ability to reuse the code implementing them. Language Oriented Programming (LOP) is a software development paradigm that aims to close this gap, through extensive use of Domain Specific Languages (DSLs). With LOP, the high-level reusable concepts become reusable DSL constructs, and their translation into code level concepts is done in the DSL implementation. Particular products are implemented using DSL code, thus reusing only high-level concepts. In this paper we provide a comparison between two implementation approaches for LOP: (a),using external DSLs with a projectional language workbench (MPS); and (b),using internal DSLs with an LOP language (Cedalion). To demonstrate how reuse is achieved in each approach, we present a small case study, where LOP is used to build a Software Product Line (SPL) of calculator software. {\textcopyright} 2011 Springer-Verlag.},
	Annote = {cited By 0},
	Doi = {10.1007/978-3-642-21347-2_13},
	Keywords = {Code reuse; Domain specific languages; Implementat,Computer software reusability,DSL; High level languages; Software design},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959648051{\&}doi=10.1007{\%}2F978-3-642-21347-2{\_}13{\&}partnerID=40{\&}md5=2211ab4c1299a3d7ba9d55095cccbe74}
}

@InProceedings{Losavio2013,
	Title = {{Graph modelling of a refactoring process for Product Line Architecture design}},
	Author = {Losavio, Francisca and Ordaz, Oscar and Levy, Nicole and Baiotto, Anthony},
	Booktitle = {2013 XXXIX Latin American Computing Conference (CLEI)},
	Year = {2013},
	Month = {oct},
	Pages = {1--12},
	Publisher = {IEEE},
	Doi = {10.1109/CLEI.2013.6670632},
	File = {:Users/mac/Downloads/bulk-download (4)/Graph modelling of a refactoring process for Product Line Architecture design.pdf:pdf},
	ISBN = {978-1-4799-1340-4},
	Url = {http://ieeexplore.ieee.org/document/6670632/}
}

@InProceedings{Lu2016,
	Title = {{Nonconformity Resolving Recommendations for Product Line Configuration}},
	Author = {Lu, Hong and Yue, Tao and Ali, Shaukat and Zhang, Li},
	Booktitle = {2016 IEEE International Conference on Software Testing, Verification and Validation (ICST)},
	Year = {2016},
	Month = {apr},
	Pages = {57--68},
	Publisher = {IEEE},
	Doi = {10.1109/ICST.2016.17},
	ISBN = {978-1-5090-1827-7},
	Url = {http://ieeexplore.ieee.org/document/7515459/}
}

@Article{Lucena2013890,
	Title = {{Contributions to the emergence and consolidation of Agent-oriented Software Engineering}},
	Author = {Lucena, Carlos and Nunes, Ingrid},
	Journal = {Journal of Systems and Software},
	Year = {2013},
	Number = {4},
	Pages = {890--904},
	Volume = {86},
	Abstract = {Many of the issues addressed with multi-agent approaches, such as distributed coordination and self-organization, are now becoming part of industrial and business systems. However, Multiagent Systems (MASs) are still not widely adopted in industry owing to the lack of a connection between {\{}MAS{\}} and software engineering. Since 2000, there is an effort to bridge this gap and to produce software engineering techniques for agent-based systems that guide the processes of design, development and maintenance. In Brazil, Agent-oriented Software Engineering (AOSE) was first investigated by the research group in the Software Engineering Laboratory (LES) at PUC-Rio, which after one decade of study in this area has built an {\{}AOSE{\}} community. This paper presents the history of {\{}AOSE{\}} at {\{}LES{\}} by discussing the sub-areas of {\{}MAS{\}} Software Engineering research and development that have been focus of the {\{}LES{\}} research group. We give examples of relevant results and present a subset of the extensive literature the group has produced during the last decade. We also report how we faced the challenges that emerged from our research by organizing and developing a research community at the intersection of software engineering, programming and {\{}MASs{\}} with a concern for scalability of solutions. },
	Annote = {{\{}SI{\}} : Software Engineering in Brazil: Retrospective and Prospective Views},
	Doi = {https://doi.org/10.1016/j.jss.2012.09.016},
	ISSN = {0164-1212},
	Keywords = {Agent-oriented Software Engineering,LES,Multiagent systems,PUC-Rio,SBES 25 years},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121212002567}
}

@Article{Lucredio2008996,
	Title = {{Software reuse: The Brazilian industry scenario}},
	Author = {Lucr{\'{e}}dio, Daniel and {dos Santos Brito}, Kellyton and Alvaro, Alexandre and Garcia, Vinicius Cardoso and de Almeida, Eduardo Santana and {de Mattos Fortes}, Renata Pontin and Meira, Silvio Lemos},
	Journal = {Journal of Systems and Software},
	Year = {2008},
	Number = {6},
	Pages = {996--1013},
	Volume = {81},
	Abstract = {This paper aims at identifying some of the key factors in adopting an organization-wide software reuse program. The factors are derived from practical experience reported by industry professionals, through a survey involving 57 Brazilian small, medium and large software organizations. Some of them produce software with commonality between applications, and have mature processes, while others successfully achieved reuse through isolated, ad hoc efforts. The paper compiles the answers from the survey participants, showing which factors were more associated with reuse success. Based on this relationship, a guide is presented, pointing out which factors should be more strongly considered by small, medium and large organizations attempting to establish a reuse program. },
	Annote = {Agile Product Line Engineering},
	Doi = {https://doi.org/10.1016/j.jss.2007.08.036},
	ISSN = {0164-1212},
	Keywords = {Best practices,Reuse success factors,Software reuse,Survey},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121207002221}
}

@Article{Lumertz20161,
	Title = {{User interfaces metamodel based on graphs}},
	Author = {Lumertz, Paulo Roberto and Ribeiro, Leila and Duarte, Lucio Mauro},
	Journal = {Journal of Visual Languages {\&} Computing},
	Year = {2016},
	Pages = {1--34},
	Volume = {32},
	Abstract = {Abstract Information systems are widely used in all business areas. These systems typically integrate a set of functionalities that implement business rules and maintain databases. Users interact with these systems and use these features through user interfaces (UI). Each {\{}UI{\}} is usually composed of menus where the user can select the desired functionality, thus accessing a new {\{}UI{\}} that corresponds to the desired feature. Hence, a system normally contains multiple UIs. However, keeping consistency between these {\{}UIs{\}} of a system from a visual (organisation, component style, etc.) and behavioral perspective is usually difficult. This problem also appears in software production lines, where it would be desirable to have patterns to guide the construction and maintenance of UIs. One possible way of defining such patterns is to use model-driven engineering (MDE). In MDE, models are defined at different levels, where the bottom level is called a metamodel. The metamodel determines the main characteristics of the models of the upper levels, serving as a guideline. Each new level must adhere to the rules defined by the lower levels. This way, if anything changes in a lower level, these changes are propagated to the levels above it. The goal of this work is to define and validate a metamodel that allows the modeling of {\{}UIs{\}} of software systems, thus allowing the definition of patterns of interface and supporting system evolution. To build this metamodel, we use a graph structure. This choice is due to the fact that a {\{}UI{\}} can be easily represented as a graph, where each {\{}UI{\}} component is a vertex and edges represent dependencies between these components. Moreover, graph theory provides support for a great number of operations and transformations that can be useful for UIs. The metamodel was defined based on the investigation of patterns that occur in UIs. We used a sample of information systems containing different types of {\{}UIs{\}} to obtain such patterns. To validate the metamodel, we built the complete {\{}UI{\}} models of one new system and of four existing real systems. This shows not only the expressive power of the metamodel, but also its versatility, since our validation was conducted using different types of systems (a desktop system, a web system, mobile system, and a multiplatform system). Moreover, it also demonstrated that the proposed approach can be used not only to build new models, but also to describe existing ones (by reverse engineering). },
	Doi = {https://doi.org/10.1016/j.jvlc.2015.10.026},
	ISSN = {1045-926X},
	Keywords = {Graph transformation,Graphs,Metamodel,User interface,User interface patterns},
	Url = {http://www.sciencedirect.com/science/article/pii/S1045926X1500083X}
}

@Article{Lung2015301,
	Title = {{On building architecture-centric product line architecture}},
	Author = {Lung, C.-H. and Balasubramaniam, B and Selvarajah, K and Elankeswaran, P and Gopalasundaram, U},
	Journal = {Requirements Engineering},
	Year = {2015},
	Number = {3},
	Pages = {301--321},
	Volume = {20},
	Abstract = {Software architects typically spend a great deal of time and effort exploring uncertainties, evaluating alternatives, and balancing the concerns of stakeholders. Selecting the best architecture to meet both the functional and non-functional requirements is a critical but difficult task, especially at the early stage of software development when there may be many uncertainties. For example, how will a technology match the operational or performance expectations in reality? This paper presents an approach to building architecture-centric product line. The main objective of the proposed approach is to support effective requirements validation and architectural prototyping for the application-level software. Architectural prototyping is practically essential to architecture design and evaluation. However, architectural prototyping practiced in the field mostly is not used to explore alternatives. Effective construction and evaluation of multiple architecture alternatives is one of the critically challenging tasks. The product line architecture advocated in this paper consists of multiple software architecture alternatives, from which the architect can select and rapidly generate a working application prototype. The paper presents a case study of developing a framework that is primarily built with robust architecture patterns in distributed and concurrent computing and includes variation mechanisms to support various applications even in different domains. The development process of the framework is an application of software product line engineering with an aim to effectively facilitate upfront requirements analysis for an application and rapid architectural prototyping to explore and evaluate architecture alternatives. {\textcopyright} Springer-Verlag London 2014.},
	Annote = {cited By 2},
	Doi = {10.1007/s00766-014-0201-3},
	Keywords = {Application programs; Computer software; Requireme,Architecture evaluation; Patterns; Requirements v,Software prototyping},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943348957{\&}doi=10.1007{\%}2Fs00766-014-0201-3{\&}partnerID=40{\&}md5=5934c044b3b58dcc93f783735a84538c}
}

@Article{Lung2016311,
	Title = {{Improving software performance and reliability in a distributed and concurrent environment with an architecture-based self-adaptive framework}},
	Author = {Lung, Chung-Horng and Zhang, Xu and Rajeswaran, Pragash},
	Journal = {Journal of Systems and Software},
	Year = {2016},
	Pages = {311--328},
	Volume = {121},
	Abstract = {Abstract More and more, modern software systems in a distributed and parallel environment are becoming highly complex and difficult to manage. A self-adaptive approach that integrates monitoring, analyzing, and actuation functionalities has the potential to accommodate an ever dynamically changing environment. This paper proposes an architecture-level self-adaptive framework with the aim of improving performance and reliability. To meet such a goal, this paper presents a Self-Adaptive Framework for Concurrency Architectures (SAFCA) that consists of multiple well-documented architectural patterns in addition to monitoring and adaptive capabilities. With this framework, a system using an architectural alternative can activate another alternative at runtime to cope with increasing demands or to recover from failure. Five adaptation mechanisms have been developed for concept demonstration and evaluation; four focus on performance improvement and one deals with failover and reliability enhancement. We have performed a number of experiments with this framework. The experimental results demonstrate that the proposed adaptive framework can mitigate the over-provisioning method commonly used in practice. As a result, resource usage becomes more efficient for most normal conditions, while the system is still able to effectively handle bursty or growing demands using an adaptive mechanism. The performance of {\{}SAFCA{\}} is also better than systems using only standalone architectural alternatives without an adaptation scheme. Moreover, the experimental results show that a fast recovery can be realized in the case of failure by conducting an architecture switchover to maintain the desired service. },
	Doi = {https://doi.org/10.1016/j.jss.2016.06.102},
	ISSN = {0164-1212},
	Keywords = {Autonomic computing,Distributed and concurrent architecture,Elastic computing,Patterns,Performance,Reliability,Software architecture},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121216300991}
}

@Article{Lutz2003253,
	Title = {{Analysis of a software product line architecture: an experience report}},
	Author = {Lutz, Robyn R and Gannod, Gerald C},
	Journal = {Journal of Systems and Software},
	Year = {2003},
	Number = {3},
	Pages = {253--267},
	Volume = {66},
	Abstract = {This paper describes experiences with the architectural specification and tool-assisted architectural analysis of a mission-critical, high-performance software product line. The approach used defines a “good? product line architecture in terms of those quality attributes required by the particular product line under development. Architectures are analyzed against several criteria by both manual and tool-supported methods. The approach described in this paper provides a structured analysis of an existing product line architecture using (1) architecture recovery and specification, (2) architecture evaluation, and (3) model checking of behavior to determine the level of robustness and fault tolerance at the architectural level that are required for all systems in the product line. Results of an application to a software product line of spaceborne telescopes are used to explain the approach and describe lessons learned. },
	Annote = {Software architecture -- Engineering quality attributes},
	Doi = {https://doi.org/10.1016/S0164-1212(02)00081-X},
	ISSN = {0164-1212},
	Url = {http://www.sciencedirect.com/science/article/pii/S016412120200081X}
}

@Article{Lytra201580,
	Title = {{Harmonizing architectural decisions with component view models using reusable architectural knowledge transformations and constraints}},
	Author = {Lytra, Ioanna and Tran, Huy and Zdun, Uwe},
	Journal = {Future Generation Computer Systems},
	Year = {2015},
	Pages = {80--96},
	Volume = {47},
	Abstract = {Abstract Architectural design decisions (ADDs) have been used in recent years for capturing design rationale and documenting architectural knowledge (AK). However, various architectural design views still provide the most common means for describing and communicating architectural design. The evolution of software systems requires that both {\{}ADDs{\}} and architectural design views are documented and maintained, which is a tedious and time-consuming task in the long run. Also, in lack of a systematic and automated support for bridging between {\{}ADDs{\}} and architectural design views, decisions and designs tend to become inconsistent over time. In our proposal, we introduce a reusable {\{}AK{\}} transformation language for supporting the automated transformation of reusable {\{}AK{\}} knowledge to component-and-connector models, the architectural design view used most commonly today. In addition, reusable consistency checking rules verify the consistency between decisions and designs. We evaluate our approach in an industrial case study and show that it offers high reusability, provides automation, and can, in principle, deal with large numbers of recurring decisions. },
	Annote = {Special Section: Advanced Architectures for the Future Generation of Software-Intensive Systems},
	Doi = {https://doi.org/10.1016/j.future.2014.11.010},
	ISSN = {0167-739X},
	Keywords = {AK transformation language,Architectural decisions,Architectural design,Architectural knowledge,Consistency checking},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167739X14002441}
}

@Article{MendezAcuna2016206,
	Title = {{Leveraging Software Product Lines Engineering in the development of external DSLs: A systematic literature review}},
	Author = {M{\'{e}}ndez-Acu{\~{n}}a, David and Galindo, Jos{\'{e}} A and Degueule, Thomas and Combemale, Beno{\^{i}}t and Baudry, Beno{\^{i}}t},
	Journal = {Computer Languages, Systems {\&} Structures},
	Year = {2016},
	Pages = {206--235},
	Volume = {46},
	Abstract = {Abstract The use of domain-specific languages (DSLs) has become a successful technique in the development of complex systems. Consequently, nowadays we can find a large variety of {\{}DSLs{\}} for diverse purposes. However, not all these {\{}DSLs{\}} are completely different; many of them share certain commonalities coming from similar modeling patterns – such as state machines or petri nets – used for several purposes. In this scenario, the challenge for language designers is to take advantage of the commonalities existing among similar {\{}DSLs{\}} by reusing, as much as possible, formerly defined language constructs. The objective is to leverage previous engineering efforts to minimize implementation from scratch. To this end, recent research in software language engineering proposes the use of product line engineering, thus introducing the notion of language product lines. Nowadays, there are several approaches that result useful in the construction of language product lines. In this article, we report on an effort for organizing the literature on language product line engineering. More precisely, we propose a definition for the life-cycle of language product lines, and we use it to analyze the capabilities of current approaches. In addition, we provide a mapping between each approach and the technological space it supports. },
	Doi = {https://doi.org/10.1016/j.cl.2016.09.004},
	ISSN = {1477-8424},
	Keywords = {Domain-specific languages,Software Product Lines Engineering,Software language engineering,Variability management},
	Url = {http://www.sciencedirect.com/science/article/pii/S1477842416300768}
}

@Article{Maazoun2016364,
	Title = {{Change impact analysis for software product lines}},
	Author = {Ma{\^{a}}zoun, Jihen and Bouassida, Nadia and Ben-Abdallah, Han{\^{e}}ne},
	Journal = {Journal of King Saud University - Computer and Information Sciences},
	Year = {2016},
	Number = {4},
	Pages = {364--380},
	Volume = {28},
	Abstract = {Abstract A software product line (SPL) represents a family of products in a given application domain. Each {\{}SPL{\}} is constructed to provide for the derivation of new products by covering a wide range of features in its domain. Nevertheless, over time, some domain features may become obsolete with the apparition of new features while others may become refined. Accordingly, the {\{}SPL{\}} must be maintained to account for the domain evolution. Such evolution requires a means for managing the impact of changes on the {\{}SPL{\}} models, including the feature model and design. This paper presents an automated method that analyzes feature model evolution, traces their impact on the {\{}SPL{\}} design, and offers a set of recommendations to ensure the consistency of both models. The proposed method defines a set of new metrics adapted to {\{}SPL{\}} evolution to identify the effort needed to maintain the {\{}SPL{\}} models consistently and with a quality as good as the original models. The method and its tool are illustrated through an example of an {\{}SPL{\}} in the Text Editing domain. In addition, they are experimentally evaluated in terms of both the quality of the maintained {\{}SPL{\}} models and the precision of the impact change management. },
	Doi = {https://doi.org/10.1016/j.jksuci.2016.01.005},
	ISSN = {1319-1578},
	Keywords = {Change impact management,Feature model,Model evolution,Software product line},
	Url = {http://www.sciencedirect.com/science/article/pii/S1319157816300167}
}

@Article{SPE:SPE645,
	Title = {{Managing infinite variability in mobile terminal software}},
	Author = {Maccari, Alessandro and Heie, Anders},
	Journal = {Software: Practice and Experience},
	Year = {2005},
	Number = {6},
	Pages = {513--537},
	Volume = {35},
	Abstract = {Mobile terminals have evolved from basic portable telephones to complex and diverse devices that encompass dozens of other features, ranging from tri-dimensional games to office suites with data transmission capabilities. Variability is value: mobile terminal manufacturers must succeed in fulfilling the requirements of hundreds of mobile telecom operators worldwide, and at the same time increase the value of their brand by adopting a common user interface style while offering the features that the target end-user category desires. This makes for practically infinite variability and creates a business problem. The complexity of the variability problem increases due to issues such as the ability to ‘plug and play' and ‘feature descension' (the down-scaling of high-end features and their introduction into lower-end models). The main lesson we have learned from our experience in this field is that the application of relatively simple architectural patterns usually eases up management of the complexity at the architectural level. However, tackling the variability problem at the technical level is ineffective unless the organization is able to ensure the application of the solutions. We analyze the main challenges that lie behind the variability problem in mobile terminals, at both technical and organizational level, and illustrate some of the solutions we have implemented together with our product developers and system architects. Our experience calls for more applied research in the area of variability management, as well as for a number of enhancements to academic curricula. Copyright {\textcopyright} 2005 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spe.645},
	ISSN = {1097-024X},
	Keywords = {mobile terminal software,product family architecture,variability management},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/spe.645}
}

@Article{SYS:SYS20197,
	Title = {{Adaptable platform-based engineering: Key enablers and outlook for the future}},
	Author = {Madni, Azad M},
	Journal = {Systems Engineering},
	Year = {2012},
	Number = {1},
	Pages = {95--107},
	Volume = {15},
	Abstract = {Platform-Based Engineering (PBE) is a cost-effective, risk-mitigated system development approach that employs a common structure from which high-quality derivative products can be developed rapidly. PBE is especially effective in decreasing development cost, risks, and lead times while increasing product quality. Appropriately scoped, platforms simplify and accelerate the development of families of systems for a particular problem domain. They encompass domain-specific components and services that reflect the commonalities of systems in the domain (which can be configured as reusable physical or informational components), and variabilities across the domain (which need to be individually developed to achieve a domain product line), along with interface conventions that ensure that they can plug-and-play with the domain infrastructure and common components. However, PBE has a potential downside. Locking into a platform strategy for the long term can severely limit an organization's ability to evolve a product. It is this recognition that spurred the development of the adaptable PBE paradigm. Adaptable PBE offers the customary benefits of PBE without compromising the long-term evolvability of the system. This paper presents the game changing technologies under the adaptable PBE rubric, and discusses the key concepts and advances needed to make adaptable PBE an enabler of evolvable systems. {\textcopyright} 2011 Wiley Periodicals, Inc. Syst Eng},
	Doi = {10.1002/sys.20197},
	ISSN = {1520-6858},
	Keywords = {Platform-Based Engineering,agility platform,architectural patterns,platform,product line architecture,resilient systems},
	Publisher = {Wiley Subscription Services, Inc., A Wiley Company},
	Url = {http://dx.doi.org/10.1002/sys.20197}
}

@Article{SPE:SPE2320,
	Title = {{Migrating legacy software to the cloud: approach and verification by means of two medical software use cases}},
	Author = {Maenhaut, Pieter-Jan and Moens, Hendrik and Ongenae, Veerle and {De Turck}, Filip},
	Journal = {Software: Practice and Experience},
	Year = {2016},
	Number = {1},
	Pages = {31--54},
	Volume = {46},
	Abstract = {Cloud computing is a technology that enables elastic, on-demand resource provisioning, allowing application developers to build highly scalable systems. Multi-tenancy, the hosting of multiple customers by a single application instance, leads to improved efficiency, improved scalability, and less costs. While these technologies make it possible to create many new applications, legacy applications can also benefit from the added flexibility and cost savings of cloud computing and multi-tenancy. In this article, we describe the steps required to migrate existing applications to a public cloud environment, and the steps required to add multi-tenancy to these applications. We present a generic approach and verify this approach by means of two case studies, a commercial medical communications software package mainly used within hospitals for nurse call systems and a schedule planner for managing medical appointments. Both case studies are subject to stringent security and performance constraints, which need to be taken into account during the migration. In our evaluation, we estimate the required investment costs and compare them to the long-term benefits of the migration. Copyright {\textcopyright} 2015 John Wiley {\&} Sons Ltd.},
	Doi = {10.1002/spe.2320},
	ISSN = {1097-024X},
	Keywords = {cloud computing,medical software,migration,multi-tenancy,software-as-a-service},
	Url = {http://dx.doi.org/10.1002/spe.2320}
}

@Article{Magdalenic20132845,
	Title = {{Autogenerator: Generation and execution of programming code on demand}},
	Author = {Magdaleni{\'{c}}, Ivan and Rado{\v{s}}evi{\'{c}}, Danijel and Orehova{\v{c}}ki, Tihomir},
	Journal = {Expert Systems with Applications},
	Year = {2013},
	Number = {8},
	Pages = {2845--2857},
	Volume = {40},
	Abstract = {While generating program files that can be executed afterwards is widely established in Automatic programming, the generation of programming code and its execution on demand without creating program files is still a challenge. In the approach presented in this paper a generator entitled Autogenerator uses the ability of scripting languages to evaluate programming code from a variable. The main benefits of this approach lie in facilitating the application change during its execution on the one hand and in dependencies update on the other. Autogenerator can be useful in the development of a common Generative programming application for previewing the application before the generation of the final release. With the aim of examining specific facets of the autogeneration process, we also conducted performance tests. Finally, the presented model of Autogenerator is verified through the development of an application for the creation and handling of Universal Business Language documents. },
	Doi = {https://doi.org/10.1016/j.eswa.2012.12.003},
	ISSN = {0957-4174},
	Keywords = {Autogenerator,Dynamic frames,Generation on demand},
	Url = {http://www.sciencedirect.com/science/article/pii/S0957417412012444}
}

@Article{Magdaleno2015452,
	Title = {{Collaboration optimization in software process composition}},
	Author = {Magdaleno, Andr{\'{e}}a Magalh{\~{a}}es and {de Oliveira Barros}, Marcio and Werner, Cl{\'{a}}udia Maria Lima and de Araujo, Renata Mendes and Batista, Carlos Freud Alves},
	Journal = {Journal of Systems and Software},
	Year = {2015},
	Pages = {452--466},
	Volume = {103},
	Abstract = {Abstract Purpose: The purpose of this paper is to describe an optimization approach to maximize collaboration in software process composition. The research question is: how to compose a process for a specific software development project context aiming to maximize collaboration among team members? The optimization approach uses heuristic search algorithms to navigate the solution space and look for acceptable solutions. Design/methodology/approach: The process composition approach was evaluated through an experimental study conducted in the context of a large oil company in Brazil. The objective was to evaluate the feasibility of composing processes for three software development projects. We have also compared genetic algorithm (GA) and hill climbing (HC) algorithms driving the optimization with a simple random search (RS) in order to determine which would be more effective in addressing the problem. In addition, human specialist point-of-view was explored to verify if the composed processes were in accordance with his/her expectations regarding size, complexity, diversity, and reasonable sequence of components. Findings: The main findings indicate that {\{}GA{\}} is more effective (best results regarding the fitness function) than {\{}HC{\}} and {\{}RS{\}} in the search of solutions for collaboration optimization in software process composition for large instances. However, all algorithms are competitive for small instances and even brute force can be a feasible alternative in such a context. These {\{}SBSE{\}} results were complemented by the feedback given by specialist, indicating his satisfaction with the correctness, diversity, adherence to the project context, and support to the project manager during the decision making in process composition. Research limitations: This work was evaluated in the context of a single company and used only three project instances. Due to confidentiality restrictions, the data describing these instances could not be disclosed to be used in other research works. The reduced size of the sample prevents generalization for other types of projects or different contexts. Implications: This research is important for practitioners who are facing challenges to handle diversity in software process definition, since it proposes an approach based on context, reuse and process composition. It also contributes to research on collaboration by presenting a collaboration management solution (COMPOOTIM) that includes both an approach to introduce collaboration in organizations through software processes and a collaboration measurement strategy. From the standpoint of software engineering looking for collaborative solutions in distributed software development, free/open source software, agile, and ecosystems initiatives, the results also indicate how to increase collaboration in software development. Originality/value: This work proposes a systematic strategy to manage collaboration in software development process composition. Moreover, it brings together a mix of computer-oriented and human-oriented studies on the search-based software engineering (SBSE) research area. Finally, this work expands the body of knowledge in {\{}SBSE{\}} to the field of software process which has not been properly explored by former research. },
	Doi = {https://doi.org/10.1016/j.jss.2014.11.036},
	ISSN = {0164-1212},
	Keywords = {Collaboration,SBSE,Software process},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121214002672}
}

@Article{Magdaleno2012351,
	Title = {{Reconciling software development models: A quasi-systematic review}},
	Author = {Magdaleno, Andr{\'{e}}a Magalh{\~{a}}es and Werner, Cl{\'{a}}udia Maria Lima and de Araujo, Renata Mendes},
	Journal = {Journal of Systems and Software},
	Year = {2012},
	Number = {2},
	Pages = {351--369},
	Volume = {85},
	Abstract = {Purpose The purpose of this paper is to characterize reconciliation among the plan-driven, agile, and free/open source software models of software development. Design/methodology/approach An automated quasi-systematic review identified 42 papers, which were then analyzed. Findings The main findings are: there exist distinct – organization, group and process – levels of reconciliation; few studies deal with reconciliation among the three models of development; a significant amount of work addresses reconciliation between plan-driven and agile development; several large organizations (such as Microsoft, Motorola, and Philips) are interested in trying to combine these models; and reconciliation among software development models is still an open issue, since it is an emerging area and research on most proposals is at an early stage. Research limitations Automated searches may not capture relevant papers in publications that are not indexed. Other data sources not amenable to execution of the protocol were not used. Data extraction was performed by only one researcher, which may increase the risk of threats to internal validity. Implications This characterization is important for practitioners wanting to be current with the state of research. This review will also assist the scientific community working with software development processes to build a common understanding of the challenges that must be faced, and to identify areas where research is lacking. Finally, the results will be useful to software industry that is calling for solutions in this area. Originality/value There is no other systematic review on this subject, and reconciliation among software development models is an emerging area. This study helps to identify and consolidate the work done so far and to guide future research. The conclusions are an important step towards expanding the body of knowledge in the field. },
	Annote = {Special issue with selected papers from the 23rd Brazilian Symposium on Software Engineering},
	Doi = {https://doi.org/10.1016/j.jss.2011.08.028},
	ISSN = {0164-1212},
	Keywords = {Agile,Free/open source software,Plan-driven,Reconciliation among development models,Software process,Systematic review},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121211002287}
}

@Article{MahdaviHezavehi2013320,
	Title = {{Variability in quality attributes of service-based software systems: A systematic literature review}},
	Author = {Mahdavi-Hezavehi, Sara and Galster, Matthias and Avgeriou, Paris},
	Journal = {Information and Software Technology},
	Year = {2013},
	Number = {2},
	Pages = {320--343},
	Volume = {55},
	Abstract = {Context Variability is the ability of a software artifact (e.g., a system, component) to be adapted for a specific context, in a preplanned manner. Variability not only affects functionality, but also quality attributes (e.g., security, performance). Service-based software systems consider variability in functionality implicitly by dynamic service composition. However, variability in quality attributes of service-based systems seems insufficiently addressed in current design practices. Objective We aim at (a) assessing methods for handling variability in quality attributes of service-based systems, (b) collecting evidence about current research that suggests implications for practice, and (c) identifying open problems and areas for improvement. Method A systematic literature review with an automated search was conducted. The review included studies published between the year 2000 and 2011. We identified 46 relevant studies. Results Current methods focus on a few quality attributes, in particular performance and availability. Also, most methods use formal techniques. Furthermore, current studies do not provide enough evidence for practitioners to adopt proposed approaches. So far, variability in quality attributes has mainly been studied in laboratory settings rather than in industrial environments. Conclusions The product line domain as the domain that traditionally deals with variability has only little impact on handling variability in quality attributes. The lack of tool support, the lack of practical research and evidence for the applicability of approaches to handle variability are obstacles for practitioners to adopt methods. Therefore, we suggest studies in industry (e.g., surveys) to collect data on how practitioners handle variability of quality attributes in service-based systems. For example, results of our study help formulate hypotheses and questions for such surveys. Based on needs in practice, new approaches can be proposed. },
	Annote = {Special Section: Component-Based Software Engineering (CBSE), 2011},
	Doi = {https://doi.org/10.1016/j.infsof.2012.08.010},
	ISSN = {0950-5849},
	Keywords = {Quality attributes,Service-based systems,Systematic literature review,Variability},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584912001772}
}

@Article{SPE:SPE2134,
	Title = {{Validating pragmatic reuse tasks by leveraging existing test suites}},
	Author = {Makady, Soha and Walker, Robert J},
	Journal = {Software: Practice and Experience},
	Year = {2013},
	Number = {9},
	Pages = {1039--1070},
	Volume = {43},
	Abstract = {Traditional industrial practice often involves the ad hoc reuse of source code that was not designed for that reuse. Such pragmatic reuse tasks play an important role in disciplined software development. Pragmatic reuse has been seen as problematic due to a lack of systematic support, and an inability to validate that the reused code continues to operate correctly within the target system. Although recent work has successfully systematized support for pragmatic reuse tasks, the issue of validation remains unaddressed. In this paper, we present a novel approach and tool to semi-automatically reuse and transform relevant portions of the test suite associated with pragmatically reused code, as a means to validate that the relevant constraints from the originating system continue to hold, while minimizing the burden on the developer. We conduct a formal experiment with experienced developers, to compare the application of our approach versus the use of a standard IDE (the ‘manual approach'). We find that, relative to the manual approach, our approach: reduces task completion time; improves instruction coverage by the reused test cases; and improves the correctness of the reused test cases. Copyright {\textcopyright} 2012 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spe.2134},
	ISSN = {1097-024X},
	Keywords = {Skipper,lightweight,pragmatic software reuse,pragmatic-reuse plan,semi-automatic,test suite reuse},
	Url = {http://dx.doi.org/10.1002/spe.2134}
}

@Article{Malhotra201785,
	Title = {{On the application of search-based techniques for software engineering predictive modeling: A systematic review and future directions}},
	Author = {Malhotra, Ruchika and Khanna, Megha and Raje, Rajeev R},
	Journal = {Swarm and Evolutionary Computation},
	Year = {2017},
	Pages = {85--109},
	Volume = {32},
	Abstract = {Abstract Software engineering predictive modeling involves construction of models, with the help of software metrics, for estimating quality attributes. Recently, the use of search-based techniques have gained importance as they help the developers and project-managers in the identification of optimal solutions for developing effective prediction models. In this paper, we perform a systematic review of 78 primary studies from January 1992 to December 2015 which analyze the predictive capability of search-based techniques for ascertaining four predominant software quality attributes, i.e., effort, defect proneness, maintainability and change proneness. The review analyses the effective use and application of search-based techniques by evaluating appropriate specifications of fitness functions, parameter settings, validation methods, accounting for their stochastic natures and the evaluation of developmental models with the use of well-known statistical tests. Furthermore, we compare the effectiveness of different models, developed using the various search-based techniques amongst themselves, and also with the prevalent machine learning techniques used in literature. Although there are very few studies which use search-based techniques for predicting maintainability and change proneness, we found that the results of the application of search-based techniques for effort estimation and defect prediction are encouraging. Hence, this comprehensive study and the associated results will provide guidelines to practitioners and researchers and will enable them to make proper choices for applying the search-based techniques to their specific situations. },
	Doi = {https://doi.org/10.1016/j.swevo.2016.10.002},
	ISSN = {2210-6502},
	Keywords = {Change prediction,Defect prediction,Effort estimation,Maintainability prediction,Search-based techniques,Software quality},
	Url = {http://www.sciencedirect.com/science/article/pii/S2210650216303418}
}

@Article{SEC:SEC1151,
	Title = {{Embedded Systems Security: A Survey of EU Research Efforts}},
	Author = {Manifavas, Charalampos and Fysarakis, Konstantinos and Papanikolaou, Alexandros and Papaefstathiou, Ioannis},
	Journal = {Security and Communication Networks},
	Year = {2015},
	Number = {11},
	Pages = {2016--2036},
	Volume = {8},
	Abstract = {Embedded systems security is a recurring theme in current research efforts, brought in the limelight by the wide adoption of ubiquitous devices. Significant funding has been allocated to various European projects on this subject area, in order to investigate and overcome the various security challenges. This paper provides an overview of recent EU research efforts pertaining to embedded systems security, where several prominent security issues and the respective proposed approaches are presented. Surveying relatively recent EU research projects, the authors identify 20 such projects that focus on embedded systems security aspects. The investigated technologies are categorised using a layered approach, to facilitate the presentation of the results; the categories comprise the node, network, and middleware and overlay layers, as well as architectures, frameworks and formal validation of the security of embedded systems. From this survey, certain patterns emerge regarding the issues investigated and the technologies researchers focus on, in order to address the said issues. Finally, the existing open issues are summarised, and directions for future research are given. Copyright {\textcopyright} 2014 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/sec.1151},
	ISSN = {1939-0122},
	Keywords = {embedded systems,resource-constrained devices,security,sensor networks},
	Url = {http://dx.doi.org/10.1002/sec.1151}
}

@Article{Manikas201684,
	Title = {{Revisiting software ecosystems Research: A longitudinal literature study}},
	Author = {Manikas, Konstantinos},
	Journal = {Journal of Systems and Software},
	Year = {2016},
	Pages = {84--103},
	Volume = {117},
	Abstract = {Abstract ‘Software ecosystems' is argued to first appear as a concept more than 10 years ago and software ecosystem research started to take off in 2010. We conduct a systematic literature study, based on the most extensive literature review in the field up to date, with two primarily aims: (a) to provide an updated overview of the field and (b) to document evolution in the field. In total, we analyze 231 papers from 2007 until 2014 and provide an overview of the research in software ecosystems. Our analysis reveals a field that is rapidly growing, both in volume and empirical focus, while becoming more mature. We identify signs of field maturity from the increase in: (i) the number of journal articles, (ii) the empirical models within the last two years, and (iii) the number of ecosystems studied. However, we note that the field is far from mature and identify a set of challenges that are preventing the field from evolving. We propose means for future research and the community to address them. Finally, our analysis shapes the view of the field having evolved outside the existing definitions of software ecosystems and thus propose the update of the definition of software ecosystems. },
	Doi = {https://doi.org/10.1016/j.jss.2016.02.003},
	ISSN = {0164-1212},
	Keywords = {Longitudinal literature study,Software ecosystem maturity,Software ecosystems},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121216000406}
}

@Article{SPE:SPE1051,
	Title = {{Towards evolvable software architectures based on systems theoretic stability}},
	Author = {Mannaert, Herwig and Verelst, Jan and Ven, Kris},
	Journal = {Software: Practice and Experience},
	Year = {2012},
	Number = {1},
	Pages = {89--116},
	Volume = {42},
	Abstract = {In today's increasingly volatile environments, evolvability is quickly becoming the most desirable characteristic of information systems. Current information systems still struggle to provide these high levels of evolvability. Based on the concept of stability from systems theory, we require that information systems should be stable with respect to a set of anticipated changes in order to exhibit high evolvability. This requires that information systems should be free from so-called combinatorial effects. Combinatorial effects occur when the impact of a change is dependent on the size of the information system. To eliminate these combinatorial effects, we propose four theorems that are constraints on the modular structure of software architectures. The theorems are prescriptive and ensure that stable information systems are built, thereby guaranteeing high evolvability. We further present five higher level modular structures called elements. These elements provide the core functionality of information systems and comply fully with the stringent constraints implied by the four theorems. The internal structure of these elements is described by design patterns which are eligible for automatic code generation. These design patterns offer a constructive proof that it is possible to build information systems in practice by applying this set of theorems. Copyright {\textcopyright} 2011 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spe.1051},
	ISSN = {1097-024X},
	Keywords = {evolvability,modularity,patterns,software architecture,systems theory},
	Publisher = {John Wiley {\&} Sons, Ltd},
	Url = {http://dx.doi.org/10.1002/spe.1051}
}

@Article{Mannaert20111210,
	Title = {{The transformation of requirements into software primitives: Studying evolvability based on systems theoretic stability}},
	Author = {Mannaert, Herwig and Verelst, Jan and Ven, Kris},
	Journal = {Science of Computer Programming},
	Year = {2011},
	Number = {12},
	Pages = {1210--1222},
	Volume = {76},
	Abstract = {Evolvability is widely considered to be a crucial characteristic of software architectures, particularly in the area of information systems. Although many approaches have been proposed for improving evolvability, most indications are that it remains challenging to deliver the required levels of evolvability. In this paper, we present a theoretical approach to how the concept of systems theoretic stability can be applied to the evolvability of software architectures of information systems. We define and formalize the transformation of a set of basic functional requirements into a set of instantiations of software constructs. We define this transformation using both a static and a dynamic perspective. In the latter perspective, we formulate the postulate that information systems should be stable against new requirements. Based on this postulate, we derive a number of design theorems for software implementation. Using this transformation we use theoretical arguments to derive that these theorems contribute to achieving stability. },
	Annote = {Special Issue on Software Evolution, Adaptability and Variability},
	Doi = {https://doi.org/10.1016/j.scico.2010.11.009},
	ISSN = {0167-6423},
	Keywords = {Normalized systems,Stability,Systems theory},
	Url = {http://www.sciencedirect.com/science/article/pii/S016764231000208X}
}

@Article{SYS:SYS20086,
	Title = {{Using parameters and discriminants for product line requirements}},
	Author = {Mannion, Mike and Kaindl, Hermann},
	Journal = {Systems Engineering},
	Year = {2008},
	Number = {1},
	Pages = {61--80},
	Volume = {11},
	Abstract = {Reuse and Requirements Engineering are very important for efficient and successful product development. However, there are many open issues for performing them well in practice, especially reuse of requirements. This paper addresses several of these issues by making use of parameters and discriminants for product line requirements. Discriminants are a special kind of requirement that differentiate one product from another. They represent qualitative variability in a product line model, while parameters represent quantitative variability. In this paper these techniques are combined in the form of parameterized discriminants. Using the techniques and a metamodel for the representation of a product line model, a stakeholder-viewpoint oriented process is defined for capturing product line requirements and constructing a product line model. From such a generic model and with well-defined semantics, selecting single product application requirements can be done systematically, guaranteeing the property that the resulting single product satisfies the constraints of the product line model. Even for unsystematic free selection as often used in practice, it can be formally verified whether this property holds. The approach is illustrated using a hypothetical mobile phone example and draws upon case study work with real-world requirements. {\textcopyright} 2007 Wiley Periodicals, Inc. Syst Eng},
	Doi = {10.1002/sys.20086},
	ISSN = {1520-6858},
	Keywords = {metamodel,product line,requirements,requirements reuse},
	Publisher = {Wiley Subscription Services, Inc., A Wiley Company},
	Url = {http://dx.doi.org/10.1002/sys.20086}
}

@Article{Marchione2010357,
	Title = {{E-contracting with price configuration for Web services and QoS}},
	Author = {Marchione, F G and Fantinato, M and {De Toledo}, M B F and {De Souza Gimenes}, I M},
	Journal = {International Journal of Web and Grid Services},
	Year = {2010},
	Number = {4},
	Pages = {357--384},
	Volume = {6},
	Abstract = {The large amount of information in electronic contracts hampers their establishment due to high complexity. An approach inspired in Software Product Line (PL) and based on feature modelling was proposed to make this process more systematic through information reuse and structuring. By assessing the feature-based approach in relation to a proposed set of requirements, it was showed that the approach does not allow the price of services and of Quality of Services (QoS) attributes to be considered in the negotiation and included in the electronic contract. Thus, this paper also presents an extension of such approach in which prices and price types associated to Web services and QoS levels are applied. An extended toolkit prototype is also presented as well as an experiment example of the proposed approach. Copyright {\textcopyright} 2010 Inderscience Enterprises Ltd.},
	Annote = {cited By 3},
	Doi = {10.1504/IJWGS.2010.036403},
	Keywords = {Business Process; E-contracting; Electronic servic,Commerce,Computer software reusability; Costs; Web service},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049489331{\&}doi=10.1504{\%}2FIJWGS.2010.036403{\&}partnerID=40{\&}md5=dc25e161202646c1cab1d910e99b89f6}
}

@Article{Marew200733,
	Title = {{Systematic functional decomposition in a product line using aspect-oriented software development: A case study}},
	Author = {Marew, T and Kim, J and Bae, D H},
	Journal = {International Journal of Software Engineering and Knowledge Engineering},
	Year = {2007},
	Number = {1},
	Pages = {33--55},
	Volume = {17},
	Abstract = {Systematic configuration management is important for successful software product lines. We can use aspect-oriented software development to decompose software product lines based on features that can ease configuration management. In this paper, we present a military maintenance product line that employs such strategy. In particular, we applied a specific approach, feature based modeling (FBM), in the construction of the system. We have extended FBM to address properties specific to product line. We will discuss the advantages of FBM when applied to product lines. Such gains include the functional decomposition of the system along user requirements (features) as aspects. Moreover, those features exhibit unidirectional dependency (i.e. among any two features, at most one depend on another) that enables developers to analyze the effect of any modification they may make on any feature. In addition, any variations can be captured as aspects which can also be incorporated easily into the core asset if such variation is deemed to be important enough to be included in the product line for further evolution. {\textcopyright} World Scientific Publishing Company.},
	Annote = {cited By 2},
	Doi = {10.1142/S0218194007003112},
	Keywords = {Aspect oriented software engineering; Feature bas,Feature extraction; Function evaluation; Mathemati,Product design},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947683339{\&}doi=10.1142{\%}2FS0218194007003112{\&}partnerID=40{\&}md5=1643033c2adca039352c12f0b2e89a0e}
}

@Article{Mariani2016157,
	Title = {{Preserving architectural styles in the search based design of software product line architectures}},
	Author = {Mariani, T and {Elita Colanzi}, T and {Regina Vergilio}, S},
	Journal = {Journal of Systems and Software},
	Year = {2016},
	Pages = {157--173},
	Volume = {115},
	Abstract = {Architectural styles help to improve the Product Line Architecture (PLA) design by providing a better organization of its elements, which results in some benefits, like flexibility, extensibility and maintainability. The PLA design can also be improved by using a search based optimization approach, taking into account different metrics, such as cohesion, coupling and feature modularization. However, the application of search operators changes the PLA organization, and consequently may violate the architectural styles rules, impacting negatively in the architecture understanding. To overcome such limitation, this work introduces a set of search operators to be used in the search based design with the goal of preserving the architectural styles during the optimization process. Such operators consider rules of the layered and client/server architectural styles, generally used in the search based design of conventional architectures and PLAs. The operators are implemented and evaluated in the context of MOA4PLA, a Multi-objective Optimization Approach for PLA Design. Results from an empirical evaluation show that the proposed operators contribute to obtain better solutions, preserving the adopted style and also improving some software metric values. {\textcopyright} 2016 Elsevier Inc. All rights reserved.},
	Annote = {cited By 0},
	Doi = {10.1016/j.jss.2016.01.039},
	Keywords = {Architectural design,Architectural style; Design of softwares; Empiric,Architecture; Computer software; Design; Modular c},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959319935{\&}doi=10.1016{\%}2Fj.jss.2016.01.039{\&}partnerID=40{\&}md5=26a38167189dc84b18847bdde4d16b12}
}

@Article{Mariani201714,
	Title = {{A systematic review on search-based refactoring}},
	Author = {Mariani, Thain{\'{a}} and Vergilio, Silvia Regina},
	Journal = {Information and Software Technology},
	Year = {2017},
	Pages = {14--34},
	Volume = {83},
	Abstract = {Abstract Context: To find the best sequence of refactorings to be applied in a software artifact is an optimization problem that can be solved using search techniques, in the field called Search-Based Refactoring (SBR). Over the last years, the field has gained importance, and many {\{}SBR{\}} approaches have appeared, arousing research interest. Objective: The objective of this paper is to provide an overview of existing {\{}SBR{\}} approaches, by presenting their common characteristics, and to identify trends and research opportunities. Method: A systematic review was conducted following a plan that includes the definition of research questions, selection criteria, a search string, and selection of search engines. 71 primary studies were selected, published in the last sixteen years. They were classified considering dimensions related to the main {\{}SBR{\}} elements, such as addressed artifacts, encoding, search technique, used metrics, available tools, and conducted evaluation. Results: Some results show that code is the most addressed artifact, and evolutionary algorithms are the most employed search technique. Furthermore, most times, the generated solution is a sequence of refactorings. In this respect, the refactorings considered are usually the ones of the Fowler's Catalog. Some trends and opportunities for future research include the use of models as artifacts, the use of many objectives, the study of the bad smells effect, and the use of hyper-heuristics. Conclusions: We have found many {\{}SBR{\}} approaches, most of them published recently. The approaches are presented, analyzed, and grouped following a classification scheme. The paper contributes to the {\{}SBR{\}} field as we identify a range of possibilities that serve as a basis to motivate future researches. },
	Doi = {https://doi.org/10.1016/j.infsof.2016.11.009},
	ISSN = {0950-5849},
	Keywords = {Evolutionary algorithms,Refactoring,Search-based software engineering},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584916303779}
}

@Article{Mariani2015173,
	Title = {{Optimizing aspect-oriented product line architectures with search-based algorithms}},
	Author = {Mariani, T and Vergilio, S R and Colanzi, T E},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2015},
	Pages = {173--187},
	Volume = {9275},
	Abstract = {The adoption of Aspect-Oriented Product Line Architectures (AOPLA) brings many benefits to the software product line design. It contributes to improve modularity, stability and to reduce feature tangling and scattering. Improvements can also be obtained with a search based and multi-objective approach, such as MOA4PLA, which generates PLAs with the best trade-off between different measures, such as cohesion, coupling and feature modularization. However, MOA4PLA operators may violate the aspect-oriented modeling (AOM) rules, impacting negatively on the architecture understanding. In order to solve this problem, this paper introduces a more adequate representation for AOPLAs and a set of search operators, called SO4ASPAR (Search Operators for Aspect-Oriented Architectures). Results from an empirical evaluation show that the proposed operators yield better solutions regarding the fitness values, besides preserving the AOM rules. {\textcopyright} Springer International Publishing Switzerland 2015.},
	Annote = {cited By 0},
	Doi = {10.1007/978-3-319-22183-0_12},
	Keywords = {Algorithms; Economic and social effects; Modular c,Aspect-oriented architecture; Aspect-Oriented Mod,Product design},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951263106{\&}doi=10.1007{\%}2F978-3-319-22183-0{\_}12{\&}partnerID=40{\&}md5=6a4fb2f275935485b04ca1ca42fe14b3}
}

@Article{EXSY:EXSY631,
	Title = {{Reusing knowledge in embedded systems modelling}},
	Author = {Marincic, Jelena and Mader, Angelika and Wieringa, Roel and Lucas, Yan},
	Journal = {Expert Systems},
	Year = {2013},
	Number = {3},
	Pages = {185--199},
	Volume = {30},
	Abstract = {Model-based design is a promising technique to improve the quality of software and the efficiency of the software development process. We are investigating how to efficiently model embedded software and its environment to verify the requirements for the system controlled by the software. The software environment consists of mechanical, electrical and other parts; modelling it involves learning how these parts work, deciding what is relevant to model and how to model it. It is not possible to fully automate these steps. There are general guidelines, but given that every modelling problem differs, much is left to the modeller's own preference, background and experience. Still, when the next generation of a system is designed, the new system will have common elements with its previous version. Therefore, lessons learned from the current model could inform future models. We propose a framework for identifying the non-formal elements of knowledge, insights and a model itself, which can support modelling of the next system generation. We will present the application of our framework on an action research case – modelling mechanical parts of a paper-inserting machine.},
	Doi = {10.1111/j.1468-0394.2012.00631.x},
	ISSN = {1468-0394},
	Keywords = {model-based design,plant modelling,reuse},
	Url = {http://dx.doi.org/10.1111/j.1468-0394.2012.00631.x}
}

@Article{Marinho2017111,
	Title = {{Deriving scientific workflows from algebraic experiment lines: A practical approach}},
	Author = {Marinho, A and de Oliveira, D and Ogasawara, E and Silva, V and Oca{\~{n}}a, K and Murta, L and Braganholo, V and Mattoso, M},
	Journal = {Future Generation Computer Systems},
	Year = {2017},
	Pages = {111--127},
	Volume = {68},
	Abstract = {The exploratory nature of a scientific computational experiment involves executing variations of the same workflow with different approaches, programs, and parameters. However, current approaches do not systematize the derivation process from the experiment definition to the concrete workflows and do not track the experiment provenance down to the workflow executions. Therefore, the composition, execution, and analysis for the entire experiment become a complex task. To address this issue, we propose the Algebraic Experiment Line (AEL). AEL uses a data-centric workflow algebra, which enriches the experiment representation by introducing a uniform data model and its corresponding operators. This representation and the AEL provenance model map concepts from the workflow execution data to the AEL derived workflows with their corresponding experiment abstract definitions. We show how AEL has improved the understanding of a real experiment in the bioinformatics area. By combining provenance data from the experiment and its corresponding executions, AEL provenance queries navigate from experiment concepts defined at high abstraction level to derived workflows and their execution data. It also shows a direct way of querying results from different trials involving activity variations and optionalities, only present at the experiment level of abstraction. {\textcopyright} 2016 Elsevier B.V.},
	Annote = {cited By 0},
	Doi = {10.1016/j.future.2016.08.016},
	Keywords = {Abstracting; Bioinformatics,Algebra,Computational experiment; Concrete workflows; Dat},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989866090{\&}doi=10.1016{\%}2Fj.future.2016.08.016{\&}partnerID=40{\&}md5=b9f956816a38755d563acf6b407b90f6}
}

@Article{Marinho2013,
	Title = {{MobiLine: A Nested Software Product Line for the domain of mobile and context-aware applications}},
	Author = {Marinho, Fabiana G. and Andrade, Rossana M C and Werner, Cl??udia and Viana, Windson and Maia, Marcio E F and Rocha, Lincoln S. and Teixeira, Eld??nae and Filho, Jo??o B Ferreira and Dantas, Val??ria L L and Lima, Fabr??cio and Aguiar, Saulo},
	Journal = {Science of Computer Programming},
	Year = {2013},
	Number = {12},
	Pages = {2381--2398},
	Volume = {78},
	Abstract = {Mobile devices are multipurpose and multi-sensor equipments supporting applications able to adapt their behavior according to changes in the user's context (device, location, time, etc.). Meanwhile, the development of mobile and context-aware software is not a simple task, mostly due to the peculiar characteristics of these devices. Although several solutions have been proposed to facilitate their development, reuse is not systematically used throughout the software development life-cycle. In this paper, we discuss an approach for the development of mobile and context-aware software using the Software Product Line (SPL) paradigm. Furthermore, a Nested SPL for the domain of mobile and context-aware applications is presented, lessons learned in the SPL development are discussed and a product for a context-aware visit guide is shown. ?? 2012 Elsevier B.V. All rights reserved.},
	Doi = {10.1016/j.scico.2012.04.009},
	File = {:Users/mac/Downloads/Sciencedirect{\_}articles{\_}13Jun2017{\_}15-34-52.618/MobiLine-A-Nested-Software-Product-Line-for-the-domain-of-mobile-and-context-aware-applications{\_}2013{\_}Science-of-Computer-Programming.pdf:pdf},
	ISBN = {0167-6423},
	ISSN = {01676423},
	Keywords = {Context-awareness,Mobility,Software product line},
	Publisher = {Elsevier B.V.},
	Url = {http://dx.doi.org/10.1016/j.scico.2012.04.009}
}

@Article{MartinezFernandez201737,
	Title = {{Benefits and drawbacks of software reference architectures: A case study}},
	Author = {Mart{\'{i}}nez-Fern{\'{a}}ndez, Silverio and Ayala, Claudia P and Franch, Xavier and Marques, Helena Martins},
	Journal = {Information and Software Technology},
	Year = {2017},
	Pages = {37--52},
	Volume = {88},
	Abstract = {AbstractContext Software Reference Architectures (SRAs) play a fundamental role for organizations whose business greatly depends on the efficient development and maintenance of complex software applications. However, little is known about the real value and risks associated with {\{}SRAs{\}} in industrial practice. Objective To investigate the current industrial practice of {\{}SRAs{\}} in a single company from the perspective of different stakeholders. Method An exploratory case study that investigates the benefits and drawbacks perceived by relevant stakeholders in nine {\{}SRAs{\}} designed by a multinational software consulting company. Results The study shows the perceptions of different stakeholders regarding the benefits and drawbacks of {\{}SRAs{\}} (e.g., both {\{}SRA{\}} designers and users agree that they benefit from reduced development costs; on the contrary, only application builders strongly highlighted the extra learning curve as a drawback associated with mastering SRAs). Furthermore, some of the {\{}SRA{\}} benefits and drawbacks commonly highlighted in the literature were remarkably not mentioned as a benefit of {\{}SRAs{\}} (e.g., the use of best practices). Likewise, other aspects arose that are not usually discussed in the literature, such as higher time-to-market for applications when their dependencies on the {\{}SRA{\}} are managed inappropriately. Conclusions This study aims to help practitioners and researchers to better understand real {\{}SRAs{\}} projects and the contexts where these benefits and drawbacks appeared, as well as some {\{}SRA{\}} improvement strategies. This would contribute to strengthening the evidence regarding {\{}SRAs{\}} and support practitioners in making better informed decisions about the expected {\{}SRA{\}} benefits and drawbacks. Furthermore, we make available the instruments used in this study and the anonymized data gathered to motivate others to provide similar evidence to help mature {\{}SRA{\}} research and practice. },
	Doi = {https://doi.org/10.1016/j.infsof.2017.03.011},
	ISSN = {0950-5849},
	Keywords = {Benefits,Case study,Drawbacks,Empirical software engineering,Reference architecture,Software architecture},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584916304438}
}

@Article{Martini2015237,
	Title = {{Investigating Architectural Technical Debt accumulation and refactoring over time: A multiple-case study}},
	Author = {Martini, Antonio and Bosch, Jan and Chaudron, Michel},
	Journal = {Information and Software Technology},
	Year = {2015},
	Pages = {237--253},
	Volume = {67},
	Abstract = {AbstractContext A known problem in large software companies is to balance the prioritization of short-term with long-term feature delivery speed. Specifically, Architecture Technical Debt is regarded as sub-optimal architectural solutions taken to deliver fast that might hinder future feature development, which, in turn, would hinder agility. Objective This paper aims at improving software management by shedding light on the current factors responsible for the accumulation of Architectural Technical Debt and to understand how it evolves over time. Method We conducted an exploratory multiple-case embedded case study in 7 sites at 5 large companies. We evaluated the results with additional cross-company interviews and an in-depth, company-specific case study in which we initially evaluate factors and models. Results We compiled a taxonomy of the factors and their influence in the accumulation of Architectural Technical Debt, and we provide two qualitative models of how the debt is accumulated and refactored over time in the studied companies. We also list a set of exploratory propositions on possible refactoring strategies that can be useful as insights for practitioners and as hypotheses for further research. Conclusion Several factors cause constant and unavoidable accumulation of Architecture Technical Debt, which leads to development crises. Refactorings are often overlooked in prioritization and they are often triggered by development crises, in a reactive fashion. Some of the factors are manageable, while others are external to the companies. {\{}ATD{\}} needs to be made visible, in order to postpone the crises according to the strategic goals of the companies. There is a need for practices and automated tools to proactively manage ATD. },
	Doi = {https://doi.org/10.1016/j.infsof.2015.07.005},
	ISSN = {0950-5849},
	Keywords = {Agile software development,Architectural Technical Debt,Qualitative model,Software architecture,Software life-cycle,Software management},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584915001287}
}

@Article{Martini20164,
	Title = {{A multiple case study on the inter-group interaction speed in large, embedded software companies employing agile}},
	Author = {Martini, A and Pareto, L and Bosch, J},
	Journal = {Journal of Software: Evolution and Process},
	Year = {2016},
	Number = {1},
	Pages = {4--26},
	Volume = {28},
	Abstract = {The adoption of Agile Software Development in large companies is a recent phenomenon of great interest both for researchers and practitioners. Although intra-team interaction is well supported by established agile practices, the critical interaction between the agile team and other parts of the organization is still unexplored in literature. Such interactions slow down the development, hindering the achievement of business goals based on speed: short time to market, quick replication of products of a product-line, and reaction time for product evolution. We have employed a two-year long multiple-case case-study, collecting data through interviews and a survey in three large companies developing embedded software. Through a combination of qualitative and quantitative analysis, we have found strong evidence that interaction challenges between the development team and other groups in the organization hinder speed and are widespread in the organizations. This paper also identifies current practices in use at the studied companies and provides detailed guidelines for novel solutions in the investigated domain. Such practices are called boundary-spanning activities in information system research and coordination theory. We present a comparison between large embedded software companies employing agile and developing a line of products based on reused assets and agile companies developing pure software. We highlight specific contextual factors and areas where novel spanning activities are needed for mitigating the interaction challenges hindering speed. Copyright {\textcopyright} 2015 John Wiley {\&} Sons, Ltd.},
	Annote = {cited By 0},
	Doi = {10.1002/smr.1757},
	Keywords = {Agile software development; Boundary spanning; Co,Embedded software; Face recognition; Software engi,Software design},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956572746{\&}doi=10.1002{\%}2Fsmr.1757{\&}partnerID=40{\&}md5=47ef8f873f968c785386cd31a24d639f}
}

@Article{IIS2:IIS201345,
	Title = {{3.2.2 Enhancing the System Development Process Performance: a Value-Based Approach}},
	Author = {Mastrofini, Manuel and Cantone, Giovanni and Seaman, Carolyn and Shull, Forrest and Diep, Madeline and Falessi, Davide},
	Journal = {INCOSE International Symposium},
	Year = {2012},
	Number = {1},
	Pages = {401--415},
	Volume = {22},
	Abstract = {When planning or controlling the system development process, a project leader needs to make decisions which take into account a number of aspects, including: availability of assets and competences, previously enacted processes in the organization, certifications the system is required to obtain, standards to comply with, interactions among process activities, contextual factors and constraints, and allocated budget and schedule.In this paper we propose a value-based approach for supporting decision making. The aim is to provide supportive information for decisions related to the system verification process. This would in turn enhance the performance of system development process by supporting the decision making process for complex systems. We report both academic and industrial empirical evaluations, which demonstrate the feasibility and effectiveness of our proposal, and thus prompt us to refine and extend our approach to sub-processes other than verification.},
	Doi = {10.1002/j.2334-5837.2012.tb01345.x},
	ISSN = {2334-5837},
	Url = {http://dx.doi.org/10.1002/j.2334-5837.2012.tb01345.x}
}

@Article{SMR:SMR204,
	Title = {{Stability assessment of evolving industrial object-oriented frameworks}},
	Author = {Mattsson, Michael and Bosch, Jan},
	Journal = {Journal of Software Maintenance: Research and Practice},
	Year = {2000},
	Number = {2},
	Pages = {79--102},
	Volume = {12},
	Abstract = {Object-oriented framework technology has become a common reuse technology in software development. As with all software, frameworks evolve over time. Once the framework has been deployed, new versions of a framework potentially cause a high maintenance cost for the products built with the framework. This fact, in combination with the high costs of developing and evolving a framework, make it important for organizations to achieve a controlled and predictable evolution of the framework's functionality and costs. We present a metrics-based framework stability assessment method, which has been applied on two industrial frameworks from the telecommunication and graphical user interface domains. First, we discuss the framework concept and the frameworks studied. Then, the stability assessment method is presented including the metrics used. The results from applying the method, as well as an analysis of each of the frameworks, are described. We continue with a set of observations regarding the method, including framework differences that seem to be invariant with respect to the method. A set of framework stability indicators based on the results is then presented. Finally, we assess the method against issues related to the management and evolution of frameworks, framework deployment, change impact analysis and benchmarking. Copyright {\textcopyright} 2000 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/(SICI)1096-908X(200003/04)12:2<79::AID-SMR204>3.0.CO;2-A},
	ISSN = {1096-908X},
	Keywords = {framework assessment,framework evolution,framework stability,object-oriented framework,object-oriented metrics,software architecture},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/(SICI)1096-908X(200003/04)12:2{\%}3C79::AID-SMR204{\%}3E3.0.CO;2-A}
}

@Article{IIS2:IIS2274,
	Title = {{Towards Systems and Software Product Line Management Implementation in Extended Enterprise}},
	Author = {Mauersberger, Ralf and Reyterou, Claude},
	Journal = {INCOSE International Symposium},
	Year = {2016},
	Number = {1},
	Pages = {1973--1987},
	Volume = {26},
	Abstract = {In large, transnational organizations like the Airbus Group, which is developing highly complex, long-leading products and services in multidisciplinary environments, the Systems and Software Product Lines Management (SPL) is of high interest because it offers the capability to manage high levels of commonality between product variants as well as maximize reusability. However, issues like the compatibility of development artefacts from one program to the other and the storage of information in several separated data repositories currently lead to manual treatment of variability and reusability, which is prone to errors, not traceable and being very limited in scope.These issues are likely to become ever more challenging and complex with future programs based on high levels of customization and challenging production ramp-up to produce complex products in very short term. In such a situation, the faster definition of new product families with the benefits of SPL should encompass the needs to integrate SPL concepts across all engineering phases and artefacts. It should also enable multi-system requirements with regard to reusability of components or incremental certification and assure efficient development, cost transparency and scalability of the solution space.In this vision paper, we present the initial results of our investigations on Systems and Software Product Line Management based on a thorough survey of State of the Art Theory and Practice approaches discussed in the scientific community and applied by other industries. Consequently we use this information to derive possible perspectives for introducing a systematic and low risk approach for SPL in order to support the divisions of Airbus Group to make a step forward in that direction.},
	Doi = {10.1002/j.2334-5837.2016.00274.x},
	ISSN = {2334-5837},
	Url = {http://dx.doi.org/10.1002/j.2334-5837.2016.00274.x}
}

@Article{JOB:JOB1772,
	Title = {{Something(s) old and something(s) new: Modeling drivers of global virtual team effectiveness}},
	Author = {Maynard, M Travis and Mathieu, John E and Rapp, Tammy L and Gilson, Lucy L},
	Journal = {Journal of Organizational Behavior},
	Year = {2012},
	Number = {3},
	Pages = {342--365},
	Volume = {33},
	Abstract = {We developed and tested a model that bridges existing team effectiveness theory with new ideas aimed at understanding the complexity of multiple team membership and virtuality. Using a sample of 60 global, virtual supply teams from a large multi-national organization, we propose that even for new team configurations, transactive memory systems and preparation activities are critical for effectiveness. We also examined the association between members' percentage of time allocated to a team, team virtuality, and interdependence on preparation activities. Our findings suggest that preparation activities related significantly to effectiveness as mediated by transactive memory systems. Furthermore, interdependence interacted with members' percentage of time allocated to the team as related to preparation activities. Specifically, members' percentage of time allocated to the team shifted from being a positive influence on preparation activities to a negative influence as team interdependence went from relatively high to relatively low levels. We discuss implications for theory, research, and practice. Copyright {\textcopyright} 2012 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/job.1772},
	ISSN = {1099-1379},
	Keywords = {global virtual teams,multiple team membership,preparation activities,transactive memory systems},
	Publisher = {John Wiley {\&} Sons, Ltd},
	Url = {http://dx.doi.org/10.1002/job.1772}
}

@Article{SMR:SMR1768,
	Title = {{Automated feature discovery via sentence selection and source code summarization}},
	Author = {McBurney, Paul W and Liu, Cheng and McMillan, Collin},
	Journal = {Journal of Software: Evolution and Process},
	Year = {2016},
	Number = {2},
	Pages = {120--145},
	Volume = {28},
	Abstract = {Programs are, in essence, a collection of implemented features. Feature discovery in software engineering is the task of identifying key functionalities that a program implements. Manual feature discovery can be time consuming and expensive, leading to automatic feature discovery tools being developed. However, these approaches typically only describe features using lists of keywords, which can be difficult for readers who are not already familiar with the source code. An alternative to keyword lists is sentence selection, in which one sentence is chosen from among the sentences in a text document to describe that document. Sentence selection has been widely studied in the context of natural language summarization but is only beginning to be explored as a solution to feature discovery. In this paper, we compare four sentence selection strategies for the purpose of feature discovery. Two are off-the-shelf approaches, while two are adaptations we propose. We present our findings as guidelines and recommendations to designers of feature discovery tools. Copyright {\textcopyright} 2016 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/smr.1768},
	ISSN = {2047-7481},
	Keywords = {feature discovery,sentence selection,source code summarization},
	Url = {http://dx.doi.org/10.1002/smr.1768}
}

@Article{CPE:CPE3666,
	Title = {{Gaspar: a compositional aspect-oriented approach for cluster applications}},
	Author = {Medeiros, B and Silva, R and Sobral, J L},
	Journal = {Concurrency and Computation: Practice and Experience},
	Year = {2016},
	Number = {8},
	Pages = {2353--2373},
	Volume = {28},
	Abstract = {This paper presents a framework that enables the development of Java applications that execute on CPUs, graphics processing units (GPUs) and clusters of CPUs/GPUs. Applications are specified in an OpenMP-like fashion, accessing data through a framework-provided data API. The framework enables the efficient execution of applications in CPU and/or GPU by relying on two key features: (i) parallelism exploitation patterns are specified by additional aspect modules; and (ii) data layout can be selected according to the target platform. This paper describes how the framework abstractions are mapped and how the framework intrinsically supports the development of applications with hybrid parallelism by composing aspect modules with a given base program. Performance results show that the framework provides a performance level similar to traditional approaches and enables better performance portability for a given base program. Copyright {\textcopyright} 2015 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/cpe.3666},
	ISSN = {1532-0634},
	Keywords = {GPU,Java,aspect-oriented programming,hierarchical/hybrid parallelism},
	Url = {http://dx.doi.org/10.1002/cpe.3666}
}

@Article{Mellado20101094,
	Title = {{Security requirements engineering framework for software product lines}},
	Author = {Mellado, Daniel and Fern{\'{a}}ndez-Medina, Eduardo and Piattini, Mario},
	Journal = {Information and Software Technology},
	Year = {2010},
	Number = {10},
	Pages = {1094--1117},
	Volume = {52},
	Abstract = {Context The correct analysis and understanding of security requirements are important because they assist in the discovery of any security or requirement defects or mistakes during the early stages of development. Security requirements engineering is therefore both a central task and a critical success factor in product line development owing to the complexity and extensive nature of software product lines (SPL). However, most of the current {\{}SPL{\}} practices in requirements engineering do not adequately address security requirements engineering. Objective The aim of this approach is to describe a holistic security requirements engineering framework with which to facilitate the development of secure {\{}SPLs{\}} and their derived products. It will conform with the most relevant security standards with regard to the management of security requirements, such as ISO/IEC 27001 and ISO/IEC 15408. Results This framework is composed of: a security requirements engineering process for {\{}SPL{\}} (SREPPLine) driven by security standards; a Security Reference Meta Model to manage the variability of those {\{}SPL{\}} artefacts related to security requirements; and a tool (SREPPLineTool) which implements the meta-model and supports the process. Method A complete explanation of the framework will be provided. The process will be formally specified with {\{}SPEM{\}} 2.0 and the repository will be formally specified with an {\{}XML{\}} grammar. The application of {\{}SREPPLine{\}} and {\{}SREPPLineTool{\}} will be illustrated through a description of a simple example as a preliminary validation. Conclusion Although there have been several attempts to fill the gap between requirements engineering and {\{}SPL{\}} requirements engineering, no systematic approach with which to define security quality requirements and to manage their variability and their related security artefacts in {\{}SPL{\}} models is, as yet, available. The contribution of this work is that of providing a systematic approach for the management of the security requirements and their variability from the early stages of product line development in order to facilitate the conformance of {\{}SPL{\}} products with the most relevant security standards. },
	Doi = {https://doi.org/10.1016/j.infsof.2010.05.007},
	ISSN = {0950-5849},
	Keywords = {ISO 27001,Product lines,Requirements engineering,Security requirement,Security requirements engineering,Security software engineering},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584910000960}
}

@Article{Mellado2008361,
	Title = {{Towards security requirements management for software product lines: A security domain requirements engineering process}},
	Author = {Mellado, D and Fern{\'{a}}ndez-Medina, E and Piattini, M},
	Journal = {Computer Standards and Interfaces},
	Year = {2008},
	Number = {6},
	Pages = {361--371},
	Volume = {30},
	Abstract = {Security and requirements engineering are one of the most important factors of success in the development of a software product line due to the complexity and extensive nature of them, given that a weakness in security can cause problems throughout the products of a product line. The main contribution of this work is that of providing a security standard-based process for software product line development, which is an add-in of activities in the domain engineering. This process deals with security requirements from the early stages of the product line lifecycle in a systematic and intuitive way especially adapted for product line based development. It is based on the use of the latest security requirements techniques, together with the integration of the Common Criteria (ISO/IEC 15408) and the ISO/IEC 17799 controls into the product line lifecycle. Additionally, it deals with security artefacts variability and traceability, providing us with a Security Core Assets Repository. Moreover, it facilitates the conformance to the most relevant security standards with regard to the management of security requirements, such as ISO/IEC 27001 and ISO/IEC 17799. Finally, we will illustrate our proposed process by describing part of a real case study, as a preliminary validation of it. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
	Annote = {cited By 23},
	Doi = {10.1016/j.csi.2008.03.004},
	Keywords = {(algorithmic) complexity; (e,3e) process; case studies; Common criteria (CC); c,Computer software; Computer software reusability;,Product development},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-44949241673{\&}doi=10.1016{\%}2Fj.csi.2008.03.004{\&}partnerID=40{\&}md5=ec78211e6b7f4d0935b70d5d7c220c10}
}

@Article{Mellado2014,
	Title = {{Secure Tropos framework for software product lines requirements engineering}},
	Author = {Mellado, Daniel and Mouratidis, Haralambos and Fern{\'{a}}ndez-Medina, Eduardo},
	Journal = {Computer Standards {\&} Interfaces},
	Year = {2014},
	Number = {4},
	Pages = {711--722},
	Volume = {36},
	Abstract = {Security and requirements engineering are two of the most important factors of success in the development of a software product line (SPL). Goal-driven security requirements engineering approaches, such as Secure Tropos, have been proposed as a suitable paradigm for elicitation of security requirements and their analysis on both a social and a technical dimension. Nevertheless, goal-driven security requirements engineering methodologies are not appropriately tailored to the specific demands of SPL, while on the other hand specific proposals of SPL engineering have traditionally ignored security requirements. This paper presents work that fills this gap by proposing “SecureTropos-SPL? framework.},
	Doi = {10.1016/j.csi.2013.12.006},
	File = {:Users/mac/Downloads/Sciencedirect{\_}articles{\_}13Jun2017{\_}14-36-55.006/Secure-Tropos-framework-for-software-product-lines-requirements-engineering{\_}2014{\_}Computer-Standards-Interfaces.pdf:pdf},
	ISSN = {09205489},
	Keywords = {Product lines,Requirements engineering,Secure Tropos,Security requirement engineering,Security requirements,security requirements},
	Publisher = {Elsevier B.V.},
	Url = {http://linkinghub.elsevier.com/retrieve/pii/S0920548913001803}
}

@Article{Menasce2007646,
	Title = {{QoS management in service-oriented architectures}},
	Author = {Menasc{\'{e}}, Daniel A and Ruan, Honglei and Gomaa, Hassan},
	Journal = {Performance Evaluation},
	Year = {2007},
	Number = {7–8},
	Pages = {646--663},
	Volume = {64},
	Abstract = {The next generation of software systems will be highly distributed, component-based and service-oriented. They will need to operate in unattended mode and possibly in hostile environments, will be composed of a large number of ‘replaceable' components discoverable at run-time, and will have to run on a multitude of unknown and heterogeneous hardware and network platforms. This paper focuses on QoS management in service-oriented architectures in which service providers (SP) provide a set of interrelated services to service consumers, and a QoS broker mediates QoS negotiations between {\{}SPs{\}} and consumers. The main contributions of this paper are: (i) the description of an architecture that includes a QoS broker and service provider software components, (ii) the specification of a secure protocol for QoS negotiation with the support of a QoS broker, (iii) the specification of an admission control mechanism used by SPs, (iv) a report on the implementation of the QoS broker and SPs, and (v) the experimental validation of the ideas presented in the paper. },
	Doi = {https://doi.org/10.1016/j.peva.2006.10.001},
	ISSN = {0166-5316},
	Keywords = {Performance,QoS,QoS broker,Service oriented architectures},
	Url = {http://www.sciencedirect.com/science/article/pii/S0166531606000940}
}

@InProceedings{Mende:2010:RDP:1868328.1868336,
	Title = {{Replication of Defect Prediction Studies: Problems, Pitfalls and Recommendations}},
	Author = {Mende, Thilo},
	Booktitle = {Proceedings of the 6th International Conference on Predictive Models in Software Engineering},
	Year = {2010},
	Address = {New York, NY, USA},
	Pages = {5:1----5:10},
	Publisher = {ACM},
	Series = {PROMISE '10},
	Doi = {10.1145/1868328.1868336},
	ISBN = {978-1-4503-0404-7},
	Keywords = {defect prediction model,replication},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/1868328.1868336}
}

@InProceedings{Mende:2009:RED:1540438.1540448,
	Title = {{Revisiting the Evaluation of Defect Prediction Models}},
	Author = {Mende, Thilo and Koschke, Rainer},
	Booktitle = {Proceedings of the 5th International Conference on Predictor Models in Software Engineering},
	Year = {2009},
	Address = {New York, NY, USA},
	Pages = {7:1----7:10},
	Publisher = {ACM},
	Series = {PROMISE '09},
	Doi = {10.1145/1540438.1540448},
	ISBN = {978-1-60558-634-2},
	Keywords = {cost-sensitive performance measures,defect prediction},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/1540438.1540448}
}

@Article{SMR:SMR402,
	Title = {{An evaluation of code similarity identification for the grow-and-prune model}},
	Author = {Mende, Thilo and Koschke, Rainer and Beckwermert, Felix},
	Journal = {Journal of Software Maintenance and Evolution: Research and Practice},
	Year = {2009},
	Number = {2},
	Pages = {143--169},
	Volume = {21},
	Abstract = {In case new functionality is required, which is similar to the existing one, developers often copy the code that implements the existing functionality and adjust the copy to the new requirements. The result of the copying is code growth. If developers face maintenance problems, because of the need to make changes multiple times for the original and all its copies, they may decide to merge the original and its copies again; that is, they prune the code. This approach was named the grow-and-prune model by Faust and Verhoef. This paper describes tool support for the grow-and-prune model in the evolution of software by identifying similar functions that may be merged. These functions are identified in two steps. First, token-based clone detection is used to detect pairs of functions sharing code. Second, Levenshtein distance (LD) measures the textual similarity among these functions. Sufficient similarity at function level is then lifted to the architectural level. The approach is evaluated by a case study for the Linux kernel. We give examples of instances of the grow-and-prune model for Linux. Then, we evaluate our technique quantitatively by measuring recall and precision with respect to an oracle. To obtain the oracle, we asked nine different developers to decide whether they believe certain functions are similar and should be merged. The evaluation shows that the recall and precision of our technique are about 75{\%}. Calculating LD on token values rather than characters is superior. The two metrics strongly correlate but the token-based calculation reduces runtime by a factor of 4.6. Clone detection is an effective filter to reduce the number of calculations of the relatively expensive LD. Copyright {\textcopyright} 2009 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/smr.402},
	ISSN = {1532-0618},
	Keywords = {clones,code similarity,software maintenance,software reusability},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/smr.402}
}

@InProceedings{Mendes:2003:ARI:900051.900091,
	Title = {{Do Adaptation Rules Improve Web Cost Estimation?}},
	Author = {Mendes, Emilia and Mosley, Nile and Counsell, Steve},
	Booktitle = {Proceedings of the Fourteenth ACM Conference on Hypertext and Hypermedia},
	Year = {2003},
	Address = {New York, NY, USA},
	Pages = {173--183},
	Publisher = {ACM},
	Series = {HYPERTEXT '03},
	Doi = {10.1145/900051.900091},
	ISBN = {1-58113-704-4},
	Keywords = {case-based reasoning,prediction models,web effort prediction,web hypermedia,web hypermedia metrics},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/900051.900091}
}

@Article{Mendonca2010311,
	Title = {{Decision-making coordination and efficient reasoning techniques for feature-based configuration}},
	Author = {Mendonca, Marcilio and Cowan, Donald},
	Journal = {Science of Computer Programming},
	Year = {2010},
	Number = {5},
	Pages = {311--332},
	Volume = {75},
	Abstract = {Software Product Lines is a contemporary approach to software development that exploits the similarities and differences within a family of systems in a particular domain of interest in order to provide a common infrastructure for deriving members of this family in a timely fashion, with high-quality standards, and at lower costs. In Software Product Lines, feature-based product configuration is the process of selecting the desired features for a given software product from a repository of features called a feature model. This process is usually carried out collaboratively by people with distinct skills and interests called stakeholders. Collaboration benefits stakeholders by allowing them to directly intervene in the configuration process. However, collaboration also raises an important side effect, i.e., the need of stakeholders to cope with decision conflicts. Conflicts arise when decisions that are locally consistent cannot be applied globally because they violate one or more constraints in the feature model. Unfortunately, current product configuration systems are typically single-user-based in the sense that they do not provide means to coordinate concurrent decision-making on the feature model. As a consequence, configuration is carried out by a single person that is in charge of representing the interests of all stakeholders and managing decision conflicts on their own. This results in an error-prone and time-consuming process that requires past decisions to be revisited continuously either to correct misinterpreted stakeholder requirements or to handle decision conflicts. Yet another challenging issue related to configuration problems is the typically high computational cost of configuration algorithms. In fact, these algorithms frequently fall into the category of NP-hard and thus can become intractable in practice. In this paper, our goal is two-fold. First, we revisit our work on Collaborative Product Configuration (CPC) in which we proposed an approach to describe and validate collaborative configuration scenarios. We discuss how collaborative configuration can be described in terms of a workflow-like plan that safely guides stakeholders during the configuration process. Second, we propose a preliminary set of reasoning algorithms tailored to the feature modelling domain that can be used to provide automated support for product configuration. In addition, we compare empirically the performance of the proposed algorithms to that of a general-purpose solution. We hope that the insights provided in this paper will encourage other researchers to develop new algorithms in the near future. },
	Annote = {Coordination Models, Languages and Applications (SAC'08)},
	Doi = {https://doi.org/10.1016/j.scico.2009.12.004},
	ISSN = {0167-6423},
	Keywords = {Automated reasoning,Constraint-based reasoning,Decision-making coordination,Feature modelling,Feature models,Product configuration,Software Product Lines},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642309001713}
}

@InProceedings{Meneely:2010:SEA:1852786.1852798,
	Title = {{Strengthening the Empirical Analysis of the Relationship Between Linus' Law and Software Security}},
	Author = {Meneely, Andrew and Williams, Laurie},
	Booktitle = {Proceedings of the 2010 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
	Year = {2010},
	Address = {New York, NY, USA},
	Pages = {9:1----9:10},
	Publisher = {ACM},
	Series = {ESEM '10},
	Doi = {10.1145/1852786.1852798},
	ISBN = {978-1-4503-0039-1},
	Keywords = {contribution network,developer network,metric,vulnerability},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/1852786.1852798}
}

@InProceedings{Meneely:2008:PFD:1453101.1453106,
	Title = {{Predicting Failures with Developer Networks and Social Network Analysis}},
	Author = {Meneely, Andrew and Williams, Laurie and Snipes, Will and Osborne, Jason},
	Booktitle = {Proceedings of the 16th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
	Year = {2008},
	Address = {New York, NY, USA},
	Pages = {13--23},
	Publisher = {ACM},
	Series = {SIGSOFT '08/FSE-16},
	Doi = {10.1145/1453101.1453106},
	ISBN = {978-1-59593-995-1},
	Keywords = {developer network,failure prediction,logistic regression,negative binomial regression,social network analysis},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/1453101.1453106}
}

@InProceedings{Metzger:2014:SPL:2593882.2593888,
	Title = {{Software Product Line Engineering and Variability Management: Achievements and Challenges}},
	Author = {Metzger, Andreas and Pohl, Klaus},
	Booktitle = {Proceedings of the on Future of Software Engineering},
	Year = {2014},
	Address = {New York, NY, USA},
	Pages = {70--84},
	Publisher = {ACM},
	Series = {FOSE 2014},
	Doi = {10.1145/2593882.2593888},
	ISBN = {978-1-4503-2865-4},
	Keywords = {Software product lines,design,quality assurance,requirements engineering,variability management,variability modeling},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2593882.2593888}
}

@Article{Meyers20111223,
	Title = {{A framework for evolution of modelling languages}},
	Author = {Meyers, Bart and Vangheluwe, Hans},
	Journal = {Science of Computer Programming},
	Year = {2011},
	Number = {12},
	Pages = {1223--1246},
	Volume = {76},
	Abstract = {In model-driven engineering, evolution is inevitable over the course of the complete life cycle of complex software-intensive systems and more importantly of entire product families. Not only instance models, but also entire modelling languages are subject to change. This is in particular true for domain-specific languages, whose language constructs are tightly coupled to an application domain. The most popular approach to evolution in the modelling domain is a manual process, with tedious and error-prone migration of artefacts such as instance models as a result. This paper provides a taxonomy for evolution of modelling languages and discusses the different evolution scenarios for various kinds of modelling artefacts, such as instance models, meta-models, and transformation models. Subsequently, the consequences of evolution and the required remedial actions are decomposed into primitive scenarios such that all possible evolutions can be covered exhaustively. These primitives are then used in a high-level framework for the evolution of modelling languages. We suggest that our structured approach enables the design of (semi-)automatic modelling language evolution solutions. },
	Annote = {Special Issue on Software Evolution, Adaptability and Variability},
	Doi = {https://doi.org/10.1016/j.scico.2011.01.002},
	ISSN = {0167-6423},
	Keywords = {Evolution,Language engineering,Model transformation,Model-driven engineering,Modelling languages},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642311000141}
}

@Article{Midtgaard2015145,
	Title = {{Systematic derivation of correct variability-aware program analyses}},
	Author = {Midtgaard, Jan and Dimovski, Aleksandar S and Brabrand, Claus and W{\c{a}}sowski, Andrzej},
	Journal = {Science of Computer Programming},
	Year = {2015},
	Pages = {145--170},
	Volume = {105},
	Abstract = {Abstract A recent line of work lifts particular verification and analysis methods to Software Product Lines (SPL). In an effort to generalize such case-by-case approaches, we develop a systematic methodology for lifting single-program analyses to {\{}SPLs{\}} using abstract interpretation. Abstract interpretation is a classical framework for deriving static analyses in a compositional, step-by-step manner. We show how to take an analysis expressed as an abstract interpretation and lift each of the abstract interpretation steps to a family of programs (SPL). This includes schemes for lifting domain types, and combinators for lifting analyses and Galois connections. We prove that for analyses developed using our method, the soundness of lifting follows by construction. The resulting variational abstract interpretation is a conceptual framework for understanding, deriving, and validating static analyses for SPLs. Then we show how to derive the corresponding variational dataflow equations for an example static analysis, a constant propagation analysis. We also describe how to approximate variability by applying variability-aware abstractions to {\{}SPL{\}} analysis. Finally, we discuss how to efficiently implement our method and present some evaluation results. },
	Doi = {https://doi.org/10.1016/j.scico.2015.04.005},
	ISSN = {0167-6423},
	Keywords = {Abstract interpretation,Software Product Lines,Software variability,Static analysis,Verification},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642315000714}
}

@Article{Moon2005551,
	Title = {{An approach to developing domain requirements as a core asset based on commonality and variability analysis in a product line}},
	Author = {{Mikyeong Moon} and {Keunhyuk Yeom} and {Heung Seok Chae} and Moon, M and Yeom, K and Chae, H S},
	Journal = {IEEE Transactions on Software Engineering},
	Year = {2005},
	Month = {jul},
	Number = {7},
	Pages = {551--569},
	Volume = {31},
	Abstract = {The methodologies of product line engineering emphasize proactive reuse to construct high-quality products more quickly that are less costly. Requirements engineering for software product families differs significantly from requirements engineering for single software products. The requirements for a product line are written for the group of systems as a whole, with requirements for individual systems specified by a delta or an increment to the generic set. Therefore, it is necessary to identify and explicitly denote the regions of commonality and points of variation at the requirements level. In this paper, we suggest a method of producing requirements that will be a core asset in the product line. We describe a process for developing domain requirements where commonality and variability in a domain are explicitly considered. A CASE environment, named DREAM, for managing commonality and valiability analysis of domain requirements is also described. We also describe a case study for an e-Travel System domain where we found that our approach to developing domain requirements based on commonality and variability analysis helped to produce domain requirements as a core asset for product lines. {\textcopyright} 2005 IEEE.},
	Annote = {From Duplicate 1 (An approach to developing domain requirements as a core asset based on commonality and variability analysis in a product line - Moon, M; Yeom, K; Chae, H S)
		cited By 78},
	Doi = {10.1109/TSE.2005.76},
	ISSN = {0098-5589},
	Keywords = {,Commonality analysis,Computer aided software engineering,Computer arch,Computer software reusability,Core assets,Domain analysi},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-25844465686{\&}doi=10.1109{\%}2FTSE.2005.76{\&}partnerID=40{\&}md5=51373b68aac3e74d68e205d6d1f5da7b http://ieeexplore.ieee.org/document/1492371/}
}

@Article{Milani201655,
	Title = {{Modelling families of business process variants: A decomposition driven method}},
	Author = {Milani, Fredrik and Dumas, Marlon and Ahmed, Naved and Matulevi{\v{c}}ius, Raimundas},
	Journal = {Information Systems},
	Year = {2016},
	Pages = {55--72},
	Volume = {56},
	Abstract = {Abstract Business processes usually do not exist as singular entities that can be managed in isolation, but rather as families of business process variants. When modelling such families of variants, analysts are confronted with the choice between modelling each variant separately, or modelling multiple or all variants in a single model. Modelling each variant separately leads to a proliferation of models that share common parts, resulting in redundancies and inconsistencies. Meanwhile, modelling all variants together leads to less but more complex models, thus hindering on comprehensibility. This paper introduces a method for modelling families of process variants that addresses this trade-off. The key tenet of the method is to alternate between steps of decomposition (breaking down processes into sub-processes) and deciding which parts should be modelled together and which ones should be modelled separately. We have applied the method to two case studies: one concerning the consolidation of existing process models, and another dealing with green-field process discovery. In both cases, the method produced fewer models with respect to the baseline and reduced duplicity by up to 50{\%} without significant impact on complexity. },
	Doi = {https://doi.org/10.1016/j.is.2015.09.003},
	ISSN = {0306-4379},
	Keywords = {Business process model consolidation,Business process modelling,Business process variant},
	Url = {http://www.sciencedirect.com/science/article/pii/S0306437915001684}
}

@InProceedings{Miranskyy:2014:ETC:2652524.2652586,
	Title = {{Effect of Temporal Collaboration Network, Maintenance Activity, and Experience on Defect Exposure}},
	Author = {Miranskyy, Andriy and Caglayan, Bora and Bener, Ayse and Cialini, Enzo},
	Booktitle = {Proceedings of the 8th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
	Year = {2014},
	Address = {New York, NY, USA},
	Pages = {27:1----27:8},
	Publisher = {ACM},
	Series = {ESEM '14},
	Doi = {10.1145/2652524.2652586},
	ISBN = {978-1-4503-2774-9},
	Keywords = {collaboration network,linear regression model,prediction quality},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2652524.2652586}
}

@Article{SYS:SYS21271,
	Title = {{Transitioning the SWFTS Program Combat System Product Family from Traditional Document-Centric to Model-Based Systems Engineering}},
	Author = {Mitchell, Steven W},
	Journal = {Systems Engineering},
	Year = {2014},
	Number = {3},
	Pages = {313--329},
	Volume = {17},
	Abstract = {Over the past four years, the Submarine Warfare Federated Tactical Systems (SWFTS) Systems Engineering {\&} Integration (SE{\&}I) program has shifted from traditional document-centric systems engineering to a model-based systems engineering (MBSE) process for managing the evolution and support of the common combat system used by most submarines in the U.S. and Royal Australian Navies. At the beginning of this transition a pilot study established technical feasibility, and projected a 13{\%} reduction in the cost of processing a baseline if MBSE were applied to support the SWFTS system of systems baseline development process. Over the course of two years of development new modeling techniques were invented, and a large-scale system of systems model was designed, implemented, and populated.Now the transition to operations has begun. In early 2012 SWFTS SE{\&}I produced its first new generation of combat system interface baselines using MBSE. The technical foundation has been established, and the workforce transition is under way. Anticipated cost reductions are still a year away as the workforce ascends the learning curve, but the program is already seeing improvements in the quality and consistency of engineering products.This paper summarizes this document-centric to model-based SE transition, describes the accomplishments and observations to date, and describes the metrics being collected to quantify the achieved return on investment once the transition is complete.},
	Doi = {10.1002/sys.21271},
	ISSN = {1520-6858},
	Keywords = {MBSE,baseline management,model-based systems engineering,product family,system evolution},
	Url = {http://dx.doi.org/10.1002/sys.21271}
}

@Article{IIS2:IIS201358,
	Title = {{4.4.3 Efficiently Managing Product Baseline Configurations in the Model-Based System Development of a Combat System Product Family}},
	Author = {Mitchell, Steven W},
	Journal = {INCOSE International Symposium},
	Year = {2012},
	Number = {1},
	Pages = {589--599},
	Volume = {22},
	Abstract = {Efficient management of product configuration baselines is a challenge in the evolution of any industrial scale product family. This is particularly true with current standards-based system modeling tools, as the standards themselves are just beginning to address this problem in a scalable fashion.In the case of the Submarine Warfare Federated Tactical Systems (SWFTS) common submarine combat system, dozens of product configurations must be managed in parallel, with many of those baselines being updated several times a year. To support this activity a new SysML modeling technique has been developed for constructing hardware and software component bills of materials. It extends the concepts of libraries with SysML catalogs to bound the complexity of the task, improving the quality and efficiency of the systems engineering process.General Terms: Systems Engineering, System Evolution},
	Doi = {10.1002/j.2334-5837.2012.tb01358.x},
	ISSN = {2334-5837},
	Keywords = {MBSE,SysML catalog,baseline management,model based systems engineering,product family,service configuration},
	Url = {http://dx.doi.org/10.1002/j.2334-5837.2012.tb01358.x}
}

@Article{Mizouni20147549,
	Title = {{A framework for context-aware self-adaptive mobile applications {\{}SPL{\}}}},
	Author = {Mizouni, Rabeb and Matar, Mohammad Abu and Mahmoud, Zaid Al and Alzahmi, Salwa and Salah, Aziz},
	Journal = {Expert Systems with Applications},
	Year = {2014},
	Number = {16},
	Pages = {7549--7564},
	Volume = {41},
	Abstract = {Abstract Mobile Applications are rapidly emerging as a convenient medium for using a variety of services. Over time and with the high penetration of smartphones in society, self-adaptation has become an essential capability required by mobile application users. In an ideal scenario, an application is required to adjust its behavior according to the current context of its use. This raises the challenge in mobile computing towards the design and development of applications that sense and react to contextual changes to provide a value-added user experience. In its general sense, context information can relate to the environment, the user, or the device status. In this paper, we propose a novel framework for building context aware and adaptive mobile applications. Based on feature modeling and Software Product Lines (SPL) concepts, this framework guides the modeling of adaptability at design time and supports context awareness and adaptability at runtime. In the core of the approach, is a feature meta-model that incorporates, in addition to {\{}SPL{\}} concepts, application feature priorities to drive the adaptability. A tool, based on that feature model, is presented to model the mobile application features and to derive the {\{}SPL{\}} members. A mobile framework, built on top of {\{}OSGI{\}} framework to dynamically adapt the application at runtime is also described. },
	Doi = {https://doi.org/10.1016/j.eswa.2014.05.049},
	ISSN = {0957-4174},
	Keywords = {Feature priority,Mobile devices,Multi-view variability model,Runtime adaptability,SPL},
	Url = {http://www.sciencedirect.com/science/article/pii/S0957417414003364}
}

@Article{Mockus:2002:TCS:567793.567795,
	Title = {{Two Case Studies of Open Source Software Development: Apache and Mozilla}},
	Author = {Mockus, Audris and Fielding, Roy T and Herbsleb, James D},
	Journal = {ACM Trans. Softw. Eng. Methodol.},
	Year = {2002},
	Number = {3},
	Pages = {309--346},
	Volume = {11},
	Address = {New York, NY, USA},
	Doi = {10.1145/567793.567795},
	ISSN = {1049-331X},
	Keywords = {Apache,Mozilla,Open source software,code ownership,defect density,repair interval},
	Publisher = {ACM},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/567793.567795}
}

@Article{BLTJ:BLTJ2229,
	Title = {{Predicting risk of software changes}},
	Author = {Mockus, Audris and Weiss, David M},
	Journal = {Bell Labs Technical Journal},
	Year = {2000},
	Number = {2},
	Pages = {169--180},
	Volume = {5},
	Abstract = {Reducing the number of software failures is one of the most challenging problems of software production. We assume that software development proceeds as a series of changes and model the probability that a change to software will cause a failure. We use predictors based on the properties of a change itself. Such predictors include size in lines of code added, deleted, and unmodified; diffusion of the change and its component subchanges, as reflected in the number of files, modules, and subsystems touched, or changed; several measures of developer experience; and the type of change and its subchanges (fault fixes or new code). The model is built on historic information and is used to predict the risk of new changes. In this paper we apply the model to 5ESS{\textregistered} software updates and find that change diffusion and developer experience are essential to predicting failures. The predictive model is implemented as a Web-based tool to allow timely prediction of change quality. The ability to predict the quality of change enables us to make appropriate decisions regarding inspection, testing, and delivery. Historic information on software changes is recorded in many commercial software projects, suggesting that our results can be easily and widely applied in practice.},
	Doi = {10.1002/bltj.2229},
	ISSN = {1538-7305},
	Publisher = {Wiley Subscription Services, Inc., A Wiley Company},
	Url = {http://dx.doi.org/10.1002/bltj.2229}
}

@Article{Mohabbati20131845,
	Title = {{Combining service-orientation and software product line engineering: A systematic mapping study}},
	Author = {Mohabbati, Bardia and Asadi, Mohsen and Ga{\v{s}}evi{\'{c}}, Dragan and Hatala, Marek and M{\"{u}}ller, Hausi A},
	Journal = {Information and Software Technology},
	Year = {2013},
	Number = {11},
	Pages = {1845--1859},
	Volume = {55},
	Abstract = {AbstractContext Service-Orientation (SO) is a rapidly emerging paradigm for the design and development of adaptive and dynamic software systems. Software Product Line Engineering (SPLE) has also gained attention as a promising and successful software reuse development paradigm over the last decade and proven to provide effective solutions to deal with managing the growing complexity of software systems. Objective This study aims at characterizing and identifying the existing research on employing and leveraging {\{}SO{\}} and SPLE. Method We conducted a systematic mapping study to identify and analyze related literature. We identified 81 primary studies, dated from 2000–2011 and classified them with respect to research focus, types of research and contribution. Result The mapping synthesizes the available evidence about combining the synergy points and integration of {\{}SO{\}} and SPLE. The analysis shows that the majority of studies focus on service variability modeling and adaptive systems by employing {\{}SPLE{\}} principles and approaches. In particular, {\{}SPLE{\}} approaches, especially feature-oriented approaches for variability modeling, have been applied to the design and development of service-oriented systems. While {\{}SO{\}} is employed in software product line contexts for the realization of product lines to reconcile the flexibility, scalability and dynamism in product derivations thereby creating dynamic software product lines. Conclusion Our study summarizes and characterizes the {\{}SO{\}} and {\{}SPLE{\}} topics researchers have investigated over the past decade and identifies promising research directions as due to the synergy generated by integrating methods and techniques from these two areas. },
	Doi = {https://doi.org/10.1016/j.infsof.2013.05.006},
	File = {:Users/mac/Downloads/1-s2.0-S0950584913001274-main.pdf:pdf},
	ISSN = {0950-5849},
	Keywords = {Service-oriented architecture,Software product lines,Systematic mapping},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584913001274}
}

@Article{Mohagheghi:2008:EIS:1363102.1363104,
	Title = {{An Empirical Investigation of Software Reuse Benefits in a Large Telecom Product}},
	Author = {Mohagheghi, Parastoo and Conradi, Reidar},
	Journal = {ACM Trans. Softw. Eng. Methodol.},
	Year = {2008},
	Number = {3},
	Pages = {13:1----13:31},
	Volume = {17},
	Address = {New York, NY, USA},
	Doi = {10.1145/1363102.1363104},
	ISSN = {1049-331X},
	Keywords = {Software reuse,fault density,product family,risks,standardization},
	Publisher = {ACM},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/1363102.1363104}
}

@Article{Mohagheghi20091646,
	Title = {{Definitions and approaches to model quality in model-based software development – A review of literature}},
	Author = {Mohagheghi, Parastoo and Dehlen, Vegard and Neple, Tor},
	Journal = {Information and Software Technology},
	Year = {2009},
	Number = {12},
	Pages = {1646--1669},
	Volume = {51},
	Abstract = {More attention is paid to the quality of models along with the growing importance of modelling in software development. We performed a systematic review of studies discussing model quality published since 2000 to identify what model quality means and how it can be improved. From forty studies covered in the review, six model quality goals were identified; i.e., correctness, completeness, consistency, comprehensibility, confinement and changeability. We further present six practices proposed for developing high-quality models together with examples of empirical evidence. The contributions of the article are identifying and classifying definitions of model quality and identifying gaps for future research. },
	Annote = {Quality of {\{}UML{\}} Models},
	Doi = {https://doi.org/10.1016/j.infsof.2009.04.004},
	ISSN = {0950-5849},
	Keywords = {Model quality,Model-driven development,Modelling,Systematic review,UML},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584909000457}
}

@Article{Mohan20071255,
	Title = {{Knowledge networking to support medical new product development}},
	Author = {Mohan, Kannan and Jain, Radhika and Ramesh, Balasubramaniam},
	Journal = {Decision Support Systems},
	Year = {2007},
	Number = {4},
	Pages = {1255--1273},
	Volume = {43},
	Abstract = {New product development (NPD) in the pharmaceutical industry is very knowledge intensive. Knowledge generated and used during medical {\{}NPD{\}} processes is fragmented and distributed across various phases and artifacts. Many challenges in medical {\{}NPD{\}} can be addressed by the integration of this fragmented knowledge. We propose the creation and use of knowledge networks to address these challenges. Based on a case study conducted in a leading pharmaceutical company, we have developed a knowledge framework that represents knowledge fragments that need to be integrated to support medical NPD. We have also developed a prototype system that supports knowledge integration using knowledge networks. We illustrate the capabilities of the system through scenarios drawn from the case study. Qualitative validation of our approach is also presented. },
	Annote = {Special Issue Clusters},
	Doi = {https://doi.org/10.1016/j.dss.2006.02.005},
	ISSN = {0167-9236},
	Keywords = {Healthcare,Knowledge integration,Knowledge networks,New product development,Pharmaceutical knowledge management},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167923606000273}
}

@Article{Mohan2007968,
	Title = {{Traceability-based knowledge integration in group decision and negotiation activities}},
	Author = {Mohan, Kannan and Ramesh, Balasubramaniam},
	Journal = {Decision Support Systems},
	Year = {2007},
	Number = {3},
	Pages = {968--989},
	Volume = {43},
	Abstract = {Group decision and negotiation (GDN) in distributed collaborative environments involves the acquisition and use of extensive knowledge. Knowledge elements that play a critical role in guiding {\{}GDN{\}} activities are distributed across different work environments that are not seamlessly integrated with each other. We argue that integrating fragmented knowledge will improve the process of {\{}GDN{\}} in software development. In this paper, we present an approach to knowledge integration using traceability. Our approach comprises of: (a) a traceability framework that identifies the key knowledge elements that are to be integrated, and (b) a prototype system that supports the acquisition, integration, and use of knowledge elements represented by the traceability framework. We illustrate the usefulness of our approach with a case study in a software development organization. },
	Annote = {Integrated Decision Support},
	Doi = {https://doi.org/10.1016/j.dss.2005.05.026},
	ISSN = {0167-9236},
	Keywords = {Collaborative software development,Decision making,Group decision and negotiation,Knowledge integration,Traceability,Work processes},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167923605000916}
}

@Article{Mohan2006,
	Title = {{Change management patterns in software product lines}},
	Author = {Mohan, K and Ramesh, B},
	Journal = {Communications of the ACM},
	Year = {2006},
	Number = {12},
	Volume = {49},
	Abstract = {A case study in a software product line (SPL) development for identifying patterns of change management and to suggest effective practices was investigated. Three commonly faced patterns of changes incorporated in product lines and change management practices that mitigate their adverse effects are independencies among changes in variants, increasing the degree of evolving variance, and reinvented variations. Indepencies among change incorporated in different variants pose a problem in product line evolution, increasing variance results in numerous constraints across variation points, and it is often difficult to leverage existing implementations of variants. Some recommendations for change management include modularizing of change and variation points, tracking the scope and life of the variations, and facilitating reuse based on knowledge sharing.},
	Annote = {cited By 8},
	Doi = {10.1145/1183236.1183269},
	Keywords = {Flexible manufacturing systems; Industrial manage,Management patterns; Software product line (SPL);,Software engineering},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33751555290{\&}doi=10.1145{\%}2F1183236.1183269{\&}partnerID=40{\&}md5=a9e3d542177ea0d01a7f3d8e077727c5}
}

@Article{Mohan2008922,
	Title = {{Improving change management in software development: Integrating traceability and software configuration management}},
	Author = {Mohan, Kannan and Xu, Peng and Cao, Lan and Ramesh, Balasubramaniam},
	Journal = {Decision Support Systems},
	Year = {2008},
	Number = {4},
	Pages = {922--936},
	Volume = {45},
	Abstract = {Software configuration management (SCM) and traceability are two prominent practices that support change management in software development. While {\{}SCM{\}} helps manage the evolution of software artifacts and their documentation, traceability helps manage knowledge about the process of the development of software artifacts. In this paper, we present the integration of traceability and {\{}SCM{\}} to help change management during the development and evolution of software artifacts. We developed a traceability model using a case study conducted in a software development organization. This model represents knowledge elements that are essential to comprehensively manage changes tracked within the change management function of {\{}SCM{\}} tools. A tool that supports the integrated practice of {\{}SCM{\}} and traceability is also presented. We illustrate the usefulness of our model and tool using a change management scenario that was drawn from our case study. We also present a qualitative study towards empirically evaluating the usefulness of our approach. },
	Annote = {Information Technology and Systems in the Internet-Era},
	Doi = {https://doi.org/10.1016/j.dss.2008.03.003},
	ISSN = {0167-9236},
	Keywords = {Change management,Process knowledge,Software configuration management,Traceability},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167923608000523}
}

@Article{Mohan2006650,
	Title = {{Supporting dynamic group decision and negotiation processes: A traceability augmented peer-to-peer network approach}},
	Author = {Mohan, Kannan and Xu, Peng and Ramesh, Balasubramaniam},
	Journal = {Information {\&} Management},
	Year = {2006},
	Number = {5},
	Pages = {650--662},
	Volume = {43},
	Abstract = {Peer-to-peer (P2P) networks are gaining popularity in supporting group decision and negotiation (GDN) activities in which ad hoc, transient groups participate. In these, multiple stakeholders create and use knowledge that is fragmented and distributed across different locations. While {\{}P2P{\}} networks help establish physical links across participants, they lack the capability to integrate knowledge fragments embedded in documents and artifacts distributed across peers. We augmented the {\{}P2P{\}} architecture with traceability to provide a way of integrating distributed knowledge. We implemented this approach in a prototype system that used a {\{}P2P{\}} networking tool. Using a case study of software development outsourcing, we showed how our approach supported critical {\{}GDN{\}} activities. Qualitative evaluation of our approach in supporting {\{}GDN{\}} was also demonstrated. },
	Doi = {https://doi.org/10.1016/j.im.2006.04.001},
	ISSN = {0378-7206},
	Keywords = {Group decision and negotiation,Knowledge integration,Peer-to-peer networks,Traceability},
	Url = {http://www.sciencedirect.com/science/article/pii/S0378720606000449}
}

@Article{Montagud2012425,
	Title = {{A systematic review of quality attributes and measures for software product lines}},
	Author = {Montagud, S and Abrah{\~{a}}o, S and Insfran, E},
	Journal = {Software Quality Journal},
	Year = {2012},
	Number = {3-4},
	Pages = {425--486},
	Volume = {20},
	Abstract = {It is widely accepted that software measures provide an appropriate mechanism for understanding, monitoring, controlling, and predicting the quality of software development projects. In software product lines (SPL), quality is even more important than in a single software product since, owing to systematic reuse, a fault or an inadequate design decision could be propagated to several products in the family. Over the last few years, a great number of quality attributes and measures for assessing the quality of SPL have been reported in literature. However, no studies summarizing the current knowledge about them exist. This paper presents a systematic literature review with the objective of identifying and interpreting all the available studies from 1996 to 2010 that present quality attributes and/or measures for SPL. These attributes and measures have been classified using a set of criteria that includes the life cycle phase in which the measures are applied; the corresponding quality characteristics; their support for specific SPL characteristics (e. g., variability, compositionality); the procedure used to validate the measures, etc. We found 165 measures related to 97 different quality attributes. The results of the review indicated that 92{\%} of the measures evaluate attributes that are related to maintainability. In addition, 67{\%} of the measures are used during the design phase of Domain Engineering, and 56{\%} are applied to evaluate the product line architecture. However, only 25{\%} of them have been empirically validated. In conclusion, the results provide a global vision of the state of the research within this area in order to help researchers in detecting weaknesses, directing research efforts, and identifying new research lines. In particular, there is a need for new measures with which to evaluate both the quality of the artifacts produced during the entire SPL life cycle and other quality characteristics. There is also a need for more validation (both theoretical and empirical) of existing measures. In addition, our results may be useful as a reference guide for practitioners to assist them in the selection or the adaptation of existing measures for evaluating their software product lines. {\textcopyright} 2011 Springer Science+Business Media, LLC.},
	Annote = {cited By 24},
	Doi = {10.1007/s11219-011-9146-7},
	Keywords = {Computer software reusability; Image quality; Life,Measures; Product line architecture; Quality attr,Quality control},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865626740{\&}doi=10.1007{\%}2Fs11219-011-9146-7{\&}partnerID=40{\&}md5=33b25cd0a25fc09685e06cbc9c988f14}
}

@Article{Montalvillo2016110,
	Title = {{Requirement-driven evolution in software product lines: A systematic mapping study}},
	Author = {Montalvillo, Leticia and D{\'{i}}az, Oscar},
	Journal = {Journal of Systems and Software},
	Year = {2016},
	Pages = {110--143},
	Volume = {122},
	Abstract = {Abstract CONTEXT. Software Product Lines (SPLs) aim to support the development of a whole family of software products through systematic reuse of shared assets. As {\{}SPLs{\}} exhibit a long life-span, evolution is an even greater concern than for single-systems. For the purpose of this work, evolution refers to the adaptation of the {\{}SPL{\}} as a result of changing requirements. Hence, evolution is triggered by requirement changes, and not by bug fixing or refactoring. OBJECTIVE. Research on {\{}SPL{\}} evolution has not been previously mapped. This work provides a mapping study along Petersen's and Kichenham's guidelines, to identify strong areas of knowledge, trends and gaps. RESULTS. We identified 107 relevant contributions. They were classified according to four facets: evolution activity (e.g., identify, analyze and plan, implement), product-derivation approach (e.g., annotation-based, composition-based), research type (e.g., solution, experience, evaluation), and asset type (i.e., variability model, {\{}SPL{\}} architecture, code assets and products). CONCLUSION. Analyses of the results indicate that “Solution proposals? are the most common type of contribution (31{\%}). Regarding the evolution activity, “Implement change? (43{\%}) and “Analyze and plan change? (37{\%}) are the most covered ones. A finer-grained analysis uncovered some tasks as being underexposed. A detailed description of the 107 papers is also included. },
	Doi = {https://doi.org/10.1016/j.jss.2016.08.053},
	ISSN = {0164-1212},
	Keywords = {Evolution,Software product lines,Systematic mapping study},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121216301510}
}

@InProceedings{Morasca:2009:BSS:1540438.1540465,
	Title = {{Building Statistically Significant Robust Regression Models in Empirical Software Engineering}},
	Author = {Morasca, Sandro},
	Booktitle = {Proceedings of the 5th International Conference on Predictor Models in Software Engineering},
	Year = {2009},
	Address = {New York, NY, USA},
	Pages = {19:1----19:10},
	Publisher = {ACM},
	Series = {PROMISE '09},
	Doi = {10.1145/1540438.1540465},
	ISBN = {978-1-60558-634-2},
	Keywords = {application,data analysis,defect prediction,effort prediction,outliers,robust regression,statistical significance},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/1540438.1540465}
}

@Article{Moros2013941,
	Title = {{Transforming and tracing reused requirements models to home automation models}},
	Author = {Moros, Bego{\~{n}}a and Toval, Ambrosio and Rosique, Francisca and S{\'{a}}nchez, Pedro},
	Journal = {Information and Software Technology},
	Year = {2013},
	Number = {6},
	Pages = {941--965},
	Volume = {55},
	Abstract = {AbstractContext Model-Driven Software Development (MDSD) has emerged as a very promising approach to cope with the inherent complexity of modern software-based systems. Furthermore, it is well known that the Requirements Engineering (RE) stage is critical for a project's success. Despite the importance of RE, {\{}MDSD{\}} approaches commonly leave textual requirements specifications to one side. Objective Our aim is to integrate textual requirements specifications into the {\{}MDSD{\}} approach by using the {\{}MDSD{\}} techniques themselves, including metamodelling and model transformations. The proposal is based on the assumption that a reuse-based Model-Driven Requirements Engineering (MDRE) approach will improve the requirements engineering stage, the quality of the development models generated from requirements models, and will enable the traces from requirements to other development concepts (such as analysis or design) to be maintained. Method The approach revolves around the Requirements Engineering Metamodel, denominated as REMM, which supports the definition of the boilerplate based textual requirements specification languages needed for the definition of model transformation from application requirements models to platform-specific application models and code. Results The approach has been evaluated through its application to Home Automation (HA) systems. The {\{}HA{\}} Requirement Specification Language denominated as {\{}HAREL{\}} is used to define application requirements models which will be automatically transformed and traced to the application model conforming to the {\{}HA{\}} Domain Specific Language. Conclusions An anonymous online survey has been conducted to evaluate the degree of acceptance by both {\{}HA{\}} application developers and {\{}MDSD{\}} practitioners. The main conclusion is that 66.7{\%} of the {\{}HA{\}} experts polled strongly agree that the automatic transformation of the requirements models to {\{}HA{\}} models improves the quality of the {\{}HA{\}} models. Moreover, 58.3{\%} of the {\{}HA{\}} participants strongly agree with the usefulness of the traceability matrix which links requirements to {\{}HA{\}} functional units in order to discover which devices are related to a specific requirement. We can conclude that the experts we have consulted agree with the proposal we are presenting here, since the average mark given is 4 out of 5. },
	Doi = {https://doi.org/10.1016/j.infsof.2012.12.003},
	ISSN = {0950-5849},
	Keywords = {Home automation models,Model driven software development,Models transformation,Requirements metamodel,Requirements reuse,Requirements traceability},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584912002388}
}

@Article{Mosser20131035,
	Title = {{“Adore?, a logical meta-model supporting business process evolution}},
	Author = {Mosser, S{\'{e}}bastien and Blay-Fornarino, Mireille},
	Journal = {Science of Computer Programming},
	Year = {2013},
	Number = {8},
	Pages = {1035--1054},
	Volume = {78},
	Abstract = {The Service Oriented Architecture (Soa) paradigm supports the assembly of atomic services to create applications that implement complex business processes. Since “real-life? processes can be very complex, composition mechanisms inspired by the Separation of Concerns paradigm (e.g. features, aspects) are good candidates to support the definition and the upcoming evolutions of large systems. We propose Adore, “an Activity meta-moDel supOrting oRchestration Evolution? to address this issue. The Adore meta-model allows process designers to express in the same formalism business processes and fragments of processes. Such fragments define additional activities that aim to be integrated into other processes and adequately support their evolution. The underlying logical foundations of Adore allow the definition of interference detection rules as logical predicate, as well as the definition of consistency properties on Adore models. Consequently, the Adore framework supports process designers while they design and then apply evolutions on large processes, managing the detection of interferences among fragments and ensuring that the composed processes are consistent and do not depend on the order of the composition. },
	Annote = {Special section on software evolution, adaptability, and maintenance {\&} Special section on the Brazilian Symposium on Programming Languages},
	Doi = {https://doi.org/10.1016/j.scico.2012.06.009},
	ISSN = {0167-6423},
	Keywords = {Business processes,Logical composition,SOA,Sep. of concerns},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642312001220}
}

@InProceedings{Murugesupillai:2011:PMS:2019136.2019149,
	Title = {{A Preliminary Mapping Study of Approaches Bridging Software Product Lines and Service-oriented Architectures}},
	Author = {Murugesupillai, Esan and Mohabbati, Bardia and Ga{\v{s}}evi{\'{c}}, Dragan},
	Booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
	Year = {2011},
	Address = {New York, NY, USA},
	Pages = {11:1----11:8},
	Publisher = {ACM},
	Series = {SPLC '11},
	Doi = {10.1145/2019136.2019149},
	ISBN = {978-1-4503-0789-5},
	Keywords = {service-oriented architecture,service-oriented product line,software product line,software variability,variability management},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2019136.2019149}
}

@Article{Myllarniemi200673,
	Title = {{Inter-organisational approach in rapid software product family development - A case study}},
	Author = {Myll{\"{a}}rniemi, V and Raatikainen, M and M{\"{a}}nnist{\"{o}}, T},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2006},
	Pages = {73--86},
	Volume = {4039 LNCS},
	Abstract = {Software product families provide an efficient means of reuse between a set of related products. However, software product families are often solely associated with intra-organisational reuse. This paper presents a case study of Fathammer, a small company developing games for different mobile devices. Reuse at Fathammer takes place at multiple levels. The game framework and engine of Fathammer is reused by partner companies that in turn produce game assets to be reused by Fathammer while developing games for various devices. Very rapid development of games is a necessity for Fathammer, whereas maintainability of games is not important. The above characteristics in particular distinguish Fathammer from other case studies and practices usually presented in the product family literature. The results show the applicability and challenges of software product family practices in the context of multiple collaborating companies and a fast-changing domain. {\textcopyright} Springer-Verlag Berlin Heidelberg 2006.},
	Annote = {cited By 3},
	Keywords = {Animation; Computer software reusability; Computer,Inter-organisational approach; Product family dev,Software engineering},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746216409{\&}partnerID=40{\&}md5=8bb8b10a47429fbecd1f5282101ed665}
}

@Article{Myllarniemi20161623,
	Title = {{Performance variability in software product lines: proposing theories from a case study}},
	Author = {Myll{\"{a}}rniemi, V and Savolainen, J and Raatikainen, M and M{\"{a}}nnist{\"{o}}, T},
	Journal = {Empirical Software Engineering},
	Year = {2016},
	Number = {4},
	Pages = {1623--1669},
	Volume = {21},
	Abstract = {In the software product line research, product variants typically differ by their functionality and quality attributes are not purposefully varied. The goal is to study purposeful performance variability in software product lines, in particular, the motivation to vary performance, and the strategy for realizing performance variability in the product line architecture. The research method was a theory-building case study that was augmented with a systematic literature review. The case was a mobile network base station product line with capacity variability. The data collection, analysis and theorizing were conducted in several stages: the initial case study results were augmented with accounts from the literature. We constructed three theoretical models to explain and characterize performance variability in software product lines: the models aim to be generalizable beyond the single case. The results describe capacity variability in a base station product line. Thereafter, theoretical models of performance variability in software product lines in general are proposed. Performance variability is motivated by customer needs and characteristics, by trade-offs and by varying operating environment constraints. Performance variability can be realized by hardware or software means; moreover, the software can either realize performance differences in an emergent way through impacts from other variability or by utilizing purposeful varying design tactics. The results point out two differences compared with the prevailing literature. Firstly, when the customer needs and characteristics enable price differentiation, performance may be varied even with no trade-offs or production cost differences involved. Secondly, due to the dominance of feature modeling, the literature focuses on the impact management realization. However, performance variability can be realized through purposeful design tactics to downgrade the available software resources and by having more efficient hardware. {\textcopyright} 2015, Springer Science+Business Media New York.},
	Annote = {cited By 0},
	Doi = {10.1007/s10664-014-9359-z},
	Keywords = {Base stations; Commerce; Computer software; Econom,Operating environment; Performance variability; P,Software design},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923360642{\&}doi=10.1007{\%}2Fs10664-014-9359-z{\&}partnerID=40{\&}md5=f7cd2d20470c075b0a65d80d8de67744}
}

@Conference{Nadi2014140,
	Title = {{Mining configuration constraints: Static analyses and empirical results}},
	Author = {Nadi, S and Berger, T and K{\"{a}}stner, C and Czarnecki, K},
	Booktitle = {Proceedings - International Conference on Software Engineering},
	Year = {2014},
	Number = {CONFCODENUMBER},
	Pages = {140--151},
	Abstract = {Highly-configurable systems allow users to tailor the software to their specific needs. Not all combinations of configuration options are valid though, and constraints arise for technical or non-technical reasons. Explicitly describing these constraints in a variability model allows reasoning about the supported configurations. To automate creating variability models, we need to identify the origin of such configuration constraints. We propose an approach which uses build-time errors and a novel feature-effect heuristic to automatically extract configuration constraints from C code. We conduct an empirical study on four highly-configurable open-source systems with existing variability models having three objectives in mind: evaluate the accuracy of our approach, determine the recoverability of existing variability-model constraints using our analysis, and classify the sources of variability-model constraints. We find that both our extraction heuristics are highly accurate (93{\%} and 77{\%} respectively), and that we can recover 19{\%} of the existing variability-models using our approach. However, we find that many of the remaining constraints require expert knowledge or more expensive analyses. We argue that our approach, tooling, and experimental results support researchers and practitioners working on variability model re-engineering, evolution, and consistency-checking techniques. {\textcopyright} 2014 ACM.},
	Annote = {cited By 34},
	Doi = {10.1145/2568225.2568283},
	Keywords = {C (programming language); Hierarchical systems; Op,Configuration constraints; Configuration options;,Static analysis},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994165595{\&}doi=10.1145{\%}2F2568225.2568283{\&}partnerID=40{\&}md5=5c2a0a81c01b1c1a6e7b881320bf850a}
}

@Article{SMR:SMR1595,
	Title = {{The Linux kernel: a case study of build system variability}},
	Author = {Nadi, Sarah and Holt, Ric},
	Journal = {Journal of Software: Evolution and Process},
	Year = {2014},
	Number = {8},
	Pages = {730--746},
	Volume = {26},
	Abstract = {Although build systems control what code gets compiled into the final built product, they are often overlooked when studying software variability. The Linux kernel is one of the biggest open source software systems supporting variability and contains over 10,000 configurable features described in its Kconfig files. To understand the role of the build system in variability implementation, we use Linux as a case study. We study its build system, Kbuild, and extract the variability constraints in its Makefiles. We first provide a quantitative analysis of the variability in Kbuild. We then study how the variability constraints in the build system affect variability anomalies detected in Linux. We concentrate on dead and undead artifacts, and by extending previous work, we show that considering build system variability constraints allows more anomalies to be detected. We provide examples of such anomalies on both the code block and source file level. Our work shows that Kbuild contains a large percentage of the variability information in Linux, so it should not be ignored during variability analysis. Nonetheless, the anomalies we find suggest that variability on the file level in Kbuild is consistent with Kconfig, whereas the constraints on the code level are harder to keep consistent with both Kbuild and Kconfig. Copyright {\textcopyright} 2013 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/smr.1595},
	ISSN = {2047-7481},
	Keywords = {Kbuild,Linux,build systems,software variability,variability anomalies},
	Url = {http://dx.doi.org/10.1002/smr.1595}
}

@InProceedings{Nagappan2006,
	Title = {{Using Historical In-Process and Product Metrics for Early Estimation of Software Failures}},
	Author = {Nagappan, Nachiappan and Ball, Thomas and Murphy, Brendan},
	Booktitle = {2006 17th International Symposium on Software Reliability Engineering},
	Year = {2006},
	Month = {nov},
	Pages = {62--74},
	Publisher = {IEEE},
	Doi = {10.1109/ISSRE.2006.50},
	ISBN = {0-7695-2684-5},
	ISSN = {1071-9458},
	Url = {http://ieeexplore.ieee.org/document/4021972/}
}

@Article{Nakagawa2013985,
	Title = {{Relevance and perspectives of {\{}AAL{\}} in Brazil}},
	Author = {Nakagawa, Elisa Y and Antonino, Pablo O and Becker, Martin and Maldonado, Jos{\'{e}} C and Storf, Holger and Villela, Karina B and Rombach, Dieter},
	Journal = {Journal of Systems and Software},
	Year = {2013},
	Number = {4},
	Pages = {985--996},
	Volume = {86},
	Abstract = {Population aging has been taking place in many countries across the globe and more recently in emerging countries. In this context, Ambient Assisted Living (AAL) has become one focus of attention, including methods, products, services, and {\{}AAL{\}} software systems that support the everyday lives of elderly people, promoting mainly their independence and dignity. From the perspective of computer science, efforts are already being dedicated to adequately developing {\{}AAL{\}} systems. However, in spite of its relevance, {\{}AAL{\}} has not been properly investigated in emerging countries, including Brazil. Thus, the contribution of this paper is to present the main perspectives of research in AAL, in particular in the area of software engineering, considering that the Brazilian population is also subject to the aging process. The main intention of this paper is to raise the interest of Brazilian researchers, as well as government and industry, for this important area. },
	Annote = {{\{}SI{\}} : Software Engineering in Brazil: Retrospective and Prospective Views},
	Doi = {https://doi.org/10.1016/j.jss.2012.10.013},
	ISSN = {0164-1212},
	Keywords = {AAL platform,Ambient Assisted Living (AAL),Population aging,Reference architecture},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121212002841}
}

@Article{Nakagawa20111670,
	Title = {{An aspect-oriented reference architecture for Software Engineering Environments}},
	Author = {Nakagawa, Elisa Y and Ferrari, Fabiano C and Sasaki, Mariela M F and Maldonado, Jos{\'{e}} C},
	Journal = {Journal of Systems and Software},
	Year = {2011},
	Number = {10},
	Pages = {1670--1684},
	Volume = {84},
	Abstract = {Reusable and evolvable Software Engineering Environments (SEEs) are essential to software production and have increasingly become a need. In another perspective, software architectures and reference architectures have played a significant role in determining the success of software systems. In this paper we present a reference architecture for SEEs, named RefASSET, which is based on concepts coming from the aspect-oriented approach. This architecture is specialized to the software testing domain and the development of tools for that domain is discussed. This and other case studies have pointed out that the use of aspects in RefASSET provides a better Separation of Concerns, resulting in reusable and evolvable SEEs. },
	Doi = {https://doi.org/10.1016/j.jss.2011.04.052},
	ISSN = {0164-1212},
	Keywords = {Aspect orientation,Reference architecture,Software Engineering Environment,Software architecture,Software testing},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121211001038}
}

@Article{Narwane2014212,
	Title = {{A cost effective approach for analyzing software product lines}},
	Author = {Narwane, G K and Krishna, S N and Bhattacharjee, A K},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2014},
	Pages = {212--223},
	Volume = {8337 LNCS},
	Abstract = {In the area of Software Product Lines(SPL), most of the research work focuses on automated analysis of SPLs and the traceability relation between the problem domain and solution domain. An SPL with few features can generate billions of products; to analyze such a large product space, we need efficient analysis operations. For a given specification, we can get many possible implementations; choosing one implementation from this is a non-trivial task. In this paper, we extend the work on analyzing software product lines to propose a cost effective approach that fetches products from a given SPL based on various factors. When there are multiple implementations for a given specification, then it is the cost factors which determine the product selection. To this end, we propose a revised formal framework for SPLs with cost factors. This approach has been implemented in a tool SPLANE-CF (SPL Analysis Engine with Cost Factors). We illustrate the efficiency of SPLANE-CF on a fairly large size case study. {\textcopyright} 2014 Springer International Publishing Switzerland.},
	Annote = {cited By 0},
	Doi = {10.1007/978-3-319-04483-5_22},
	Keywords = {Automated analysis; Cost-effective approach; Effi,Computer software; Cost effectiveness; Costs; Inte,Cost benefit analysis},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958534666{\&}doi=10.1007{\%}2F978-3-319-04483-5{\_}22{\&}partnerID=40{\&}md5=5f3ce1b31fba5eef3c2ba304c81841a1}
}

@Article{Nasir20159,
	Title = {{Fault-tolerant context development and requirement validation in {\{}ERP{\}} systems}},
	Author = {Nasir, S Zafar and Mahmood, Tariq and Shaikh, M Shahid and Shaikh, Zubair A},
	Journal = {Computer Standards {\&} Interfaces},
	Year = {2015},
	Pages = {9--19},
	Volume = {37},
	Abstract = {Abstract This paper presents context development and requirement validation to overcome maintenance problems in Enterprise Resource Planning (ERP) systems. Using {\{}ERP{\}} data of a local petroleum firm, we employ knowledge integration to dynamically validate users' requirements, and to gather, analyze, and represent context through knowledge models. We also employ context-awareness to model the {\{}ERP{\}} context, along with a user requirement model. We employ context affinity to determine impact of these models on requirements' validation. We apply fault-tolerance on these models by using data mining to pre-identify delays in delivery of petroleum products, and to predict faulty contextual {\{}ERP{\}} product configuration. },
	Doi = {https://doi.org/10.1016/j.csi.2014.05.001},
	ISSN = {0920-5489},
	Keywords = {Context development,Data Mining,Enterprise Resource Planning,Fault tolerance,Requirement validation},
	Url = {http://www.sciencedirect.com/science/article/pii/S0920548914000695}
}

@Article{Nasr201782,
	Title = {{Automated extraction of product comparison matrices from informal product descriptions}},
	Author = {Nasr, S B and B{\'{e}}can, G and Acher, M and {Ferreira Filho}, J B and Sannier, N and Baudry, B and Davril, J.-M.},
	Journal = {Journal of Systems and Software},
	Year = {2017},
	Pages = {82--103},
	Volume = {124},
	Abstract = {Domain analysts, product managers, or customers aim to capture the important features and differences among a set of related products. A case-by-case reviewing of each product description is a laborious and time-consuming task that fails to deliver a condense view of a family of product. In this article, we investigate the use of automated techniques for synthesizing a product comparison matrix (PCM) from a set of product descriptions written in natural language. We describe a tool-supported process, based on term recognition, information extraction, clustering, and similarities, capable of identifying and organizing features and values in a PCM – despite the informality and absence of structure in the textual descriptions of products. We evaluate our proposal against numerous categories of products mined from BestBuy. Our empirical results show that the synthesized PCMs exhibit numerous quantitative, comparable information that can potentially complement or even refine technical descriptions of products. The user study shows that our automatic approach is capable of extracting a significant portion of correct features and correct values. This approach has been implemented in MatrixMiner a web environment with an interactive support for automatically synthesizing PCMs from informal product descriptions. MatrixMiner also maintains traceability with the original descriptions and the technical specifications for further refinement or maintenance by users. {\textcopyright} 2016 Elsevier Inc.},
	Annote = {cited By 0},
	Doi = {10.1016/j.jss.2016.11.018},
	Keywords = {Automated extraction; Automated techniques; Autom,Hardware; Software engineering,Reverse engineering},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996593342{\&}doi=10.1016{\%}2Fj.jss.2016.11.018{\&}partnerID=40{\&}md5=6c44aaadfbc3fb4cbc5292ddb3503d98}
}

@Article{Nassif2013144,
	Title = {{Towards an early software estimation using log-linear regression and a multilayer perceptron model}},
	Author = {Nassif, Ali Bou and Ho, Danny and Capretz, Luiz Fernando},
	Journal = {Journal of Systems and Software},
	Year = {2013},
	Number = {1},
	Pages = {144--160},
	Volume = {86},
	Abstract = {Software estimation is a tedious and daunting task in project management and software development. Software estimators are notorious in predicting software effort and they have been struggling in the past decades to provide new models to enhance software estimation. The most critical and crucial part of software estimation is when estimation is required in the early stages of the software life cycle where the problem to be solved has not yet been completely revealed. This paper presents a novel log-linear regression model based on the use case point model (UCP) to calculate the software effort based on use case diagrams. A fuzzy logic approach is used to calibrate the productivity factor in the regression model. Moreover, a multilayer perceptron (MLP) neural network model was developed to predict software effort based on the software size and team productivity. Experiments show that the proposed approach outperforms the original {\{}UCP{\}} model. Furthermore, a comparison between the {\{}MLP{\}} and log-linear regression models was conducted based on the size of the projects. Results demonstrate that the {\{}MLP{\}} model can surpass the regression model when small projects are used, but the log-linear regression model gives better results when estimating larger projects. },
	Doi = {https://doi.org/10.1016/j.jss.2012.07.050},
	ISSN = {0164-1212},
	Keywords = {Log-linear regression model,Multilayer perceptron,Software effort estimation,Use case points},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121212002221}
}

@Article{Navas20131073,
	Title = {{Reconciling run-time evolution and resource-constrained embedded systems through a component-based development framework}},
	Author = {Navas, Juan F and Babau, Jean-Philippe and Pulou, Jacques},
	Journal = {Science of Computer Programming},
	Year = {2013},
	Number = {8},
	Pages = {1073--1098},
	Volume = {78},
	Abstract = {This paper deals with the evolution of embedded systems software at run-time. To accomplish such software evolution activities in resource-constrained embedded systems, we propose a component-based, execution time evolution infrastructure, that reconciles richness of evolution alternatives and performance requirements. The proposition is based on fine-grained optimization of embedded components, and on off-site component reifications called mirrors, which are representations of components that allow us to treat evolution concerns remotely and hence to reduce the memory footprint. An evaluation on a real-world evolution scenario shows the efficiency and relevance of our approach. },
	Annote = {Special section on software evolution, adaptability, and maintenance {\&} Special section on the Brazilian Symposium on Programming Languages},
	Doi = {https://doi.org/10.1016/j.scico.2012.08.004},
	ISSN = {0167-6423},
	Keywords = {Architecture,Components,Embedded software,Evolution,Optimization,Reifications},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642312001633}
}

@Article{SPIP:SPIP327,
	Title = {{Evolving strategies for software architecture and reuse}},
	Author = {Nedstam, Josef and Staples, Mark},
	Journal = {Software Process: Improvement and Practice},
	Year = {2007},
	Number = {3},
	Pages = {295--309},
	Volume = {12},
	Abstract = {To achieve their business objectives, software developing companies employ different technical and managerial strategies concerning architecture and reuse. These strategies include component-based development, software platforms, product lines and highly configurable code bases. Frameworks for describing these strategies have recently emerged, presenting them in orders of ‘increasing maturity', with researchers declaring specific architectural strategies to be more mature than others. Such frameworks can be useful in helping a company realize a particular architectural strategy but they do not provide guidelines concerning which architectural strategies are appropriate for companies in particular situations.Different companies have different needs—the business context and business goals of a company will determine which architectural strategy is most suitable for that company. There is no universally ‘most mature' strategy. In this article, we have studied architectural situations in thirteen companies in order to determine why and how these companies have moved between architectural strategies and how these relate to reuse and business goals and conditions. We present a framework for describing these and provide guidelines for companies about how to traverse the maze of architectural evolution. Copyright {\textcopyright} 2007 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spip.327},
	ISSN = {1099-1670},
	Keywords = {architectural evolution,empirical software engineering,software architecture,strategic reuse},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/spip.327}
}

@Article{Nekvi:2014:IRC:2666081.2629432,
	Title = {{Impediments to Regulatory Compliance of Requirements in Contractual Systems Engineering Projects: A Case Study}},
	Author = {Nekvi, Md Rashed I and Madhavji, Nazim H},
	Journal = {ACM Trans. Manage. Inf. Syst.},
	Year = {2014},
	Number = {3},
	Pages = {15:1----15:35},
	Volume = {5},
	Address = {New York, NY, USA},
	Doi = {10.1145/2629432},
	ISSN = {2158-656X},
	Keywords = {Legal requirements elicitation,compliance of requirements,effort estimation,rail infrastructure system},
	Publisher = {ACM},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2629432}
}

@Article{CostaNeto20132333,
	Title = {{A design rule language for aspect-oriented programming}},
	Author = {Neto, Alberto Costa and Bonif{\'{a}}cio, Rodrigo and Ribeiro, M{\'{a}}rcio and Pontual, Carlos Eduardo and Borba, Paulo and Castor, Fernando},
	Journal = {Journal of Systems and Software},
	Year = {2013},
	Number = {9},
	Pages = {2333--2356},
	Volume = {86},
	Abstract = {Abstract Aspect-oriented programming is known as a technique for modularizing crosscutting concerns. However, constructs aimed to support crosscutting modularity might actually break class modularity. As a consequence, class developers face changeability, parallel development and comprehensibility problems, because they must be aware of aspects whenever they develop or maintain a class. At the same time, aspects are vulnerable to changes in classes, since there is no contract specifying the points of interaction amongst these elements. These problems can be mitigated by using adequate design rules between classes and aspects. We present a design rule specification language and explore its benefits since the initial phases of the development process, specially with the aim of supporting modular development of classes and aspects. We discuss how our language improves crosscutting modularity without breaking class modularity. We evaluate it using a real case study and compare it with other approaches. },
	Doi = {https://doi.org/10.1016/j.jss.2013.03.104},
	ISSN = {0164-1212},
	Keywords = {Aspect-oriented programming,Design rules,Modularity},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121213000861}
}

@Article{SouzaNeto201684,
	Title = {{Designing service-based applications in the presence of non-functional properties: A mapping study}},
	Author = {Neto, Pl{\'{a}}cido A Souza and Vargas-Solar, Genoveva and da Costa, Umberto Souza and Musicante, Martin A},
	Journal = {Information and Software Technology},
	Year = {2016},
	Pages = {84--105},
	Volume = {69},
	Abstract = {AbstractContext The development of distributed software systems has become an important problem for the software engineering community. Service-based applications are a common solution for this kind of systems. Services provide a uniform mechanism for discovering, integrating and using these resources. In the development of service based applications not only the functionality of services and compositions should be considered, but also conditions in which the system operates. These conditions are called non-functional requirements (NFR). The conformance of applications to {\{}NFR{\}} is crucial to deliver software that meets the expectations of its users. Objective This paper presents the results of a systematic mapping carried out to analyze how {\{}NFR{\}} have been addressed in the development of service-based applications in the last years, according to different points of view. Method Our analysis applies the systematic mapping approach. It focuses on the analysis of publications organized by categories called facets, which are combined to answer specific research questions. The facets compose a classification schema which is part of the contribution and results. Results This paper presents our findings on how {\{}NFR{\}} have been supported in the development of service-based applications by proposing a classification scheme consisting in five facets: (i) programming paradigm (object/service oriented); (ii) contribution (methodology, system, middleware); (iii) software process phase; (iv) technique or mathematical model used for expressing NFR; and (v) the types of {\{}NFR{\}} addressed by the papers, based on the classification proposed by the ISO/IEC 9126 specification. The results of our systematic mapping are presented as bubble charts that provide a quantitative analysis to show the frequencies of publications for each facet. The paper also proposes a qualitative analysis based on these plots. This analysis discusses how {\{}NFR{\}} (quality properties) have been addressed in the design and development of service-based applications, including methodologies, languages and tools devised to support different phases of the software process. Conclusion This systematic mapping showed that {\{}NFR{\}} are not fully considered in all software engineering phases for building service based applications. The study also let us conclude that work has been done for providing models and languages for expressing {\{}NFR{\}} and associated middleware for enforcing them at run time. An important finding is that {\{}NFR{\}} are not fully considered along all software engineering phases and this opens room for proposing methodologies that fully model NFR. The data collected by our work and used for this systematic mapping are available in https://github.com/placidoneto/systematic-mapping{\_}service-based-app{\_}nfr. },
	Doi = {https://doi.org/10.1016/j.infsof.2015.09.004},
	ISSN = {0950-5849},
	Keywords = {Non-functional requirements,Service-based software process,Systematic mapping},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584915001573}
}

@Article{Neves201542,
	Title = {{Safe evolution templates for software product lines}},
	Author = {Neves, L and Borba, P and Alves, V and Turnes, L and Teixeira, L and Sena, D and Kulesza, U},
	Journal = {Journal of Systems and Software},
	Year = {2015},
	Pages = {42--58},
	Volume = {106},
	Abstract = {Abstract Software product lines enable generating related software products from reusable assets. Adopting a product line strategy can bring significant quality and productivity improvements. However, evolving a product line can be risky, since it might impact many products. When introducing new features or improving its design, it is important to make sure that the behavior of existing products is not affected. To ensure that, one usually has to analyze different types of artifacts, an activity that can lead to errors. To address this issue, in this work we discover and analyze concrete evolution scenarios from five different product lines. We discover a total of 13 safe evolution templates, which are generic transformations that developers can apply when evolving compositional and annotative product lines, with the goal of preserving the behavior of existing products. We also evaluate the templates by analyzing the evolution history of these product lines. In this evaluation, we observe that the templates can address the modifications that developers performed in the analyzed scenarios, which corroborates the expressiveness of our template set. We also observe that the templates could also have helped to avoid the errors that we identified during our analysis. },
	Doi = {https://doi.org/10.1016/j.jss.2015.04.024},
	ISSN = {0164-1212},
	Keywords = {Evolution,Refinement,Software product lines},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121215000801}
}

@Article{Ng201566,
	Title = {{Integrating software engineering theory and practice using essence: A case study}},
	Author = {Ng, Pan-Wei},
	Journal = {Science of Computer Programming},
	Year = {2015},
	Pages = {66--78},
	Volume = {101},
	Abstract = {Abstract Software engineering is complex and success depends on many inter-related factors. Theory Based Software Engineering (TBSE) is about providing a practical way for software teams to understand the relationships and the influence of these factors to thereby adapt the way they work. This paper proposes an approach to {\{}TBSE{\}} based on Essence, a software engineering kernel distilled by the {\{}SEMAT{\}} (Software Engineering Method and Theory) initiative. Essence supports {\{}TBSE{\}} by providing a domain model that is useful for organizing and relating software engineering factors. Essence also helps make recommended practices precise and actionable to software teams. We provide a step-by-step application of our approach on an industrial software process improvement case study. The case study achieved 21{\%} productivity gains and 58{\%} decrease in defects. But more importantly than these results, it demonstrates the value of Essence in supporting TBSE. },
	Annote = {Towards general theories of software engineering},
	Doi = {https://doi.org/10.1016/j.scico.2014.11.009},
	ISSN = {0167-6423},
	Keywords = {Essence,Kernel,SEMAT,Software engineering,Theory},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642314005413}
}

@Article{STVR:STVR439,
	Title = {{Automated verification and testing of user-interactive undo features in database applications}},
	Author = {Ngo, Minh Ngoc and Tan, Hee Beng Kuan},
	Journal = {Software Testing, Verification and Reliability},
	Year = {2012},
	Number = {4},
	Pages = {245--265},
	Volume = {22},
	Abstract = {User-interactive undo is a recovery facility that enables users to correct mistakes easily by canceling or re-executing operations that have already been executed. This paper presents an interesting common structural property that has been discovered in programs that implementing user-interactive undo features. The property shows that there is a one-to-one correspondence between program statements that raise erroneous effects and those statements that can undo these effects. Statistical validation has been conducted which gives evidences to show that this property holds for 99 per cent of all the cases. An approach for automated verification of user-interactive undo features in database applications through the use of this empirical property is further proposed. Based on the verification results, test cases are automatically generated to confirm the correctness of these features. A case study has been conducted to evaluate the performance of the proposed verification and testing approach in terms of fault detection capability. Copyright {\textcopyright} 2010 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/stvr.439},
	ISSN = {1099-1689},
	Keywords = {database applications,empirical properties,software testing,user-interactive undo,verification},
	Publisher = {John Wiley {\&} Sons, Ltd},
	Url = {http://dx.doi.org/10.1002/stvr.439}
}

@Article{Nguyen2013321,
	Title = {{A web services variability description language (WSVL) for business users oriented service customization}},
	Author = {Nguyen, T and Colman, A and Han, J},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2013},
	Pages = {321--334},
	Volume = {7652 LNCS},
	Abstract = {To better facilitate business users in customizing Web services, customization options need to be described at a high level of abstraction. In contrast to related efforts that describe customization options at the technical level of service description, we propose a Web Services Variability description Language (WSVL) that facilitates the representation of such options at business level. The language has several advantages. Firstly, it does not require people, who perform customization, to have knowledge of Web service technologies. Thus, the language enables business users-friendly service customization. Secondly, the language captures not only what can be customized, but also how and where customization operations should happen in a service-oriented way. This self-described property removes the need for a separate procedure for governing service customization. Consequently, this property eases the adoption of the language. We elaborate the design of the language using a case study and describe its usages from both consumers and providers' viewpoints. {\textcopyright} 2013 Springer-Verlag.},
	Annote = {cited By 0},
	Doi = {10.1007/978-3-642-38333-5_34},
	Keywords = {Feature modeling; Service customization; Service d,Quality of service; Systems engineering; Web serv,Websites},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891308404{\&}doi=10.1007{\%}2F978-3-642-38333-5{\_}34{\&}partnerID=40{\&}md5=9f61afa271af6d2fde8e81f80bbf48c3}
}

@Article{Nicolas20091291,
	Title = {{On the generation of requirements specifications from software engineering models: A systematic literature review}},
	Author = {Nicol{\'{a}}s, Joaqu{\'{i}}n and Toval, Ambrosio},
	Journal = {Information and Software Technology},
	Year = {2009},
	Number = {9},
	Pages = {1291--1307},
	Volume = {51},
	Abstract = {System and software requirements documents play a crucial role in software engineering in that they must both communicate requirements to clients in an understandable manner and define requirements in precise detail for system developers. The benefits of both lists of textual requirements (usually written in natural language) and software engineering models (usually specified in graphical form) can be brought together by combining the two approaches in the specification of system and software requirements documents. If, moreover, textual requirements are generated from models in an automatic or closely monitored form, the effort of specifying those requirements is reduced and the completeness of the specification and the management of the requirements traceability are improved. This paper presents a systematic review of the literature related to the generation of textual requirements specifications from software engineering models. },
	Doi = {https://doi.org/10.1016/j.infsof.2009.04.001},
	ISSN = {0950-5849},
	Keywords = {Requirements document generation from software eng,Specification generation from software engineering,Systematic literature review,Textual requirements generation from software engi},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584909000378}
}

@Article{SMR:SMR1734,
	Title = {{An optimization-based tool to support the cost-effective production of software architecture documentation}},
	Author = {Nicoletti, Matias and Schiaffino, Silvia and Diaz-Pace, J Andres},
	Journal = {Journal of Software: Evolution and Process},
	Year = {2015},
	Number = {9},
	Pages = {674--699},
	Volume = {27},
	Abstract = {Some of the challenges faced by most software projects are tight budget constraints and schedules, which often make managers and developers prioritize the delivery of a functional product over other engineering activities, such as software documentation. In particular, having little or low-quality documentation of the software architecture of a system can have negative consequences for the project, as the architecture is the main container of the key design decisions to fulfill the stakeholders' goals. To further complicate this situation, generating and maintaining architectural documentation is a non-trivial and time-consuming activity. In this context, we present a tool approach that aims at (i) assisting the documentation writer in their tasks and (ii) ensuring a cost-effective documentation process by means of optimization techniques. Our tool, called SADHelper, follows the principle of producing reader-oriented documentation, in order to focus the available, and often limited, resources on generating just enough documentation that satisfies the stakeholders' concerns. The approach was evaluated in two experiments with users of software architecture documents, with encouraging results. These results show evidence that our tool can be useful to reduce the documentation costs and even improve the documentation quality, as perceived by their stakeholders. Copyright {\textcopyright} 2015 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/smr.1734},
	ISSN = {2047-7481},
	Keywords = {architecture documentation,multi-objective optimization,software architecture,stakeholders,tool support,wikis},
	Url = {http://dx.doi.org/10.1002/smr.1734}
}

@Article{Nieke2016563,
	Title = {{User profiles for context-aware reconfiguration in software product lines}},
	Author = {Nieke, M and Mauro, J and Seidl, C and Yu, I C},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2016},
	Pages = {563--578},
	Volume = {9953 LNCS},
	Abstract = {Software Product Lines (SPLs) are a mechanism to capture families of closely related software systems by modeling commonalities and variability. Although user customization has a growing importance in software systems and is a vital sales argument, SPLs currently only allow user customization at deploy-time. In this paper, we extend the notion of context-aware SPLs by means of user profiles, containing a linearly ordered set of preferences. Preferences have priorities, meaning that a low priority preference can be neglected in favor of a higher prioritized one. We present a reconfiguration engine checking the validity of the current configuration and, if necessary, reconfiguring the SPL while trying to fulfill the preferences of the active user profile. Thus, users can be assured about the reconfiguration engine providing the most suitable configuration for them. Moreover, we demonstrate the feasibility of our approach using a case study based on existing car customizability. {\textcopyright} Springer International Publishing AG 2016.},
	Annote = {cited By 1},
	Doi = {10.1007/978-3-319-47169-3_44},
	Keywords = {Computer software,Context- awareness; Dynamic software product line,Engines; Formal methods; Reconfigurable hardware;},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994052020{\&}doi=10.1007{\%}2F978-3-319-47169-3{\_}44{\&}partnerID=40{\&}md5=0d33fcf39f92d98c12a680d97977251d}
}

@Article{Niemela20071107,
	Title = {{Capturing quality requirements of product family architecture}},
	Author = {Niemel{\"{a}}, E and Immonen, A},
	Journal = {Information and Software Technology},
	Year = {2007},
	Number = {11-12},
	Pages = {1107--1120},
	Volume = {49},
	Abstract = {Software quality is one of the major issues with software intensive systems. Moreover, quality is a critical success factor in software product families exploiting shared architecture and common components in a set of products. Our contribution is the QRF (Quality Requirements of a software Family) method, which explicitly focuses on how quality requirements have to be defined, represented and transformed to architectural models. The method has been applied to two experiments; one in a laboratory environment and the other in industry. The use of the QRF method is exemplified by the Distribution Service Platform (DiSeP), the laboratory experiment. The lessons learned are also based on our experiences of applying the method in industrial settings. {\textcopyright} 2006 Elsevier B.V. All rights reserved.},
	Annote = {cited By 35},
	Doi = {10.1016/j.infsof.2006.11.003},
	File = {:Users/mac/Downloads/Sciencedirect{\_}articles{\_}13Jun2017{\_}15-34-52.618/Capturing-quality-requirements-of-product-family-architecture{\_}2007{\_}Information-and-Software-Technology.pdf:pdf},
	Keywords = {Computer software; Distribution functions; Industr,Industrial settings; Quality requirement; Softwar,Quality assurance},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-34848888028{\&}doi=10.1016{\%}2Fj.infsof.2006.11.003{\&}partnerID=40{\&}md5=e13e12d15019d1467a408d04910818f6}
}

@Article{Nikolov201553,
	Title = {{Integration of {\{}DSLs{\}} and Migration of Models: A Case Study in the Cloud Computing Domain}},
	Author = {Nikolov, Nikolay and Rossini, Alessandro and Kritikos, Kyriakos},
	Journal = {Procedia Computer Science},
	Year = {2015},
	Pages = {53--66},
	Volume = {68},
	Abstract = {Abstract Domain-specific languages (DSLs) are high-level software languages representing concepts in a particular domain. In real-world scenarios, it is common to adopt multiple {\{}DSLs{\}} to solve different aspects of a specific problem. As any other software artefact, {\{}DSLs{\}} evolve independently in response to changing requirements, which leads to two challenges. First, the concepts from the {\{}DSLs{\}} have to be integrated into a single language. Second, models that conform to an old version of the language have to be migrated to conform to its current version. In this paper, we discuss how we tackled the challenge of integrating the {\{}DSLs{\}} that comprise the Cloud Application Modelling and Execution Language (CAMEL) by leveraging upon Eclipse Modeling Framework (EMF) and Object Constraint Language (OCL). Moreover, we propose a solution to the challenge of persisting and automatically migrating {\{}CAMEL{\}} models based on Connected Data Objects (CDO) and Edapt. },
	Annote = {1st International Conference on Cloud Forward: From Distributed to Complete Computing},
	Doi = {https://doi.org/10.1016/j.procs.2015.09.223},
	ISSN = {1877-0509},
	Keywords = {CAMEL,CDO,EMF,Edapt,OCL,cloud computing,domain-specific language,metamodel migration,model co-evolution,model-driven engineering},
	Url = {http://www.sciencedirect.com/science/article/pii/S1877050915030689}
}

@Conference{Niu2009137,
	Title = {{Concept analysis for product line requirements}},
	Author = {Niu, N and Easterbrook, S},
	Booktitle = {Proceedings of the 8th ACM International Conference on Aspect-Oriented Software Development, AOSD'09},
	Year = {2009},
	Pages = {137--148},
	Abstract = {Traditional methods characterize a software product line's requirements using either functional or quality criteria. This appears to be inadequate to assess modularity, detect interferences, and analyze trade-offs. We take advantage of both symmetric and asymmetric views of aspects, and perform formal concept analysis to examine the functional and quality requirements of an evolving product line. The resulting concept lattice provides a rich notion which allows remarkable insights into the modularity and interactions of requirements. We formulate a number of problems that aspect-oriented product line requirements engineering should address, and present our solutions according to the concept lattice. We describe a case study applying our approach to analyze a mobile game product line's requirements, and review lessons learned. Copyright 2009 ACM.},
	Annote = {cited By 22},
	Doi = {10.1145/1509239.1509259},
	Keywords = {Aspect-oriented; Concept analysis; Concept Lattice,Computer software selection and evaluation,Computer systems programming; Large scale systems},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-70450233890{\&}doi=10.1145{\%}2F1509239.1509259{\&}partnerID=40{\&}md5=360d1e15f4a2d43e2879a8d655560200}
}

@Article{IIS2:IIS2184,
	Title = {{A Case for Product Lines}},
	Author = {Nolan, Andy J and Pickard, Andrew C and Fisher, Steve and Beasley, Richard},
	Journal = {INCOSE International Symposium},
	Year = {2016},
	Number = {1},
	Pages = {645--660},
	Volume = {26},
	Abstract = {It would seem obvious that a business could benefit from a Product Line, but what is the business case, when can we expect benefits and how much do we need to invest? Under what conditions will it not be cost beneficial to adopt a Product Line?A Product Line is complicated, strategic in nature and it is probably beyond the capability of most people to simply “judge? if it is the right strategy. It was evident that in order for Rolls-Royce to understand Product Lines, and to make the right trade decisions, we would need to develop a Product Line cost/benefit estimation tool. The tool estimated both the costs to develop assets as well as the costs to deploy the assets onto a project.This paper summarizes the estimation tool we developed and some of the conclusions it revealed. Although the concept was used on a Software Product Line, the principles have been duplicated in hardware and Systems Engineering but are not discussed in this paper.},
	Doi = {10.1002/j.2334-5837.2016.00184.x},
	ISSN = {2334-5837},
	Url = {http://dx.doi.org/10.1002/j.2334-5837.2016.00184.x}
}

@Article{Noor200869,
	Title = {{A collaborative method for reuse potential assessment in reengineering-based product line adoption}},
	Author = {Noor, M A and Gr{\"{u}}nbacher, P and Hoyer, C},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2008},
	Pages = {69--83},
	Volume = {5082 LNCS},
	Abstract = {Software product lines are rarely developed from scratch. Instead the development of a product line by reengineering existing systems is a more common scenario, which relies on the collaboration of diverse stakeholders to lay its foundations. The paper describes a collaborative scoping approach for organizations migrating existing products to a product line. The approach uses established practices from the field of reengineering and architectural recovery and synthesizes them in a collaborative process. The proposed approach employs best practices and tools from the area of collaboration engineering to achieve effective collaboration. The paper presents a case study as initial validation of the proposed approach. {\textcopyright} 2008 Springer-Verlag Berlin Heidelberg.},
	Annote = {cited By 3},
	Doi = {10.1007/978-3-540-85279-7_6},
	Keywords = {Architectural recovery; Collaboration; Collaborat,Collaboration; Engineering techniques; Product li,Software design; Software engineering; Transients;,Technology; Software engineering},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-50949096881{\&}doi=10.1007{\%}2F978-3-540-85279-7{\_}6{\&}partnerID=40{\&}md5=a9c312b4655b961bdb0e1ec0ae66a3fc}
}

@Article{Noor2008868,
	Title = {{Agile product line planning: A collaborative approach and a case study}},
	Author = {Noor, Muhammad A and Rabiser, Rick and Gr{\"{u}}nbacher, Paul},
	Journal = {Journal of Systems and Software},
	Year = {2008},
	Number = {6},
	Pages = {868--882},
	Volume = {81},
	Abstract = {Agile methods and product line engineering (PLE) have both proven successful in increasing customer satisfaction and decreasing time to market under certain conditions. Key characteristics of agile methods are lean and highly iterative development with a strong emphasis on stakeholder involvement. {\{}PLE{\}} leverages reuse through systematic approaches such as variability modeling or product derivation. Integrating agile approaches with product line engineering is an interesting proposition which – not surprisingly – entails several challenges: Product lines (PL) rely on complex plans and models to ensure their long-term evolution while agile methods emphasize simplicity and short-term value-creation for customers. When incorporating agility in product line engineering, it is thus essential to define carefully how agile principles can support particular {\{}PLE{\}} processes. For instance, the processes of defining and setting up a product line (domain engineering) and deriving products (application engineering) differ significantly in practices and focus with implications on the suitability of agile principles. This paper presents practical experiences of adopting agile principles in product line planning (a domain engineering activity). ThinkLets, i.e., collaborative practices from the area of collaboration engineering, are the building blocks of the presented approach as they codify agile principles such as stakeholder involvement, rapid feedback, or value-based prioritization. We discuss how our approach balances agility and the intrinsic needs of product line planning. A case study carried out with an industrial partner indicates that the approach is practicable, usable, and useful. },
	Annote = {Agile Product Line Engineering},
	Doi = {https://doi.org/10.1016/j.jss.2007.10.028},
	ISSN = {0164-1212},
	Keywords = {Agile methods,Collaboration engineering,Product line engineering,Product line planning},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121207002531}
}

@Article{SMR:SMR1870,
	Title = {{Toward automated quality-centric product line configuration using intentional variability}},
	Author = {Noorian, Mahdi and Bagheri, Ebrahim and Du, Weichang},
	Journal = {Journal of Software: Evolution and Process},
	Pages = {n/a----n/a},
	Abstract = {Software product line engineering is a discipline that facilitates a systematic reuse-based approach by formally representing commonalities and variabilities between the applications of a target domain. As one of the main artifacts of the software product line, a feature model represents the possible configuration space and can be customized based on the stakeholders' needs. Considering the complexity of the variabilities represented by feature models and the diversity of the stakeholders' expectations, the configuration process can be viewed as a complex optimization problem. In previous research, researchers have bridged the gap between requirement and product line engineering by integrating feature models and goal models. In this paper, we propose an approach for the configuration process that seeks to satisfy the stakeholders' requirements as well as the feature models' structural and integrity constraints. We model stakeholders' functional and nonfunctional needs and their preferences using requirement engineering goal models. We formalize the structure of the feature model, the stakeholders' objectives, and their preferences in the form of an integer linear program to conduct a semi-automated feature model configuration process. Our experimental results show that the proposed configuration framework is scalable when considering both functional and nonfunctional requirements of stakeholders.},
	Doi = {10.1002/smr.1870},
	ISSN = {2047-7481},
	Keywords = {configuration process,feature model,goal model,software product line},
	Url = {http://dx.doi.org/10.1002/smr.1870}
}

@InProceedings{Norick:2010:END:1852786.1852864,
	Title = {{Effects of the Number of Developers on Code Quality in Open Source Software: A Case Study}},
	Author = {Norick, Brandon and Krohn, Justin and Howard, Eben and Welna, Ben and Izurieta, Clemente},
	Booktitle = {Proceedings of the 2010 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
	Year = {2010},
	Address = {New York, NY, USA},
	Pages = {62:1----62:1},
	Publisher = {ACM},
	Series = {ESEM '10},
	Doi = {10.1145/1852786.1852864},
	ISBN = {978-1-4503-0039-1},
	Keywords = {code quality,number of developers,open source software},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/1852786.1852864}
}

@Article{NunezVarela2017164,
	Title = {{Source code metrics: A systematic mapping study}},
	Author = {Nu{\~{n}}ez-Varela, Alberto S and P{\'{e}}rez-Gonzalez, H{\'{e}}ctor G and Mart{\'{i}}nez-Perez, Francisco E and Soubervielle-Montalvo, Carlos},
	Journal = {Journal of Systems and Software},
	Year = {2017},
	Pages = {164--197},
	Volume = {128},
	Abstract = {AbstractContext Source code metrics are essential components in the software measurement process. They are extracted from the source code of the software, and their values allow us to reach conclusions about the quality attributes measured by the metrics. Objectives This paper aims to collect source code metrics related studies, review them, and perform an analysis, while providing an overview on the current state of source code metrics and their current trends. Method A systematic mapping study was conducted. A total of 226 studies, published between the years 2010 and 2015, were selected and analyzed. Results Almost 300 source code metrics were found. Object oriented programming is the most commonly studied paradigm with the Chidamber and Kemerer metrics, lines of code, McCabe's cyclomatic complexity, and number of methods and attributes being the most used metrics. Research on aspect and feature oriented programming is growing, especially for the current interest in programming concerns and software product lines. Conclusions Object oriented metrics have gained much attention, but there is a current need for more studies on aspect and feature oriented metrics. Software fault prediction, complexity and quality assessment are recurrent topics, while concerns, big scale software and software product lines represent current trends. },
	Doi = {https://doi.org/10.1016/j.jss.2017.03.044},
	ISSN = {0164-1212},
	Keywords = {Aspect-oriented metrics,Feature-oriented metrics,Object-oriented metrics,Software metrics,Source code metrics,Systematic mapping study},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121217300663}
}

@Article{SPE:SPE2200,
	Title = {{Heuristic expansion of feature mappings in evolving program families}},
	Author = {Nunes, Camila and Garcia, Alessandro and Lucena, Carlos and Lee, Jaejoon},
	Journal = {Software: Practice and Experience},
	Year = {2014},
	Number = {11},
	Pages = {1315--1349},
	Volume = {44},
	Abstract = {Establishing explicit mappings between features and their implementation elements in code is one of the critical factors to maintain and evolve software systems successfully. This is especially important when developers have to evolve program families, which have evolved from one single core system to similar but different systems to accommodate various requirements from customers. Many techniques and tools have emerged to assist developers in the feature mapping activity. However, existing techniques and tools for feature mapping are limited as they operate on a single program version individually. Additionally, existing approaches are limited to recover features on demand, that is, developers have to run the tools for each family member version individually. In this paper, we propose a cohesive suite of five mapping heuristics addressing those two limitations. These heuristics explore the evolution history of the family members in order to expand feature mappings in evolving program families. The expansion refers to the action of automatically generating the feature mappings for each family member version by systematically considering its previous change history. The mapping expansion starts from seed mappings and continually tracks the features of the program family, thus eliminating the need of on demand algorithms. Additionally, we present the MapHist tool that provides support to the application of the proposed heuristics. We evaluate the accuracy of our heuristics through two evolving program families from our industrial partners. Copyright {\textcopyright} 2013 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spe.2200},
	ISSN = {1097-024X},
	Keywords = {evolving program families,experimental evaluation,feature mappings,heuristics},
	Url = {http://dx.doi.org/10.1002/spe.2200}
}

@Article{Nunes20092254,
	Title = {{Assessment of the design modularity and stability of multi-agent system product lines}},
	Author = {Nunes, C and Kulesza, U and Sant'Anna, C and Nunes, I and Garcia, A and Lucena, C},
	Journal = {Journal of Universal Computer Science},
	Year = {2009},
	Number = {11},
	Pages = {2254--2283},
	Volume = {15},
	Abstract = {A multi-agent system product line (MAS-PL) defines an architecture whose design and implementation is accomplished using software agents to address its common and variable features. MAS-PL promotes the large-scale reuse of common and variable agency features across multiple MAS applications. The development of MAS-PLs can be achieved through MAS-specific platforms and implementation techniques, such as conditional compilation and aspect-oriented programming (AOP). However, there is not much evidence on how these techniques provide better modularity, allowing the conception of stable MAS-PL designs. This paper presents a quantitative study on the design modularity and stability of an evolving MAS-PL. The MAS-PL was built following the reactive product line adoption approach. The product line was developed and evolved based on several versions of a conference management web-based system, named Expert Committee (EC). Our evaluation is made through a series of change scenarios related to new agency features, which are agent characteristics that enhance the system with autonomous behavior. The quantitative study consists of a systematic comparison between two different versions of the EC MAS-PL based on a MAS-specific platform, called JADE. One version was implemented with object-oriented and conditional compilation techniques. The other one relied on AOP. Our analysis was driven by well-known modularity and change impact metrics.},
	Annote = {cited By 4},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350176494{\&}partnerID=40{\&}md5=4c31d9416f2203717f92a4ce9001eca2}
}

@Article{Nunes2009716,
	Title = {{A domain analysis approach for multi-agent systems product lines}},
	Author = {Nunes, I and Kulesza, U and Nunes, C and {De Lucena}, C J P and Cirilo, E},
	Journal = {Lecture Notes in Business Information Processing},
	Year = {2009},
	Pages = {716--727},
	Volume = {24 LNBIP},
	Abstract = {In this paper, we propose an approach for documenting and modeling Multi-agent System Product Lines (MAS-PLs) in the domain analysis stage. MAS-PLs are the integration between two promising techniques, software product lines and agent-oriented software engineering, aiming at incorporating their respective benefits and helping the industrial exploitation of agent technology. Our approach explores the scenario of including agency features to existing web applications and is based on PASSI, an agent-oriented methodology, to which we added some extensions to address agency variability. A case study, OLIS (OnLine Intelligent Services), illustrates our approach. {\textcopyright} 2009 Springer Berlin Heidelberg.},
	Annote = {cited By 1},
	Doi = {10.1007/978-3-642-01347-8_59},
	Keywords = {Agent Oriented Software Engineering; Agent techno,Information systems; Software agents; Software eng,Multi agent systems},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-65949110866{\&}doi=10.1007{\%}2F978-3-642-01347-8{\_}59{\&}partnerID=40{\&}md5=f7d7f126f58f0f6b380dd9c52a08ec54}
}

@Article{Nurdiani2016162,
	Title = {{The impacts of agile and lean practices on project constraints: A tertiary study}},
	Author = {Nurdiani, Indira and B{\"{o}}rstler, J{\"{u}}rgen and Fricker, Samuel A},
	Journal = {Journal of Systems and Software},
	Year = {2016},
	Pages = {162--183},
	Volume = {119},
	Abstract = {Abstract The growing interest in Agile and Lean software development is reflected in the increasing number of secondary studies on the benefits and limitations of Agile and Lean processes and practices. The aim of this tertiary study is to consolidate empirical evidence regarding Agile and Lean practices and their respective impacts on project constraints as defined in the Project Management Body of Knowledge (PMBOK): scope, quality, schedule, budget, resources, communication, and risk. In this tertiary study, 13 secondary studies were included for detailed analysis. Given the heterogeneity of the data, we were unable to perform a rigorous synthesis. Instead, we mapped the identified Agile and Lean practices, and their impacts on the project constraints described in PMBOK. From 13 secondary studies, we identified 13 Agile and Lean practices. Test-Driven Development (TDD) is studied in ten secondary studies, meanwhile other practices are studied in only one or two secondary studies. This tertiary study provides a consolidated view of the impacts of Agile and Lean practices. The result of this tertiary study indicates that {\{}TDD{\}} has a positive impact on external quality. However, due to insufficient data or contradictory results, we were unable to make inferences on other Agile and Lean practices. Implications for research and practice are further discussed in the paper. },
	Doi = {https://doi.org/10.1016/j.jss.2016.06.043},
	ISSN = {0164-1212},
	Keywords = {Agile software development,Lean software development,Project constraints,Tertiary study},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121216300863}
}

@Article{O’Leary20121014,
	Title = {{The Pro-PD Process Model for Product Derivation within software product lines}},
	Author = {O'Leary, P{\'{a}}draig and de Almeida, Eduardo Santana and Richardson, Ita},
	Journal = {Information and Software Technology},
	Year = {2012},
	Number = {9},
	Pages = {1014--1028},
	Volume = {54},
	Abstract = {Background The derivation of products from a software product line is a time consuming and expensive activity. Despite recognition that an effective process could alleviate many of the difficulties associated with product derivation, existing approaches have different scope, emphasise different aspects of the derivation process and are frequently too specialised to serve as a general solution. Objective To define a systematic process that will provide a structured approach to the derivation of products from a software product line, based on a set of tasks, roles and artefacts. Method Through a series of research stages using sources in industry and academia, this research has developed a Process Model for Product Derivation (Pro-PD). We document the evidence for the construction of Pro-PD and the design decisions taken. We evaluate Pro-PD through comparison with prominent existing approaches and standards. Results This research presents a Process Model for Product Derivation (Pro-PD). Pro-PD describes the tasks, roles and work artefacts used to derive products from a software product line. Conclusion In response to a need for methodological support, we developed Pro-PD (Process Model for Product Derivation). Pro-PD was iteratively developed and evaluated through four research stages. Our research is a first step toward an evidence-based methodology for product derivation and a starting point for the definition of a product derivation approach. },
	Doi = {https://doi.org/10.1016/j.infsof.2012.03.008},
	ISSN = {0950-5849},
	Keywords = {Process,Product derivation,Software product lines},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584912000572}
}

@Article{SMR:SMR498,
	Title = {{An agile process model for product derivation in software product line engineering}},
	Author = {O'Leary, P{\'{a}}draig and McCaffery, Fergal and Thiel, Steffen and Richardson, Ita},
	Journal = {Journal of Software: Evolution and Process},
	Year = {2012},
	Number = {5},
	Pages = {561--571},
	Volume = {24},
	Doi = {10.1002/smr.498},
	ISSN = {2047-7481},
	Keywords = {agile approaches,product derivation,software product lines},
	Publisher = {John Wiley {\&} Sons, Ltd},
	Url = {http://dx.doi.org/10.1002/smr.498}
}

@Article{Oberweis2007909,
	Title = {{Product lines for digital information products}},
	Author = {Oberweis, Andreas and Pankratius, Victor and Stucky, Wolffried},
	Journal = {Information Systems},
	Year = {2007},
	Number = {6},
	Pages = {909--939},
	Volume = {32},
	Abstract = {The growth of the Web has fueled the creation, storage, and exchange of digital information products (DIPs), whose main purpose is the delivery of information, entertainment, education, or training. Very often, after their initial creation, the growing amount of content also leads to a more complicated maintenance, since updates typically occur rather often and the potential variability of modifications is not limited in advance. Moreover, commonalities between different parts of similar information products are not exploited, which often leads to redundancy. At the moment, there is hardly any attempt to compose information products from different sources and to produce more complex information products in a coordinated way. To help remedy this situation, this paper introduces the Product Lines for digitAl iNformation producTs (PLANT) approach, which applies the concept of software product lines to DIPs. The {\{}PLANT{\}} approach explicitly manages the commonalities of similar {\{}DIPs{\}} by defining common requirements, limiting variability in advance, as well as planning and coordinating reuse. This article focuses on the modeling of such product lines. In particular, the developed general concepts will be exemplified throughout the paper in the area of e-learning, in which {\{}DIPs{\}} play an important role. The application of {\{}PLANT{\}} in other areas and an implemented tool that supports the creation of information products in a product line are outlined as well. },
	Doi = {https://doi.org/10.1016/j.is.2006.09.003},
	ISSN = {0306-4379},
	Keywords = {Digital information products,Digital products,Feature models,Software product lines,Workflow management,e-learning},
	Url = {http://www.sciencedirect.com/science/article/pii/S0306437906000809}
}

@Article{Ognjanovic20131094,
	Title = {{A stratified framework for handling conditional preferences: An extension of the analytic hierarchy process}},
	Author = {Ognjanovi{\'{c}}, Ivana and Ga{\v{s}}evi{\'{c}}, Dragan and Bagheri, Ebrahim},
	Journal = {Expert Systems with Applications},
	Year = {2013},
	Number = {4},
	Pages = {1094--1115},
	Volume = {40},
	Abstract = {Representing and reasoning over different forms of preferences is of crucial importance to many different fields, especially where numerical comparisons need to be made between critical options. Focusing on the well-known Analytical Hierarchical Process (AHP) method, we propose a two-layered framework for addressing different kinds of conditional preferences which include partial information over preferences and preferences of a lexicographic kind. The proposed formal two-layered framework, called CS-AHP, provides the means for representing and reasoning over conditional preferences. The framework can also effectively order decision outcomes based on conditional preferences in a way that is consistent with well-formed preferences. Finally, the framework provides an estimation of the potential number of violations and inconsistencies within the preferences. We provide and report extensive performance analysis for the proposed framework from three different perspectives, namely time-complexity, simulated decision making scenarios, and handling cyclic and partially defined preferences. },
	Doi = {https://doi.org/10.1016/j.eswa.2012.08.026},
	ISSN = {0957-4174},
	Keywords = {AHP method,Comparative preferences,Conditional preferences,Lexicographic order,S-AHP method,Well-formed preferences},
	Url = {http://www.sciencedirect.com/science/article/pii/S0957417412009876}
}

@InProceedings{Oh:2010:EOM:1943628.1943634,
	Title = {{Evaluating Ontology Modularization Approaches}},
	Author = {Oh, Sunju and Yeom, Heon Y and Ahn, Joongho},
	Booktitle = {Proceedings of the 8th International Conference on Frontiers of Information Technology},
	Year = {2010},
	Address = {New York, NY, USA},
	Pages = {6:1----6:6},
	Publisher = {ACM},
	Series = {FIT '10},
	Doi = {10.1145/1943628.1943634},
	ISBN = {978-1-4503-0342-2},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/1943628.1943634}
}

@Article{deOliveira201442,
	Title = {{Evaluating Lehman's laws of software evolution within software product lines: A preliminary empirical study}},
	Author = {de Oliveira, R P and de Almeida, E S and {da Silva Gomes}, G S},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2014},
	Pages = {42--57},
	Volume = {8919},
	Abstract = {The evolution of a single system is a task where we deal with the modification of a single product. Lehman's laws of software evolution were broadly evaluated within this type of systems and the results shown that these single systems evolve according to his stated laws over time. However, when dealing with Software Product Lines (SPL), we need to deal with the modification of several products which include common, variable and product specific assets. Because of the several assets within SPL, each stated law may have a different behavior for each asset kind. Nonetheless, we do not know if the stated laws are still valid for SPL since they were not yet evaluated in this context. Thus, this paper details an empirical investigation where four of the Lehman's Laws (LL) of Software Evolution were used in an SPL industrial project to understand how the SPL assets evolve over time. This project relates to an application in the medical domain developed in a medium-size company in Brazil. It contains 45 modules and a total of 70.652 bug requests in the tracking system, gathered along the past 10 years. We employed two techniques - the KPSS Test and linear regression analysis, to assess the relationship between LL and SPL assets. Finally, results showed that three laws were supported based on the data employed (continuous change, increasing complexity, and declining quality). The other law (continuing growth) was partly supported, depending on the SPL evaluated asset (common, variable or product-specific). {\textcopyright} Springer International Publishing Switzerland 2014.},
	Annote = {cited By 2},
	Keywords = {Computer software reusability,Empirical investigation; Empirical studies; Indus,Regression analysis},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919658085{\&}partnerID=40{\&}md5=514813541629f54954884174adcb8687}
}

@Article{deOliveira2014666,
	Title = {{Defining and validating a feature-driven requirements engineering approach}},
	Author = {de Oliveira, R P and Blanes, D and Gonzalez-Huerta, J and Insfran, E and Abrah{\~{a}}o, S and Cohen, S and de Almeida, E S},
	Journal = {Journal of Universal Computer Science},
	Year = {2014},
	Number = {5},
	Pages = {666--691},
	Volume = {20},
	Abstract = {The specification of requirements is a key activity for achieving the goals of any software project and it has long been established and recognized by researchers and practitioners. Within Software Product Lines (SPL), this activity is even more critical owing to the need to deal with common, variable, and product-specific requirements, not only for a single product but for the whole set of products. In this paper, we present a Feature-Driven Requirements Engineering approach (FeDRE) that provides support to the requirements specification of SPL. The approach realizes features into functional requirements by considering the variability captured in a feature model. It also provides detailed guidelines on how to associate chunks of features from a feature model and to consider them as the context for the Use Case specification. The evaluation of the approach is illustrated in a case study for developing an SPL of mobile applications for emergency notifications. This case study was applied within 14 subjects, 8 subjects from Universitat Polit{\`{e}}cnica de Val{\`{e}}ncia and 6 subjects from Federal University of Bahia. Evaluations concerning the perceived ease of use, perceived usefulness, effectiveness and efficiency as regards requirements analysts using the approach are also presented. The results show that FeDRE was perceived as easy to learn and useful by the participants. {\textcopyright} J.UCS.},
	Annote = {cited By 1},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904769020{\&}partnerID=40{\&}md5=31d21d19a0249965fa4b5f0742277f51}
}

@Article{Oliveira20112234,
	Title = {{ReuseTool—An extensible tool support for object-oriented framework reuse}},
	Author = {Oliveira, Toacy C and Alencar, Paulo and Cowan, Don},
	Journal = {Journal of Systems and Software},
	Year = {2011},
	Number = {12},
	Pages = {2234--2252},
	Volume = {84},
	Abstract = {Object-oriented frameworks have become a popular paradigm used to improve the software development lifecycle. They promote reuse by providing a semi-complete architecture that can be extended through an instantiation process to integrate the needs of the new software application. Instantiation processes are typically enacted in an ad-hoc manner, which may lead to tedious and error-prone procedures. This work leverages our previous work on the definition of RDL, a language to facilitate the description of instantiation process, and describe the ReuseTool, which is an extensible tool to execute {\{}RDL{\}} programs and assist framework reuse by manipulating {\{}UML{\}} Diagrams. The ReuseTool integrates a {\{}RDL{\}} Compiler and a Workflow Engine to control most of the activities required to extend a framework design and, therefore, incorporates application-specific needs. This work also describes how the tool can be extended to incorporate new reuse activities and provides information of its use based on an exploratory Case Study. },
	Doi = {https://doi.org/10.1016/j.jss.2011.06.030},
	ISSN = {0164-1212},
	Keywords = {Object-oriented framework,Software process,Software reuse,UML},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121211001531}
}

@Article{Olszak2012131,
	Title = {{Remodularizing Java programs for improved locality of feature implementations in source code}},
	Author = {Olszak, Andrzej and J{\o}rgensen, Bo N{\o}rregaard},
	Journal = {Science of Computer Programming},
	Year = {2012},
	Number = {3},
	Pages = {131--151},
	Volume = {77},
	Abstract = {Explicit traceability between features and source code is known to help programmers to understand and modify programs during maintenance tasks. However, the complex relations between features and their implementations are not evident from the source code of object-oriented Java programs. Consequently, the implementations of individual features are difficult to locate, comprehend, and modify in isolation. In this paper, we present a novel remodularization approach that improves the representation of features in the source code of Java programs. Both forward and reverse restructurings are supported through on-demand bidirectional restructuring between feature-oriented and object-oriented decompositions. The approach includes a feature location phase based on tracing of program execution, a feature representation phase that reallocates classes into a new package structure based on single-feature and multi-feature packages, and an annotation-based reverse transformation of code. Case studies performed on two open-source projects indicate that our approach requires relatively little manual effort and reduces tangling and scattering of feature implementations in the source code. },
	Annote = {Feature-Oriented Software Development (FOSD 2009)},
	Doi = {https://doi.org/10.1016/j.scico.2010.10.007},
	ISSN = {0167-6423},
	Keywords = {Feature location,Features,Fragile decomposition problem,Remodularization},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642310001917}
}

@InProceedings{Olumofin,
	Title = {{Extending the ATAM Architecture Evaluation to Product Line Architectures}},
	Author = {Olumofin, F.G. and Misic, V.B.},
	Booktitle = {5th Working IEEE/IFIP Conference on Software Architecture (WICSA'05)},
	Pages = {45--56},
	Publisher = {IEEE},
	Doi = {10.1109/WICSA.2005.33},
	ISBN = {0-7695-2548-2},
	Url = {http://ieeexplore.ieee.org/document/1620090/}
}

@Article{Olumofin2007309,
	Title = {{A holistic architecture assessment method for software product lines}},
	Author = {Olumofin, F G and Mi{\v{s}}i{\'{c}}, V B},
	Journal = {Information and Software Technology},
	Year = {2007},
	Number = {4},
	Pages = {309--323},
	Volume = {49},
	Abstract = {The success of architecture-centric development of software product lines is critically dependent upon the availability of suitable architecture assessment methods. While a number of architecture assessment methods are available and some of them have been widely used in the process of evaluating single product architectures, none of them is equipped to deal with the main challenges of product line development. In this paper we present an adaptation of the Architecture Tradeoff Analysis Method (ATAM) for the task of assessing product line architectures. The new method, labeled Holistic Product Line Architecture Assessment (HoPLAA), uses a holistic approach that focuses on risks and quality attribute tradeoffs - not only for the common product line architecture, but for the individual product architectures as well. In addition, it prescribes a qualitative analytical treatment of variation points using scenarios. The use of the new method is illustrated through a case study. {\textcopyright} 2006 Elsevier B.V. All rights reserved.},
	Annote = {cited By 15},
	Doi = {10.1016/j.infsof.2006.05.003},
	Keywords = {Architecture Tradeoff Analysis Method (ATAM); Sof,Computer architecture; Information technology; Ris,Computer software},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846550580{\&}doi=10.1016{\%}2Fj.infsof.2006.05.003{\&}partnerID=40{\&}md5=3881718a2b055a96a89780dbdce15ba3}
}

@Article{SPE:SPE541,
	Title = {{Horizontal communication: a style to compose control software}},
	Author = {van Ommering, Rob},
	Journal = {Software: Practice and Experience},
	Year = {2003},
	Number = {12},
	Pages = {1117--1150},
	Volume = {33},
	Abstract = {Consumer products become more complex and diverse, integrating functions that were previously available only in separate products. We believe that to build such products efficiently, a compositional approach is required. While this is quite feasible in hardware, we would like to achieve the same in software, especially in the low-level software that drives the hardware. We found this to be possible, but only if we let software components communicate horizontally, exchanging information along software channels that mirror the hardware signal topology. In this paper a concrete protocol implementing this style of control is described and many examples are given of its use. Copyright {\textcopyright} 2003 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spe.541},
	ISSN = {1097-024X},
	Keywords = {composition,control software,diversity,product family,product population,software components},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/spe.541}
}

@Article{SYS:SYS21373,
	Title = {{Applying Composable Architectures to the Design and Development of a Product Line of Complex Systems}},
	Author = {Oster, Christopher and Kaiser, Michael and Kruse, Jonathan and Wade, Jon and Cloutier, Rob},
	Journal = {Systems Engineering},
	Year = {2016},
	Number = {6},
	Pages = {522--534},
	Volume = {19},
	Abstract = {This paper investigates a composable design methodology leveraging SysML to manage mission flexible product lines, and reviews the application of this methodology to a spacecraft product line. This methodology extends the SysML language with a mathematical and Boolean constraint language allowing for the capture of product line rules as an alternative to a more traditional variation tree. Finally, this paper reviews future work underway to extend this methodology.},
	Doi = {10.1002/sys.21373},
	ISSN = {1520-6858},
	Keywords = {architectural design,decision analysis/management,defense and security,government,model-based systems engineering (MBSE),system integration},
	Url = {http://dx.doi.org/10.1002/sys.21373}
}

@Article{SYS:SYS21256,
	Title = {{Ecosystem requirements for composability and reuse: An investigation into ecosystem factors that support adoption of composable practices for engineering design}},
	Author = {Oster, Christopher and Wade, Jon},
	Journal = {Systems Engineering},
	Year = {2013},
	Number = {4},
	Pages = {439--452},
	Volume = {16},
	Abstract = {Composability is a systems architecture and design concept focusing on composing new systems from known components, designs, product lines, and reference architectures as opposed to focusing on “blank sheet? designs based on requirements decomposition alone. The concept of composability has been a goal of the US Department of Defense (DoD) for many years, most recently taking the form of Platform-based Engineering. Despite this focus, the goal of effective modularity and design reuse has been somewhat elusive in the aerospace and defense sectors. This paper describes an ecosystem construct which incorporates market factors, business practices, and trends that occur in industries where composability and reuse have taken hold in order to identify a path forward for effective adoption of composability in the aerospace and defense marketplace. A number of examples of composable design are described, followed by proposals for necessary changes within the DoD ecosystem to facilitate its support. {\textcopyright} 2013 Wiley Periodicals, Inc. Syst Eng 16},
	Doi = {10.1002/sys.21256},
	ISSN = {1520-6858},
	Keywords = {architecture,composable,ecosystem,modular: market factor,reuse},
	Publisher = {Wiley Subscription Services, Inc., A Wiley Company},
	Url = {http://dx.doi.org/10.1002/sys.21256}
}

@InProceedings{Otsuka:2011:SIC:2019136.2019159,
	Title = {{Small Inexpensive Core Asset Construction for Large Gainful Product Line Development: Developing a Communication System Firmware Product Line}},
	Author = {Otsuka, Jun and Kawarabata, Kouichi and Iwasaki, Takashi and Uchiba, Makoto and Nakanishi, Tsuneo and Hisazumi, Kenji},
	Booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
	Year = {2011},
	Address = {New York, NY, USA},
	Pages = {20:1----20:5},
	Publisher = {ACM},
	Series = {SPLC '11},
	Doi = {10.1145/2019136.2019159},
	ISBN = {978-1-4503-0789-5},
	Keywords = {case study,core assets,feature modeling,product line},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2019136.2019159}
}

@Article{Ouni201755,
	Title = {{Search-based software library recommendation using multi-objective optimization}},
	Author = {Ouni, Ali and Kula, Raula Gaikovina and Kessentini, Marouane and Ishio, Takashi and German, Daniel M and Inoue, Katsuro},
	Journal = {Information and Software Technology},
	Year = {2017},
	Pages = {55--75},
	Volume = {83},
	Abstract = {Abstract Context: Software library reuse has significantly increased the productivity of software developers, reduced time-to-market and improved software quality and reusability. However, with the growing number of reusable software libraries in code repositories, finding and adopting a relevant software library becomes a fastidious and complex task for developers. Objective: In this paper, we propose a novel approach called LibFinder to prevent missed reuse opportunities during software maintenance and evolution. The goal is to provide a decision support for developers to easily find “useful? third-party libraries to the implementation of their software systems. Method: To this end, we used the non-dominated sorting genetic algorithm (NSGA-II), a multi-objective search-based algorithm, to find a trade-off between three objectives : 1) maximizing co-usage between a candidate library and the actual libraries used by a given system, 2) maximizing the semantic similarity between a candidate library and the source code of the system, and 3) minimizing the number of recommended libraries. Results: We evaluated our approach on 6083 different libraries from Maven Central super repository that were used by 32,760 client systems obtained from Github super repository. Our results show that our approach outperforms three other existing search techniques and a state-of-the art approach, not based on heuristic search, and succeeds in recommending useful libraries at an accuracy score of 92{\%}, precision of 51{\%} and recall of 68{\%}, while finding the best trade-off between the three considered objectives. Furthermore, we evaluate the usefulness of our approach in practice through an empirical study on two industrial Java systems with developers. Results show that the top 10 recommended libraries was rated by the original developers with an average of 3.25 out of 5. Conclusion: This study suggests that (1) library usage history collected from different client systems and (2) library semantics/content embodied in library identifiers should be balanced together for an efficient library recommendation technique. },
	Doi = {https://doi.org/10.1016/j.infsof.2016.11.007},
	ISSN = {0950-5849},
	Keywords = {Multi-objective optimization,Search-based software engineering,Software library,Software reuse},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584916303652}
}

@Article{Paivarinta2015124,
	Title = {{Theorizing about software development practices}},
	Author = {P{\"{a}}iv{\"{a}}rinta, Tero and Smolander, Kari},
	Journal = {Science of Computer Programming},
	Year = {2015},
	Pages = {124--135},
	Volume = {101},
	Abstract = {Abstract The paper focuses on the challenge of generating theoretical support for software development, especially when human software developers are involved in the software development process. We outline a model, “Coat Hanger?, for theorizing about development practices. The model focuses on the intended rationale for the actual realization and resulting impacts of using particular practices in varying contexts. To illustrate the use of the model, we have studied recent practice-oriented articles in the journal Science of Computer Programming. A survey of articles in the journal between 2010 and 2013 showed that out of 371 articles, only four studied software development in professional organizations with actual software practitioners as informants. The Coat Hanger model was then used to identify the theoretical strengths and weaknesses of these four practice descriptions. The analysis is used as the basis to declare the potential of our model as a conceptual aid for more structured theorizing about software development practices. The contribution of the model is the introduction of a concretization of how theorizing can be done through reflection-in-action, instead of regarding research on software practices plainly from the prevailing viewpoint of technical rationality. },
	Annote = {Towards general theories of software engineering},
	Doi = {https://doi.org/10.1016/j.scico.2014.11.012},
	ISSN = {0167-6423},
	Keywords = {Practice,Reflection-in-action,Software development,Theorizing,Theory},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642314005449}
}

@Article{SMR:SMR1698,
	Title = {{A proposed model for reuse of software requirements in requirements catalog}},
	Author = {Pacheco, C L and Garcia, I A and Calvo-Manzano, J A and Arcilla, M},
	Journal = {Journal of Software: Evolution and Process},
	Year = {2015},
	Number = {1},
	Pages = {1--21},
	Volume = {27},
	Abstract = {Nowadays, the demand for higher quality in software products causes the increased complexity of these products. These two aspects (quality and complexity) combined with productivity are key elements for improving the competitiveness between software companies and differentiating success from failure in the software industry. In this sense, software reuse, and requirements reuse in particular, has been widely recognized as an effective way to improve software quality and productivity. In this paper, we propose a model to reuse the software requirements in a Requirements Catalog. This research aims to define the guidelines to carry out the requirements reuse process through four principal activities: searching, selecting, adapting, and implementing. Also, the commercial tool Computer-Aided Requirements Engineering Rational RequisitePro was tailored to make feasible the implementation of the proposed model; in this sense, a tool plug-in was developed. Finally, a case study is presented to demonstrate the feasibility of the proposed model. Copyright {\textcopyright} 2014 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/smr.1698},
	ISSN = {2047-7481},
	Keywords = {requirements engineering,requirements reuse process,software enterprises,software tool integration,traceability},
	Url = {http://dx.doi.org/10.1002/smr.1698}
}

@Article{Palviainen201417,
	Title = {{A semi-automatic end-user programming approach for smart space application development}},
	Author = {Palviainen, Marko and Kuusij{\"{a}}rvi, Jarkko and Ovaska, Eila},
	Journal = {Pervasive and Mobile Computing},
	Year = {2014},
	Pages = {17--36},
	Volume = {12},
	Abstract = {Abstract This article describes a semi-automatic end-user programming approach that: (i) assists in the creation of easy-to-apply Semantic End-User Application Programming Interfaces(S-APIs) for the {\{}APIs{\}} of legacy software components; and (ii) enables the usage of S-APIs in command-oriented and goal-oriented end-user application programming. Furthermore, a reference implementation is presented for the approach that provides visual programming tools and an agent-based execution environment for smart space applications. The use of the approach is exemplified and tested in a case study in which S-APIs are created for a home automation system and for a personal assistant application, and then utilized in end-user programming performed in desktop and mobile environments. },
	Doi = {https://doi.org/10.1016/j.pmcj.2013.04.002},
	ISSN = {1574-1192},
	Keywords = {Command-oriented end-user programming,Goal-oriented end-user programming,Ontology,Smart Modeler,Smart space application},
	Url = {http://www.sciencedirect.com/science/article/pii/S157411921300062X}
}

@Article{SMR:SMR334,
	Title = {{Organizational evolution of digital signal processing software development}},
	Author = {Pantsar-Syv{\"{a}}niemi, Susanna and Taramaa, Jorma and Niemel{\"{a}}, Eila},
	Journal = {Journal of Software Maintenance and Evolution: Research and Practice},
	Year = {2006},
	Number = {4},
	Pages = {293--305},
	Volume = {18},
	Abstract = {A base station, as a network element, has become an increasingly software-intensive system. Digital signal processing (DSP) software is hard real-time software that is a part of the software system needed in a base station. This article reports practical experiences related to organizing the development of embedded software in the telecommunication industry, at Nokia Networks. The article introduces the main factors influencing the development of DSP software and also compares the evolutionary process under study with both selected organizational models for a software product line and a multistage model for the software life cycle. We believe it is vitally important to formulate the organization according to the software architecture, and it is essential to have a dedicated development organization with long-term responsibility for the software. History shows that without long-term responsibility, there is no software reuse. In this paper we introduce a new organizational model for product line development. This new hybrid model clarifies long-term responsibilities in large software organizations with hundreds of staff members and formulates the organization according to the software architecture. Our case needs a couple more constraints to keep it in the evolution stage of the software life cycle. Thus, we extend the evolution phase in the multistage model to make it relevant for embedded, hard real-time software. Copyright {\textcopyright} 2006 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/smr.334},
	ISSN = {1532-0618},
	Keywords = {embedded software,software architecture,software development,software maintenance,software product line,software reuse},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/smr.334}
}

@Article{Parejo2016287,
	Title = {{Multi-objective test case prioritization in highly configurable systems: A case study}},
	Author = {Parejo, Jos{\'{e}} A and S{\'{a}}nchez, Ana B and Segura, Sergio and Ruiz-Cort{\'{e}}s, Antonio and Lopez-Herrejon, Roberto E and Egyed, Alexander},
	Journal = {Journal of Systems and Software},
	Year = {2016},
	Pages = {287--310},
	Volume = {122},
	Abstract = {Abstract Test case prioritization schedules test cases for execution in an order that attempts to accelerate the detection of faults. The order of test cases is determined by prioritization objectives such as covering code or critical components as rapidly as possible. The importance of this technique has been recognized in the context of Highly-Configurable Systems (HCSs), where the potentially huge number of configurations makes testing extremely challenging. However, current approaches for test case prioritization in {\{}HCSs{\}} suffer from two main limitations. First, the prioritization is usually driven by a single objective which neglects the potential benefits of combining multiple criteria to guide the detection of faults. Second, instead of using industry-strength case studies, evaluations are conducted using synthetic data, which provides no information about the effectiveness of different prioritization objectives. In this paper, we address both limitations by studying 63 combinations of up to three prioritization objectives in accelerating the detection of faults in the Drupal framework. Results show that non–functional properties such as the number of changes in the features are more effective than functional metrics extracted from the configuration model. Results also suggest that multi-objective prioritization typically results in faster fault detection than mono-objective prioritization. },
	Doi = {https://doi.org/10.1016/j.jss.2016.09.045},
	ISSN = {0164-1212},
	Keywords = {Automated software testing,Highly-configurable systems,Test case prioritization,Variability},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121216301935}
}

@Article{MeimandiParizi2015463,
	Title = {{Automated test generation technique for aspectual features in AspectJ}},
	Author = {Parizi, Reza Meimandi and Ghani, Abdul Azim Abdul and Lee, Sai Peck},
	Journal = {Information and Software Technology},
	Year = {2015},
	Pages = {463--493},
	Volume = {57},
	Abstract = {AbstractContext Aspect-oriented programming (AOP) has been promoted as a means for handling the modularization of software systems by raising the abstraction level and reducing the scattering and tangling of crosscutting concerns. Studies from literature have shown the usefulness and application of {\{}AOP{\}} across various fields of research and domains. Despite this, research shows that {\{}AOP{\}} is currently used in a cautious way due to its natural impact on testability and maintainability. Objective To realize the benefits of {\{}AOP{\}} and to increase its adoption, aspects developed using {\{}AOP{\}} should be subjected to automated testing. Automated testing, as one of the most pressing needs of the software industry to reduce both effort and costs in assuring correctness, is a delicate issue in testing aspect-oriented programs that still requires advancement and has a way to go before maturity. Method Previous attempts and studies in automated test generation process for aspect-oriented programs have been very limited. This paper proposes a rigorous automated test generation technique, called RAMBUTANS, with its tool support based on guided random testing for the AspectJ programs. Results The paper reports the results of a thorough empirical study of 9 AspectJ benchmark programs, including non-trivial and larger software, by means of mutation analysis to compare {\{}RAMBUTANS{\}} and the four existing automated {\{}AOP{\}} testing approaches for testing aspects in terms of fault detection effectiveness and test effort efficiency. The results of the experiment and statistical tests supplemented by effect size measures presented evidence of the effectiveness and efficiency of the proposed technique at 99{\%} confidence level (i.e. p {\textless} 0.01). Conclusion The study showed that the resulting randomized tests were reasonably good for {\{}AOP{\}} testing, thus the proposed technique could be worth using as an effective and efficient AOP-specific automated test generation technique. },
	Doi = {https://doi.org/10.1016/j.infsof.2014.05.020},
	ISSN = {0950-5849},
	Keywords = {AspectJ,Automated test generation,Empirical study,Software testing,Testing tool},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584914001372}
}

@Article{SMR:SMR1658,
	Title = {{Mobile situation-aware framework for developing smart mobile software}},
	Author = {Park, Joonseok and Kang, Taejun and Yeom, Keunhyuk},
	Journal = {Journal of Software: Evolution and Process},
	Year = {2014},
	Number = {12},
	Pages = {1213--1232},
	Volume = {26},
	Abstract = {Advances in mobile technology and significantly increasing utilization of mobile devices such as smartphones and tablets have resulted in a paradigm shift from PC-centric computing to mobile computing. The results of careful analysis conducted of this mobile landscape indicate that there is a growing demand for smart, user-centric, situation-aware mobile software. Invariably, concomitant with this demand is the need for methodologies that can provide support the development of this type of software. In this paper, we propose a semantic framework, called the mobile situation-aware framework, which supports efficient modeling, construction, processing, management, and inference of mobile situation information. The framework comprises two phases, situation modeling and situation construction, and can serve as a guide and a template in the development of situation-aware applications for the mobile environment. A case study in which mobile situation-aware framework is utilized in the development of a semantic media player verifies the efficacy of the proposed framework. Copyright {\textcopyright} 2014 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/smr.1658},
	ISSN = {2047-7481},
	Keywords = {mobile situation-aware framework,mobile software development,situation awareness development process,smart mobile software},
	Url = {http://dx.doi.org/10.1002/smr.1658}
}

@Article{SPE:SPE1116,
	Title = {{UML design pattern metamodel-level constraints for the maintenance of software evolution}},
	Author = {Park, Jaeyong and Lee, Seok-Won and Rine, David C},
	Journal = {Software: Practice and Experience},
	Year = {2013},
	Number = {7},
	Pages = {835--866},
	Volume = {43},
	Abstract = {Software maintenance including design is difficult because it is usually performed on someone else's work over a period of time. Maintaining a pattern-based design is especially much more difficult when the information on specific patterns that have been used are not available in the corresponding design documents. Also, finding a maintainer who has a similar level of knowledge on specific patterns that the initial designer had is not easy. Pattern-based design, the use of design patterns during the software design process, has become widely used in the object-oriented community because of its many benefits such as its reuse. However, the defects in pattern-based design can be introduced during the design maintenance phase when the changes are made to the requirements and the initial design, but the conformance to the original patterns is neglected. This conformance process is laborious and time consuming; no systematic process exists to guide the defects discovery and maintenance. Also, deep and correct knowledge of design patterns and their characteristics are required because this process is conducted by human experts. Failure to follow this conformance process and to maintain correct designs during software design evolution may cause serious problems in later software development and maintenance stages by not providing the benefits of pattern-based design that would have been possible if followed correctly. There is a strong need of a systematic design and maintenance method for preventing defects in design patterns introduced during the evolution of pattern-based software design and its maintenance. Because conventional UML design methods do not provide such systematic ways of assessing pattern-based design conformance to the evolutionary changes, we have developed the pattern instance changes with UML profiles (PICUP) design method, which is an improved design method for perfective and corrective UML pattern-based design maintenance and assessment. Design pattern in UML profiles (DPUP) is developed for the use of instantiation, maintenance, and assessment of UML pattern-based designs to support the formal specification of a design pattern. DPUPs, as the main part of the PICUP design method, provide metamodel-level UML design constraints using UML stereotype notations and metamodel-level Object Constraint Language design constraints. Assessment of pattern-based designs in UML class diagram with the corresponding DPUPs enforces maintainers to make necessary and correct changes to keep the principles of the original designs. Pattern-related information is annotated using stereotype notations to help assess pattern-based designs when changes are made. Furthermore, the structural conformance checking of a given UML pattern-based design can be automated by using the assessment tool. The major contributions of this paper are: (i) specifying design patterns using extended UML profile; (ii) providing a means of how to instantiate pattern-based designs from DPUPs with naming conventions; and (iii) providing design constraints for maintaining pattern-based design to guide correct changes of a given design in PICUP method. Explanatory type case studies were performed to better understand and evaluate the effectiveness of the PICUP design method with DPUPs. Answers to the case study questionnaires and the pattern defects discovered from the case studies support the hypothesis that the PICUP method is an improved design method that ensures structural conformance of UML pattern-based designs to the corresponding design patterns during perfective and corrective design maintenance for software systems. Copyright {\textcopyright} 2011 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spe.1116},
	ISSN = {1097-024X},
	Keywords = {UML,design pattern,software maintenance},
	Url = {http://dx.doi.org/10.1002/spe.1116}
}

@Article{Parra20111247,
	Title = {{Unifying design and runtime software adaptation using aspect models}},
	Author = {Parra, Carlos and Blanc, Xavier and Cleve, Anthony and Duchien, Laurence},
	Journal = {Science of Computer Programming},
	Year = {2011},
	Number = {12},
	Pages = {1247--1260},
	Volume = {76},
	Abstract = {Software systems are seen more and more as evolutive systems. At the design phase, software is constantly in adaptation by the building process itself, and at runtime, it can be adapted in response to changing conditions in the executing environment such as location or resources. Adaptation is generally difficult to specify because of its cross-cutting impact on software. This article introduces an approach to unify adaptation at design and at runtime based on Aspect Oriented Modeling. Our approach proposes a unified aspect metamodel and a platform that realizes two different weaving processes to achieve design and runtime adaptations. This approach is used in a Dynamic Software Product Line which derives products that can be configured at design time and adapted at runtime in order to dynamically fit new requirements or resource changes. Such products are implemented using the Service Component Architecture and Java. Finally, we illustrate the use of our approach based on an adaptive e-shopping scenario. The main advantages of this unification are: a clear separation of concerns, the self-contained aspect model that can be weaved during the design and execution, and the platform independence guaranteed by two different types of weaving. },
	Annote = {Special Issue on Software Evolution, Adaptability and Variability},
	Doi = {https://doi.org/10.1016/j.scico.2010.12.005},
	File = {:Users/mac/Downloads/Sciencedirect{\_}articles{\_}13Jun2017{\_}14-36-55.006/Unifying-design-and-runtime-software-adaptation-using-aspect-models{\_}2011{\_}Science-of-Computer-Programming.pdf:pdf},
	ISSN = {0167-6423},
	Keywords = {Aspect oriented modeling,Software product lines},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642310002303}
}

@Article{Pascual2015392,
	Title = {{Applying multiobjective evolutionary algorithms to dynamic software product lines for reconfiguring mobile applications}},
	Author = {Pascual, Gustavo G and Lopez-Herrejon, Roberto E and Pinto, M{\'{o}}nica and Fuentes, Lidia and Egyed, Alexander},
	Journal = {Journal of Systems and Software},
	Year = {2015},
	Pages = {392--411},
	Volume = {103},
	Abstract = {Abstract Mobile applications require dynamic reconfiguration services (DRS) to self-adapt their behavior to the context changes (e.g., scarcity of resources). Dynamic Software Product Lines (DSPL) are a well-accepted approach to manage runtime variability, by means of late binding the variation points at runtime. During the system's execution, the {\{}DRS{\}} deploys different configurations to satisfy the changing requirements according to a multiobjective criterion (e.g., insufficient battery level, requested quality of service). Search-based software engineering and, in particular, multiobjective evolutionary algorithms (MOEAs), can generate valid configurations of a {\{}DSPL{\}} at runtime. Several approaches use {\{}MOEAs{\}} to generate optimum configurations of a Software Product Line, but none of them consider {\{}DSPLs{\}} for mobile devices. In this paper, we explore the use of {\{}MOEAs{\}} to generate at runtime optimum configurations of the {\{}DSPL{\}} according to different criteria. The optimization problem is formalized in terms of a Feature Model (FM), a variability model. We evaluate six existing {\{}MOEAs{\}} by applying them to 12 different FMs, optimizing three different objectives (usability, battery consumption and memory footprint). The results are discussed according to the particular requirements of a {\{}DRS{\}} for mobile applications, showing that {\{}PAES{\}} and NSGA-II are the most suitable algorithms for mobile environments. },
	Doi = {https://doi.org/10.1016/j.jss.2014.12.041},
	ISSN = {0164-1212},
	Keywords = {DSPL,Dynamic reconfiguration,Evolutionary algorithms},
	Url = {http://www.sciencedirect.com/science/article/pii/S016412121400291X}
}

@Article{Pascual2015127,
	Title = {{Self-adaptation of mobile systems driven by the Common Variability Language}},
	Author = {Pascual, Gustavo G and Pinto, M{\'{o}}nica and Fuentes, Lidia},
	Journal = {Future Generation Computer Systems},
	Year = {2015},
	Pages = {127--144},
	Volume = {47},
	Abstract = {Abstract The execution context in which pervasive systems or mobile computing run changes continually. Hence, applications for these systems require support for self-adaptation to the continual context changes. Most of the approaches for self-adaptive systems implement a reconfiguration service that receives as input the list of all possible configurations and the plans to switch between them. In this paper we present an alternative approach for the automatic generation of application configurations and the reconfiguration plans at runtime. With our approach, the generated configurations are optimal as regards different criteria, such as functionality or resource consumption (e.g. battery or memory). This is achieved by: (1) modelling architectural variability at design-time using the Common Variability Language (CVL), and (2) using a genetic algorithm that finds nearly-optimal configurations at run-time using the information provided by the variability model. We also specify a case study and we use it to evaluate our approach, showing that it is efficient and suitable for devices with scarce resources. },
	Annote = {Special Section: Advanced Architectures for the Future Generation of Software-Intensive Systems},
	Doi = {https://doi.org/10.1016/j.future.2014.08.015},
	ISSN = {0167-739X},
	Keywords = {Architectural variability,CVL,Context,Dynamic reconfiguration,Genetic algorithm,Pervasive systems},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167739X14001630}
}

@Article{Paz201667,
	Title = {{A Model to Guide Dynamic Adaptation Planning in Self-Adaptive Systems}},
	Author = {Paz, Andr{\'{e}}s and Arboleda, Hugo},
	Journal = {Electronic Notes in Theoretical Computer Science},
	Year = {2016},
	Pages = {67--88},
	Volume = {321},
	Abstract = {Abstract Self-adaptive enterprise applications have the ability to continuously reconfigure themselves according to changes in their execution contexts or user requirements. The infrastructure managing such systems is based on IBM's MAPE-K reference model: a Monitor and an Analyzer to sense and interpret context data, a Planner and an Executor to create and apply structural adaptation plans, and a Knowledge manager to share relevant information. In this paper we present a formal model, built on the principles of constraint satisfaction, to address dynamic adaptation planning for self-adaptive enterprise applications. We formalize, modify and extend the approach presented in [H. Arboleda, J. F. D{\'{i}}az, V. Vargas, and J.-C. Royer, “Automated reasoning for derivation of modeldriven spls,? in SPLC'10 MAPLE'10, 2010, pp. 181–188] for working with self-adaptation infrastructures in order to provide automated reasoning on the dynamic creation of structural adaptation plans. We use a running example to demonstrate the applicability of such model, even in situations where complex interactions arise between context elements and the target self-adaptive enterprise application. },
	Annote = {{\{}CLEI{\}} 2015, the {\{}XLI{\}} Latin American Computing Conference},
	Doi = {https://doi.org/10.1016/j.entcs.2016.02.005},
	ISSN = {1571-0661},
	Keywords = {Automated Reasoning,Dynamic Adaptation Planning,Self-Adaptive Enterprise Applications},
	Url = {http://www.sciencedirect.com/science/article/pii/S1571066116300056}
}

@Article{Pena200771,
	Title = {{Designing and managing evolving systems using a {\{}MAS{\}} product line approach}},
	Author = {Pe{\~{n}}a, Joaquin and Hinchey, Michael G and Resinas, Manuel and Sterritt, Roy and Rash, James L},
	Journal = {Science of Computer Programming},
	Year = {2007},
	Number = {1},
	Pages = {71--86},
	Volume = {66},
	Abstract = {We view an evolutionary system as being a software product line. The core architecture is the unchanging part of the system, and each version of the system may be viewed as a product from the product line. Each “product? may be described as the core architecture with some agent-based additions. The result is a multiagent system software product line. We describe an approach to such a software product line-based approach using the MaCMAS agent-oriented methodology. The approach scales to enterprise architectures as a multiagent system is an appropriate means of representing a changing enterprise architecture and the interaction between components in it. In addition, we reduce the gap between the enterprise architecture and the software architecture. },
	Annote = {Special Issue on the 5th International Workshop on System/Software Architectures (IWSSA'06)},
	Doi = {https://doi.org/10.1016/j.scico.2006.10.007},
	ISSN = {0167-6423},
	Keywords = {Enterprise architecture evolution,Multiagent systems product lines,Swarm-based systems},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642306002498}
}

@Conference{Peake201571,
	Title = {{Analysis of software binaries for reengineering-driven product line architecture - An industrial case study}},
	Author = {Peake, I D and Blech, J O and Fernando, L and Sharma, D and Ramaswamy, S and Kande, M},
	Booktitle = {Electronic Proceedings in Theoretical Computer Science, EPTCS},
	Year = {2015},
	Pages = {71--82},
	Volume = {182},
	Abstract = {This paper describes a method for the recovering of software architectures from a set of similar (but unrelated) software products in binary form. One intention is to drive refactoring into software product lines and combine architecture recovery with run time binary analysis and existing clustering methods. Using our runtime binary analysis, we create graphs that capture the dependencies between different software parts. These are clustered into smaller component graphs, that group software parts with high interactions into larger entities. The component graphs serve as a basis for further software product line work. In this paper, we concentrate on the analysis part of the method and the graph clustering. We apply the graph clustering method to a real application in the context of automation / robot configuration software tools. {\textcopyright} Peake et al.},
	Annote = {cited By 0},
	Doi = {10.4204/EPTCS.182.6},
	Keywords = {Application programs,Architecture recovery; Clustering methods; Config,Bins; Cluster analysis; Computer software; Embedde},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014954197{\&}doi=10.4204{\%}2FEPTCS.182.6{\&}partnerID=40{\&}md5=b9a280dcf333f8991d728f97c88b6d77}
}

@Article{Pedrycz2003383,
	Title = {{N4: computing with neural receptive fields}},
	Author = {Pedrycz, W and Chun, M G and Succi, G},
	Journal = {Neurocomputing},
	Year = {2003},
	Number = {1–2},
	Pages = {383--401},
	Volume = {55},
	Abstract = {In this study, we introduce a new neural architecture called {\{}N4{\}} that is based on a collection of local receptive fields realized in the form of referential neural networks. While the network exhibits some similarities to other structures of modular neural networks (such as expert networks), it comes with a number of unique features. Especially, its receptive fields exhibit high flexibility by being formed by neural networks. Subsequently, the processing therein is of referential nature. A ?skeleton? (structure) of the network is completed through unsupervised learning that is aimed at “discovering? and structuring the main dependencies in data. More specifically, the design of the network consists of two phases. First, a blueprint of the network is formed and this involves the prototypes obtained through clustering of training data. This structural development of the network is followed by further refinement in a form of parametric training of the individual neural receptive fields. The study provides a detailed analysis and learning of the network and includes experimental investigations. },
	Annote = {Support Vector Machines},
	Doi = {https://doi.org/10.1016/S0925-2312(02)00630-6},
	ISSN = {0925-2312},
	Keywords = {Fuzzy clustering,Local neural networks,Modular neural networks,Nearest neighbor classification rule,Neural receptive fields,Prototypes,RBF neural networks},
	Url = {http://www.sciencedirect.com/science/article/pii/S0925231202006306}
}

@Article{Pedrycz2011739,
	Title = {{A model of job satisfaction for collaborative development processes}},
	Author = {Pedrycz, Witold and Russo, Barbara and Succi, Giancarlo},
	Journal = {Journal of Systems and Software},
	Year = {2011},
	Number = {5},
	Pages = {739--752},
	Volume = {84},
	Abstract = {Modern software development relies on collaborative work as a means for sharing knowledge, distributing tasks and responsibilities, reducing risk of failures, and increasing the overall quality of the software product. Such objectives are achieved with a continuous share of the programmers' daily working life that inevitably influences the programmers' job satisfaction. One of the major challenges in process management is to determine the causes of this satisfaction. Traditional research models job satisfaction with social aspects of collaborative work like communication, work sustainability, and work environment. This study reflects on existing models of job satisfaction in collaborative environments, creates one for modern software development processes, and validates it with a retrospective comparative survey run on a sample of 108 respondents. In addition, the work investigates the impact on job satisfaction and its model of the agile practice of Pair Programming that pushes job sharing to the extreme. With this intent, the questionnaire also collected feedback from pair programmers whose responses were used for a comparative analysis. The results demonstrate that Pair Programming has actually a strong positive effect on satisfaction, work sustainability, and communication. },
	Doi = {https://doi.org/10.1016/j.jss.2010.12.018},
	ISSN = {0164-1212},
	Keywords = {Job satisfaction,Log linear model,Pair programming},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121210003407}
}

@Article{Peng20122707,
	Title = {{Self-tuning of software systems through dynamic quality tradeoff and value-based feedback control loop}},
	Author = {Peng, Xin and Chen, Bihuan and Yu, Yijun and Zhao, Wenyun},
	Journal = {Journal of Systems and Software},
	Year = {2012},
	Number = {12},
	Pages = {2707--2719},
	Volume = {85},
	Abstract = {Quality requirements of a software system cannot be optimally met, especially when it is running in an uncertain and changing environment. In principle, a controller at runtime can monitor the change impact on quality requirements of the system, update the expectations and priorities from the environment, and take reasonable actions to improve the overall satisfaction. In practice, however, existing controllers are mostly designed for tuning low-level performance indicators instead of high-level requirements. By maintaining a live goal model to represent runtime requirements and linking the overall satisfaction of quality requirements to an indicator of earned business value, we propose a control-theoretic self-tuning method that can dynamically tune the preferences of different quality requirements, and can autonomously make tradeoff decisions through our Preference-Based Goal Reasoning procedure. The reasoning procedure results in an optimal configuration of the variation points by selecting the right alternative of OR-decomposed goals and such a configuration is mapped onto corresponding system architecture reconfigurations. The effectiveness of our self-tuning method is evaluated by earned business value, comparing our results with those obtained using static and ad hoc methods. },
	Annote = {Self-Adaptive Systems},
	Doi = {https://doi.org/10.1016/j.jss.2012.04.079},
	ISSN = {0164-1212},
	Keywords = {Earned business value,Feedback control theory,Goal-oriented reasoning,Preference,Self-tuning},
	Url = {http://www.sciencedirect.com/science/article/pii/S016412121200132X}
}

@Article{Peng2013664,
	Title = {{Improving feature location using structural similarity and iterative graph mapping}},
	Author = {Peng, Xin and Xing, Zhenchang and Tan, Xi and Yu, Yijun and Zhao, Wenyun},
	Journal = {Journal of Systems and Software},
	Year = {2013},
	Number = {3},
	Pages = {664--676},
	Volume = {86},
	Abstract = {Locating program element(s) relevant to a particular feature is an important step in efficient maintenance of a software system. The existing feature location techniques analyse each feature independently and perform a one-time analysis after being provided an initial input. As a result, these techniques are sensitive to the quality of the input. In this paper, we propose to address the above issues in feature location using an iterative context-aware approach. The underlying intuition is that features are not independent of each other, and the structure of source code resembles the structure of features. The distinguishing characteristics of the proposed approach are: (1) it takes into account the structural similarity between a feature and a program element to determine feature-element relevance and (2) it employs an iterative process to propagate the relevance of the established mappings between a feature and a program element to the neighbouring features and program elements. We evaluate our approach using two different systems, DirectBank, a small-scale industry financial system, and Linux kernel, a large-scale open-source operating system. Our evaluation suggests that the proposed approach is more robust and can significantly increase the recall of feature location with only a minor decrease of precision. },
	Doi = {https://doi.org/10.1016/j.jss.2012.10.270},
	ISSN = {0164-1212},
	Keywords = {Feature location,Information retrieval,Structural similarity,Traceability recovery},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121212003007}
}

@Article{Peng2011707,
	Title = {{Analyzing evolution of variability in a software product line: From contexts and requirements to features}},
	Author = {Peng, X and Yu, Y and Zhao, W},
	Journal = {Information and Software Technology},
	Year = {2011},
	Number = {7},
	Pages = {707--721},
	Volume = {53},
	Abstract = {Context: In the long run, features of a software product line (SPL) evolve with respect to changes in stakeholder requirements and system contexts. Neither domain engineering nor requirements engineering handles such co-evolution of requirements and contexts explicitly, making it especially hard to reason about the impact of co-changes in complex scenarios. Objective: In this paper, we propose a problem-oriented and value-based analysis method for variability evolution analysis. The method takes into account both kinds of changes (requirements and contexts) during the life of an evolving software product line. Method: The proposed method extends the core requirements engineering ontology with the notions to represent variability-intensive problem decomposition and evolution. On the basis of problemorientation, the analysis method identifies candidate changes, detects influenced features, and evaluates their contributions to the value of the SPL. Results and Conclusion: The process of applying the analysis method is illustrated using a concrete case study of an evolving enterprise software system, which has confirmed that tracing back to requirements and contextual changes is an effective way to understand the evolution of variability in the software product line. {\textcopyright} 2011 Elsevier B.V. All rights reserved.},
	Annote = {cited By 10},
	Doi = {10.1016/j.infsof.2011.01.001},
	Keywords = {Context; Evolution; Feature; Requirements; Softwar,Ontology,Requirements engineering},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955070030{\&}doi=10.1016{\%}2Fj.infsof.2011.01.001{\&}partnerID=40{\&}md5=dba8bf360fcd61e03ea82701cae2869b}
}

@Article{Pereira201666,
	Title = {{Quantitative and qualitative empirical analysis of three feature modeling tools}},
	Author = {Pereira, J A and Constantino, K and Figueiredo, E and Saake, G},
	Journal = {Communications in Computer and Information Science},
	Year = {2016},
	Pages = {66--88},
	Volume = {703},
	Abstract = {During the last couple of decades, feature modeling tools have played a significant role in the improvement of software productivity and quality by assisting tasks in software product line (SPL). SPL decomposes a large-scale software system in terms of their functionalities. The goal of the decomposition is to create well-structured individual software systems that can meet different users' requirements. Thus, feature modeling tools provides means to manage the inter-dependencies among reusable common and variable functionalities, called features. There are several tools to support variability management by modeling features in SPL. The variety of tools in the current literature makes it difficult to understand what kinds of tasks are supported and how much effort can be reduced by using these tools. In this paper, we present the results of an empirical study aiming to support SPL engineers choosing the feature modeling tool that best fits their needs. This empirical study compares and analyzes three tools, namely SPLOT, FeatureIDE, and pure::variants. These tools are analyzed based on data from 119 participants. Each participant used one tool for typical feature modeling tasks, such as create a model, update a model, automated analysis of the model, and product configuration. Finally, analysis concerning the perceived ease of use, usefulness, effectiveness, and efficiency are presented. {\textcopyright} Springer International Publishing AG 2016.},
	Annote = {cited By 0},
	Doi = {10.1007/978-3-319-56390-9_4},
	Keywords = {Computer software; Software design,Feature models; Featureide; Pure::variants; Softw,Software engineering},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019123685{\&}doi=10.1007{\%}2F978-3-319-56390-9{\_}4{\&}partnerID=40{\&}md5=4640e9e6abae4822bfe3de6c9a50a71d}
}

@Article{ITOR:ITOR12414,
	Title = {{Heuristic and exact algorithms for product configuration in software product lines}},
	Author = {Pereira, Juliana Alves and Maciel, Lucas and Noronha, Thiago F and Figueiredo, Eduardo},
	Journal = {International Transactions in Operational Research},
	Pages = {n/a----n/a},
	Abstract = {Software product line (SPL) is a set of software applications that share a common set of features satisfying the specific needs of a particular market segment. SPL engineering is a paradigm to develop software applications that commonly use a feature model to capture and document common and variable features, and their relationships. A big challenge is to derive one product among all possible products in the SPL, which satisfies the business and customer requirements. This task is known as product configuration. Although product configuration has been extensively investigated in the literature, customer's preferences are frequently neglected. In this paper, we propose a novel approach to configure a product that considers both qualitative and quantitative feature properties. We model the product configuration task as a combinatorial optimization problem, and heuristic and exact algorithms are proposed. As far as we are concerned, this proposal is the first work in the literature that considers feature properties in both leaf and nonleaf features. Computational experiments showed that the best of our heuristics found optimal solutions for all instances where those are known. For the instances where optimal solutions are not known, our heuristic outperformed the best solution obtained by a one-hour run of the exact algorithm by up to 67.89{\%}.},
	Doi = {10.1111/itor.12414},
	ISSN = {1475-3995},
	Keywords = {combinatorial optimization,heuristic,product configuration,search-based software engineering,software product line},
	Url = {http://dx.doi.org/10.1111/itor.12414}
}

@InProceedings{Pereira:2016:FPR:2993236.2993249,
	Title = {{A Feature-based Personalized Recommender System for Product-line Configuration}},
	Author = {Pereira, Juliana Alves and Matuszyk, Pawel and Krieter, Sebastian and Spiliopoulou, Myra and Saake, Gunter},
	Booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
	Year = {2016},
	Address = {New York, NY, USA},
	Pages = {120--131},
	Publisher = {ACM},
	Series = {GPCE 2016},
	Doi = {10.1145/2993236.2993249},
	ISBN = {978-1-4503-4446-3},
	Keywords = {Personalized Recommendations,Product-Line Configuration,Recommenders,Software Product Lines},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2993236.2993249}
}

@Article{SMR:SMR1799,
	Title = {{Framing program comprehension as fault localization}},
	Author = {Perez, Alexandre and Abreu, Rui},
	Journal = {Journal of Software: Evolution and Process},
	Year = {2016},
	Number = {10},
	Pages = {840--862},
	Volume = {28},
	Abstract = {Program comprehension is a time-consuming task performed during the process of reusing, reengineering, and enhancing existing systems. There are tools to assist comprehension by means of dynamic analysis, but most cannot identify the topology and the interactions of certain functionality in need of change. We propose an approach, coined Spectrum-based Feature Comprehension (SFC), that borrows techniques from software-fault localization that were proven to be effective even when debugging large applications. SFC analyses the program by exploiting run-time information from test case executions to identify the components that are important for a given feature, helping software engineers to understand how a program is structured and each of the functionality's dependencies are. We present a toolset, coined PANGOLIN, that implements SFC and displays its report to the user using an intuitive visualization. A user study presented demonstrating PANGOLIN's efficiency in locating components that should be inspected when changing a certain functionality. Participants using SFC spent a median of 50 min locating the feature with greater accuracy, whereas participants using coverage tools took 60 min. Finally, we also detail the Participatory Feature Detection approach, where user interactions with the system are captured; removing the hindrance of requiring pre-existing automated tests. Copyright {\textcopyright} 2016 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/smr.1799},
	ISSN = {2047-7481},
	Keywords = {fault diagnosis,program comprehension,software evolution and maintenance},
	Url = {http://dx.doi.org/10.1002/smr.1799}
}

@Article{Pessoa201754,
	Title = {{Building reliable and maintainable Dynamic Software Product Lines: An investigation in the Body Sensor Network domain}},
	Author = {Pessoa, Leonardo and Fernandes, Paula and Castro, Thiago and Alves, Vander and Rodrigues, Gena{\'{i}}na N and Carvalho, Hervaldo},
	Journal = {Information and Software Technology},
	Year = {2017},
	Pages = {54--70},
	Volume = {86},
	Abstract = {Abstract Context: Dependability is a key requirement, especially in safety-critical applications. Many of these applications have changing context and configurations at runtime to achieve functional and quality goals and can be realized as Dynamic Software Product Lines (DSPLs). {\{}DSPL{\}} constitutes an emerging but promising research area. Nevertheless, ensuring dependability in {\{}DSPLs{\}} remains insufficiently explored, especially in terms of reliability and maintainability. This compromises quality assurance and applicability of {\{}DSPLs{\}} in safety-critical domains, such as Body Sensor Network (BSN). Objective: To address this issue, we propose an approach to developing reliable and maintainable {\{}DSPLs{\}} in the context of the {\{}BSN{\}} domain. Method: Adaptation plans are instances of a Domain Specific Language (DSL) describing reliability goals and adaptability at runtime. These instances are automatically checked for reliability goal satisfiability before being deployed and interpreted at runtime to provide more suitable adaptation goals complying with evolving needs perceived by a domain specialist. Results: The approach is evaluated in the {\{}BSN{\}} domain. Results show that reliability and maintainability could be provided with execution and reconfiguration times of around 30 ms, notification and adaptation plan update time over the network around 5 s, and space consumption around 5 MB. Conclusion: The method is feasible at reasonable cost. The incurred benefits are reliable vital signal monitoring for the patient—thus providing early detection of serious health issues and the possibility of proactive treatment—and a maintainable infrastructure allowing medical {\{}DSL{\}} instance update to suit the needs of the domain specialist and ultimately of the patient. },
	Doi = {https://doi.org/10.1016/j.infsof.2017.02.002},
	ISSN = {0950-5849},
	Keywords = {Adaptiveness,Body Sensor Network,Context-awareness,Dynamic Software Product Lines,Maintainability,Reliability},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584917301386}
}

@Article{Petersen20151,
	Title = {{Guidelines for conducting systematic mapping studies in software engineering: An update}},
	Author = {Petersen, Kai and Vakkalanka, Sairam and Kuzniarz, Ludwik},
	Journal = {Information and Software Technology},
	Year = {2015},
	Pages = {1--18},
	Volume = {64},
	Abstract = {AbstractContext Systematic mapping studies are used to structure a research area, while systematic reviews are focused on gathering and synthesizing evidence. The most recent guidelines for systematic mapping are from 2008. Since that time, many suggestions have been made of how to improve systematic literature reviews (SLRs). There is a need to evaluate how researchers conduct the process of systematic mapping and identify how the guidelines should be updated based on the lessons learned from the existing systematic maps and {\{}SLR{\}} guidelines. Objective To identify how the systematic mapping process is conducted (including search, study selection, analysis and presentation of data, etc.); to identify improvement potentials in conducting the systematic mapping process and updating the guidelines accordingly. Method We conducted a systematic mapping study of systematic maps, considering some practices of systematic review guidelines as well (in particular in relation to defining the search and to conduct a quality assessment). Results In a large number of studies multiple guidelines are used and combined, which leads to different ways in conducting mapping studies. The reason for combining guidelines was that they differed in the recommendations given. Conclusion The most frequently followed guidelines are not sufficient alone. Hence, there was a need to provide an update of how to conduct systematic mapping studies. New guidelines have been proposed consolidating existing findings. },
	Doi = {https://doi.org/10.1016/j.infsof.2015.03.007},
	ISSN = {0950-5849},
	Keywords = {Guidelines,Software engineering,Systematic mapping studies},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584915000646}
}

@Article{Petre2010533,
	Title = {{Editorial}},
	Author = {Petre, Marian and van der Hoek, Andr{\'{e}} and Baker, Alex},
	Journal = {Design Studies},
	Year = {2010},
	Number = {6},
	Pages = {533--544},
	Volume = {31},
	Annote = {Special Issue Studying Professional Software Design},
	Doi = {https://doi.org/10.1016/j.destud.2010.09.001},
	ISSN = {0142-694X},
	Url = {http://www.sciencedirect.com/science/article/pii/S0142694X10000633}
}

@Article{Pillat201595,
	Title = {{BPMNt: A {\{}BPMN{\}} extension for specifying software process tailoring}},
	Author = {Pillat, Raquel M and Oliveira, Toacy C and Alencar, Paulo S C and Cowan, Donald D},
	Journal = {Information and Software Technology},
	Year = {2015},
	Pages = {95--115},
	Volume = {57},
	Abstract = {AbstractContext Although {\{}SPEM{\}} 2.0 has great potential for software process modeling, it does not provide concepts or formalisms for precise modeling of process behavior. Indeed, {\{}SPEM{\}} fails to address process simulation, execution, monitoring and analysis, which are important activities in process management. On the other hand, {\{}BPMN{\}} 2.0 is a widely used notation to model business processes that has associated tools and techniques to facilitate the aforementioned process management activities. Using {\{}BPMN{\}} to model software development processes can leverage BPMN's infrastructure to improve the quality of these processes. However, {\{}BPMN{\}} lacks an important feature to model software processes: a mechanism to represent process tailoring. Objective This paper proposes BPMNt, a conservative extension to {\{}BPMN{\}} that aims at creating a tailoring representation mechanism similar to the one found in {\{}SPEM{\}} 2.0. Method We have used the {\{}BPMN{\}} 2.0 extensibility mechanism to include the representation of specific tailoring relationships namely suppression, local contribution, and local replacement, which establish links between process elements (such as in the case of SPEM). Moreover, this paper also presents some rules to ensure the consistency of {\{}BPMN{\}} models when using tailoring relationships. Results In order to evaluate our proposal we have implemented a tool to support the {\{}BPMNt{\}} approach and have applied it for representing real process adaptations in the context of an academic management system development project. Results of this study showed that the approach and its support tool can successfully be used to adapt BPMN-based software processes in real scenarios. Conclusion We have proposed an approach to enable reuse and adaptation of BPMN-based software process models as well as derivation traceability between models through tailoring relationships. We believe that bringing such capabilities into {\{}BPMN{\}} will open new perspectives to software process management. },
	Doi = {https://doi.org/10.1016/j.infsof.2014.09.004},
	ISSN = {0950-5849},
	Keywords = {BPMN,Process modeling,Software process tailoring},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584914002031}
}

@Article{Pinto2012525,
	Title = {{Deriving detailed design models from an aspect-oriented {\{}ADL{\}} using {\{}MDD{\}}}},
	Author = {Pinto, M{\'{o}}nica and Fuentes, Lidia and Fern{\'{a}}ndez, Luis},
	Journal = {Journal of Systems and Software},
	Year = {2012},
	Number = {3},
	Pages = {525--545},
	Volume = {85},
	Abstract = {Software architects can separate crosscutting concerns more appropriately by using an aspect-oriented ADL, concretely AO-ADL. This paper illustrates how aspect-orientation and model-driven development technologies can be used to enhance the system design phase; by automatically deriving detailed designs that take into account the “aspects? identified at the architectural level. Specifically, we have defined model-to-model transformation rules to automatically generate either aspect-oriented or object-oriented {\{}UML{\}} 2.0 models, closing the gap between {\{}ADLs{\}} and the notations used at the detailed design phase. By using AO-ADL it is possible to specify separately crosscutting concerns and base functionality. Another advantage of using AO-ADL is that it allows the specification of parameterizable architectures, promoting the definition of architectural templates. AO-ADL, then, enforces the specification of crosscutting concerns as separate architectural templates, which can be later instantiated and integrated with the core functionality of the system being developed. The AO-ADL language and the transformation rules from AO-ADL to {\{}UML{\}} 2.0 are available throughout the AO-ADL Tool Suite, which can be used to progressively refine and elaborate aspect-oriented software architectures. These refined architectures are the starting point of the detailed design phase. This means that our approach provides support to automatically generate a skeleton of the detailed design that preserves the information about the crosscutting and the non-crosscutting functionalities identified and modelled at the architecture level. },
	Annote = {Novel approaches in the design and implementation of systems/software architecture},
	Doi = {https://doi.org/10.1016/j.jss.2011.05.026},
	ISSN = {0164-1212},
	Keywords = {AO-ADL,ATL,Aspect-oriented software development,Model-driven development,Software architectures,Theme/UML,UML 2.0},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121211001269}
}

@Article{Pinto20111165,
	Title = {{Specifying aspect-oriented architectures in AO-ADL}},
	Author = {Pinto, M{\'{o}}nica and Fuentes, Lidia and Troya, Jos{\'{e}} Mar{\'{i}}a},
	Journal = {Information and Software Technology},
	Year = {2011},
	Number = {11},
	Pages = {1165--1182},
	Volume = {53},
	Abstract = {Context Architecture description languages (ADLs) are a well-accepted approach to software architecture representation. The majority of well-known {\{}ADLs{\}} are defined by means of components and connectors. Architectural connectors are mainly used to model interactions among components, specifying component communication and coordination separately. However, there are other properties that cut across several components and also affect component interactions (e.g. security). Objective It seems reasonable therefore to model how such crosscutting properties affect component interactions as part of connectors. Method Using an aspect-oriented approach, the AO-ADL architecture description language extends the classical connector semantics with enough expressiveness to model the influences of such crosscutting properties on component interactions (defined as ‘aspectual compositions' in connectors). Results This paper describes the AO-ADL language putting special emphasis on the extended connectors used to specify aspectual and non-aspectual compositions between concrete components. The contributions of AO-ADL are validated using concern-oriented metrics available in the literature. Conclusion The measured indicators show that using AO-ADL it is possible to specify more reusable and scalable software architectures. },
	Annote = {{\{}AMOST{\}} 2010AMOST 2010},
	Doi = {https://doi.org/10.1016/j.infsof.2011.04.003},
	ISSN = {0950-5849},
	Keywords = {Aspect-Oriented Software Development,Languages,Metrics,Software Architectures,Software Engineering},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584911001005}
}

@InProceedings{Pinzger:2008:DNP:1453101.1453105,
	Title = {{Can Developer-module Networks Predict Failures?}},
	Author = {Pinzger, Martin and Nagappan, Nachiappan and Murphy, Brendan},
	Booktitle = {Proceedings of the 16th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
	Year = {2008},
	Address = {New York, NY, USA},
	Pages = {2--12},
	Publisher = {ACM},
	Series = {SIGSOFT '08/FSE-16},
	Doi = {10.1145/1453101.1453105},
	ISBN = {978-1-59593-995-1},
	Keywords = {developer contribution network,failure prediction,network centrality measures,social network analysis},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/1453101.1453105}
}

@Article{Pleuss20122261,
	Title = {{Model-driven support for product line evolution on feature level}},
	Author = {Pleuss, Andreas and Botterweck, Goetz and Dhungana, Deepak and Polzer, Andreas and Kowalewski, Stefan},
	Journal = {Journal of Systems and Software},
	Year = {2012},
	Number = {10},
	Pages = {2261--2274},
	Volume = {85},
	Abstract = {Software Product Lines (SPL) are an engineering technique to efficiently derive a set of similar products from a set of shared assets. In particular in conjunction with model-driven engineering, {\{}SPL{\}} engineering promises high productivity benefits. There is however, a lack of support for systematic management of {\{}SPL{\}} evolution, which is an important success factor as a product line often represents a long term investment. In this article, we present a model-driven approach for managing {\{}SPL{\}} evolution on feature level. To reduce complexity we use model fragments to cluster related elements. The relationships between these fragments are specified using feature model concepts itself leading to a specific kind of feature model called EvoFM. A configuration of EvoFM represents an evolution step and can be transformed to a concrete instance of the product line (i.e., a feature model for the corresponding point in time). Similarly, automatic transformations allow the derivation of an EvoFM from a given set of feature models. This enables retrospective analysis of historic evolution and serves as a starting point for introduction of EvoFM, e.g., to plan future evolution steps. },
	Annote = {Automated Software Evolution},
	Doi = {https://doi.org/10.1016/j.jss.2011.08.008},
	ISSN = {0164-1212},
	Keywords = {Evolving systems,Feature modeling,Model-driven engineering,Software Product Lines},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121211002093}
}

@InProceedings{Pohl2013,
	Title = {{Measuring the structural complexity of feature models}},
	Author = {Pohl, Richard and Stricker, Vanessa and Pohl, Klaus},
	Booktitle = {2013 28th IEEE/ACM International Conference on Automated Software Engineering (ASE)},
	Year = {2013},
	Month = {nov},
	Pages = {454--464},
	Publisher = {IEEE},
	Doi = {10.1109/ASE.2013.6693103},
	ISBN = {978-1-4799-0215-6},
	Url = {http://ieeexplore.ieee.org/document/6693103/}
}

@Article{Poort20121995,
	Title = {{RCDA: Architecting as a risk- and cost management discipline}},
	Author = {Poort, Eltjo R and van Vliet, Hans},
	Journal = {Journal of Systems and Software},
	Year = {2012},
	Number = {9},
	Pages = {1995--2013},
	Volume = {85},
	Abstract = {We propose to view architecting as a risk- and cost management discipline. This point of view helps architects identify the key concerns to address in their decision making, by providing a simple, relatively objective way to assess architectural significance. It also helps business stakeholders to align the architect's activities and results with their own goals. We examine the consequences of this point of view on the architecture process. The point of view is the basis of RCDA, the Risk- and Cost Driven Architecture approach. So far, more than 150 architects have received {\{}RCDA{\}} training. For a majority of the trainees, {\{}RCDA{\}} has a significant positive impact on their architecting work. },
	Annote = {Selected papers from the 2011 Joint Working IEEE/IFIP Conference on Software Architecture (WICSA 2011)},
	Doi = {https://doi.org/10.1016/j.jss.2012.03.071},
	ISSN = {0164-1212},
	Keywords = {Cost management,Risk Management,Software architecture},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121212000994}
}

@Article{Preuveneers2016162,
	Title = {{Systematic scalability assessment for feature oriented multi-tenant services}},
	Author = {Preuveneers, Davy and Heyman, Thomas and Berbers, Yolande and Joosen, Wouter},
	Journal = {Journal of Systems and Software},
	Year = {2016},
	Pages = {162--176},
	Volume = {116},
	Abstract = {Abstract Recent software engineering paradigms such as software product lines, supporting development techniques like feature modeling, and cloud provisioning models such as platform and infrastructure as a service, allow for great flexibility during both software design and deployment, resulting in potentially large cost savings. However, all this flexibility comes with a catch: as the combinatorial complexity of optional design features and deployment variability increases, the difficulty of assessing system qualities such as scalability and quality of service increases too. And if the software itself is not scalable (for instance, because of a specific set of selected features), deploying additional service instances is a futile endeavor. Clearly there is a need to systematically measure the impact of feature selection on scalability, as the potential cost savings can be completely mitigated by the risk of having a system that is unable to meet service demand. In this work, we document our results on systematic load testing for automated quality of service and scalability analysis. The major contribution of our work is tool support and a methodology to analyze the scalability of these distributed, feature oriented multi-tenant software systems in a continuous integration process. We discuss our approach to select features for load testing such that a representative set of feature combinations is used to elicit valuable information on the performance impact and feature interactions. Additionally, we highlight how our methodology and framework for performance and scalability prediction differs from state-of-practice solutions. We take the viewpoint of both the tenant of the service and the service provider, and report on our experiences applying the approach to an industrial use case in the domain of electronic payments. We conclude that the integration of systematic scalability tests in a continuous integration process offers strong advantages to software developers and service providers, such as the ability to quantify the impact of new features in existing service compositions, and the early detection of hidden feature interactions that may negatively affect the overall performance of multi-tenant services. },
	Doi = {https://doi.org/10.1016/j.jss.2015.12.024},
	ISSN = {0164-1212},
	Keywords = {Distributed systems,Scalability,Tool support},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121215002897}
}

@Article{Purao:2003:PMO:857076.857090,
	Title = {{Product Metrics for Object-oriented Systems}},
	Author = {Purao, Sandeep and Vaishnavi, Vijay},
	Journal = {ACM Comput. Surv.},
	Year = {2003},
	Number = {2},
	Pages = {191--221},
	Volume = {35},
	Address = {New York, NY, USA},
	Doi = {10.1145/857076.857090},
	ISSN = {0360-0300},
	Keywords = {Software metrics,measurement theory,object-oriented metrics,object-oriented product metrics,object-oriented systems},
	Publisher = {ACM},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/857076.857090}
}

@Article{Qu2014544,
	Title = {{Pattern mining of cloned codes in software systems}},
	Author = {Qu, Wei and Jia, Yuanyuan and Jiang, Michael},
	Journal = {Information Sciences},
	Year = {2014},
	Pages = {544--554},
	Volume = {259},
	Abstract = {Pattern mining of cloned codes in software systems is a challenging task due to various modifications and the large size of software codes. Most existing approaches adopt a token-based software representation and use sequential analysis for pattern mining of cloned codes. Due to the intrinsic limitations of such spatial space analysis, these methods have difficulties handling statement reordering, insertion and control replacement. Recently, graph-based models such as program dependent graph have been exploited to solve these issues. Although they can improve the performance in terms of accuracy, they introduce additional problems. Their computational complexity is very high and dramatically increases with the software size, thus limiting their applications in practice. In this paper, we propose a novel pattern mining framework for cloned codes in software systems. It efficiently exploits software's spatial space information as well as graph space information and thus can mine accurate patterns of cloned codes for software systems. Preliminary experimental results have demonstrated the superior performance of the proposed approach compared with other methods. },
	Doi = {https://doi.org/10.1016/j.ins.2010.04.022},
	ISSN = {0020-0255},
	Keywords = {Pattern mining,Software clone detection,Software engineering,Software reuse detection},
	Url = {http://www.sciencedirect.com/science/article/pii/S0020025510001787}
}

@InProceedings{Quesada-Lopez:2014:FPS:2652524.2652595,
	Title = {{Function Point Structure and Applicability Validation Using the ISBSG Dataset: A Replicated Study}},
	Author = {Quesada-L{\'{o}}pez, Christian and Jenkins, Marcelo},
	Booktitle = {Proceedings of the 8th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
	Year = {2014},
	Address = {New York, NY, USA},
	Pages = {66:1----66:1},
	Publisher = {ACM},
	Series = {ESEM '14},
	Doi = {10.1145/2652524.2652595},
	ISBN = {978-1-4503-2774-9},
	Keywords = {business application,experimental procedure,function points,replication study,software functional size measurement},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2652524.2652595}
}

@Article{Quinton201655,
	Title = {{SALOON: A platform for selecting and configuring cloud environments}},
	Author = {Quinton, C and Romero, D and Duchien, L},
	Journal = {Software - Practice and Experience},
	Year = {2016},
	Number = {1},
	Pages = {55--78},
	Volume = {46},
	Abstract = {Migrating legacy systems or deploying a new application to a cloud environment has recently become very trendy, because the number of cloud providers available is still increasing. These cloud environments provide a wide range of resources at different levels of functionality, which must be appropriately configured by stakeholders for the application to run properly. Handling this variability during the configuration and deployment stages is known as a complex and error-prone process, usually made in an ad hoc manner. In this paper, we propose SALOON, a software product lines-based platform to face these issues. We describe the architecture of the SALOON platform, which relies on feature models combined with a domain model used to select among cloud environments a well-suited one. SALOON supports stakeholders while configuring the selected cloud environment in a consistent way and automates the deployment of such configurations through the generation of executable configuration scripts. This paper also reports on some experiments, showing that using SALOON significantly reduces time to configure a cloud environment compared with a manual approach and provides a reliable way to find a correct and suitable configuration. Moreover, our empirical evaluation shows that our approach is effective and scalable to properly deal with a significant number of cloud environments. {\textcopyright} 2015 John Wiley {\&} Sons, Ltd.},
	Annote = {cited By 5},
	Doi = {10.1002/spe.2311},
	Keywords = {Cloud computing; Computer software; Legacy systems,Cloud environments; Cloud providers; Empirical ev,Software design},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955684730{\&}doi=10.1002{\%}2Fspe.2311{\&}partnerID=40{\&}md5=89bb5c31cca858f908e56667d4988d6a}
}

@Article{SPIP:SPIP211,
	Title = {{Characterizing configurable software product families and their derivation}},
	Author = {Raatikainen, Mikko and Soininen, Timo and M??nnist??, Tomi and Mattila, Antti and M{\"{a}}nnist{\"{o}}, Tomi and Mattila, Antti},
	Journal = {Software Process: Improvement and Practice},
	Year = {2005},
	Number = {1},
	Pages = {41--60},
	Volume = {10},
	Doi = {10.1002/spip.211},
	File = {:Users/mac/Downloads/Raatikainen{\_}et{\_}al-2005-Software{\_}Process-{\_}Improvement{\_}and{\_}Practice.pdf:pdf},
	ISSN = {1099-1670},
	Keywords = {Case study,Configurable product base,Configurable software product family,Product derivation,Software product family,case study,configurable product base,configurable software product family,product derivation,software product family},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/spip.211}
}

@Article{Raatikainen2004403,
	Title = {{A case study of two configurable software product families}},
	Author = {Raatikainen, M and Soininen, T and M{\"{a}}nnist{\"{o}}, T and Mattila, A},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2004},
	Pages = {403--421},
	Volume = {3014},
	Abstract = {A configurable software product family allows the deployment of individual products without customer-specific design or programming effort. Despite the fact that such software product families have recently gained research interest, there are only few empirical studies on them. This paper presents some results of a descriptive case study undertaken in two companies that develop and deploy configurable software product families. The similarities found in comparisons between characteristics of the configurable software product families were remarkable, although the companies, products, and application domains were different. The study shows that the configurable software product family approach is already applied in the industry. Furthermore, the approach seems to be a feasible and even efficient way to systematically develop a family of products and manage the variability within it. {\textcopyright} Springer-Verlag Berlin Heidelberg 2004.},
	Annote = {cited By 4},
	Keywords = {Application programs,Configurable software product family; Empirical s,Product design},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-35048821987{\&}partnerID=40{\&}md5=c80c150668f0c1fff6ec434df422f942}
}

@InProceedings{Rabai:2011:DBM:2107556.2107563,
	Title = {{Data Base Management Systems Trends: An Empirical Study}},
	Author = {Rabai, Latifa Ben Arfa},
	Booktitle = {Proceedings of the Second Kuwait Conference on e-Services and e-Systems},
	Year = {2011},
	Address = {New York, NY, USA},
	Pages = {7:1----7:6},
	Publisher = {ACM},
	Series = {KCESS '11},
	Doi = {10.1145/2107556.2107563},
	ISBN = {978-1-4503-0793-2},
	Keywords = {data base management systems,extrinsic factors,historical trends,intrinsic factors,software technology trends,successful trends},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2107556.2107563}
}

@Article{Rabiser2010324,
	Title = {{Requirements for product derivation support: Results from a systematic literature review and an expert survey}},
	Author = {Rabiser, R and Gr{\"{u}}nbacher, P and Dhungana, D},
	Journal = {Information and Software Technology},
	Year = {2010},
	Number = {3},
	Pages = {324--346},
	Volume = {52},
	Abstract = {Context: An increasing number of publications in product line engineering address product derivation, i.e., the process of building products from reusable assets. Despite its importance, there is still no consensus regarding the requirements for product derivation support. Objective: Our aim is to identify and validate requirements for tool-supported product derivation. Method: We identify the requirements through a systematic literature review and validate them with an expert survey. Results: We discuss the resulting requirements and provide implementation examples from existing product derivation approaches. Conclusions: We conclude that key requirements are emerging in the research literature and are also considered relevant by experts in the field. {\textcopyright} 2009 Elsevier B.V. All rights reserved.},
	Annote = {cited By 59},
	Doi = {10.1016/j.infsof.2009.11.001},
	Keywords = {Building products; Expert survey; Product derivati,Computer software; Surveys,Production engineering},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-75449091148{\&}doi=10.1016{\%}2Fj.infsof.2009.11.001{\&}partnerID=40{\&}md5=dd02663c17d0c088444aea71f6824956}
}

@Article{Rabiser2011285,
	Title = {{Key activities for product derivation in software product lines}},
	Author = {Rabiser, Rick and O'Leary, P{\'{a}}draig and Richardson, Ita},
	Journal = {Journal of Systems and Software},
	Year = {2011},
	Number = {2},
	Pages = {285--300},
	Volume = {84},
	Abstract = {More and more organizations adopt software product lines to leverage extensive reuse and deliver a multitude of benefits such as increased quality and productivity and a decrease in cost and time-to-market of their software development. When compared to the vast amount of research on developing product lines, relatively little work has been dedicated to the actual use of product lines to derive individual products, i.e., the process of product derivation. Existing approaches to product derivation have been developed independently for different aims and purposes. While the definition of a general approach applicable to every domain may not be possible, it would be interesting for researchers and practitioners to know which activities are common in existing approaches, i.e., what are the key activities in product derivation. In this paper we report on how we compared two product derivation approaches developed by the authors in two different, independent research projects. Both approaches independently sought to identify product derivation activities, one through a process reference model and the other through a tool-supported derivation approach. Both approaches have been developed and validated in research industry collaborations with different companies. Through the comparison of the approaches we identify key product derivation activities. We illustrate the activities' importance with examples from industry collaborations. To further validate the activities, we analyze three existing product derivation approaches for their support for these activities. The validation provides evidence that the identified activities are relevant to product derivation and we thus conclude that they should be considered (e.g., as a checklist) when developing or evaluating a product derivation approach. },
	Doi = {https://doi.org/10.1016/j.jss.2010.09.042},
	ISSN = {0164-1212},
	Keywords = {Process,Product derivation,Software product lines},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121210002700}
}

@InProceedings{Rahmat:2016:AMP:2857546.2857608,
	Title = {{Actor in Multi Product Line}},
	Author = {Rahmat, Azizah and Kassim, Suzana and Selamat, Mohd Hasan and Hassan, Sa'adah},
	Booktitle = {Proceedings of the 10th International Conference on Ubiquitous Information Management and Communication},
	Year = {2016},
	Address = {New York, NY, USA},
	Pages = {61:1----61:8},
	Publisher = {ACM},
	Series = {IMCOM '16},
	Doi = {10.1145/2857546.2857608},
	ISBN = {978-1-4503-4142-4},
	Keywords = {Software product line,actor,cross-domain reference architecture,multi product line,reference architecture},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2857546.2857608}
}

@Article{SPE:SPE908,
	Title = {{Towards generic representation of web applications: solutions and trade-offs}},
	Author = {Rajapakse, Damith C and Jarzabek, Stan},
	Journal = {Software: Practice and Experience},
	Year = {2009},
	Number = {5},
	Pages = {501--530},
	Volume = {39},
	Abstract = {Server pages (also called dynamic pages) render a generic web page into many similar ones. The technique is commonly used for implementing web application user interfaces (UIs). Yet our previous study found a high rate of repetitions (also called ‘clones') in web applications, particularly in UIs. The finding raised the question as to why such repetitions had not been averted with the use of server pages. For an answer, we conducted an experiment using PHP server pages to explore how far server pages can be pushed to achieve generic web applications. Our initial findings suggested that generic representation obtained using server pages sometimes compromises certain important system qualities such as run-time performance. It may also complicate the use of WYSIWYG editors. We have analysed the nature of these trade-offs, and now propose a mixed-strategy approach to obtain optimum generic representation of web applications without unnecessary compromise to critical system qualities and user experience. The mixed-strategy approach applies the generative technique of XVCL to achieve genericity at the meta-level representation of a web application, leaving repetitions to the actual web application. Our experiments show that the mixed-strategy approach can achieve a good level of genericity without conflicting with other system qualities. Our findings should open the way for others to better-informed decisions regarding generic design solutions, which should in turn lead to simpler, more maintainable and more reusable web applications. Copyright {\textcopyright} 2008 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spe.908},
	ISSN = {1097-024X},
	Keywords = {Web applications,clone unification,code duplication,design patterns,genericity,meta-programming},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/spe.908}
}

@InProceedings{Ramler:2010:USC:1852786.1852848,
	Title = {{The Usual Suspects: A Case Study on Delivered Defects Per Developer}},
	Author = {Ramler, Rudolf and Klammer, Claus and Natschl{\"{a}}ger, Thomas},
	Booktitle = {Proceedings of the 2010 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
	Year = {2010},
	Address = {New York, NY, USA},
	Pages = {48:1----48:4},
	Publisher = {ACM},
	Series = {ESEM '10},
	Doi = {10.1145/1852786.1852848},
	ISBN = {978-1-4503-0039-1},
	Keywords = {case study,defect introduction,defect removal,post-release defects},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/1852786.1852848}
}

@Article{Ramos20081207,
	Title = {{Embedded software revitalization through component mining and software product line techniques}},
	Author = {Ramos, M A and Penteado, R A D},
	Journal = {Journal of Universal Computer Science},
	Year = {2008},
	Number = {8},
	Pages = {1207--1227},
	Volume = {14},
	Abstract = {The mining of generic software components from legacy systems can be used as an auxiliary technique to revitalize systems. This paper presents a software maintenance approach that uses such technique to revitalize one or more embedded legacy systems simultaneously and, in addition, create a core of reusable assets that can be used to support the development of new similar products. Software Product Line techniques are used to support the tasks of domain modelling and software component development. A real case study in the domain of Point of Sale (POS) terminals is presented and it illustrates the use of the proposed approach to revitalize three similar embedded legacy systems, simultaneously. It also shows how it is possible, through the created core of reusable assets, to deliver variations of these systems to meet the requirements of a wide family of POS terminals with different hardware configurations. {\textcopyright} J.UCS.},
	Annote = {cited By 2},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-46049088149{\&}partnerID=40{\&}md5=3196ea29d6331ce88e4ea332ebd0a55a}
}

@Article{Rattan20131165,
	Title = {{Software clone detection: A systematic review}},
	Author = {Rattan, Dhavleesh and Bhatia, Rajesh and Singh, Maninder},
	Journal = {Information and Software Technology},
	Year = {2013},
	Number = {7},
	Pages = {1165--1199},
	Volume = {55},
	Abstract = {Context Reusing software by means of copy and paste is a frequent activity in software development. The duplicated code is known as a software clone and the activity is known as code cloning. Software clones may lead to bug propagation and serious maintenance problems. Objective This study reports an extensive systematic literature review of software clones in general and software clone detection in particular. Method We used the standard systematic literature review method based on a comprehensive set of 213 articles from a total of 2039 articles published in 11 leading journals and 37 premier conferences and workshops. Results Existing literature about software clones is classified broadly into different categories. The importance of semantic clone detection and model based clone detection led to different classifications. Empirical evaluation of clone detection tools/techniques is presented. Clone management, its benefits and cross cutting nature is reported. Number of studies pertaining to nine different types of clones is reported. Thirteen intermediate representations and 24 match detection techniques are reported. Conclusion We call for an increased awareness of the potential benefits of software clone management, and identify the need to develop semantic and model clone detection techniques. Recommendations are given for future research. },
	Doi = {https://doi.org/10.1016/j.infsof.2013.01.008},
	ISSN = {0950-5849},
	Keywords = {Clone detection,Model based clone,Semantic clones,Software clone,Systematic literature review},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584913000323}
}

@Article{Rebelo20131137,
	Title = {{Optimizing generated aspect-oriented assertion checking code for {\{}JML{\}} using program transformations: An empirical study}},
	Author = {Reb{\^{e}}lo, Henrique and Lima, Ricardo and Leavens, Gary T and Corn{\'{e}}lio, M{\'{a}}rcio and Mota, Alexandre and Oliveira, C{\'{e}}sar},
	Journal = {Science of Computer Programming},
	Year = {2013},
	Number = {8},
	Pages = {1137--1156},
	Volume = {78},
	Abstract = {The AspectJ {\{}JML{\}} compiler (ajmlc) explores aspect-oriented programming (AOP) mechanisms to implement {\{}JML{\}} specifications, such as pre- and postconditions, and enforce them during runtime. This compiler was created to improve source-code modularity. Some experiments were conducted to evaluate the performance of the code generated through ajmlc. Results demonstrated that the strategy of adopting {\{}AOP{\}} to implement {\{}JML{\}} specifications is very promising. However, there is still a need for optimization of the generated code's bytecode size and running time. This paper presents a catalog of transformations which represent the optimizations implemented in the new optimized version of the ajmlc compiler. We employ such transformations to reduce the bytecode size and running time of the code generated through the ajmlc compiler. Aiming at demonstrating the impact of such transformation on the code quality, we conduct an empirical study using four applications in optimized and non-optimized versions generated by ajmlc. We show that our {\{}AOP{\}} transformations provide a significant improvement, regarding bytecode size and running time. },
	Annote = {Special section on software evolution, adaptability, and maintenance {\&} Special section on the Brazilian Symposium on Programming Languages},
	Doi = {https://doi.org/10.1016/j.scico.2012.09.003},
	ISSN = {0167-6423},
	Keywords = {Aspect-oriented programming,JML,Program transformation},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642312001682}
}

@Article{ReinhartzBerger2010491,
	Title = {{Towards automatization of domain modeling}},
	Author = {Reinhartz-Berger, Iris},
	Journal = {Data {\&} Knowledge Engineering},
	Year = {2010},
	Number = {5},
	Pages = {491--515},
	Volume = {69},
	Abstract = {A domain model, which captures the common knowledge and the possible variability allowed among applications in a domain, may assist in the creation of other valid applications in that domain. However, to create such domain models is not a trivial task: it requires expertise in the domain, reaching a very high level of abstraction, and providing flexible, yet formal, artifacts. In this paper an approach, called Semi-automated Domain Modeling (SDM), to create draft domain models from applications in those domains, is presented. {\{}SDM{\}} takes a repository of application models in a domain and matches, merges, and generalizes them into sound draft domain models that include the commonality and variability allowed in these domains. The similarity of the different elements is measured, with consideration of syntactic, semantic, and structural aspects. Unlike ontology and schema integration, these models capture both structural and behavioral aspects of the domain. Running {\{}SDM{\}} on small repositories of project management applications and scheduling systems, we found that the approach may provide reasonable draft domain models, whose comprehensibility, correctness, completeness, and consistency levels are satisfactory. },
	Doi = {https://doi.org/10.1016/j.datak.2010.01.002},
	ISSN = {0169-023X},
	Keywords = {DSL,Domain analysis,Domain engineering,Metamodeling,Product line engineering,UML},
	Url = {http://www.sciencedirect.com/science/article/pii/S0169023X10000030}
}

@Article{ReinhartzBerger201781,
	Title = {{Investigating styles in variability modeling: Hierarchical vs. constrained styles}},
	Author = {Reinhartz-Berger, Iris and Figl, Kathrin and Haugen, {\O}ystein},
	Journal = {Information and Software Technology},
	Year = {2017},
	Pages = {81--102},
	Volume = {87},
	Abstract = {AbstractContext A common way to represent product lines is with variability modeling. Yet, there are different ways to extract and organize relevant characteristics of variability. Comprehensibility of these models and the ease of creating models are important for the efficiency of any variability management approach. Objective The goal of this paper is to investigate the comprehensibility of two common styles to organize variability into models – hierarchical and constrained – where the dependencies between choices are specified either through the hierarchy of the model or as cross-cutting constraints, respectively. Method We conducted a controlled experiment with a sample of 90 participants who were students with prior training in modeling. Each participant was provided with two variability models specified in Common Variability Language (CVL) and was asked to answer questions requiring interpretation of provided models. The models included 9–20 nodes and 8–19 edges and used the main variability elements. After answering the questions, the participants were asked to create a model based on a textual description. Results The results indicate that the hierarchical modeling style was easier to comprehend from a subjective point of view, but there was also a significant interaction effect with the degree of dependency in the models, that influenced objective comprehension. With respect to model creation, we found that the use of a constrained modeling style resulted in higher correctness of variability models. Conclusions Prior exposure to modeling style and the degree of dependency among elements in the model determine what modeling style a participant chose when creating the model from natural language descriptions. Participants tended to choose a hierarchical style for modeling situations with high dependency and a constrained style for situations with low dependency. Furthermore, the degree of dependency also influences the comprehension of the variability model. },
	Doi = {https://doi.org/10.1016/j.infsof.2017.01.012},
	ISSN = {0950-5849},
	Keywords = {Cognitive aspects,Comprehensibility,Empirical research,Feature modeling,Hierarchical modeling,Product line engineering,Textual constraints,Variability modeling},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584917300800}
}

@Article{Reinhartz-Berger2014678,
	Title = {{Comprehensibility of UML-based software product line specifications A controlled experiment}},
	Author = {Reinhartz-Berger, I and Sturm, A},
	Journal = {Empirical Software Engineering},
	Year = {2014},
	Number = {3},
	Pages = {678--713},
	Volume = {19},
	Abstract = {Software Product Line Engineering (SPLE) deals with developing artifacts that capture the common and variable aspects of software product families. Domain models are one kind of such artifacts. Being developed in early stages, domain models need to specify commonality and variability and guide the reuse of the artifacts in particular software products. Although different modeling methods have been proposed to manage and support these activities, the assessment of these methods is still in an inceptive stage. In this work, we examined the comprehensibility of domain models specified in ADOM, a UML-based SPLE method. In particular, we conducted a controlled experiment in which 116 undergraduate students were required to answer comprehension questions regarding a domain model that was equipped with explicit reuse guidance and/or variability specification. We found that explicit specification of reuse guidance within the domain model helped understand the model, whereas explicit specification of variability increased comprehensibility only to a limited extent. Explicit specification of both reuse guidance and variability often provided intermediate results, namely, results that were better than specification of variability without reuse guidance, but worse than specification of reuse guidance without variability. All these results were perceived in different UML diagram types, namely, use case, class, and sequence diagrams and for different commonality-, variability-, and reuse-related aspects. {\textcopyright} 2012 Springer Science+Business Media, LLC.},
	Annote = {cited By 3},
	Doi = {10.1007/s10664-012-9234-8},
	Keywords = {Computer software reusability,Domain model; Empirical evaluations; Software pro,Experiments; Query languages; Software design; Spe},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899943828{\&}doi=10.1007{\%}2Fs10664-012-9234-8{\&}partnerID=40{\&}md5=8c6715fe527f714fb9a98aa9dd230798}
}

@Article{ReinhartzBerger20091275,
	Title = {{Utilizing domain models for application design and validation}},
	Author = {Reinhartz-Berger, Iris and Sturm, Arnon},
	Journal = {Information and Software Technology},
	Year = {2009},
	Number = {8},
	Pages = {1275--1289},
	Volume = {51},
	Abstract = {Domain analysis enables identifying families of applications and capturing their terminology in order to assist and guide system developers to design valid applications in the domain. One major way of carrying out the domain analysis is modeling. Several studies suggest using metamodeling techniques, feature-oriented approaches, or architectural-based methods for modeling domains and specifying applications in those domains. However, these methods mainly focus on representing the domain knowledge, providing insufficient guidelines (if any) for creating application models that satisfy the domain rules and constraints. In particular, validation of the application models which include application-specific knowledge is insufficiently dealt. In order to fill these lacks, we propose a general approach, called Application-based {\{}DOmain{\}} Modeling (ADOM), which enables specifying domains and applications similarly, (re)using domain knowledge in application models, and validating the application models against the relevant domain models. In this paper we present the {\{}ADOM{\}} approach, demonstrating its application to {\{}UML{\}} 2.0 class and sequence diagrams. },
	Doi = {https://doi.org/10.1016/j.infsof.2009.03.005},
	ISSN = {0950-5849},
	Keywords = {Domain analysis,Domain engineering,Feature oriented,Metamodeling,Software product line engineering,Variability management},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584909000366}
}

@Article{ReinhartzBerger2013320,
	Title = {{Comparing functionality of software systems: An ontological approach}},
	Author = {Reinhartz-Berger, Iris and Sturm, Arnon and Wand, Yair},
	Journal = {Data {\&} Knowledge Engineering},
	Year = {2013},
	Pages = {320--338},
	Volume = {87},
	Abstract = {Abstract Organizations can reduce the costs and enhance the quality of required software by adapting existing software systems. Software adaptation decisions often involve comparing alternatives on two criteria: (1) how well a system meets users' requirements and (2) the effort required for adapting the system. These criteria reflect two points of view — of users and of developers. Common to both views is the notion of functionality, which software developers have traditionally used for effort estimation utilizing concepts such as function points. However, users involved in selecting systems are not necessarily familiar with such concepts. We propose an approach for comparing software functionality from users' point of view. The approach employs ontological concepts to define functionality in terms of system behaviors. To evaluate whether or not the approach is also usable by software developers, we conducted an exploratory experiment. In the experiment, software engineering students ranked descriptions of software systems on the amount of changes needed to adapt the systems to given requirements. The results demonstrated that the ontological approach was usable after a short training and provided results comparable to ranking done by expert software developers. We also compared the ontological approach to a method which employed function point concepts. The results showed no statistically significant differences in performance, but there seemed to be an advantage to the ontological approach for cases that were difficult to analyze. Moreover, it took less time to apply the ontological approach than the function point-based approach, and the difference was statistically significant. },
	Doi = {https://doi.org/10.1016/j.datak.2012.09.005},
	ISSN = {0169-023X},
	Keywords = {Development effort estimation,Function point analysis,Ontologies,Requirements engineering,Software comparison,Variability management},
	Url = {http://www.sciencedirect.com/science/article/pii/S0169023X12001073}
}

@Conference{Reuys2005519,
	Title = {{Model-based system testing of software product families}},
	Author = {Reuys, A and Kamsties, E and Pohl, K and Reis, S},
	Booktitle = {Lecture Notes in Computer Science},
	Year = {2005},
	Pages = {519--534},
	Volume = {3520},
	Abstract = {In software product family engineering reusable artifacts are produced during domain engineering and applications are built from these artifacts during application engineering. Modeling variability of current and future applications is the key for enabling reuse. The proactive reuse leads to a reduction in development costs and a shorter time to market. Up to now, these benefits have been realized for the constructive development phases, but not for testing. This paper presents the ScenTED technique (Scenario based TEst case Derivation), which aims at reducing effort in product family testing. ScenTED is a model-based, reuse-oriented technique for test case derivation in the system test of software product families. Reuse of test cases is ensured by preserving variability during test case derivation. Thus, concepts known from model-based testing in single system engineering, e.g., coverage metrics, must be adapted. Experiences with our technique gained from an industrial case study are discussed and prototypical tool support is illustrated. {\textcopyright} Springer-Verlag Berlin Heidelberg 2005.},
	Annote = {cited By 42},
	Keywords = {Case study; Software products; Software testing;,Computer aided software engineering; Computer appl,Computer software},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-25144488545{\&}partnerID=40{\&}md5=2a8d773e2826d7dacc6688b3e8a82ef3}
}

@Article{Riaz201514,
	Title = {{How have we evaluated software pattern application? A systematic mapping study of research design practices}},
	Author = {Riaz, Maria and Breaux, Travis and Williams, Laurie},
	Journal = {Information and Software Technology},
	Year = {2015},
	Pages = {14--38},
	Volume = {65},
	Abstract = {AbstractContext Software patterns encapsulate expert knowledge for constructing successful solutions to recurring problems. Although a large collection of software patterns is available in literature, empirical evidence on how well various patterns help in problem solving is limited and inconclusive. The context of these empirical findings is also not well understood, limiting applicability and generalizability of the findings. Objective To characterize the research design of empirical studies exploring software pattern application involving human participants. Method We conducted a systematic mapping study to identify and analyze 30 primary empirical studies on software pattern application, including 24 original studies and 6 replications. We characterize the research design in terms of the questions researchers have explored and the context of empirical research efforts. We also classify the studies in terms of measures used for evaluation, and threats to validity considered during study design and execution. Results Use of software patterns in maintenance is the most commonly investigated theme, explored in 16 studies. Object-oriented design patterns are evaluated in 14 studies while 4 studies evaluate architectural patterns. We identified 10 different constructs with 31 associated measures used to evaluate software patterns. Measures for ‘efficiency' and ‘usability' are commonly used to evaluate the problem solving process. While measures for ‘completeness', ‘correctness' and ‘quality' are commonly used to evaluate the final artifact. Overall, ‘time to complete a task' is the most frequently used measure, employed in 15 studies to measure ‘efficiency'. For qualitative measures, studies do not report approaches for minimizing biases 27{\%} of the time. Nine studies do not discuss any threats to validity. Conclusion Subtle differences in study design and execution can limit comparison of findings. Establishing baselines for participants' experience level, providing appropriate training, standardizing problem sets, and employing commonly used measures to evaluate performance can support replication and comparison of results across studies. },
	Doi = {https://doi.org/10.1016/j.infsof.2015.04.002},
	ISSN = {0950-5849},
	Keywords = {Empirical design,Empirical evaluation,Mapping study,Software pattern,Systematic review},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584915000774}
}

@InProceedings{Ribeiro:2011:AIC:2019136.2019155,
	Title = {{An Approach for Implementing Core Assets in Service-oriented Product Lines}},
	Author = {Ribeiro, Heberth Braga G and de Almeida, Eduardo Santana and {de Lemos Meira}, Silvio R},
	Booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
	Year = {2011},
	Address = {New York, NY, USA},
	Pages = {17:1----17:4},
	Publisher = {ACM},
	Series = {SPLC '11},
	Doi = {10.1145/2019136.2019155},
	ISBN = {978-1-4503-0789-5},
	Keywords = {service-oriented architecture,service-oriented product lines,software development process,software product lines},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2019136.2019155}
}

@Article{Riehle2016,
	Title = {{Inner Source in Platform-Based Product Engineering}},
	Author = {Riehle, Dirk and Capraro, Maximilian and Kips, Detlef and Horn, Lars},
	Journal = {IEEE Transactions on Software Engineering},
	Year = {2016},
	Month = {dec},
	Number = {12},
	Pages = {1162--1177},
	Volume = {42},
	Doi = {10.1109/TSE.2016.2554553},
	ISSN = {0098-5589},
	Url = {http://ieeexplore.ieee.org/document/7452676/}
}

@Article{Rincon201561,
	Title = {{Method to Identify Corrections of Defects on Product Line Models}},
	Author = {Rinc{\'{o}}n, L and Giraldo, G and Mazo, R and Salinesi, C and Diaz, D},
	Journal = {Electronic Notes in Theoretical Computer Science},
	Year = {2015},
	Pages = {61--81},
	Volume = {314},
	Abstract = {Abstract Software product line engineering is a promising paradigm for developing software intensive systems. Among their proven benefits are reduced time to market, better asset reuse and improved software quality. To achieve this, the collection of products of the product line are specified by means of product line models. Feature Models (FMs) are a common notation to represent product lines that express the set of feature combinations that software products can have. Experience shows that these models can have defects. Defects in {\{}FMs{\}} be inherited to the products configured from these models. Consequently, defects must be early identified and corrected. Several works reported in scientific literature, deal with identification of defects in FMs. However, only few of these proposals are able to explain how to fix defects, and only some corrections are suggested. This paper proposes a new method to detect all possible corrections from a defective product line model. The originality of the contribution is that corrections can be found when the method systematically eliminates dependencies from the FMs. The proposed method was applied on 78 distinct {\{}FMs{\}} with sizes up to 120 dependencies. Evaluation indicates that the method proposed in this paper scale up, is accurate, and sometimes useful in real scenarios. },
	Annote = {{\{}CLEI{\}} 2014, the {\{}XL{\}} Latin American Conference in Informatic},
	Doi = {https://doi.org/10.1016/j.entcs.2015.05.005},
	ISSN = {1571-0661},
	Keywords = {Corrections,Defects,Features Models,Software Engineering,Software product lines},
	Url = {http://www.sciencedirect.com/science/article/pii/S1571066115000286}
}

@Article{Rincon2014111,
	Title = {{An ontological rule-based approach for analyzing dead and false optional features in feature models}},
	Author = {Rinc{\'{o}}n, L F and Giraldo, G. L. and Mazo, R. and Salinesi, C. and Rinc??n, L. F. and Giraldo, G. L. and Mazo, R. and Salinesi, C.},
	Journal = {Electronic Notes in Theoretical Computer Science},
	Year = {2014},
	Pages = {111--132},
	Volume = {302},
	Abstract = {Feature models are a common way to represent variability requirements of software product lines by expressing the set of feature combinations that software products can have. Assuring quality of feature models is thus of paramount importance for assuring quality in software product line engineering. However, feature models can have several types of defects that disminish benefits of software product line engineering.Two of such defects are dead features and false optional features. Several state-of-the-art techniques identify these defects, but only few of them tackle the problem of identifying their causes. Besides, the explanations they provide are cumbersome and hard to understand by humans. In this paper, we propose an ontological rule-based approach to: (a) identify dead and false optional features; (b)identify certain causes of these defects; and (c) explain these causes in natural language helping modelers to correct found defects. We represent our approach with a feature model taken from literature. A preliminary empirical evaluation of our approach over 31 FMs shows that our proposal is effective, accurate and scalable to 150 features. {\textcopyright} 2014 Elsevier B.V.},
	Annote = {From Duplicate 2 (An ontological rule-based approach for analyzing dead and false optional features in feature models - Rinc{\'{o}}n, L F; Giraldo, G L; Mazo, R; Salinesi, C)
		cited By 13},
	Doi = {10.1016/j.entcs.2014.01.023},
	File = {:Users/mac/Downloads/Sciencedirect{\_}articles{\_}13Jun2017{\_}14-36-55.006/An-Ontological-Rule-Based-Approach-for-Analyzing-Dead-and-False-Optional-Features-in-Feature-Models{\_}2014{\_}Electronic-Notes-in-Theoretical-Computer-Scie.pdf:pdf},
	ISSN = {15710661},
	Keywords = {Defects,Feature Models,Ontologies,Software Engineering},
	Publisher = {Elsevier B.V.},
	Url = {http://dx.doi.org/10.1016/j.entcs.2014.01.023 https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894169420{\&}doi=10.1016{\%}2Fj.entcs.2014.01.023{\&}partnerID=40{\&}md5=c8eb87067b63d1f08d18a4f24d963b13}
}

@Article{SMR:SMR411,
	Title = {{A profile-based approach for maintaining software architecture: an industrial experience report}},
	Author = {Riva, Claudio and Selonen, Petri and Syst{\"{a}}, Tarja and Xu, Jianli},
	Journal = {Journal of Software Maintenance and Evolution: Research and Practice},
	Year = {2011},
	Number = {1},
	Pages = {3--20},
	Volume = {23},
	Abstract = {This paper presents our experiences in building a UML-based approach for maintaining software products of a large-scale industrial product family. It enables software architects to define rules and constraints for a product family architecture to be enforced on individual product architectures. The target system of our study was the Nokia ISA platform for a mobile phone product family, a complex software system comprising thousands of components and several million lines of code. We outline our approach and the accompanying tools, and report our experiences and lessons learned in assessing the architectural integrity of 10 ISA platform releases and the associated products. Copyright {\textcopyright} 2009 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/smr.411},
	ISSN = {1532-0618},
	Keywords = {software architectures,software maintenance,software product-lines},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/smr.411}
}

@Conference{Rosko2014317,
	Title = {{Predicting the changeability of software product lines for business application}},
	Author = {Ro{\v{s}}ko, Z},
	Booktitle = {Information Systems Development: Transforming Organisations and Society Through Information Systems - Proceedings of the 23rd International Conference on Information Systems Development, ISD 2014},
	Year = {2014},
	Pages = {317--328},
	Abstract = {The changeability, a sub-characteristic of maintainability, refers to the level of effort which is required to do modifications to a software product line (SPL) application component. Assuming dependencies between SPL application components and reference architecture implementation (a platform), this paper empirically investigates the relationship between 7 design metrics and changeability of 46 server components of a product line for business applications. In addition, we investigated the usefulness of Platform Responsibility (PR) metric as an indicator of product line component changeability. The results show that most of the design metrics are strongly related to the changeability of server component and also indicate statistically significant correlation between Maintainability Index (MI) and PR metric. The assessment is based on a case study of the implementation of the product line for business applications in a financial institution. The results show that PR metric can be used as good predictor of changeability in the software product line environment.},
	Annote = {cited By 0},
	Keywords = {Application programs,Changeability; Metrics; Platform responsibility;,Computer software; Computer software reusability;},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923658645{\&}partnerID=40{\&}md5=782da325791f9138c38f9e67a7fb3ae0}
}

@Article{Rosko2014145,
	Title = {{A case study of software product line for business applications changeability prediction}},
	Author = {Ro{\v{s}}ko, Z and Strahonja, V},
	Journal = {Journal of Information and Organizational Sciences},
	Year = {2014},
	Number = {2},
	Pages = {145--160},
	Volume = {38},
	Abstract = {The changeability, a sub-characteristic of maintainability, refers to the level of effort which is required to do modifications to a software product line (SPL) application component. Assuming dependencies between SPL application components and reference architecture implementation (a platform), this paper empirically investigates the relationship between 7 design metrics and changeability of 46 server components of a product line for business applications. In addition, we investigated the usefulness of Platform Responsibility (PR) metric as an indicator of product line component changeability. The results show that most of the design metrics are strongly related to the changeability of server component and also indicate statistically significant correlation between Maintainability Index (MI) and PR metric. The assessment is based on a case study of the implementation of the product line for business applications in a financial institution. The results show that PR metric can be used as good predictor of changeability in the software product line environment. {\textcopyright} 2014, University of Zagreb. All rights reserved.},
	Annote = {cited By 0},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84918540616{\&}partnerID=40{\&}md5=5c9f9c6b3eeb9a82151b8ed9178db021}
}

@InProceedings{Robinson:2010:IIA:1852786.1852814,
	Title = {{Improving Industrial Adoption of Software Engineering Research: A Comparison of Open and Closed Source Software}},
	Author = {Robinson, Brian and Francis, Patrick},
	Booktitle = {Proceedings of the 2010 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
	Year = {2010},
	Address = {New York, NY, USA},
	Pages = {21:1----21:10},
	Publisher = {ACM},
	Series = {ESEM '10},
	Doi = {10.1145/1852786.1852814},
	ISBN = {978-1-4503-0039-1},
	Keywords = {empirical studies,industrial adoption,software comparison},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/1852786.1852814}
}

@Article{Robinson:2010:SCS:1842713.1842717,
	Title = {{A Survey of Customization Support in Agent-based Business Process Simulation Tools}},
	Author = {Robinson, William N and Ding, Yi},
	Journal = {ACM Trans. Model. Comput. Simul.},
	Year = {2010},
	Number = {3},
	Pages = {14:1----14:29},
	Volume = {20},
	Address = {New York, NY, USA},
	Doi = {10.1145/1842713.1842717},
	ISSN = {1049-3301},
	Keywords = {Agent-based modeling,application frameworks,encapsulation,event-driven simulation,modularity,software product line engineering},
	Publisher = {ACM},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/1842713.1842717}
}

@Article{RodriguezCovili20132009,
	Title = {{A lightweight and distributed middleware to provide presence awareness in mobile ubiquitous systems}},
	Author = {Rodr{\'{i}}guez-Covili, Juan and Ochoa, Sergio F},
	Journal = {Science of Computer Programming},
	Year = {2013},
	Number = {10},
	Pages = {2009--2025},
	Volume = {78},
	Abstract = {Abstract Several researchers have identified the need to count on presence awareness in ubiquitous systems that support mobile activities, particularly when these systems are used to perform loosely-coupled mobile work. In such a work style, mobile users conduct face-to-face on-demand interactions, therefore counting on awareness information about the position and availability of potential collaborators becomes mandatory for these applications. Most proposed solutions that provide user presence awareness involve centralized components, have reusability limitations, or simply address a part of that service. This article presents a lightweight and fully distributed middleware named Moware, which allows developers to embed presence awareness services in mobile ubiquitous systems in a simple way. The article also describes the Moware architecture, its main components and strategies used to deal with several aspects of the presence awareness support. These design strategies can be reused by software designers to provide presence awareness capabilities into middleware and specific software applications. Moware services were embedded in a mobile ubiquitous system that supports inspectors during the construction inspection process. The preliminary results indicate that the middleware was easy to use for developers, and its services were useful for the end-users. },
	Annote = {Special section on Language Descriptions Tools and Applications (LDTA'08 {\&} '09) {\&} Special section on Software Engineering Aspects of Ubiquitous Computing and Ambient Intelligence (UCAmI 2011)},
	Doi = {https://doi.org/10.1016/j.scico.2013.02.003},
	ISSN = {0167-6423},
	Keywords = {Loosely-coupled mobile work,Middleware,Mobile ubiquitous computing,Presence awareness},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642313000282}
}

@Article{Rodrigues2012112,
	Title = {{Dependability analysis in the Ambient Assisted Living Domain: An exploratory case study}},
	Author = {Rodrigues, Gena{\'{i}}na Nunes and Alves, Vander and Silveira, Renato and Laranjeira, Luiz A},
	Journal = {Journal of Systems and Software},
	Year = {2012},
	Number = {1},
	Pages = {112--131},
	Volume = {85},
	Abstract = {Ambient Assisted Living (AAL) investigates the development of systems involving the use of different types of sensors, which monitor activities and vital signs of lonely elderly people in order to detect emergency situations or deviations from desirable medical patterns. Instead of requiring the elderly person to manually push a button to request assistance, state-of-the-art {\{}AAL{\}} solutions automate the process by ‘perceiving' lonely elderly people in their home environment through various sensors and performing appropriate actions under the control of the underlying software. Dependability in the {\{}AAL{\}} domain is a critical requirement, since poor system availability, reliability, safety, or integrity may cause inappropriate emergency assistance to potentially have fatal consequences. Nevertheless, contemporary research has not focused on assessing dependability in this domain. This work attempts to fill this gap presenting an approach which relies on modern quantitative and qualitative dependability analysis techniques based on software architecture. The analysis method presented in this paper consists of conversion patterns from Unified Modeling Language (UML) behavior models of the {\{}AAL{\}} software architecture into a formal executable specification, based on a probabilistic process algebra description language, which enables a sound quantitative and qualitative analysis. The {\{}UML{\}} models specify system component interactions and are annotated with component failure probabilities and system usage profile information. The resulting formal specification is executed on PRISM, a model checking tool adequate for the purpose of our analysis in order to identify a set of domain-specific dependability properties expressed declaratively in Probabilistic Computational Tree Logic (PCTL). The benefits of using these techniques are twofold. Firstly, they allow us to seamlessly integrate the analysis during subsequent software lifecycle stages in critical scenarios. Secondly, we identify the components which have the highest impact on software system dependability, and therefore, be able to address software architecture and individual software component problems prior to implementation and the occurrence of critical errors. },
	Annote = {Dynamic Analysis and Testing of Embedded Software},
	Doi = {https://doi.org/10.1016/j.jss.2011.07.037},
	ISSN = {0164-1212},
	Keywords = {Ambient Assisted Living,Component-based system,Dependability analysis},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121211002056}
}

@Article{SMR:SMR1778,
	Title = {{The v-algorithm for discovering software process lines}},
	Author = {{Rojas Blum}, Fabian and Simmonds, Jocelyn and Bastarrica, Mar{\'{i}}a Cecilia},
	Journal = {Journal of Software: Evolution and Process},
	Year = {2016},
	Number = {9},
	Pages = {783--799},
	Volume = {28},
	Abstract = {A software company can define a software process line (SPrL) to deal with projects with different characteristics. This entails defining a base process and its variation points; the SPrL is then tailored to each project. This approach avoids the co-evolution problems but is expensive to set up. In companies that register project events, this information could be used to discover the SPrL. However, traditional discovery algorithms focus on extracting a single process, which can be overly complex and would not be useful for managing future projects. Filtering out less frequent behavior leads to the discovery of simpler models, but these may not include relevant behavior. To address these issues, we propose the v-algorithm, which discovers a SPrL from process logs. Two thresholds split the log into three clusters based on relation frequency. The first one is used to generate the base process, the second one is used to identify variable elements, and the last one is discarded. We used the v-algorithm to discover the SPrL of Mobius, a small Chilean software company. We also discuss how the values of the thresholds affect the process discovery quality dimensions, extending existing metrics to the SPrL case. Copyright {\textcopyright} 2016 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/smr.1778},
	ISSN = {2047-7481},
	Keywords = {noise in logs,process discovery,software process lines,variability},
	Url = {http://dx.doi.org/10.1002/smr.1778}
}

@Article{Romanovsky2011158,
	Title = {{Refactoring the documentation of software product lines}},
	Author = {Romanovsky, K and Koznov, D and Minchin, L},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2011},
	Pages = {158--170},
	Volume = {4980 LNCS},
	Abstract = {One of the most vital techniques in the context of software product line (SPL) evolution is refactoring - extracting and refining reusable assets and improving SPL architecture in such a way that the behavior of existing products remains unchanged. We extend the idea of SPL refactoring to technical documentation because reuse techniques could effectively be applied to this area and reusable assets evolve and should be maintained. Various XML-based technologies for documentation development are widely spread today, and XML-specifications appear to be a good field for formal transformations. We base our research on the DocLine technology; the main goal of which is to introduce adaptive reuse into documentation development. We define a model of refactoring-based documentation development process, a set of refactoring operations, and describe their implementation in the DocLine toolset. Also, we present an experiment in which we applied the proposed approach to the documentation of a telecommunication systems SPL. {\textcopyright} 2011 IFIP International Federation for Information Processing.},
	Annote = {cited By 3},
	Doi = {10.1007/978-3-642-22386-0_12},
	Keywords = {Computer software reusability; Computer software,Development process; Documentation of software; Fo,Formal methods; Network architecture; Software de},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053157402{\&}doi=10.1007{\%}2F978-3-642-22386-0{\_}12{\&}partnerID=40{\&}md5=7f8bfaa737b162c6996848360257c634}
}

@Article{LaRosa2011313,
	Title = {{Configurable multi-perspective business process models}},
	Author = {Rosa, Marcello La and Dumas, Marlon and ter Hofstede, Arthur H M and Mendling, Jan},
	Journal = {Information Systems},
	Year = {2011},
	Number = {2},
	Pages = {313--340},
	Volume = {36},
	Abstract = {A configurable process model provides a consolidated view of a family of business processes. It promotes the reuse of proven practices by providing analysts with a generic modeling artifact from which to derive individual process models. Unfortunately, the scope of existing notations for configurable process modeling is restricted, thus hindering their applicability. Specifically, these notations focus on capturing tasks and control-flow dependencies, neglecting equally important ingredients of business processes such as data and resources. This research fills this gap by proposing a configurable process modeling notation incorporating features for capturing resources, data and physical objects involved in the performance of tasks. The proposal has been implemented in a toolset that assists analysts during the configuration phase and guarantees the correctness of the resulting process models. The approach has been validated by means of a case study from the film industry. },
	Annote = {Special Issue: Semantic Integration of Data, Multimedia, and Services},
	Doi = {https://doi.org/10.1016/j.is.2010.07.001},
	ISSN = {0306-4379},
	Keywords = {Business process,Configurable process model,EPC},
	Url = {http://www.sciencedirect.com/science/article/pii/S0306437910000633}
}

@Article{Rosemann20071,
	Title = {{A configurable reference modelling language}},
	Author = {Rosemann, M and van der Aalst, W M P},
	Journal = {Information Systems},
	Year = {2007},
	Number = {1},
	Pages = {1--23},
	Volume = {32},
	Abstract = {Enterprise Systems (ES) are comprehensive off-the-shelf packages that have to be configured to suit the requirements of an organization. Most {\{}ES{\}} solutions provide reference models that describe the functionality and structure of the system. However, these models do not capture the potential configuration alternatives. This paper discusses the shortcomings of current reference modelling languages using Event-Driven Process Chains (EPCs) as an example. We propose Configurable {\{}EPCs{\}} (C-EPCs) as an extended reference modelling language which allows capturing the core configuration patterns. A formalization of this language as well as examples for typical configurations are provided. A program of further research including the identification of a comprehensive list of configuration patterns, deriving possible notations for reference model configurations and testing the quality of these proposed extensions in experiments and focus groups is presented. },
	Doi = {https://doi.org/10.1016/j.is.2005.05.003},
	ISSN = {0306-4379},
	Keywords = {Configuration,Enterprise systems,Event-Driven Process Chains,Reference model},
	Url = {http://www.sciencedirect.com/science/article/pii/S0306437905000487}
}

@Article{Rosenmuller20091493,
	Title = {{Tailor-made data management for embedded systems: A case study on Berkeley {\{}DB{\}}}},
	Author = {Rosenm{\"{u}}ller, Marko and Apel, Sven and Leich, Thomas and Saake, Gunter},
	Journal = {Data {\&} Knowledge Engineering},
	Year = {2009},
	Number = {12},
	Pages = {1493--1512},
	Volume = {68},
	Abstract = {Applications in the domain of embedded systems are diverse and store an increasing amount of data. In order to satisfy the varying requirements of these applications, data management functionality is needed that can be tailored to the applications' needs. Furthermore, the resource restrictions of embedded systems imply a need for data management that is customized to the hardware platform. In this paper, we present an approach for decomposing data management software for embedded systems using feature-oriented programming. The result of such a decomposition is a software product line that allows us to generate tailor-made data management systems. While existing approaches for tailoring software have significant drawbacks regarding customizability and performance, a feature-oriented approach overcomes these limitations, as we will demonstrate. In a non-trivial case study on Berkeley DB, we evaluate our approach and compare it to other approaches for tailoring DBMS. },
	Annote = {Including Special Section: 21st {\{}IEEE{\}} International Symposium on Computer-Based Medical Systems (IEEE {\{}CBMS{\}} 2008) – Seven selected and extended papers on Biomedical Data Mining},
	Doi = {https://doi.org/10.1016/j.datak.2009.07.013},
	ISSN = {0169-023X},
	Keywords = {Embedded systems,Feature-oriented programming,FeatureC++,Software product lines,Tailor-made data management},
	Url = {http://www.sciencedirect.com/science/article/pii/S0169023X09001128}
}

@Article{SPE:SPE999,
	Title = {{Assessing architectural drift in commercial software development: a case study}},
	Author = {Rosik, Jacek and {Le Gear}, Andrew and Buckley, Jim and Babar, Muhammad Ali and Connolly, Dave},
	Journal = {Software: Practice and Experience},
	Year = {2011},
	Number = {1},
	Pages = {63--86},
	Volume = {41},
	Abstract = {Objectives: Software architecture is perceived as one of the most important artefacts created during a system's design. However, implementations often diverge from their intended architectures: a phenomenon called architectural drift. The objective of this research is to assess the occurrence of architectural drift in the context of de novo software development, to characterize it, and to evaluate whether its detection leads to inconsistency removal. Method: An in vivo, longitudinal case study was performed during the development of a commercial software system, where an approach based on Reflexion Modelling was employed to detect architectural drift. Observation and think-aloud data, captured during the system's development, were assessed for the presence and types of architectural drift. When divergences were identified, the data were further analysed to see if identification led to the removal of these divergences. Results: The analysed system diverged from the intended architecture, during the initial implementation of the system. Surprisingly however, this work showed that Reflexion Modelling served to conceal some of the inconsistencies, a finding that directly contradicts the high regard that this technique enjoys as an architectural evaluation tool. Finally, the analysis illustrated that detection of inconsistencies was insufficient to prompt their removal, in the small, informal team context studied. Conclusions: Although the utility of the approach for detecting inconsistencies was demonstrated in most cases, it also served to hide several inconsistencies and did not act as a trigger for their removal. Hence additional efforts must be taken to lessen architectural drift and several improvements in this regard are suggested. Copyright {\textcopyright} 2010 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spe.999},
	ISSN = {1097-024X},
	Keywords = {architecture consistency,architecture degeneration,reflection models,software/program verification},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/spe.999}
}

@Article{DelRosso20081,
	Title = {{Software performance tuning of software product family architectures: Two case studies in the real-time embedded systems domain}},
	Author = {Rosso, Christian Del},
	Journal = {Journal of Systems and Software},
	Year = {2008},
	Number = {1},
	Pages = {1--19},
	Volume = {81},
	Abstract = {Software performance is an important non-functional quality attribute and software performance evaluation is an essential activity in the software development process. Especially in embedded real-time systems, software design and evaluation are driven by the needs to optimize the limited resources, to respect time deadlines and, at the same time, to produce the best experience for end-users. Software product family architectures add additional requirements to the evaluation process. In this case, the evaluation includes the analysis of the optimizations and tradeoffs for the whole products in the family. Performance evaluation of software product family architectures requires knowledge and a clear understanding of different domains: software architecture assessments, software performance and software product family architecture. We have used a scenario-driven approach to evaluate performance and dynamic memory management efficiency in one Nokia software product family architecture. In this paper we present two case studies. Furthermore, we discuss the implications and tradeoffs of software performance against evolvability and maintenability in software product family architectures. },
	Doi = {https://doi.org/10.1016/j.jss.2007.07.006},
	ISSN = {0164-1212},
	Keywords = {Dynamic memory management,Embedded real-time systems,Software architecture assessments,Software performance,Software product family},
	Url = {http://www.sciencedirect.com/science/article/pii/S016412120700180X}
}

@Article{Rothberg:2016:TSC:3093335.2993252,
	Title = {{Towards Scalable Configuration Testing in Variable Software}},
	Author = {Rothberg, Valentin and Dietrich, Christian and Ziegler, Andreas and Lohmann, Daniel},
	Journal = {SIGPLAN Not.},
	Year = {2016},
	Number = {3},
	Pages = {156--167},
	Volume = {52},
	Address = {New York, NY, USA},
	Doi = {10.1145/3093335.2993252},
	ISSN = {0362-1340},
	Keywords = {Configurability,Linux,Sampling,Software Product Lines,Software Testing},
	Publisher = {ACM},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/3093335.2993252}
}

@Article{Rubin2015627,
	Title = {{Cloned product variants: from ad-hoc to managed software product lines}},
	Author = {Rubin, J and Czarnecki, K and Chechik, M},
	Journal = {International Journal on Software Tools for Technology Transfer},
	Year = {2015},
	Number = {5},
	Pages = {627--646},
	Volume = {17},
	Abstract = {We focus on the problem of managing a collection of related software product variants realized via cloning. By analyzing three industrial case studies of organizations with cloned product lines, we conclude that an efficient management of clones relies on both refactoring cloned variants into a single-copy product line representation and improving development experience when maintaining existing clones. We propose a framework that consists of seven conceptual operators for cloned product line management and show that these operators are adequate to realize development activities we observed in the analyzed case studies. We discuss options for implementing the operators and benefits of the operator-based view. {\textcopyright} 2014, Springer-Verlag Berlin Heidelberg.},
	Annote = {cited By 3},
	Doi = {10.1007/s10009-014-0347-9},
	Keywords = {Cloning,Computer software,Development activity; Development experiences; Ef},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941423205{\&}doi=10.1007{\%}2Fs10009-014-0347-9{\&}partnerID=40{\&}md5=7472e5524c685cc539f316498ae8f0c1}
}

@Article{ZelaRuiz2016113,
	Title = {{Quality of Service Conflict During Web Service Monitoring: A Case Study}},
	Author = {Ruiz, Jael Zela and Rubira, Cec{\'{i}}lia M},
	Journal = {Electronic Notes in Theoretical Computer Science},
	Year = {2016},
	Pages = {113--127},
	Volume = {321},
	Abstract = {Abstract Web services have become one of the most used technologies in service-oriented systems. Its popularity is due to its property to adapt to any context. As a consequence of the increasing number of Web services on the Internet and its important role in many applications today, Web service quality has become a crucial requirement and demanded by service consumers. Terms of quality levels are written between service providers and service consumers to ensure a degree of quality. The use of monitoring tools to control service quality levels is very important. Quality attributes suffer variations in their values during runtime, this is produced by many factors such as a memory leak, deadlock, race data, inconsistent data, etc. However, sometimes monitoring tools can impact negatively affecting the quality of service when they are not properly used and configured, producing possible conflicts between quality attributes. This paper aims to show the impact of monitoring tools over service quality, two of the most important quality attributes – performance and accuracy – were chosen to be monitored. A case study is conducted to present and evaluate the relationship between performance and accuracy over a Web service. As a result, conflict is found between performance and accuracy, where performance was the most affected, because it presented a degradation in its quality level during monitoring. },
	Annote = {{\{}CLEI{\}} 2015, the {\{}XLI{\}} Latin American Computing Conference},
	Doi = {https://doi.org/10.1016/j.entcs.2016.02.007},
	ISSN = {1571-0661},
	Keywords = {Accuracy,Conflict,Monitoring Tools,Performance,Quality Attributes,Quality of Service,SOA,Web Services},
	Url = {http://www.sciencedirect.com/science/article/pii/S1571066116300081}
}

@Article{SMR:SMR1594,
	Title = {{Uses and applications of Software {\&} Systems Process Engineering Meta-Model process models. A systematic mapping study}},
	Author = {Ruiz-Rube, Iv{\'{a}}n and Dodero, Juan Manuel and Palomo-Duarte, Manuel and Ruiz, Mercedes and Gawn, David},
	Journal = {Journal of Software: Evolution and Process},
	Year = {2013},
	Number = {9},
	Pages = {999--1025},
	Volume = {25},
	Abstract = {Software process engineering is a discipline, which aims to study and improve software development and maintenance processes. The explicit definition of software processes is essential. To this end, the Object Management Group consortium proposed the Software {\&} Systems Process Engineering Meta-Model (SPEM) that exploits the benefits of the Model Driven Architecture paradigm applied to software process models, instead of software specification models. The aim of this study is to discover evidence clusters and evidence deserts in the use and application of SPEM from a business process management point of view. To reach the proposed objective, we have undertaken a systematic mapping study of the existing scientific literature.The reviewed literature deals mainly with process modeling and, to a lesser extent, with process adaptability, verification, and validation, enactment and evaluation. Wide agreement exists in using the SPEM meta-model to develop different types of methods and processes. Further research efforts are needed in areas related to enactment and evaluation of software processes. There is a need to evolve to a new version of the meta-model that incorporates the improvements proposed by different authors. Copyright {\textcopyright} 2013 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/smr.1594},
	ISSN = {2047-7481},
	Keywords = {SPEM,business process management,model-driven engineering,software process engineering,systematic mapping study},
	Url = {http://dx.doi.org/10.1002/smr.1594}
}

@Article{SYS:SYS21247,
	Title = {{Leveraging Variability Modeling Techniques for Architecture Trade Studies and Analysis}},
	Author = {Ryan, Jessica and Sarkani, Shahram and Mazzuchi, Thomas},
	Journal = {Systems Engineering},
	Year = {2014},
	Number = {1},
	Pages = {10--25},
	Volume = {17},
	Abstract = {Increasing complexity in modern systems, as well as cost and schedule constraints, require a new paradigm of Systems Engineering to fulfill stakeholder needs. Challenges facing efficient trade studies include poor tool interoperability, lack of simulation coordination (design parameters), and requirements flow down. A recent trend toward Model Based Systems Engineering (MBSE) includes flexible architecture definition, program documentation, requirements traceability, and Systems Engineering reuse. As a rapidly evolving practice, MBSE still lacks governing standards and commonly accepted frameworks. This paper proposes a framework for efficient architecture definition using MBSE in conjunction with domain specific simulation to evaluate alternatives. Variant modeling techniques are introduced as an extension to capturing parameterized architecture options. Initial exploration applies such variant modeling techniques to design for adaptability trade study criteria, as a means to evaluate candidate architecture configurations against multiple requirement sets. A general framework is provided, followed with a specific example including a method for designing a trade study, defining candidate architectures, planning simulations to fulfill requirements, and designing a weighted decision analysis to optimize system objectives.},
	Doi = {10.1002/sys.21247},
	ISSN = {1520-6858},
	Keywords = {Model Based Systems Engineering,SysML},
	Url = {http://dx.doi.org/10.1002/sys.21247}
}

@Article{Ryssel201283,
	Title = {{Automatic library migration for the generation of hardware-in-the-loop models}},
	Author = {Ryssel, Uwe and Ploennigs, Joern and Kabitzsch, Klaus},
	Journal = {Science of Computer Programming},
	Year = {2012},
	Number = {2},
	Pages = {83--95},
	Volume = {77},
	Abstract = {Embedded systems are widely used in several applications nowadays. As they integrate hard- and software elements, their functionality and reliability are often tested by hardware-in-the-loop methods, in which the system under test runs in a simulated environment. Due to the rising complexity of the embedded functions, performance limitations and practicability reasons, the simulations are often specialized to test specific aspects of the embedded system and develop a high diversity by themselves. This diversity is difficult to manage for a user and results in erroneously selected test components and compatibility problems in the test configuration. This paper presents a generative programming approach that handles the diversity of test libraries. Compatibility issues are explicitly evaluated by a new interface concept. Furthermore, a novel model analyzer facilitates the efficient application in practice by migrating existing libraries. The approach is evaluated for an example from the automotive domain using MATLAB/Simulink. },
	Annote = {Special Issue on Automatic Program Generation for Embedded Systems},
	Doi = {https://doi.org/10.1016/j.scico.2010.06.005},
	ISSN = {0167-6423},
	Keywords = {Function-block-based design,Generative programming,Library migration,Structural comparison},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642310001115}
}

@Article{Sanchez20122504,
	Title = {{From Teleo-Reactive specifications to architectural components: A model-driven approach}},
	Author = {S{\'{a}}nchez, Pedro and Alonso, Diego and Morales, Jos{\'{e}} Miguel and Navarro, Pedro Javier},
	Journal = {Journal of Systems and Software},
	Year = {2012},
	Number = {11},
	Pages = {2504--2518},
	Volume = {85},
	Abstract = {The Teleo-Reactive approach designed by N.J. Nilsson offers a high-level programming model that permits the development of reactive systems, such as robotic vehicles. Teleo-Reactive programs are written in a manner that allows engineers to define the behaviour of the system while taking into account goals and changes in the state of the environment. This article presents a systematic approach that makes it possible to derive architectural models, with structural descriptions and behaviour, from Teleo-Reactive Programs. The development of reactive systems can therefore benefit significantly from a combination of two approaches: (1) the Teleo-Reactive approach, which is oriented towards a description of the system from the standpoint of the goals identified and the state of the environment and (2) the architectural approach, which is oriented towards the design of component-based software, in which decisions are conditioned by the need to reuse already tested solutions. The integration of this work into a development environment that allows code to be generated via model transformations opens up new possibilities in the development of this type of systems. The proposal is validated through a case study that is representative of the domain, and a survey carried out with post-graduate students. },
	Doi = {https://doi.org/10.1016/j.jss.2012.05.067},
	ISSN = {0164-1212},
	Keywords = {Component-based software development,Model-driven software development,Reactive systems,Robotics,Teleo-Reactive programs},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121212001537}
}

@Article{Sabouri2012296,
	Title = {{Reducing the model checking cost of product lines using static analysis techniques}},
	Author = {Sabouri, H and Khosravi, R},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2012},
	Pages = {296--312},
	Volume = {7253 LNCS},
	Abstract = {Software product line engineering is a paradigm to develop software applications using platforms and mass customization. Component based approaches play an important role in development of product lines: Components represent features, and different component combinations lead to different products. The number of combinations is exponential in the number of features, which makes the cost of product line model checking high. In this paper, we propose two techniques to reduce the number of component combinations that have to be verified. The first technique is using the static slicing approach to eliminate the features that do not affect the property. The second technique is analyzing the property and extracting sufficient conditions of property satisfaction/violation, to identify products that satisfy or violate the property without model checking. We apply these techniques on a vending machine case study to show the applicability and effectiveness of our approach. The results show that the number of generated states and time of model checking is reduced significantly using the proposed reduction techniques. {\textcopyright} 2012 Springer-Verlag.},
	Annote = {cited By 0},
	Doi = {10.1007/978-3-642-35743-5_18},
	Keywords = {Analysis techniques; Component based approach; Mas,Computer software,Model checking},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871569046{\&}doi=10.1007{\%}2F978-3-642-35743-5{\_}18{\&}partnerID=40{\&}md5=6d26c0ec246632a3495074aec70d8c8b}
}

@Article{Saeed20161094,
	Title = {{The experimental applications of search-based techniques for model-based testing: Taxonomy and systematic literature review}},
	Author = {Saeed, Aneesa and Hamid, Siti Hafizah Ab and Mustafa, Mumtaz Begum},
	Journal = {Applied Soft Computing},
	Year = {2016},
	Pages = {1094--1117},
	Volume = {49},
	Abstract = {AbstractContext Model-based testing (MBT) aims to generate executable test cases from behavioral models of software systems. {\{}MBT{\}} gains interest in industry and academia due to its provision of systematic, automated, and comprehensive testing. Researchers have successfully applied search-based techniques (SBTs) by automating the search for an optimal set of test cases at reasonable cost compared to other more expensive techniques. Thus, there is a recent surge toward the applications of {\{}SBTs{\}} for {\{}MBT{\}} because the generated test cases are optimal and have low computational cost. However, successful, future {\{}SBTs{\}} for {\{}MBT{\}} applications demand deep insight into its existing experimental applications that underlines stringent issues and challenges, which is lacking in the literature. Objective The objective of this study is to comprehensively analyze the current state-of-the-art of the experimental applications of {\{}SBTs{\}} for {\{}MBT{\}} and present the limitations of the current literature to direct future research. Method We conducted a systematic literature review (SLR) using 72 experimental papers from six data sources. We proposed a taxonomy based on the literature to categorize the characteristics of the current applications. Results The results indicate that the majority of the existing applications of {\{}SBTs{\}} for {\{}MBT{\}} focus on functional and structural coverage purposes, as opposed to stress testing, regression testing and graphical user interface (GUI) testing. We found research gaps in the existing applications in five areas: applying multi-objective SBTs, proposing hybrid techniques, handling complex constraints, addressing data and requirement-based adequacy criteria, and adapting landscape visualization. Only twelve studies proposed and empirically evaluated the {\{}SBTs{\}} for complex systems in MBT. Conclusion This extensive systematic analysis of the existing literature based on the proposed taxonomy enables to assist researchers in exploring the existing research efforts and reveal the limitations that need additional investigation. },
	Doi = {https://doi.org/10.1016/j.asoc.2016.08.030},
	ISSN = {1568-4946},
	Keywords = {Model-based testing,Search-based techniques,Software testing,Systematic literature review,Taxonomy,Test case generation},
	Url = {http://www.sciencedirect.com/science/article/pii/S1568494616304240}
}

@Article{Saeed20161,
	Title = {{Empirical validating the cognitive effectiveness of a new feature diagrams visual syntax}},
	Author = {Saeed, M and Saleh, F and Al-Insaif, S and El-Attar, M},
	Journal = {Information and Software Technology},
	Year = {2016},
	Pages = {1--26},
	Volume = {71},
	Abstract = {Context Feature models are commonly used to capture and communicate the commonality and variability of features in a Software Product Line. The core component of Feature models is feature diagrams, which graphically depict features in a hierarchical form. In previous work we have proposed a new notation that aims to improve the cognitive effectiveness of feature diagrams. Objective The objective of this paper is to empirically validate the cognitive effectiveness of the new feature diagrams notation in comparison to its original form. Methods We use two distinct empirical user-studies to validate the new notation. The first empirical study uses the survey approach while the second study is a subject-based experiment. The survey study investigates the semantic transparency of the new notation while the second study investigates the speed and accuracy of reading the notation. Results The results of the studies indicate that the proposed changes have significantly improved its cognitive effectiveness. Conclusions The cognitive effectiveness of feature diagrams has been improved, however there remains further research for full acceptance of the new notation by its potential user community.},
	Annote = {cited By 2},
	Doi = {10.1016/j.infsof.2015.10.012},
	Keywords = {Commonality and variability; Context features; Em,Computer software; Semantics; Software design; Sur,Graphic methods},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952767067{\&}doi=10.1016{\%}2Fj.infsof.2015.10.012{\&}partnerID=40{\&}md5=b8c024993f085b6cbd539a0319569487}
}

@Article{Saeed2014180,
	Title = {{Evaluating the Cognitive Effectiveness of the Visual Syntax of Feature Diagrams}},
	Author = {Saeed, M and Saleh, F and Al-Insaif, S and El-Attar, M},
	Journal = {Communications in Computer and Information Science},
	Year = {2014},
	Pages = {180--194},
	Volume = {432 CCIS},
	Abstract = {[Context and Motivation] Feature models are widely used in the Software Product Line (SPL) domain to capture and communicate the commonality and variability of features in a product line. Feature models contain feature diagrams that graphically depict features in a hierarchical form. [Problem/Question] Many research works have been devoted to enriching the visual syntax of feature diagrams to extend its expressiveness to capture additional types of semantics, however, there is a lack of research that evaluates the visual perception of feature models by its readers. Models serve a dual purpose: to brainstorm and communicate. A very sophisticated yet unreadable model is arguably useless. To date, there has not been a scientific evaluation of the cognitive effectiveness of the visual syntax of feature diagrams. [Principle Ideas] This paper presents a scientific evaluation of the cognitive effectiveness of feature diagrams. The evaluation approach is based on theory and empirical evidence mainly from the cognitive science field. [Contribution] The evaluation reveals drawbacks in the visual notation of feature diagrams. The paper concludes with some recommendations for improvement to remedy the identified flaws. {\textcopyright} Springer-Verlag Berlin Heidelberg 2014.},
	Annote = {cited By 1},
	Doi = {10.1007/978-3-662-43610-3_14},
	Keywords = {Cognitive science; Commonality and variability; E,Computer software; Requirements engineering; Seman,Graphic methods},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904750779{\&}doi=10.1007{\%}2F978-3-662-43610-3{\_}14{\&}partnerID=40{\&}md5=9604755a9b0ee53a178a597d51bd687a}
}

@InProceedings{Salay:2014:LMT:2568225.2568267,
	Title = {{Lifting Model Transformations to Product Lines}},
	Author = {Salay, Rick and Famelis, Michalis and Rubin, Julia and {Di Sandro}, Alessio and Chechik, Marsha},
	Booktitle = {Proceedings of the 36th International Conference on Software Engineering},
	Year = {2014},
	Address = {New York, NY, USA},
	Pages = {117--128},
	Publisher = {ACM},
	Series = {ICSE 2014},
	Doi = {10.1145/2568225.2568267},
	ISBN = {978-1-4503-2756-5},
	Keywords = {Model Driven Engineering,Model Transformations,Software Product Lines},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2568225.2568267}
}

@Article{Salvaneschi201520,
	Title = {{ContextErlang: A language for distributed context-aware self-adaptive applications}},
	Author = {Salvaneschi, Guido and Ghezzi, Carlo and Pradella, Matteo},
	Journal = {Science of Computer Programming},
	Year = {2015},
	Pages = {20--43},
	Volume = {102},
	Abstract = {Abstract Self-adaptive software modifies its behavior at run time to satisfy changing requirements in a dynamic environment. Context-oriented programming (COP) has been recently proposed as a specialized programming paradigm for context-aware and adaptive systems. {\{}COP{\}} mostly focuses on run time adaptation of the application's behavior by supporting modular descriptions of behavioral variations. However, self-adaptive applications must satisfy additional requirements, such as distribution and concurrency, support for unforeseen changes and enforcement of correct behavior in the presence of dynamic change. Addressing these issues at the language level requires a holistic design that covers all aspects and takes into account the possibly cumbersome interaction of those features, for example concurrency and dynamic change. We present ContextErlang, a {\{}COP{\}} programming language in which adaptive abstractions are seamlessly integrated with distribution and concurrency. We define ContextErlang's formal semantics, validated through an executable prototype, and we show how it supports formal proofs that the language design ensures satisfaction of certain safety requirements. We provide empirical evidence that ContextErlang is an effective solution through case studies and a performance assessment. We also show how the same design principles that lead to the development of ContextErlang can be followed to systematically design contextual extensions of other languages. A concrete example is presented concerning ContextScala. },
	Doi = {https://doi.org/10.1016/j.scico.2014.11.016},
	ISSN = {0167-6423},
	Keywords = {Concurrency,Context,Context-oriented programming,Distribution,Self-adaptive software},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642314005577}
}

@Article{Salvaneschi20121801,
	Title = {{Context-oriented programming: A software engineering perspective}},
	Author = {Salvaneschi, Guido and Ghezzi, Carlo and Pradella, Matteo},
	Journal = {Journal of Systems and Software},
	Year = {2012},
	Number = {8},
	Pages = {1801--1817},
	Volume = {85},
	Abstract = {The implementation of context-aware systems can be supported through the adoption of techniques at the architectural level such as middlewares or component-oriented architectures. It can also be supported by suitable constructs at the programming language level. Context-oriented programming (COP) is emerging as a novel paradigm for the implementation of this kind of software, in particular in the field of mobile and ubiquitous computing. The {\{}COP{\}} paradigm tackles the issue of developing context-aware systems at the language-level, introducing ad hoc language abstractions to manage adaptations modularization and their dynamic activation. In this paper we review the state of the art in the field of {\{}COP{\}} in the perspective of the benefits that this technique can provide to software engineers in the design and implementation of context-aware applications. },
	Doi = {https://doi.org/10.1016/j.jss.2012.03.024},
	ISSN = {0164-1212},
	Keywords = {Context,Context-awareness,Context-oriented programming},
	Url = {http://www.sciencedirect.com/science/article/pii/S016412121200074X}
}

@Article{Sanchez2012,
	Title = {{Software product line engineering for e-learning applications: A case study}},
	Author = {Sanchez, P and Garcia-Saiz, D and Zorrilla, M},
	Journal = {2012 International Symposium on Computers in Education, SIIE 2012},
	Year = {2012},
	Abstract = {As a consequence of the massive adoption of internet, different e-learning platforms, such as Moodle or WebCT, have been incorporated into educational institutions, ranging from schools to universities. As expected, a software market of auxiliary applications for these platforms has also emerged. One of the challenges these auxiliary applications must face is how to deal with the variability inherent to these different but similar platforms. Thus, we strongly believe the use of Software Product Line engineering techniques, whose goal is the effective production of similar software systems, can be greatly beneficial in this particular market segment. We demonstrate this idea by means of refactoring a data mining application, called ElWM (E-learning Web Miner), which works on e-learning platforms into a Software Product Line. We discuss the benefits obtained. {\textcopyright} 2012 UOLS.},
	File = {:Users/mac/Downloads/bulk-download (5)/Software product line engineering for e-learning applications A case study.pdf:pdf},
	ISBN = {9788493981471 (ISBN)},
	Keywords = {Commerce,Data mining applications,E-learning,E-learning platforms,Education,Educational institutions,Market segment,Refactorings,Societies and institutions,Software Product Line,Software design,Software markets,Software product line engineerings,Software systems,e-Learning application},
	Url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84873163071{\&}partnerID=40{\&}md5=b2c57d2da46d34dd9c72458d262a1704}
}

@Article{IIS2:IIS202789,
	Title = {{6.3.1 Threads of Reasoning: A Case Study}},
	Author = {Sandee, J H and van den Bosch, P F A and Heemels, W.P.M.H. and Muller, G J and Verhoef, M H G},
	Journal = {INCOSE International Symposium},
	Year = {2006},
	Number = {1},
	Pages = {895--909},
	Volume = {16},
	Abstract = {In the design of technology intensive products like copiers, wafer steppers and televisions, one searches for a product that satisfies the product requirements as well as the business drivers. The main need in an early design phase is to bring structure in the typical chaos of uncertainty and the huge amount of realization options present. Potential realization choices all have advantages and disadvantages, which cause tensions and conflicts. The earlier the (essential) conflicts and tensions are identified, the better it is. Turning them from implicit to explicit helps the system architect in making the trade-off consciously or at least in selecting the most important tensions and conflicts that require further in-depth investigation. In this respect we demonstrate the effectiveness of a technique called “threads of reasoning?. The illustrative case study is the design of the paper flow control (sensors, actuators, control architecture, etc.) in a high-volume copier/printer.},
	Doi = {10.1002/j.2334-5837.2006.tb02789.x},
	ISSN = {2334-5837},
	Url = {http://dx.doi.org/10.1002/j.2334-5837.2006.tb02789.x}
}

@Article{Santiago20121340,
	Title = {{Model-Driven Engineering as a new landscape for traceability management: A systematic literature review}},
	Author = {Santiago, Iv{\'{a}}n and Jim{\'{e}}nez, {\'{A}}lvaro and Vara, Juan Manuel and Castro, Valeria De and Bollati, Ver{\'{o}}nica A and Marcos, Esperanza},
	Journal = {Information and Software Technology},
	Year = {2012},
	Number = {12},
	Pages = {1340--1356},
	Volume = {54},
	Abstract = {Context Model-Driven Engineering provides a new landscape for dealing with traceability in software development. Objective Our goal is to analyze the current state of the art in traceability management in the context of Model-Driven Engineering. Method We use the systematic literature review based on the guidelines proposed by Kitchenham. We propose five research questions and six quality assessments. Results Of the 157 relevant studies identified, 29 have been considered primary studies. These studies have resulted in 17 proposals. Conclusion The evaluation shows that the most addressed operations are storage, {\{}CRUD{\}} and visualization, while the most immature operations are exchange and analysis traceability information. },
	Annote = {Special Section on Software Reliability and Security},
	Doi = {https://doi.org/10.1016/j.infsof.2012.07.008},
	ISSN = {0950-5849},
	Keywords = {Model-Driven Engineering,Systematic literature review,Traceability},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584912001346}
}

@Article{SPE:SPE2428,
	Title = {{Variability management of plugin-based systems using feature models}},
	Author = {Santos, Andr{\'{e}} L},
	Journal = {Software: Practice and Experience},
	Year = {2017},
	Number = {7},
	Pages = {959--970},
	Volume = {47},
	Abstract = {Plugin-based systems are typically realized with resort to a component framework that offers an infrastructure for assembling plugin components, which can be composed to form system variants. Feature models have been proposed as an abstraction to manage software variability, where feature configurations describe variants of a software system. In this paper, we propose an automated approach to map the artifacts of plugin-based component frameworks to feature models. We describe a methodology for structuring the architecture of a plugin-based system, so that the variability space and variants are reflected in a feature model and its configurations. We materialized the proposed approach for the Eclipse Equinox component framework in a tool to visualize the variability of plugin-based systems in feature diagrams, which can be used to generate system variants. We carried out an experiment where we developed a small plugin-based product line on top of Equinox in the context of an advanced software development course. Copyright {\textcopyright} 2016 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spe.2428},
	ISSN = {1097-024X},
	Keywords = {Equinox,components,feature models,plugin-based systems,variability management},
	Url = {http://dx.doi.org/10.1002/spe.2428}
}

@Article{Schaler2012597,
	Title = {{Building information system variants with tailored database schemas using features}},
	Author = {Sch{\"{a}}ler, M and Leich, T and Rosenm{\"{u}}ller, M and Saake, G},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2012},
	Pages = {597--612},
	Volume = {7328 LNCS},
	Abstract = {Database schemas are an integral part of many information systems (IS). New software-engineering methods, such as software product lines, allow engineers to create a high number of different programs tailored to the customer needs from a common code base. Unfortunately, these engineering methods usually do not take the database schema into account. Particularly, a tailored client program requires a tailored database schema as well to form a consistent IS. In this paper, we show the challenges of tailoring relational database schemas in software product lines. Furthermore, we present an approach to treat the client and database part of an IS in the same way using a variable database schema. Additionally, we show the benefits and discuss disadvantages of the approach during the evolution of an industrial case study, covering a time span of more than a year. {\textcopyright} 2012 Springer-Verlag Berlin Heidelberg.},
	Annote = {cited By 5},
	Doi = {10.1007/978-3-642-31095-9_39},
	Keywords = {Building information system; Client programs; Cust,Database systems,Industrial applications; Information systems; Sys},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867778420{\&}doi=10.1007{\%}2F978-3-642-31095-9{\_}39{\&}partnerID=40{\&}md5=fe5e71afb4fa5e1b7a8c410ca5825877}
}

@InProceedings{Schaefer:2015:STC:2815782.2815799,
	Title = {{SPLicing TABASCO: Custom-Tailored Software Product Line Variants from Taxonomy-Based Toolkits}},
	Author = {Schaefer, Ina and Seidl, Christoph and Cleophas, Loek and Watson, Bruce W},
	Booktitle = {Proceedings of the 2015 Annual Research Conference on South African Institute of Computer Scientists and Information Technologists},
	Year = {2015},
	Address = {New York, NY, USA},
	Pages = {34:1----34:10},
	Publisher = {ACM},
	Series = {SAICSIT '15},
	Doi = {10.1145/2815782.2815799},
	ISBN = {978-1-4503-3683-3},
	Keywords = {Software Product Line (SPL) adoption,Taxonomy-Based Software Construction (TABASCO) to},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2815782.2815799}
}

@Article{SPIP:SPIP215,
	Title = {{Systematic management of software product lines}},
	Author = {Schmid, Klaus and Biffl, Stefan},
	Journal = {Software Process: Improvement and Practice},
	Year = {2005},
	Number = {1},
	Pages = {61--76},
	Volume = {10},
	Abstract = {Software product lines can effectively facilitate large-scale reuse and can thus bring about order of magnitude improvements in terms of time to market (TTM), costs, and quality. This comes at the price of a more complex development environment in which many interdependencies are created through shared generic assets.Owing to this complexity, the specific strategy chosen for product line development can be expected to have a strong impact on the benefits that can be gained from product line development. This is systematically studied in this work, as we vary different strategies and apply them to various forms of products lines. On the basis of the analysis of the performed simulations, we were able to determine optimal, heuristic strategies to the integrated management of the product line.As a result of the analysis, we identify strategies and guidelines that can be employed by practitioners in order to improve the success of their management of a software product line. Copyright {\textcopyright} 2005 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spip.215},
	ISSN = {1099-1670},
	Keywords = {heuristic management,multiproject management,product line,project management,simulation,software product lines},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/spip.215}
}

@Article{Schobbens2007456,
	Title = {{Generic semantics of feature diagrams}},
	Author = {Schobbens, P.-Y. and Heymans, P and Trigaux, J.-C. and Bontemps, Y},
	Journal = {Computer Networks},
	Year = {2007},
	Number = {2},
	Pages = {456--479},
	Volume = {51},
	Abstract = {Feature Diagrams (FDs) are a family of popular modelling languages used to address the feature interaction problem, particularly in software product lines, FDs were first introduced by Kang as part of the FODA (Feature-Oriented Domain Analysis) method back in 1990. Afterwards, various extensions of FODA FDs were introduced to compensate for a purported ambiguity and lack of precision and expressiveness. However, they never received a formal semantics, which is the hallmark of precision and unambiguity and a prerequisite for efficient and safe tool automation. The reported work is intended to contribute a more rigorous approach to the definition, understanding, evaluation, selection and implementation of FD languages. First, we provide a survey of FD variants. Then, we give them a formal semantics, thanks to a generic construction that we call Free Feature Diagrams (FFDs). This demonstrates that FDs can be precise and unambiguous. This also defines their expressiveness. Many variants are expressively complete, and thus the endless quest for extensions actually cannot be justified by expressiveness. A finer notion is thus needed to compare these expressively complete languages. Two solutions are well-established: succinctness and embeddability, that express the naturalness of a language. We show that the expressively complete FDs fall into two succinctness classes, of which we of course recommend the most succinct. Among the succinct expressively complete languages, we suggest a new, simple one that is not harmfully redundant: Varied FD (VFD). Finally, we study the execution time that tools will need to solve useful problems in these languages. {\textcopyright} 2006 Elsevier B.V. All rights reserved.},
	Annote = {cited By 158},
	Doi = {10.1016/j.comnet.2006.08.008},
	Keywords = {Computer aided software engineering; Computer prog,Feature diagram; Feature interaction; Formal sema,Semantics},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750723486{\&}doi=10.1016{\%}2Fj.comnet.2006.08.008{\&}partnerID=40{\&}md5=f069b02d12f436e866959ebaf1a38eb4}
}

@InProceedings{Scholz:2011:ADF:2019136.2019144,
	Title = {{Automatic Detection of Feature Interactions Using the Java Modeling Language: An Experience Report}},
	Author = {Scholz, Wolfgang and Th{\"{u}}m, Thomas and Apel, Sven and Lengauer, Christian},
	Booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
	Year = {2011},
	Address = {New York, NY, USA},
	Pages = {7:1----7:8},
	Publisher = {ACM},
	Series = {SPLC '11},
	Doi = {10.1145/2019136.2019144},
	ISBN = {978-1-4503-0789-5},
	Keywords = {FeatureHouse,JML,feature interaction,software product lines},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2019136.2019144}
}

@InProceedings{Schroeter:2012:MFM:2404962.2404986,
	Title = {{Multi-perspectives on Feature Models}},
	Author = {Schroeter, Julia and Lochau, Malte and Winkelmann, Tim},
	Booktitle = {Proceedings of the 15th International Conference on Model Driven Engineering Languages and Systems},
	Year = {2012},
	Address = {Berlin, Heidelberg},
	Pages = {252--268},
	Publisher = {Springer-Verlag},
	Series = {MODELS'12},
	Doi = {10.1007/978-3-642-33666-9_17},
	ISBN = {978-3-642-33665-2},
	Keywords = {automated view composition,customization,feature models,preconfiguration,software product lines},
	Url = {http://0-dx.doi.org.fama.us.es/10.1007/978-3-642-33666-9{\_}17}
}

@InProceedings{Schultis:2014:ACI:2635868.2635876,
	Title = {{Architecture Challenges for Internal Software Ecosystems: A Large-scale Industry Case Study}},
	Author = {Schultis, Klaus-Benedikt and Elsner, Christoph and Lohmann, Daniel},
	Booktitle = {Proceedings of the 22Nd ACM SIGSOFT International Symposium on Foundations of Software Engineering},
	Year = {2014},
	Address = {New York, NY, USA},
	Pages = {542--552},
	Publisher = {ACM},
	Series = {FSE 2014},
	Doi = {10.1145/2635868.2635876},
	ISBN = {978-1-4503-3056-5},
	Keywords = {Software ecosystem,case study,collaboration,decentralized software engineering,software architecture,software product line},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2635868.2635876}
}

@Article{Schwagerl201619,
	Title = {{Filtered model-driven product line engineering with superMod: The home automation case}},
	Author = {Schw{\"{a}}gerl, F and Buchmann, T and Westfechtel, B},
	Journal = {Communications in Computer and Information Science},
	Year = {2016},
	Pages = {19--41},
	Volume = {586},
	Abstract = {Software Product Line Engineering promises to increase the productivity of software development. In the literature, a plan-driven process has been established that is divided up into domain and application engineering. We argue that the strictly sequential order of its process activities implies several disadvantages such as increased complexity, late customer feedback, and duplicate maintenance. SuperMod is a novel model-driven tool based upon a filtered editing model oriented towards version control. The tool provides integrated support for domain and application engineering, offering an iterative and incremental style of development. In this paper, we apply SuperMod to a well-known case study, the Home Automation System product line. We learn that the tool supports a broad variety of iterative and incremental development processes, ranging from phase-structured to feature-driven. Furthermore, it can mitigate the disadvantages of the traditional software product line development process. {\textcopyright} Springer International Publishing Switzerland 2016.},
	Annote = {cited By 1},
	Doi = {10.1007/978-3-319-30142-6_2},
	Keywords = {Automation; Computer software; Software engineerin,Filtered editing; Home automation; Model-driven E,Software design},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960436440{\&}doi=10.1007{\%}2F978-3-319-30142-6{\_}2{\&}partnerID=40{\&}md5=b27a8d001069fd67759b48dbcc44f29b}
}

@Article{Schwanke20041273,
	Title = {{Experience with the architectural design of a modest product family}},
	Author = {Schwanke, R W and Lutz, R R},
	Journal = {Software - Practice and Experience},
	Year = {2004},
	Number = {13},
	Pages = {1273--1296},
	Volume = {34},
	Abstract = {Many product families are modest in the sense that they consist of a sequence of incremental products with, at any point in time, only a few distinct products available and minimal variations among the products. Such product families, nevertheless, are often large, complex systems, widely deployed, and possessing stringent safety and performance requirements. This paper describes a case study that tends to confirm the value of using a product-line approach for the architectural design of a modest product family. The paper describes the process, design alternatives, and lessons learned, both positive and negative, from the architectural design of one such family of medical image analysis products. Realized benefits included identifying previously unrecognized common behavior and sets of features that were likely to change together, aligning the architecture with specific market needs and with the organization, and reducing unplanned dependencies. Most interesting were the unanticipated benefits, including decoupling the product-family architecture from the order of implementation of features, and using the product-family architecture as a 'guiding star' with subsequent releases moving toward, rather than away from, the planned architecture. Copyright {\textcopyright} 2004 John Wiley {\&} Sons, Ltd.},
	Annote = {cited By 7},
	Doi = {10.1002/spe.613},
	Keywords = {Architecture; Image analysis; Large scale systems;,Medical image analysis; Modest product family; Pr,Product development},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-8344271997{\&}doi=10.1002{\%}2Fspe.613{\&}partnerID=40{\&}md5=d475d60b8742bd21ec76e32c111f28f0}
}

@Article{SYS:SYS21279,
	Title = {{A Reference Architecture for Mobile SOA}},
	Author = {Sefid-Dashti, Behrouz and Habibi, Jafar},
	Journal = {Systems Engineering},
	Year = {2014},
	Number = {4},
	Pages = {407--425},
	Volume = {17},
	Abstract = {Integration of mobile devices into a service-oriented system involves the consideration of architectural design principles and trade-offs among interdependent design decisions to cope with limitations of mobile devices in terms of resources available within a device and aspects of network connectivity. Mobile devices consume some services and provide others. A reference architecture (RA) that provides solutions, baseline and future insights is the priority in designing efficient mobile service oriented architecture (SOA). This paper discusses nuances of the concept of RA. The paper presents a survey of 26 mobile SOA patterns and proposes mobile SOA RA that extends Open Group SOA RA based on the catalogue of mobile SOA patterns proposed in this paper. Evaluations are made and discussed for both RA and its possible instantiation.},
	Doi = {10.1111/sys.21279},
	ISSN = {1520-6858},
	Keywords = {mobile SOA pattern catalogue,mobile SOA reference architecture,reference architecture},
	Url = {http://dx.doi.org/10.1111/sys.21279}
}

@Article{STVR:STVR1566,
	Title = {{Automated metamorphic testing of variability analysis tools}},
	Author = {Segura, Sergio and Dur{\'{a}}n, Amador and S{\'{a}}nchez, Ana B and Berre, Daniel Le and Lonca, Emmanuel and Ruiz-Cort{\'{e}}s, Antonio},
	Journal = {Software Testing, Verification and Reliability},
	Year = {2015},
	Number = {2},
	Pages = {138--163},
	Volume = {25},
	Abstract = {Variability determines the capability of software applications to be configured and customized. A common need during the development of variability-intensive systems is the automated analysis of their underlying variability models, for example, detecting contradictory configuration options. The analysis operations that are performed on variability models are often very complex, which hinders the testing of the corresponding analysis tools and makes difficult, often infeasible, to determine the correctness of their outputs, that is, the well-known oracle problem in software testing. In this article, we present a generic approach for the automated detection of faults in variability analysis tools overcoming the oracle problem. Our work enables the generation of random variability models together with the exact set of valid configurations represented by these models. These test data are generated from scratch using stepwise transformations and assuring that certain constraints (a.k.a. metamorphic relations) hold at each step. To show the feasibility and generalizability of our approach, it has been used to automatically test several analysis tools in three variability domains: feature models, common upgradeability description format documents and Boolean formulas. Among other results, we detected 19 real bugs in 7 out of the 15 tools under test. Copyright {\textcopyright} 2015 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/stvr.1566},
	ISSN = {1099-1689},
	Keywords = {automated testing,metamorphic testing,software testing,software variability},
	Url = {http://dx.doi.org/10.1002/stvr.1566}
}

@Article{Segura20111124,
	Title = {{Mutation testing on an object-oriented framework: An experience report}},
	Author = {Segura, Sergio and Hierons, Robert M and Benavides, David and Ruiz-Cort{\'{e}}s, Antonio},
	Journal = {Information and Software Technology},
	Year = {2011},
	Number = {10},
	Pages = {1124--1136},
	Volume = {53},
	Abstract = {Context The increasing presence of Object-Oriented (OO) programs in industrial systems is progressively drawing the attention of mutation researchers toward this paradigm. However, while the number of research contributions in this topic is plentiful, the number of empirical results is still marginal and mostly provided by researchers rather than practitioners. Objective This article reports our experience using mutation testing to measure the effectiveness of an automated test data generator from a user perspective. Method In our study, we applied both traditional and class-level mutation operators to FaMa, an open source Java framework currently being used for research and commercial purposes. We also compared and contrasted our results with the data obtained from some motivating faults found in the literature and two real tools for the analysis of feature models, FaMa and SPLOT. Results Our results are summarized in a number of lessons learned supporting previous isolated results as well as new findings that hopefully will motivate further research in the field. Conclusion We conclude that mutation testing is an effective and affordable technique to measure the effectiveness of test mechanisms in {\{}OO{\}} systems. We found, however, several practical limitations in current tool support that should be addressed to facilitate the work of testers. We also missed specific techniques and tools to apply mutation testing at the system level. },
	Annote = {Special Section on Mutation Testing},
	Doi = {https://doi.org/10.1016/j.infsof.2011.03.006},
	ISSN = {0950-5849},
	Keywords = {Automated analysis,Feature models,Mutation testing,Test adequacy,Test data generation},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584911000826}
}

@Article{Segura2011245,
	Title = {{Automated metamorphic testing on the analyses of feature models}},
	Author = {Segura, Sergio and Hierons, Robert M and Benavides, David and Ruiz-Cort{\'{e}}s, Antonio},
	Journal = {Information and Software Technology},
	Year = {2011},
	Number = {3},
	Pages = {245--258},
	Volume = {53},
	Abstract = {Context A feature model (FM) represents the valid combinations of features in a domain. The automated extraction of information from {\{}FMs{\}} is a complex task that involves numerous analysis operations, techniques and tools. Current testing methods in this context are manual and rely on the ability of the tester to decide whether the output of an analysis is correct. However, this is acknowledged to be time-consuming, error-prone and in most cases infeasible due to the combinatorial complexity of the analyses, this is known as the oracle problem. Objective In this paper, we propose using metamorphic testing to automate the generation of test data for feature model analysis tools overcoming the oracle problem. An automated test data generator is presented and evaluated to show the feasibility of our approach. Method We present a set of relations (so-called metamorphic relations) between input {\{}FMs{\}} and the set of products they represent. Based on these relations and given a {\{}FM{\}} and its known set of products, a set of neighbouring {\{}FMs{\}} together with their corresponding set of products are automatically generated and used for testing multiple analyses. Complex {\{}FMs{\}} representing millions of products can be efficiently created by applying this process iteratively. Results Our evaluation results using mutation testing and real faults reveal that most faults can be automatically detected within a few seconds. Two defects were found in FaMa and another two in SPLOT, two real tools for the automated analysis of feature models. Also, we show how our generator outperforms a related manual suite for the automated analysis of feature models and how this suite can be used to guide the automated generation of test cases obtaining important gains in efficiency. Conclusion Our results show that the application of metamorphic testing in the domain of automated analysis of feature models is efficient and effective in detecting most faults in a few seconds without the need for a human oracle. },
	Doi = {https://doi.org/10.1016/j.infsof.2010.11.002},
	ISSN = {0950-5849},
	Keywords = {Automated analysis,Feature models,Metamorphic testing,Mutation testing,Product lines,Test data generation},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584910001904}
}

@Article{Segura20143975,
	Title = {{Automated generation of computationally hard feature models using evolutionary algorithms}},
	Author = {Segura, Sergio and Parejo, Jos{\'{e}} A and Hierons, Robert M and Benavides, David and Ruiz-Cort{\'{e}}s, Antonio},
	Journal = {Expert Systems with Applications},
	Year = {2014},
	Number = {8},
	Pages = {3975--3992},
	Volume = {41},
	Abstract = {Abstract A feature model is a compact representation of the products of a software product line. The automated extraction of information from feature models is a thriving topic involving numerous analysis operations, techniques and tools. Performance evaluations in this domain mainly rely on the use of random feature models. However, these only provide a rough idea of the behaviour of the tools with average problems and are not sufficient to reveal their real strengths and weaknesses. In this article, we propose to model the problem of finding computationally hard feature models as an optimization problem and we solve it using a novel evolutionary algorithm for optimized feature models (ETHOM). Given a tool and an analysis operation, {\{}ETHOM{\}} generates input models of a predefined size maximizing aspects such as the execution time or the memory consumption of the tool when performing the operation over the model. This allows users and developers to know the performance of tools in pessimistic cases providing a better idea of their real power and revealing performance bugs. Experiments using {\{}ETHOM{\}} on a number of analyses and tools have successfully identified models producing much longer executions times and higher memory consumption than those obtained with random models of identical or even larger size. },
	Doi = {https://doi.org/10.1016/j.eswa.2013.12.028},
	ISSN = {0957-4174},
	Keywords = {Automated analysis,Evolutionary algorithms,Feature models,Performance testing,Search-based testing,Software product lines},
	Url = {http://www.sciencedirect.com/science/article/pii/S0957417413010038}
}

@InProceedings{Segura:2014:AVA:2642937.2642939,
	Title = {{Automated Variability Analysis and Testing of an E-commerce Site.: An Experience Report}},
	Author = {Segura, Sergio and S{\'{a}}nchez, Ana B and Ruiz-Cort{\'{e}}s, Antonio},
	Booktitle = {Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
	Year = {2014},
	Address = {New York, NY, USA},
	Pages = {139--150},
	Publisher = {ACM},
	Series = {ASE '14},
	Doi = {10.1145/2642937.2642939},
	ISBN = {978-1-4503-3013-8},
	Keywords = {automated testing,e-commerce,experience report,feature modelling,variability},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2642937.2642939}
}

@Conference{Seidl2014,
	Title = {{Capturing variability in space and time with hyper feature models}},
	Author = {Seidl, C and Schaefer, I and A{\ss}mann, U},
	Booktitle = {ACM International Conference Proceeding Series},
	Year = {2014},
	Abstract = {Software product lines (SPLs) and software ecosystems (SECOs) are approaches to capturing families of closely related software systems in terms of common and variable functionality. SPLs and especially SECOs are subject to evolution to adapt to new or changed requirements resulting in different versions of the software family and its variable assets. These versions may have to be maintained and used for products even after they were superseded by newer versions. Variability models describing valid combinations of variable assets, such as feature models, capture variability in space (configuration), but not variability in time (evolution) making it impossible to respect versions of variable assets in product definitions on a conceptual level. In this paper, we propose Hyper Feature Models (HFMs) explicitly providing feature versions as configurable units for product definition. Furthermore, we provide a version-aware constraint language to specify dependencies between features and ranges of feature versions as well as a procedure to automatically select valid combinations of versions for a pre-configuration of features. We demonstrate our approach in a case study. {\textcopyright} 2014 ACM.},
	Annote = {cited By 1},
	Doi = {10.1145/2556624.2556625},
	Keywords = {Computer software; Ecosystems,Software design,constraint; evolution; Feature modeling; Software},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897616868{\&}doi=10.1145{\%}2F2556624.2556625{\&}partnerID=40{\&}md5=34fbf588d062e02046188f3b76e3196d}
}

@Article{Seidl201789,
	Title = {{Generative software product line development using variability-aware design patterns}},
	Author = {Seidl, C and Schuster, S and Schaefer, I},
	Journal = {Computer Languages, Systems and Structures},
	Year = {2017},
	Pages = {89--111},
	Volume = {48},
	Abstract = {Software Product Lines (SPLs) are an approach to reuse in-the-large that models a set of closely related software systems in terms of commonalities and variabilities. Design patterns are best practices for addressing recurring design problems in object-oriented source code. In the practice of implementing SPL, instances of certain design patterns are employed to handle variability, which makes these “variability-aware design patterns? a best practice for SPL design. However, currently there is no dedicated method for proactively developing SPLs using design patterns suitable for realizing variable functionality. In this paper, we present a method to perform generative SPL development with design patterns. We use role models to capture design patterns and their relation to a variability model. We further allow mapping of individual design pattern roles to (parts of) implementation elements to be generated (e.g., classes, methods) and check the conformance of the realization with the specification of the pattern. We provide definitions for the variability-aware versions of the design patterns Observer, Strategy, Template Method and Composite. Furthermore, we support generation of realizations in Java, C++ and UML class diagrams utilizing annotative, compositional and transformational variability realization mechanisms. Hence, we support proactive development of SPLs using design patterns to apply best practices for the realization of variability. We realize our concepts within the Eclipse IDE and demonstrate them within a case study. {\textcopyright} 2016 Elsevier Ltd},
	Annote = {cited By 0},
	Doi = {10.1016/j.cl.2016.08.006},
	Keywords = {C++ (programming language); Computer software; Com,Product design,Software product line (SPLs); Software product li},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995752793{\&}doi=10.1016{\%}2Fj.cl.2016.08.006{\&}partnerID=40{\&}md5=85cb107ea1b92dcce82154a31efd6979}
}

@Article{SPE:SPE1077,
	Title = {{A component-based middleware platform for reconfigurable service-oriented architectures}},
	Author = {Seinturier, Lionel and Merle, Philippe and Rouvoy, Romain and Romero, Daniel and Schiavoni, Valerio and Stefani, Jean-Bernard},
	Journal = {Software: Practice and Experience},
	Year = {2012},
	Number = {5},
	Pages = {559--583},
	Volume = {42},
	Abstract = {ThetextitService Component Architecture (SCA) is a technology-independent standard for developing distributed Service-oriented Architectures (SOA). The SCA standard promotes the use of components and architecture descriptors, and mostly covers the lifecycle steps of implementation and deployment. Unfortunately, SCA does not address the governance of SCA applications and provides no support for the maintenance of deployed components. This article covers this issue and introduces the FRASCATI platform, a run-time support for SCA with dynamic reconfiguration capabilities and run-time management features. This article presents the internal component-based architecture of the FRASCATI platform, and highlights its key features. The component-based design of the FRASCATI platform introduces many degrees of flexibility and configurability in the platform itself and it can host the SOA applications. This article reports on micro-benchmarks highlighting that run-time manageability in the FRASCATI platform does not decrease its performance when compared with the de facto reference SCA implementation: Apache TUSCANY. Finally, a smart home scenario illustrates the extension capabilities and the various reconfigurations of the FRASCATI platform. Copyright {\textcopyright} 2011 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spe.1077},
	ISSN = {1097-024X},
	Keywords = {SCA,SOA,component,middleware,reconfiguration},
	Publisher = {John Wiley {\&} Sons, Ltd},
	Url = {http://dx.doi.org/10.1002/spe.1077}
}

@Article{IIS2:IIS202835,
	Title = {{11.4.1 Generating Predictive Models Using Decision Trees and Neural Networks for Large-Scale Systems Engineering}},
	Author = {Selby, Richard W},
	Journal = {INCOSE International Symposium},
	Year = {2006},
	Number = {1},
	Pages = {1585--1596},
	Volume = {16},
	Abstract = {Systems engineering must tackle the challenges of computational systems that are increasingly large-scale and software-intensive in terms of system size, component breadth and maturity, and development heterogeneity. This research describes and empirically evaluates techniques for generating predictive models for enabling large-scale system development and management. We describe two types of metric-driven decision models, decision trees and neural networks, which classify software components in large systems according to their likelihood of having user-specified properties such as high fault-proneness or high development effort. The metric-driven decision models enable coarse-grain analysis of large-scale multi-component heterogeneous systems, and they identify high-payoff areas for directing the application of fine-grain analysis techniques for fault detection or redesign. The decision models serve as metric integration mechanisms that enable the synergistic use of numerous metrics simultaneously and integrate measurements collected by development tools or infrastructure. Model generation techniques automatically generate the decision models to calibrate them to new projects and organizations.We evaluate the predictive effectiveness of the decision models in terms of correctness, consistency, and completeness using fault and effort data from large NASA systems. Correctness is defined as the percent of components correctly identified, consistency is defined as 100{\%} minus the percent of false positives, and completeness is defined as 100{\%} minus the percent of false negatives. On average, the decision models had 83.44{\%} correctness, 71.96{\%} consistency, and 65.25{\%} completeness in predictions of high fault and high effort software components. The network models had 89.63{\%} correctness, 79.49{\%} consistency, and 69.09{\%} completeness, while the tree models had 77.25{\%} correctness, 64.42{\%} consistency, and 61.40{\%} completeness. Non-parametric ANOVA comparisons showed that the network models were statistically more accurate than the tree models ($\alpha$ {\textless} 0.0001).},
	Doi = {10.1002/j.2334-5837.2006.tb02835.x},
	ISSN = {2334-5837},
	Url = {http://dx.doi.org/10.1002/j.2334-5837.2006.tb02835.x}
}

@Article{Sellier2008299,
	Title = {{Managing requirements inter-dependency for software product line derivation}},
	Author = {Sellier, D and Mannion, M and Mansell, J X},
	Journal = {Requirements Engineering},
	Year = {2008},
	Number = {4},
	Pages = {299--313},
	Volume = {13},
	Abstract = {Software Product Line Engineering (SPLE) can reduce software development costs, reduce time to market and improve product quality. A software product line is a set of software products sharing a set of common features but containing variation points. Successful SPLE requires making selection decisions at variation points effectively and efficiently. A significant challenge is how to identify, represent and manage the inter-dependency of selection decisions for requirements. We developed the concept of a meta-model for requirement decision models to bring formalism and consistency to the structure and to model inter-dependencies between requirement selection decisions. Here we present a meta-model for requirement selection decisions that includes inter-dependencies and we use a mobile phone worked example to illustrate our approach. To support our method, we developed two separate tools, V-Define (for domain decision model construction) and V-Resolve (for new product derivation). Finally the results of a metal processing product line case study using the tools are described. {\textcopyright} Springer-Verlag London Limited 2008.},
	Annote = {cited By 5},
	Doi = {10.1007/s00766-008-0066-4},
	Keywords = {Case studies; Common features; Decision models; D,Computer software selection and evaluation,Concurrent engineering; Decision making; Productio},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-54249094126{\&}doi=10.1007{\%}2Fs00766-008-0066-4{\&}partnerID=40{\&}md5=32aa8c667a85021263ab3ed83b09389b}
}

@Article{Sen2013657,
	Title = {{Testing a data-intensive system with generated data interactions: The Norwegian customs and excise case study}},
	Author = {Sen, S and Gotlieb, A},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2013},
	Pages = {657--671},
	Volume = {7908 LNCS},
	Abstract = {Testing data-intensive systems is paramount to increase our reliance on e-governance services. An incorrectly computed tax can have catastrophic consequences in terms of public image. Testers at Norwegian Customs and Excise reveal that faults occur from interactions between database features such as field values. Taxation rules, for example, are triggered due to an interaction between 10,000 items, 88 country groups, and 934 tax codes. There are about 12.9 trillion 3-wise interactions. Finding interactions to uncover specific faults is like finding a needle in a haystack. Can we surgically generate a test database for interactions that interest testers? We address this question with a methodology and tool Faktum to automatically populate a test database that covers all T-wise interactions for selected features. Faktum generates a constraint model of interactions in Alloy and solves it using a divide-and-combine strategy. Our experiments demonstrate scalability of our methodology and we project its industrial applications. {\textcopyright} 2013 Springer-Verlag.},
	Annote = {cited By 3},
	Doi = {10.1007/978-3-642-38709-8_42},
	Keywords = {Alloying; Cerium alloys; Industrial applications;,Database schemas; Entity relationship diagrams; Fe,Database systems},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879862386{\&}doi=10.1007{\%}2F978-3-642-38709-8{\_}42{\&}partnerID=40{\&}md5=52156c86c764b18c8dfc654a55c61079}
}

@InProceedings{Sena:2012:MVS:2591028.2600820,
	Title = {{Modularization of Variabilities from Software Product Lines of Web Information Systems (in Portuguese)}},
	Author = {Sena, Dem{\'{o}}stenes and Pinto, Felipe and Lima, Gleydson and Santos, Jadson and Lima, Jalerson and Kulesza, Uir{\'{a}} and Pereira, David and Fernandes, Victor and Vianna, Alexandre},
	Booktitle = {Proceedings of the 9th Latin-American Conference on Pattern Languages of Programming},
	Year = {2012},
	Address = {New York, NY, USA},
	Pages = {11:1----11:15},
	Publisher = {ACM},
	Series = {SugarLoafPLoP '12},
	Doi = {10.1145/2591028.2600820},
	ISBN = {978-1-4503-2787-9},
	Keywords = {design patterns,information systems,software product lines},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2591028.2600820}
}

@Article{Sepulveda201616,
	Title = {{Requirements modeling languages for software product lines: A systematic literature review}},
	Author = {Sep{\'{u}}lveda, S and Cravero, A and Cachero, C},
	Journal = {Information and Software Technology},
	Year = {2016},
	Pages = {16--36},
	Volume = {69},
	Abstract = {Context: Software product lines (SPLs) have reached a considerable level of adoption in the software industry, having demonstrated their cost-effectiveness for developing higher quality products with lower costs. For this reason, in the last years the requirements engineering community has devoted much effort to the development of a myriad of requirements modelling languages for SPLs. Objective: In this paper, we review and synthesize the current state of research of requirements modelling languages used in SPLs with respect to their degree of empirical validation, origin and context of use, level of expressiveness, maturity, and industry adoption. Method: We have conducted a systematic literature review with six research questions that cover the main objective. It includes 54 studies, published from 2000 to 2013. Results: The mean level of maturity of the modelling languages is 2.59 over 5, with 46{\%} of them falling within level 2 or below -no implemented abstract syntax reported-. They show a level of expressiveness of 0.7 over 1.0. Some constructs (feature, mandatory, optional, alternative, exclude and require) are present in all the languages, while others (cardinality, attribute, constraint and label) are less common. Only 6{\%} of the languages have been empirically validated, 41{\%} report some kind of industry adoption and 71{\%} of the languages are independent from any development process. Last but not least, 57{\%} of the languages have been proposed by the academia, while 43{\%} have been the result of a joint effort between academia and industry. Conclusions: Research on requirements modeling languages for SPLs has generated a myriad of languages that differ in the set of constructs provided to express SPL requirements. Their general lack of empirical validation and adoption in industry, together with their differences in maturity, draws the picture of a discipline that still needs to evolve. {\textcopyright} 2015 Elsevier B.V. All rights reserved.},
	Annote = {cited By 5},
	Doi = {10.1016/j.infsof.2015.08.007},
	Keywords = {Computational linguistics; Computer software; Cost,Development process; Empirical validation; Requir,Modeling languages},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946595413{\&}doi=10.1016{\%}2Fj.infsof.2015.08.007{\&}partnerID=40{\&}md5=718ae3d53a66e2e0acba045e425380dd}
}

@Article{Shen201152,
	Title = {{Towards feature-oriented variability reconfiguration in dynamic software product lines}},
	Author = {Shen, L and Peng, X and Liu, J and Zhao, W},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2011},
	Pages = {52--68},
	Volume = {6727 LNCS},
	Abstract = {Dynamic Software Product Line (DSPL) provides a new paradigm for developing self-adaptive systems with the principles of software product line engineering. DSPL emphasizes variability analysis and design at development time and variability binding and reconfiguration at runtime, thus requires some kinds of variability mechanisms to map high-level variations (usually represented by features) to low-level implementation and support runtime reconfiguration. Existing work on DSPL usually assumes that variation features can be directly mapped to coarse-grained elements like services, components or plug-ins, making the methods hard to be applied for traditional software systems. In this paper, we propose a feature-oriented method to support runtime variability reconfiguration in DSPLs. The method introduces the concept of role model, an intermediate level between feature variations and implementations to improve their traceability. On the other hand, the method involves a reference implementation framework based on dynamic aspect mechanisms to implement the runtime reconfiguration. We illustrate the process of applying the proposed method with a concrete case study, which helps to validate the effectiveness of our method. {\textcopyright} 2011 Springer-Verlag.},
	Annote = {cited By 13},
	Doi = {10.1007/978-3-642-21347-2_5},
	Keywords = {Adaptive systems; Dynamics; Network architecture;,Coarse-grained; Development time; Dynamic software,Computer software reusability},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959638959{\&}doi=10.1007{\%}2F978-3-642-21347-2{\_}5{\&}partnerID=40{\&}md5=f47c66eef11a4c469a6cc6a12583940c}
}

@Article{Shen2009170,
	Title = {{Feature-driven and incremental variability generalization in software product line}},
	Author = {Shen, L and Peng, X and Zhao, W},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2009},
	Pages = {170--180},
	Volume = {5791 LNCS},
	Abstract = {In the lifecycle of a software product line (SPL), incremental generalization is usually required to extend the variability of existing core assets to support the new or changed application requirements. In addition, the generalization should conform to the evolved SPL requirements which are usually represented by a feature model. In this paper, we propose a feature-driven and incremental variability generalization method based on the aspect-oriented variability implementation techniques. It addresses a set of basic scenarios where program-level JBoss-AOP based reference implementations respond to the feature-level variability generalization patterns. It also provides the corresponding guidance to compose these patterns in more complex cases. Based on the method, we present a case study and related discussions. {\textcopyright} 2009 Springer Berlin Heidelberg.},
	Annote = {cited By 0},
	Doi = {10.1007/978-3-642-04211-9_17},
	Keywords = {Application requirements; Aspect-oriented; Core as,Computer software reusability,Speech recognition},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350381125{\&}doi=10.1007{\%}2F978-3-642-04211-9{\_}17{\&}partnerID=40{\&}md5=3b8d03018048714ba3902c69c190962f}
}

@Article{Shi2012270,
	Title = {{Integration testing of software product lines using compositional symbolic execution}},
	Author = {Shi, J and Cohen, M B and Dwyer, M B},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2012},
	Pages = {270--284},
	Volume = {7212 LNCS},
	Abstract = {Software product lines are families of products defined by feature commonality and variability, with a well-managed asset base. Recent work in testing of software product lines has exploited similarities across development phases to reuse shared assets and reduce test effort. The use of feature dependence graphs has also been employed to reduce testing effort, but little work has focused on code level analysis of dataflow between features. In this paper we present a compositional symbolic execution technique that works in concert with a feature dependence graph to extract the set of possible interaction trees in a product family. It composes these to incrementally and symbolically analyze feature interactions. We experiment with two product lines and determine that our technique can reduce the overall number of interactions that must be considered during testing, and requires less time to run than a traditional symbolic execution technique. {\textcopyright} 2012 Springer-Verlag Berlin Heidelberg.},
	Annote = {cited By 15},
	Doi = {10.1007/978-3-642-28872-2_19},
	Keywords = {Code-level analysis; Commonality and variability;,Data flow analysis; Software testing; Trees (math,Software engineering},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859125355{\&}doi=10.1007{\%}2F978-3-642-28872-2{\_}19{\&}partnerID=40{\&}md5=bbbff7d1460a18a668bf5090f0f82715}
}

@Article{IIS2:IIS202644,
	Title = {{4.4.2 The Mission Analysis Discipline: Bringing focus to the fuzziness about Attaining Good Architectures}},
	Author = {Shupp, Jeffrey K},
	Journal = {INCOSE International Symposium},
	Year = {2003},
	Number = {1},
	Pages = {596--606},
	Volume = {13},
	Abstract = {The systems and software engineering communities have experienced a veritable explosion of activity surrounding architectures and their role in developing complex systems. This activity has moved forward on two fundamental fronts: the ability to document an architecture, and the ability to evaluate one. Both of these themes have as their foundation the need for well articulated requirements. The architecture is the highest level design of a system, and as such is a response to the requirements. This paper addresses a third, largely overlooked, segment of the early systems development cycle called Mission Analysis, and the focus on the solution trade space through the generation of Technical Business Strategies. Making this topic a recognized discipline in its own right will help to focus early system development work in a way that hopefully makes the overall process less reliant on super system architects to produce good solutions for the long run.},
	Doi = {10.1002/j.2334-5837.2003.tb02644.x},
	ISSN = {2334-5837},
	Url = {http://dx.doi.org/10.1002/j.2334-5837.2003.tb02644.x}
}

@Article{Siegmund2013491,
	Title = {{Scalable prediction of non-functional properties in software product lines: Footprint and memory consumption}},
	Author = {Siegmund, N and Rosenm{\"{u}}ller, M and K{\"{a}}stner, C and Giarrusso, P G and Apel, S and Kolesnikov, S S},
	Journal = {Information and Software Technology},
	Year = {2013},
	Number = {3},
	Pages = {491--507},
	Volume = {55},
	Abstract = {Context: A software product line is a family of related software products, typically created from a set of common assets. Users select features to derive a product that fulfills their needs. Users often expect a product to have specific non-functional properties, such as a small footprint or a bounded response time. Because a product line may have an exponential number of products with respect to its features, it is usually not feasible to generate and measure non-functional properties for each possible product. Objective: Our overall goal is to derive optimal products with respect to non-functional requirements by showing customers which features must be selected. Method: We propose an approach to predict a product's non-functional properties based on the product's feature selection. We aggregate the influence of each selected feature on a non-functional property to predict a product's properties. We generate and measure a small set of products and, by comparing measurements, we approximate each feature's influence on the non-functional property in question. As a research method, we conducted controlled experiments and evaluated prediction accuracy for the non-functional properties footprint and main-memory consumption. But, in principle, our approach is applicable for all quantifiable non-functional properties. Results: With nine software product lines, we demonstrate that our approach predicts the footprint with an average accuracy of 94{\%}, and an accuracy of over 99{\%} on average if feature interactions are known. In a further series of experiments, we predicted main memory consumption of six customizable programs and achieved an accuracy of 89{\%} on average. Conclusion: Our experiments suggest that, with only few measurements, it is possible to accurately predict non-functional properties of products of a product line. Furthermore, we show how already little domain knowledge can improve predictions and discuss trade-offs between accuracy and required number of measurements. With this technique, we provide a basis for many reasoning and product-derivation approaches. {\textcopyright} 2012 Elsevier B.V. All rights reserved.},
	Annote = {cited By 28},
	Doi = {10.1016/j.infsof.2012.07.020},
	Keywords = {Controlled experiment; Customizable; Domain knowle,Experiments; Measurements; Network architecture;,Forecasting},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872961131{\&}doi=10.1016{\%}2Fj.infsof.2012.07.020{\&}partnerID=40{\&}md5=6b156f0c57fa225ac4233c5604b091d6}
}

@Article{Siek2011423,
	Title = {{A language for generic programming in the large}},
	Author = {Siek, Jeremy G and Lumsdaine, Andrew},
	Journal = {Science of Computer Programming},
	Year = {2011},
	Number = {5},
	Pages = {423--465},
	Volume = {76},
	Abstract = {Generic programming is an effective methodology for developing reusable software libraries. Many programming languages provide generics and have features for describing interfaces, but none completely support the idioms used in generic programming. To address this need we developed the language G . The central feature of G is the concept, a mechanism for organizing constraints on generics that is inspired by the needs of modern C++ libraries. G provides modular type checking and separate compilation (even of generics). These characteristics support modular software development, especially the smooth integration of independently developed components. In this article we present the rationale for the design of G and demonstrate the expressiveness of G with two case studies: porting the Standard Template Library and the Boost Graph Library from C++ to G . The design of G shares much in common with the concept extension proposed for the next C++ Standard (the authors participated in its design) but there are important differences described in this article. },
	Annote = {Special Issue on Generative Programming and Component Engineering (Selected Papers from {\{}GPCE{\}} 2004/2005)},
	Doi = {https://doi.org/10.1016/j.scico.2008.09.009},
	ISSN = {0167-6423},
	Keywords = {Associated types,Concepts,Functors,Generic programming,Generics,Modules,Polymorphism,Programming language design,Signatures,Software reuse,Type classes,Virtual types},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642308001123}
}

@Article{Sillitti2004393,
	Title = {{Measures for mobile users: an architecture}},
	Author = {Sillitti, Alberto and Janes, Andrea and Succi, Giancarlo and Vernazza, Tullio},
	Journal = {Journal of Systems Architecture},
	Year = {2004},
	Number = {7},
	Pages = {393--405},
	Volume = {50},
	Abstract = {Software measures are important to evaluate software properties like complexity, reusability, maintainability, effort required, etc. Collecting such data is difficult because of the lack of tools that perform acquisition automatically. It is not possible to implement a manual data collection because it is error prone and very time expensive. Moreover, developers often work in teams and sometimes in different places using laptops. These conditions require tools that collect data automatically, can work offline and merge data from different developers working in the same project. This paper presents {\{}PROM{\}} (PRO Metrics), a distributed Java based tool designed to collect automatically software measures. This tool uses a distributed architecture based on plug-ins, integrated in most popular development tools, and the {\{}SOAP{\}} communication protocol. },
	Annote = {Adaptable System/Software Architectures},
	Doi = {https://doi.org/10.1016/j.sysarc.2003.09.005},
	ISSN = {1383-7621},
	Keywords = {Development monitoring,Java,Process metrics,Product metrics},
	Url = {http://www.sciencedirect.com/science/article/pii/S1383762103001607}
}

@Article{RodriguesdaSilva2015139,
	Title = {{Model-driven engineering: A survey supported by the unified conceptual model}},
	Author = {da Silva, Alberto Rodrigues},
	Journal = {Computer Languages, Systems {\&} Structures},
	Year = {2015},
	Pages = {139--155},
	Volume = {43},
	Abstract = {Abstract During the last decade a new trend of approaches has emerged, which considers models not just documentation artefacts, but also central artefacts in the software engineering field, allowing the creation or automatic execution of software systems starting from those models. These proposals have been classified generically as Model-Driven Engineering (MDE) and share common concepts and terms that need to be abstracted, discussed and understood. This paper presents a survey on {\{}MDE{\}} based on a unified conceptual model that clearly identifies and relates these essential concepts, namely the concepts of system, model, metamodel, modeling language, transformations, software platform, and software product. In addition, this paper discusses the terminologies relating MDE, MDD, {\{}MDA{\}} and others. This survey is based on earlier work, however, contrary to those, it intends to give a simple, broader and integrated view of the essential concepts and respective terminology commonly involved in the MDE, answering to key questions such as: What is a model? What is the relation between a model and a metamodel? What are the key facets of a modeling language? How can I use models in the context of a software development process? What are the relations between models and source code artefacts and software platforms? and What are the relations between MDE, MDD, {\{}MDA{\}} and other {\{}MD{\}} approaches? },
	Doi = {https://doi.org/10.1016/j.cl.2015.06.001},
	ISSN = {1477-8424},
	Keywords = {Metamodel,Model,Model-driven approaches,Model-driven engineering,Modeling language,Software system},
	Url = {http://www.sciencedirect.com/science/article/pii/S1477842415000408}
}

@Article{daSilva2009105,
	Title = {{Refactoring of Crosscutting Concerns with Metaphor-Based Heuristics}},
	Author = {da Silva, Bruno Carreiro and Figueiredo, Eduardo and Garcia, Alessandro and Nunes, Daltro},
	Journal = {Electronic Notes in Theoretical Computer Science},
	Year = {2009},
	Pages = {105--125},
	Volume = {233},
	Abstract = {It has been advocated that Aspect-Oriented Programming (AOP) is an effective technique to improve software maintainability through explicit support for modularising crosscutting concerns. However, in order to take the advantages of AOP, there is a need for supporting the systematic refactoring of crosscutting concerns to aspects. Existing techniques for aspect-oriented refactoring are too fine-grained and do not take the concern structure into consideration. This paper presents two categories towards a metaphor-based classification of crosscutting concerns driven by their manifested shapes through a system's modular structure. The proposed categories provide an intuitive and fundamental terminology for detecting concern-oriented design flaws and identifying refactorings in terms of recurring crosscutting structures. On top of this classification, we define a suite of metaphor-based refactorings to guide the “aspectisation? of each concern category. We evaluate our technique by classifying concerns of 23 design patterns and by proposing refactorings to aspectise them according to observations made in previous empirical studies. Based on our experience, we also determine a catalogue of potential additional categories and heuristics for refactoring of crosscutting concerns. },
	Annote = {Proceedings of the International Workshop on Software Quality and Maintainability (SQM 2008)},
	Doi = {https://doi.org/10.1016/j.entcs.2009.02.064},
	ISSN = {1571-0661},
	Keywords = {Aspect-oriented programming,Crosscutting concerns,Design heuristics,Metaphor-based classification,Refactoring},
	Url = {http://www.sciencedirect.com/science/article/pii/S1571066109000693}
}

@InProceedings{daSilva:2014:ESD:2577080.2577096,
	Title = {{An Empirical Study on How Developers Reason About Module Cohesion}},
	Author = {da Silva, Bruno C and Sant'Anna, Claudio N and Chavez, Christina von F G},
	Booktitle = {Proceedings of the 13th International Conference on Modularity},
	Year = {2014},
	Address = {New York, NY, USA},
	Pages = {121--132},
	Publisher = {ACM},
	Series = {MODULARITY '14},
	Doi = {10.1145/2577080.2577096},
	ISBN = {978-1-4503-2772-5},
	Keywords = {empirical software engineering,module cohesion},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2577080.2577096}
}

@Article{daSilva20131316,
	Title = {{Team building criteria in software projects: A mix-method replicated study}},
	Author = {da Silva, Fabio Q B and Fran{\c{c}}a, A C{\'{e}}sar C and Suassuna, Marcos and {de Sousa Mariz}, Leila M R and Rossiley, Isabella and de Miranda, Regina C G and Gouveia, Tatiana B and Monteiro, Cleviton V F and Lucena, Evisson and Cardozo, Elisa S F and Espindola, Edval},
	Journal = {Information and Software Technology},
	Year = {2013},
	Number = {7},
	Pages = {1316--1340},
	Volume = {55},
	Abstract = {Context The internal composition of a work team is an important antecedent of team performance and the criteria used to select team members play an important role in determining team composition. However, there are only a handful of empirical studies about the use of team building criteria in the software industry. Objective The goal of this article is to identify criteria used in industrial practice to select members of a software project team, and to look for relationships between the use of these criteria and project success. In addition, we expect to contribute with findings about the use of replication in empirical studies involving human factors in software engineering. Method Our research was based on an iterative mix-method, replication strategy. In the first iteration, we used qualitative research to identify team-building criteria interviewing software project managers from industry. Then, we performed a cross-sectional survey to assess the correlations of the use of these criteria and project success. In the second iteration, we used the results of a systematic mapping study to complement the set of team building criteria. Finally, we performed a replication of the survey research with variations to verify and improve the results. Results Our results showed that the consistent use team building criteria correlated significantly with project success, and the criteria related to human factors, such as personality and behavior, presented the strongest correlations. The results of the replication did not reproduce the results of the original survey with respect to the correlations between criteria and success goals. Nevertheless, the variations in the design and the difference in the sample of projects allowed us to conclude that the two results were compatible, increasing our confidence on the existence of the correlations. Conclusion Our findings indicated that carefully selecting team member for software teams is likely to positively influence the projects in which these teams participate. Besides, it seems that the type of development method used can moderate (increase or decrease) this influence. In addition, our study showed that the choice of sampling technique is not straightforward given the many interacting factors affecting this type of investigation. },
	Doi = {https://doi.org/10.1016/j.infsof.2012.11.006},
	ISSN = {0950-5849},
	Keywords = {People management,Software engineering,Software teams,Team building criteria,Team effectiveness},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584912002327}
}

@Article{daSilva2011899,
	Title = {{Six years of systematic literature reviews in software engineering: An updated tertiary study}},
	Author = {da Silva, Fabio Q B and Santos, Andr{\'{e}} L M and Soares, S{\'{e}}rgio and Fran{\c{c}}a, A C{\'{e}}sar C and Monteiro, Cleviton V F and Maciel, Felipe Farias},
	Journal = {Information and Software Technology},
	Year = {2011},
	Number = {9},
	Pages = {899--913},
	Volume = {53},
	Abstract = {Context Since the introduction of evidence-based software engineering in 2004, systematic literature review (SLR) has been increasingly used as a method for conducting secondary studies in software engineering. Two tertiary studies, published in 2009 and 2010, identified and analysed 54 {\{}SLRs{\}} published in journals and conferences in the period between 1st January 2004 and 30th June 2008. Objective In this article, our goal was to extend and update the two previous tertiary studies to cover the period between 1st July 2008 and 31st December 2009. We analysed the quality, coverage of software engineering topics, and potential impact of published {\{}SLRs{\}} for education and practice. Method We performed automatic and manual searches for {\{}SLRs{\}} published in journals and conference proceedings, analysed the relevant studies, and compared and integrated our findings with the two previous tertiary studies. Results We found 67 new {\{}SLRs{\}} addressing 24 software engineering topics. Among these studies, 15 were considered relevant to the undergraduate educational curriculum, and 40 appeared of possible interest to practitioners. We found that the number of {\{}SLRs{\}} in software engineering is increasing, the overall quality of the studies is improving, and the number of researchers and research organisations worldwide that are conducting {\{}SLRs{\}} is also increasing and spreading. Conclusion Our findings suggest that the software engineering research community is starting to adopt {\{}SLRs{\}} consistently as a research method. However, the majority of the {\{}SLRs{\}} did not evaluate the quality of primary studies and fail to provide guidelines for practitioners, thus decreasing their potential impact on software engineering practice. },
	Annote = {Studying work practices in Global Software Engineering},
	Doi = {https://doi.org/10.1016/j.infsof.2011.04.004},
	ISSN = {0950-5849},
	Keywords = {Mapping studies,Software engineering,Systematic reviews,Tertiary studies},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584911001017}
}

@Article{SPE:SPE1078,
	Title = {{Agile software product lines: a systematic mapping study}},
	Author = {da Silva, Ivonei Freitas and {da Mota Silveira Neto}, Paulo Anselmo and O'Leary, P{\'{a}}draig and de Almeida, Eduardo Santana and {de Lemos Meira}, Silvio Romero},
	Journal = {Software: Practice and Experience},
	Year = {2011},
	Number = {8},
	Pages = {899--920},
	Volume = {41},
	Doi = {10.1002/spe.1078},
	ISSN = {1097-024X},
	Keywords = {Agile methods,Agile principles,software product lines,systematic mapping study},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/spe.1078}
}

@Article{DaSilva2014,
	Title = {{Software product line scoping and requirements engineering in a small and medium-sized enterprise: An industrial case study}},
	Author = {da Silva, Ivonei Freitas and {da Mota Silveira Neto}, Paulo Anselmo and O'Leary, P{\'{a}}draig and de Almeida, Eduardo Santana and Meira, Silvio Romero De Lemos},
	Journal = {Journal of Systems and Software},
	Year = {2014},
	Month = {feb},
	Pages = {189--206},
	Volume = {88},
	Doi = {10.1016/j.jss.2013.10.040},
	File = {:Users/mac/Downloads/1-s2.0-S0164121213002598-main-2.pdf:pdf},
	ISSN = {01641212},
	Keywords = {requirements engineering},
	Publisher = {Elsevier Inc.},
	Url = {http://linkinghub.elsevier.com/retrieve/pii/S0164121213002598}
}

@Article{Silva201719,
	Title = {{A systematic review on search based mutation testing}},
	Author = {Silva, Rodolfo Adamshuk and {do Rocio Senger de Souza}, Simone and de Souza, Paulo S{\'{e}}rgio Lopes},
	Journal = {Information and Software Technology},
	Year = {2017},
	Pages = {19--35},
	Volume = {81},
	Abstract = {AbstractContext Search Based Software Testing refers to the use of meta-heuristics for the optimization of a task in the context of software testing. Meta-heuristics can solve complex problems in which an optimum solution must be found among a large amount of possibilities. The use of meta-heuristics in testing activities is promising because of the high number of inputs that should be tested. Previous studies on search based software testing have focused on the application of meta-heuristics for the optimization of structural and functional criteria. Recently, some researchers have proposed the use of {\{}SBST{\}} for mutation testing and explored solutions for the cost of application of this testing criterion. Objective The objective is to identify how {\{}SBST{\}} has been explored in the context of mutation testing, how fitness functions are defined and the challenges and research opportunities in the application of meta-heuristic search techniques. Method A systematic review involving 263 papers published between 1996 and 2014 examined the studies on the use of meta-heuristic search techniques for the optimization of mutation testing. Results The results show meta-heuristic search techniques have been applied for the optimization of test data generation, mutant generation and selection of effective mutation operators. Five meta-heuristic techniques, namely Genetic Algorithm, Ant Colony, Bacteriological Algorithm, Hill Climbing and Simulated Annealing have been used in search based mutation testing. The review addressed different fitness functions used to guide the search. Conclusion Search based mutation testing is a field of interest, however, some issues remain unexplored. For instance, the use of meta-heuristics for the selection of effective mutation operators was identified in only one study. The results have pointed a range of possibilities for new studies to be developed, i.e., identification of equivalent mutants, experimental studies and application to different domains, such as concurrent programs. },
	Doi = {https://doi.org/10.1016/j.infsof.2016.01.017},
	ISSN = {0950-5849},
	Keywords = {Meta-heuristic,Mutation testing,Search based software testing},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584916300167}
}

@Article{Simidchieva2007109,
	Title = {{Representing process variation with a process family}},
	Author = {Simidchieva, B I and Clarke, L A and Osterweil, L J},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2007},
	Pages = {109--120},
	Volume = {4470 LNCS},
	Abstract = {The formalization of process definitions has been an invaluable aid in many domains. However, noticeable variations in processes start to emerge as precise details are added to process definitions. While each such variation gives rise to a different process, these processes might more usefully be considered as variants of each other, rather than completely different processes. This paper proposes that it is beneficial to regard such an appropriately close set of process variants as a process family. The paper suggests a characterization of what might comprise a process family and introduces a formal approach to defining families based upon this characterization. To illustrate this approach, we describe a case study that demonstrates the different variations we observed in processes that define how dispute resolution is performed at the U.S. National Mediation Board. We demonstrate how our approach supports the definition of this set of process variants as a process family. {\textcopyright} Springer-Verlag Berlin Heidelberg 2007.},
	Annote = {cited By 21},
	Keywords = {Formal methods; Set theory,Process engineering,Process family; Process instance generation; Proce},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-37149016114{\&}partnerID=40{\&}md5=819724924b0a96138361ad0c3279ccb0}
}

@Article{Sinnema2008584,
	Title = {{Industrial validation of {\{}COVAMOF{\}}}},
	Author = {Sinnema, Marco and Deelstra, Sybren},
	Journal = {Journal of Systems and Software},
	Year = {2008},
	Number = {4},
	Pages = {584--600},
	Volume = {81},
	Abstract = {{\{}COVAMOF{\}} is a variability management framework for product families that was developed to reduce the number of iterations required during product derivation and to reduce the dependency on experts. In this paper, we present the results of an experiment with {\{}COVAMOF{\}} in industry. The results show that with COVAMOF, engineers that are not involved in the product family were now capable of deriving the products in 100{\%} of the cases, compared to 29{\%} of the cases without COVAMOF. For experts, the use of {\{}COVAMOF{\}} reduced the number of iterations by 42{\%}, and the total derivation time by 38{\%}. },
	Annote = {Selected papers from the 10th Conference on Software Maintenance and Reengineering (CSMR 2006)},
	Doi = {https://doi.org/10.1016/j.jss.2007.06.002},
	ISSN = {0164-1212},
	Keywords = {Industrial validation,Product family engineering,Software Variability Management},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121207001422}
}

@InProceedings{Smith:2001:MCS:381473.381613,
	Title = {{Mining Components for a Software Architecture and a Product Line: The Options Analysis for Reengineering (OAR) Method}},
	Author = {Smith, Dennis and O'Brien, Liam and Bergey, John},
	Booktitle = {Proceedings of the 23rd International Conference on Software Engineering},
	Year = {2001},
	Address = {Washington, DC, USA},
	Pages = {728----},
	Publisher = {IEEE Computer Society},
	Series = {ICSE '01},
	ISBN = {0-7695-1050-7},
	Keywords = {reengineering,reverse engineering,software architectures,software product lines},
	Url = {http://0-dl.acm.org.fama.us.es/citation.cfm?id=381473.381613}
}

@Article{Smith20091155,
	Title = {{A document driven methodology for developing a high quality Parallel Mesh Generation Toolbox}},
	Author = {Smith, S and Yu, W},
	Journal = {Advances in Engineering Software},
	Year = {2009},
	Number = {11},
	Pages = {1155--1167},
	Volume = {40},
	Abstract = {This paper motivates the value of using a document driven methodology to improve the quality of scientific computing applications by illustrating the design and documentation of a Parallel Mesh Generation Toolbox (PMGT). Formal mathematical specification is promoted for writing unambiguous requirements, which can be used to judge the correctness and thus the reliability of PMGT. Mathematics is also shown to improve understandability, reusability and maintainability through modelling software modules as finite state machines. The proposed methodology includes explicit traceability between requirements, design, implementation and test cases. Traceability improves the verification of completeness and consistency and it allows for proper change management. To improve the reliability of PMGT, given the challenge that the correct solution is unknown a priori, an automated testing approach is adopted to verify the known properties of a correct solution, such as conformality and counterclockwise vertex numbering. },
	Doi = {https://doi.org/10.1016/j.advengsoft.2009.05.003},
	ISSN = {0965-9978},
	Keywords = {Mesh generation,Scientific computing,Software engineering,Software quality},
	Url = {http://www.sciencedirect.com/science/article/pii/S0965997809001306}
}

@Article{Snook2008112,
	Title = {{Rigorous engineering of product-line requirements: A case study in failure management}},
	Author = {Snook, Colin and Poppleton, Michael and Johnson, Ian},
	Journal = {Information and Software Technology},
	Year = {2008},
	Number = {1–2},
	Pages = {112--129},
	Volume = {50},
	Abstract = {We consider the failure detection and management function for engine control systems as an application domain where product line engineering is indicated. The need to develop a generic requirement set – for subsequent system instantiation – is complicated by the addition of the high levels of verification demanded by this safety-critical domain, subject to avionics industry standards. We present our case study experience in this area as a candidate method for the engineering, validation and verification of generic requirements using domain engineering and Formal Methods techniques and tools. For a defined class of systems, the case study produces a generic requirement set in {\{}UML{\}} and an example system instance. Domain analysis and engineering produce a validated model which is integrated with the formal specification/verification method B by the use of our UML-B profile. The formal verification both of the generic requirement set, and of a simple system instance, is demonstrated using our U2B, ProB and prototype Requirements Manager tools. This work is a demonstrator for a tool-supported method which will be an output of {\{}EU{\}} project {\{}RODIN{\}} (This work is conducted in the setting of the {\{}EU{\}} funded Research Project: {\{}IST{\}} 511599 {\{}RODIN{\}} (Rigorous Open Development Environment for Complex Systems) http://rodin.cs.ncl.ac.uk/). The use of existing and prototype formal verification and support tools is discussed. The method, developed in application to this novel combination of product line, failure management and safety-critical engineering, is evaluated and considered to be applicable to a wide range of domains. },
	Annote = {Special issue with two special sections. Section 1: Most-cited software engineering articles in 2001. Section 2: Requirement engineering: Foundation for software quality},
	Doi = {https://doi.org/10.1016/j.infsof.2007.10.010},
	ISSN = {0950-5849},
	Keywords = {Formal specification,Generic requirements,Product line,Refinement,Tools,UML-B,Verification},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584907001176}
}

@Article{Soares20131006,
	Title = {{Comparing approaches to analyze refactoring activity on software repositories}},
	Author = {Soares, Gustavo and Gheyi, Rohit and Murphy-Hill, Emerson and Johnson, Brittany},
	Journal = {Journal of Systems and Software},
	Year = {2013},
	Number = {4},
	Pages = {1006--1022},
	Volume = {86},
	Abstract = {Some approaches have been used to investigate evidence on how developers refactor their code, whether refactorings activities may decrease the number of bugs, or improve developers' productivity. However, there are some contradicting evidence in previous studies. For instance, some investigations found evidence that if the number of refactoring changes increases in the preceding time period the number of defects decreases, different from other studies. They have used different approaches to evaluate refactoring activities. Some of them identify committed behavior-preserving transformations in software repositories by using manual analysis, commit messages, or dynamic analysis. Others focus on identifying which refactorings are applied between two programs by using manual inspection or static analysis. In this work, we compare three different approaches based on manual analysis, commit message (Ratzinger's approach) and dynamic analysis (SafeRefactor's approach) to detect whether a pair of versions determines a refactoring, in terms of behavioral preservation. Additionally, we compare two approaches (manual analysis and Ref-Finder) to identify which refactorings are performed in each pair of versions. We perform both comparisons by evaluating their accuracy, precision, and recall in a randomly selected sample of 40 pairs of versions of JHotDraw, and 20 pairs of versions of Apache Common Collections. While the manual analysis presents the best results in both comparisons, it is not as scalable as the automated approaches. Ratzinger's approach is simple and fast, but presents a low recall; differently, SafeRefactor is able to detect most applied refactorings, although limitations in its test generation backend results for some kinds of subjects in low precision values. Ref-Finder presented a low precision and recall in our evaluation. },
	Annote = {{\{}SI{\}} : Software Engineering in Brazil: Retrospective and Prospective Views},
	Doi = {https://doi.org/10.1016/j.jss.2012.10.040},
	ISSN = {0164-1212},
	Keywords = {Automated analysis,Manual analysis,Refactoring,Repository},
	Url = {http://www.sciencedirect.com/science/article/pii/S016412121200297X}
}

@Article{Sobernig20161670,
	Title = {{Quantifying structural attributes of system decompositions in 28 feature-oriented software product lines: An exploratory study}},
	Author = {Sobernig, S and Apel, S and Kolesnikov, S and Siegmund, N},
	Journal = {Empirical Software Engineering},
	Year = {2016},
	Number = {4},
	Pages = {1670--1705},
	Volume = {21},
	Abstract = {A key idea of feature orientation is to decompose a software product line along the features it provides. Feature decomposition is orthogonal to object-oriented decomposition—it crosscuts the underlying package and class structure. It has been argued often that feature decomposition improves system structure by reducing coupling and by increasing cohesion. However, recent empirical findings suggest that this is not necessarily the case. In this exploratory, observational study, we investigate the decompositions of 28 feature-oriented software product lines into classes, features, and feature-specific class fragments. The product lines under investigation are implemented using the feature-oriented programming language Fuji. In particular, we quantify and compare the internal attributes import coupling and cohesion of the different product-line decompositions in a systematic, reproducible manner. For this purpose, we adopt three established software measures (e.g., coupling between units, CBU; internal-ratio unit dependency, IUD) as well as standard concentration statistics (e.g., Gini coefficient). In our study, we found that feature decomposition can be associated with higher levels of structural coupling in a product line than a decomposition into classes. Although coupling can be concentrated in very few features in most feature decompositions, there are not necessarily hot-spot features in all product lines. Interestingly, feature cohesion is not necessarily higher than class cohesion, whereas features are more equal in serving dependencies internally than classes of a product line. Our empirical study raises critical questions about alleged advantages of feature decomposition. At the same time, we demonstrate how our measurement approach of coupling and cohesion has potential to support static and dynamic analyses of software product lines (i.e., type checking and feature-interaction detection) by facilitating product sampling. {\textcopyright} 2014, Springer Science+Business Media New York.},
	Annote = {cited By 0},
	Doi = {10.1007/s10664-014-9336-6},
	Keywords = {Adhesion; Computer programming languages; Computer,Feature-oriented programming; Fuji; Software Meas,Object oriented programming},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908355432{\&}doi=10.1007{\%}2Fs10664-014-9336-6{\&}partnerID=40{\&}md5=cc1eb006ed351a5be2ec4619fb3e256f}
}

@Article{Sobernig2016140,
	Title = {{Extracting reusable design decisions for UML-based domain-specific languages: A multi-method study}},
	Author = {Sobernig, Stefan and Hoisl, Bernhard and Strembeck, Mark},
	Journal = {Journal of Systems and Software},
	Year = {2016},
	Pages = {140--172},
	Volume = {113},
	Abstract = {Abstract When developing domain-specific modeling languages (DSMLs), software engineers have to make a number of important design decisions on the {\{}DSML{\}} itself, or on the software-development process that is applied to develop the DSML. Thus, making well-informed design decisions is a critical factor in developing DSMLs. To support this decision-making process, the model-driven development community has started to collect established design practices in terms of patterns, guidelines, story-telling, and procedural models. However, most of these documentation practices do not capture the details necessary to reuse the rationale behind these decisions in other {\{}DSML{\}} projects. In this paper, we report on a three-year research effort to compile and to empirically validate a catalog of structured decision descriptions (decision records) for UML-based DSMLs. This catalog is based on design decisions extracted from 90 {\{}DSML{\}} projects. These projects were identified—among others—via an extensive systematic literature review (SLR) for the years 2005–2012. Based on more than 8,000 candidate publications, we finally selected 84 publications for extracting design-decision data. The extracted data were evaluated quantitatively using a frequent-item-set analysis to obtain characteristic combinations of design decisions and qualitatively to document recurring documentation issues for UML-based DSMLs. We revised the collected decision records based on this evidence and made the decision-record catalog for developing UML-based {\{}DSMLs{\}} publicly available. Furthermore, our study offers insights into {\{}UML{\}} usage (e.g. diagram types) and into the adoption of {\{}UML{\}} extension techniques (e.g. metamodel extensions, profiles). },
	Doi = {https://doi.org/10.1016/j.jss.2015.11.037},
	ISSN = {0164-1212},
	Keywords = {Design decision,Design rationale,Domain-specific language,Domain-specific modeling,Model-driven development,Unified modeling language},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121215002617}
}

@Article{SPIP:SPIP214,
	Title = {{Managing variability of self-customizable systems through composable components}},
	Author = {Şora, Ioana and Creţu, Vladimir and Verbaeten, Pierre and Berbers, Yolande},
	Journal = {Software Process: Improvement and Practice},
	Year = {2005},
	Number = {1},
	Pages = {77--95},
	Volume = {10},
	Abstract = {Self-customizable systems must adapt themselves to evolving user requirements or to their changing environment. One way to address this problem is through automatic component composition, systematically (re-)building systems according to the current requirements by composing reusable components. Our work addresses requirements-driven composition of multi-flow architectures.This article presents the central element of our automated runtime customization approach, the concept of composable components: the internal configuration of a composable component is not fixed, but is variable in the limits of its structural constraints. In this article, we introduce the mechanism of structural constraints as a way of managing the variability of customizable systems. Composition is performed in a top–down stepwise refinement manner, while recursively composing the internal structures of the composable components according to external requirements over the invariant structural constraints.The final section of the article presents our cases of practical validation. Copyright {\textcopyright} 2005 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spip.214},
	ISSN = {1099-1670},
	Keywords = {requirements-driven automated runtime composition},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/spip.214}
}

@Article{Soumeya20141345,
	Title = {{Study of advanced separation of concerns approaches using the GoF design patterns: A quantitative and qualitative comparison}},
	Author = {Soumeya, Debboub and Djamel, Meslati},
	Journal = {Information and Software Technology},
	Year = {2014},
	Number = {10},
	Pages = {1345--1359},
	Volume = {56},
	Abstract = {AbstractContext Since the emergence of the aspect oriented paradigm, several studies have been conducted to test the contribution of this new paradigm compared to the object paradigm. However, in addition to this type of studies, we need also comparative studies that assess the aspect approaches mutually. The motivations of the latter include the enhancement of each aspect approach, devising hybrid approaches or merely helping developers choosing the suitable approach according to their needs. Comparing advanced separation of concerns approaches is the context of our work. Objective We aim at making an assessment of how the aspect approaches deal with crosscutting concerns. This assessment is based on quantitative attributes such as coupling and cohesion that evaluate the modularity as well as on qualitative observations. Method We selected three of well-known aspect approaches: AspectJ, {\{}JBoss{\}} {\{}AOP{\}} and CaesarJ, all the three based on Java. We conducted then, a comparative study using the GoF design patterns. In order to be fair we asked a group of Master students to achieve the implementation of all patterns with the three approaches. The use of these implementations as hypothetical benchmarks allowed us to achieve two kinds of comparison: a quantitative one based on structural and performance metrics, and qualitative one based on observations collected during the implementation phase. Results The quantitative comparison shows some advantages like the using of fewer components with AspectJ and the strong cohesion with CaesarJ and weaknesses, as the high internal coupling caused by the inner classes of CaesarJ. The qualitative comparison gives comments about the approach understandability and others qualitative concepts. Conclusion This comparison highlighted strengths and weaknesses of each approach, and provided a referential work that can help choosing the right approach during software development, enhancing aspect approaches or devising hybrid approaches that combine best features. },
	Doi = {https://doi.org/10.1016/j.infsof.2014.04.015},
	ISSN = {0950-5849},
	Keywords = {Advanced separation of concerns,Aspect oriented programming,AspectJ,CaesarJ,Empirical assessment,JBoss AOP},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584914000962}
}

@Article{SousaFerreira201465,
	Title = {{On the use of feature-oriented programming for evolving software product lines - A comparative study}},
	Author = {{Sousa Ferreira}, G C and Gaia, F N and Figueiredo, E and {De Almeida Maia}, M},
	Journal = {Science of Computer Programming},
	Year = {2014},
	Number = {PART A},
	Pages = {65--85},
	Volume = {93},
	Abstract = {Feature-oriented programming (FOP) is a programming technique based on composition mechanisms, called refinements. It is often assumed that feature-oriented programming is more suitable than other variability mechanisms for implementing Software Product Lines (SPLs). However, there is no empirical evidence to support this claim. In fact, recent research work found out that some composition mechanisms might degenerate the SPL modularity and stability. However, there is no study investigating these properties focusing on the FOP composition mechanisms. This paper presents quantitative and qualitative analysis of how feature modularity and change propagation behave in the context of two evolving SPLs, namely WebStore and MobileMedia. Quantitative data have been collected from the SPLs developed in three different variability mechanisms: FOP refinements, conditional compilation, and object-oriented design patterns. Our results suggest that FOP requires few changes in source code and a balanced number of added modules, providing better support than other techniques for non-intrusive insertions. Therefore, it adheres closer to the Open-Closed principle. Additionally, FOP seems to be more effective tackling modularity degeneration, by avoiding feature tangling and scattering in source code, than conditional compilation and design patterns. These results are based not only on the variability mechanism itself, but also on careful SPL design. However, the aforementioned results are weaker when the design needs to cope with crosscutting and fine-grained features. {\textcopyright} 2013 Elsevier B.V. All rights reserved.},
	Annote = {cited By 12},
	Doi = {10.1016/j.scico.2013.10.010},
	Keywords = {Computer programming; Software engineering,Computer software,Conditional compilations; Design Patterns; Featur},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905459866{\&}doi=10.1016{\%}2Fj.scico.2013.10.010{\&}partnerID=40{\&}md5=b909f275cd0c20a8c0c799592c8c29de}
}

@Article{deSouza2015378,
	Title = {{Knowledge management initiatives in software testing: A mapping study}},
	Author = {de Souza, {\'{E}}rica Ferreira and {de Almeida Falbo}, Ricardo and Vijaykumar, Nandamudi L},
	Journal = {Information and Software Technology},
	Year = {2015},
	Pages = {378--391},
	Volume = {57},
	Abstract = {AbstractContext Software testing is a knowledge intensive process, and, thus, Knowledge Management (KM) principles and techniques should be applied to manage software testing knowledge. Objective This study conducts a survey on existing research on {\{}KM{\}} initiatives in software testing, in order to identify the state of the art in the area as well as the future research. Aspects such as purposes, types of knowledge, technologies and research type are investigated. Method The mapping study was performed by searching seven electronic databases. We considered studies published until December 2013. The initial resulting set was comprised of 562 studies. From this set, a total of 13 studies were selected. For these 13, we performed snowballing and direct search to publications of researchers and research groups that accomplished these studies. Results From the mapping study, we identified 15 studies addressing {\{}KM{\}} initiatives in software testing that have been reviewed in order to extract relevant information on a set of research questions. Conclusions Although only a few studies were found that addressed {\{}KM{\}} initiatives in software testing, the mapping shows an increasing interest in the topic in the recent years. Reuse of test cases is the perspective that has received more attention. From the {\{}KM{\}} point of view, most of the studies discuss aspects related to providing automated support for managing testing knowledge by means of a {\{}KM{\}} system. Moreover, as a main conclusion, the results show that {\{}KM{\}} is pointed out as an important strategy for increasing test effectiveness, as well as for improving the selection and application of suited techniques, methods and test cases. On the other hand, inadequacy of existing {\{}KM{\}} systems appears as the most cited problem related to applying {\{}KM{\}} in software testing. },
	Doi = {https://doi.org/10.1016/j.infsof.2014.05.016},
	ISSN = {0950-5849},
	Keywords = {Knowledge management,Mapping study,Software testing},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584914001335}
}

@Article{Souza20131172,
	Title = {{Evidence of software inspection on feature specification for software product lines}},
	Author = {Souza, I S and {Da Silva Gomes}, G S and {Da Mota Silveira Neto}, P A and {Do Carmo Machado}, I and {De Almeida}, E S and {De Lemos Meira}, S R},
	Journal = {Journal of Systems and Software},
	Year = {2013},
	Number = {5},
	Pages = {1172--1190},
	Volume = {86},
	Abstract = {In software product lines (SPL), scoping is a phase responsible for capturing, specifying and modeling features, and also their constraints, interactions and variations. The feature specification task, performed in this phase, is usually based on natural language, which may lead to lack of clarity, non-conformities and defects. Consequently, scoping analysts may introduce ambiguity, inconsistency, omissions and non-conformities. In this sense, this paper aims at gathering evidence about the effects of applying an inspection approach to feature specification for SPL. Data from a SPL reengineering project were analyzed in this work and the analysis indicated that the correction activity demanded more effort. Also, Pareto's principle showed that incompleteness and ambiguity reported higher non-conformity occurrences. Finally, the Poisson regression analysis showed that sub-domain risk information can be a good indicator for prioritization of sub-domains in the inspection activity.{\textcopyright} 2012 Elsevier Inc. All rights reserved.},
	Annote = {cited By 3},
	Doi = {10.1016/j.jss.2012.11.044},
	Keywords = {Computer software selection and evaluation; Inspe,Empirical studies; Inspection activities; Poisson,Specifications},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875245673{\&}doi=10.1016{\%}2Fj.jss.2012.11.044{\&}partnerID=40{\&}md5=625c750c31f8fc3e8ca34cbc8fdb2dea}
}

@Article{vanderSpek20111261,
	Title = {{Applying a dynamic threshold to improve cluster detection of {\{}LSI{\}}}},
	Author = {van der Spek, Pieter and Klusener, Steven},
	Journal = {Science of Computer Programming},
	Year = {2011},
	Number = {12},
	Pages = {1261--1274},
	Volume = {76},
	Abstract = {Latent Semantic Indexing (LSI) is a standard approach for extracting and representing the meaning of words in a large set of documents. Recently it has been shown that it is also useful for identifying concerns in source code. The tree cutting strategy plays an important role in obtaining the clusters, which identify the concerns. In this contribution the authors compare two tree cutting strategies: the Dynamic Hybrid cut and the commonly used fixed height threshold. Two case studies have been performed on the source code of Philips Healthcare to compare the results using both approaches. While some of the settings are particular to the Philips-case, the results show that applying a dynamic threshold, implemented by the Dynamic Hybrid cut, is an improvement over the fixed height threshold in the detection of clusters representing relevant concerns. This makes the approach as a whole more usable in practice. },
	Annote = {Special Issue on Software Evolution, Adaptability and Variability},
	Doi = {https://doi.org/10.1016/j.scico.2010.12.004},
	ISSN = {0167-6423},
	Keywords = {Clustering,Feature extraction,Latent Semantic Indexing,Reverse engineering,Software architecture},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642310002297}
}

@Conference{Sree-Kumar201646,
	Title = {{Analysis of feature models using alloy: A survey}},
	Author = {Sree-Kumar, A and Planas, E and Claris{\'{o}}, R},
	Booktitle = {Electronic Proceedings in Theoretical Computer Science, EPTCS},
	Year = {2016},
	Pages = {46--60},
	Volume = {206},
	Abstract = {Feature Models (FMs) are a mechanism to model variability among a family of closely related software products, i.e. a software product line (SPL). Analysis of FMs using formal methods can reveal defects in the specification such as inconsistencies that cause the product line to have no valid products. A popular framework used in research for FM analysis is Alloy, a light-weight formal modeling notation equipped with an efficient model finder. Several works in the literature have proposed different strategies to encode and analyze FMs using Alloy. However, there is little discussion on the relative merits of each proposal, making it difficult to select the most suitable encoding for a specific analysis need. In this paper, we describe and compare those strategies according to various criteria such as the expressivity of the FM notation or the efficiency of the analysis. This survey is the first comparative study of research targeted towards using Alloy for FM analysis. This review aims to identify all the best practices on the use of Alloy, as a part of a framework for the automated extraction and analysis of rich FMs from natural language requirement specifications. {\textcopyright} 2016, Open Publishing Association. All rights reserved.},
	Annote = {cited By 0},
	Doi = {10.4204/EPTCS.206.5},
	Keywords = {Alloying; Computer software; Encoding (symbols); F,Automated extraction; Comparative studies; Featur,Software design},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991721727{\&}doi=10.4204{\%}2FEPTCS.206.5{\&}partnerID=40{\&}md5=c3c203c894234ccd60b1596137a58f3d}
}

@Article{Srikanth2016122,
	Title = {{Test case prioritization of build acceptance tests for an enterprise cloud application: An industrial case study}},
	Author = {Srikanth, Hema and Cashman, Mikaela and Cohen, Myra B},
	Journal = {Journal of Systems and Software},
	Year = {2016},
	Pages = {122--135},
	Volume = {119},
	Abstract = {Abstract The use of cloud computing brings many new opportunities for companies to deliver software in a highly-customizable and dynamic way. One such paradigm, Software as a Service (SaaS), allows users to subscribe and unsubscribe to services as needed. While beneficial to both subscribers and SaaS service providers, failures escaping to the field in these systems can potentially impact an entire customer base. Build Acceptance Testing (BAT) is a black box technique performed to validate the quality of a SaaS system every time a build is generated. In BAT, the same set of test cases is executed simultaneously across many different servers, making this a time consuming test process. Since {\{}BAT{\}} contains the most critical use cases, it may not be obvious which tests to perform first, given that the time to complete all test cases across different servers in any given day may be insufficient. While all tests must be eventually run, it is critical to run those tests first which are likely to find failures. In this work, we ask if it is possible to prioritize {\{}BAT{\}} tests for improved time to fault detection and present several different approaches, each based on the services executed when running each BAT. In an empirical study on a production enterprise system, we first analyze the historical data from several months in the field, and then use that data to derive the prioritization order for the current development BATs. We then examine if the orders change significantly when we consider fault severity using a cost-based prioritization metric. We find that the prioritization order in which we run the tests does matter, and that the use of historical information is a good heuristic for this order. Prioritized tests have an increase in the rate of fault detection, with the average percent of faults detected (APFD) increasing from less than 0.30 to as high as 0.77 on a scale of zero to one. Although severity slightly changes which order performs best, we see that there are clusters of orderings, ones which improve time to early fault detection ones which don't. },
	Doi = {https://doi.org/10.1016/j.jss.2016.06.017},
	ISSN = {0164-1212},
	Keywords = {Cloud computing,Prioritization,Regression testing,Software as a service},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121216300851}
}

@Article{Stallinger201321,
	Title = {{Enhancing ISO/IEC 15288 with reuse and product management: An add-on process reference model}},
	Author = {Stallinger, Fritz and Neumann, Robert},
	Journal = {Computer Standards {\&} Interfaces},
	Year = {2013},
	Number = {1},
	Pages = {21--32},
	Volume = {36},
	Abstract = {Abstract To support the transformation of system engineering from the project-based development of highly customer-specific solutions to the reuse and customization of ‘system products', we integrate a process reference model for reuse- and product-oriented industrial engineering and a process reference model extending ISO/IEC 12207 on software life cycle processes with software- and system-level product management. We synthesize the key process elements of both models to enhance ISO/IEC 15288 on system life cycle processes with product- and reuse-oriented engineering and product management practices as an integrated framework for process assessment and improvement in contexts where systems are developed and evolved as products. },
	Doi = {https://doi.org/10.1016/j.csi.2013.07.006},
	ISSN = {0920-5489},
	Keywords = {ISO/IEC 15288,Process reference model,Product management,Reuse,Systems engineering},
	Url = {http://www.sciencedirect.com/science/article/pii/S0920548913000718}
}

@Article{Stallinger201293,
	Title = {{Extending ISO/IEC 12207 with software product management: A process reference model proposal}},
	Author = {Stallinger, F and Neumann, R},
	Journal = {Communications in Computer and Information Science},
	Year = {2012},
	Pages = {93--106},
	Volume = {290 CCIS},
	Abstract = {Software product management is generally expected to link and integrate business and product related goals with core software engineering and software life cycle activities. Empirical research demonstrates the positive effect of mature software product management practices on key software development performance indicators. Nevertheless, the various frameworks available for software product management have distinct and diverse focus points, are often linked or incorporated with specific development paradigms, or lack integration with or addressing of core software engineering activities. On the other hand, traditional software process improvement approaches generally lack the provision of explicit or detailed software product management activities. - In this paper we build on the results of preceding research on identifying a lack of software product management practices within ISO/IEC 12207 and on deriving key outcomes of software product management activities from selected software product management frameworks. Based on these results we propose a process reference model for software product management that can be integrated with the process reference model as defined in ISO/IEC 12207 for software life cycle processes. {\textcopyright} 2012 Springer-Verlag.},
	Annote = {cited By 7},
	Doi = {10.1007/978-3-642-30439-2_9},
	Keywords = {Benchmarking; Life cycle; Models; Software design,ISO/IEC; ISO/IEC 15504; Process assessments; Proce,Management},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862188967{\&}doi=10.1007{\%}2F978-3-642-30439-2{\_}9{\&}partnerID=40{\&}md5=bc10ccce40d79f40f8b241c020b2da7c}
}

@Article{SMR:SMR489,
	Title = {{Integrating ISO/IEC 15504 conformant process assessment and organizational reuse enhancement}},
	Author = {Stallinger, Fritz and Pl{\"{o}}sch, Reinhold and Pomberger, Gustav and Vollmar, Jan},
	Journal = {Journal of Software Maintenance and Evolution: Research and Practice},
	Year = {2010},
	Number = {4},
	Pages = {307--324},
	Volume = {22},
	Abstract = {Improving reuse in industrial engineering is more and more recognized as a key to economic success by providers of industrial solutions. It generally increases the quality of the engineered systems, shortens engineering time, and decreases engineering costs for the development of customer-specific industrial solutions. We therefore developed an integrated set of methods for assessing an organization's reuse practices, identifying its reuse potential and guiding the selection and implementation of improvement actions.While the assessment of reuse practices is performed according to the principles of ISO/IEC 15504 for process assessment, identification of improvements for reuse in industrial engineering is much more complex compared to ‘traditional' process capability-oriented improvement. Orthogonal to improving along the process capability dimension it also involves strategic decisions on the overall design of the engineering process, the pursued engineering and reuse paradigms, the desired organizational reuse maturity stages, etc.To tackle this problem, we developed a potentials analysis method that assures compliance of the improvements with the strategic goals of the organization. It bridges the gap between the process assessment and the improvement action planning parts of the methodology by supporting the selection of reuse practices considering strategic issues and business goals. Copyright {\textcopyright} 2009 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spip.443},
	ISSN = {1532-0618},
	Keywords = {ISO/IEC 15504,maturity,potentials analysis,reuse,reuse assessment},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/spip.443}
}

@Article{SMR:SMR470,
	Title = {{Developing measurement systems: an industrial case study}},
	Author = {Staron, Miroslaw and Meding, Wilhelm and Karlsson, G{\"{o}}ran and Nilsson, Christer},
	Journal = {Journal of Software Maintenance and Evolution: Research and Practice},
	Year = {2011},
	Number = {2},
	Pages = {89--107},
	Volume = {23},
	Abstract = {The process of measuring in software engineering has already been standardized in the ISO/IEC 15939 standard, where activities related to identifying, creating, and evaluating of measures are described. In the process of measuring software entities, however, an organization usually needs to create custom measurement systems, which are intended to collect, analyze, and present data for a specific purpose. In this paper, we present a proven industrial process for developing measurement systems including the artifacts and deliverables important for a successful deployment of measurement systems in industry. The process has been elicited during a case study at Ericsson and is used in the organization for over 3 years when the paper was written. The process is supported by a framework that simplifies the implementation of the measurement systems and shortens the time from the initial idea to a working measurement system by the factor of 5 compared with using a standard development process not tailored for measurement systems. Copyright {\textcopyright} 2010 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/smr.470},
	ISSN = {1532-0618},
	Keywords = {measurement,software engineering,software metrics},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/smr.470}
}

@Article{SPE:SPE699,
	Title = {{Model-centric software architecture reconstruction}},
	Author = {Stoermer, Christoph and Rowe, Anthony and O'Brien, Liam and Verhoef, Chris},
	Journal = {Software: Practice and Experience},
	Year = {2006},
	Number = {4},
	Pages = {333--363},
	Volume = {36},
	Abstract = {Much progress has been achieved in defining methods, techniques, and tools for software architecture reconstruction (SAR). However, less progress has been achieved in constructing reasoning frameworks from existing systems that support organizations in architecture analysis and design decisions. These reasoning frameworks are necessary, for example, to assemble existing components and deploy them in new system configurations. We propose a model-centric approach where this kind of reasoning is driven by the analysis of quality attribute scenarios. The scenarios and the related quality attribute models guide the SAR effort by focusing on the elicitation of model relevant artifacts. The approach further drives the model construction towards the analytical support of What If scenarios that explore responses stimulated by new requirements, such as new deployments of existing components. The paper provides two real-world case studies. The first case study introduces the model-centric reconstruction approach in the context of a large satellite tracking system. The second case study provides the construction of a time performance model for an existing embedded system in the automotive industry. The model allows us to perform cost-efficient predictions of component assemblies in new customer configurations. Copyright {\textcopyright} 2005 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spe.699},
	ISSN = {1097-024X},
	Keywords = {LIN bus,architecture,models,performance,prediction,quality attributes,software architecture reconstruction,views},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/spe.699}
}

@Article{Stoica201542,
	Title = {{System components of a general theory of software engineering}},
	Author = {Stoica, Anca-Juliana and Pelckmans, Kristiaan and Rowe, William},
	Journal = {Science of Computer Programming},
	Year = {2015},
	Pages = {42--65},
	Volume = {101},
	Abstract = {Abstract The contribution of this paper to a general theory of software engineering is twofold: it presents the model system concept, and it integrates the software engineering design process into a decision making theory and a value-based decision-under-risk process. The model system concept is defined as a collection of interconnected and consistent components that work together for defining, developing, and delivering a software system. This model system concept is used to represent the multiple facets of a software engineering project such as stakeholders and models related to domain/environment, success, decision, product, process, and property. The model system concept is derived from software development practices in the industry and academia. The theoretical decision framework acts as a central governance component for a given software engineering project. Applying this decision framework allows for effectively managing risks and uncertainties related to success in the project building stage. Especially, this puts the design process in an economic perspective, where concepts such as value-of-waiting, value-of-information and possible outcomes can be coped with explicitly. In practice, the decision framework allows for the optimal control of modern adaptive software development. In particular, one can use dynamic programming to find the optimal sequence of decisions to be made considering a defined time horizon. In this way we can relate our contribution to a theory of software engineering to the well-studied areas of automatic control, optimization, decision theory and Bayesian analysis. Computational case studies exemplify the conceptual innovations proposed in this paper. },
	Annote = {Towards general theories of software engineering},
	Doi = {https://doi.org/10.1016/j.scico.2014.11.008},
	ISSN = {0167-6423},
	Keywords = {Adaptive software development,General theory of software engineering,Model systems,Optimal decision-under-risk process,Theoretic Decision Framework},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642314005401}
}

@Article{Stol20111319,
	Title = {{A comparative study of challenges in integrating Open Source Software and Inner Source Software}},
	Author = {Stol, Klaas-Jan and Babar, Muhammad Ali and Avgeriou, Paris and Fitzgerald, Brian},
	Journal = {Information and Software Technology},
	Year = {2011},
	Number = {12},
	Pages = {1319--1336},
	Volume = {53},
	Abstract = {Context Several large software-developing organizations have adopted Open Source Software development (OSSD) practices to develop in-house components that are subsequently integrated into products. This phenomenon is also known as “Inner Source?. While there have been several reports of successful cases of this phenomenon, little is known about the challenges that practitioners face when integrating software that is developed in such a setting. Objective The objective of this study was to shed light on challenges related to building products with components that have been developed within an Inner Source development environment. Method Following an initial systematic literature review to generate seed category data constructs, we performed an in-depth exploratory case study in an organization that has a significant track record in the implementation of Inner Source. Data was gathered through semi-structured interviews with participants from a range of divisions across the organization. Interviews were transcribed and analyzed using qualitative data analysis techniques. Results We have identified a number of challenges and approaches to address them, and compared the findings to challenges related to development with {\{}OSS{\}} products reported in the literature. We found that many challenges identified in the case study could be mapped to challenges related to integration of OSS. Conclusion The results provide important insights into common challenges of developing with {\{}OSS{\}} and Inner Source and may help organizations to understand how to improve their software development practices by adopting certain {\{}OSSD{\}} practices. The findings also identify the areas that need further research. },
	Doi = {https://doi.org/10.1016/j.infsof.2011.06.007},
	ISSN = {0950-5849},
	Keywords = {Case study,Challenges,Empirical studies,Inner Source,Open Source Software,Software development},
	Url = {http://www.sciencedirect.com/science/article/pii/S095058491100142X}
}

@Article{Strasunskas20121239,
	Title = {{Domain model-driven software engineering: A method for discovery of dependency links}},
	Author = {Strasunskas, Darijus and Hakkarainen, Sari E},
	Journal = {Information and Software Technology},
	Year = {2012},
	Number = {11},
	Pages = {1239--1249},
	Volume = {54},
	Abstract = {Context Dependency management often suffers from labor intensity and complexity in creating and maintaining the dependency relations in practice. This is even more critical in a distributed development, in which developers are geographically distributed and a wide variety of tools is used. In those settings, different interpretations of software requirements or usage of different terminologies make it challenging to predict the change impact. Objective is (a) to describe a method facilitating change management in geographically distributed software engineering by effective discovery and establishment of dependency links using domain models; (b) to evaluate the effectiveness of the proposed method. Method A domain model, providing a common reference point, is used to manage development objects and to automatically support dependency discovery. We propose to associate (annotate) development objects with the concepts from the model. These associations are used to compute dependency among development objects, and are stepwise refined to direct dependency links (i.e. enabling product traceability). To evaluate the method, we conducted a laboratory-based randomized experiment on two real cases. Six participants were using an implemented prototype and two comparable tools to perform simulated tasks. Results In the paper we elaborate on the proposed method discussing its functional steps. Results from the experiment show that the method can be effectively used to assist in discovery of dependency links. Users have discovered on average fourteen percent more dependency links than by using the comparable tools. Conclusions The proposed method advocates the use of domain models throughout the whole development life-cycle and is apt to facilitate multi-site software engineering. The experimental study and results suggest that the method is effective in the discovery of dependencies among development objects. },
	Doi = {https://doi.org/10.1016/j.infsof.2012.06.004},
	ISSN = {0950-5849},
	Keywords = {Dependency management,Domain model-based information system design,Information systems development,Randomized experiment,Software engineering},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584912001073}
}

@Article{Strecker:2012:ADC:2211616.2211620,
	Title = {{Accounting for Defect Characteristics in Evaluations of Testing Techniques}},
	Author = {Strecker, Jaymie and Memon, Atif M},
	Journal = {ACM Trans. Softw. Eng. Methodol.},
	Year = {2012},
	Number = {3},
	Pages = {17:1----17:43},
	Volume = {21},
	Address = {New York, NY, USA},
	Doi = {10.1145/2211616.2211620},
	ISSN = {1049-331X},
	Keywords = {Defects,GUI testing,faults},
	Publisher = {ACM},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2211616.2211620}
}

@Article{SPE:SPE936,
	Title = {{An approach for the systematic development of domain-specific languages}},
	Author = {Strembeck, Mark and Zdun, Uwe},
	Journal = {Software: Practice and Experience},
	Year = {2009},
	Number = {15},
	Pages = {1253--1292},
	Volume = {39},
	Abstract = {Building tailored software systems for a particular application domain is a complex task. For this reason, domain-specific languages (DSLs) receive a constantly growing attention in recent years. So far the main focus of DSL research is on case studies and experience reports for the development of individual DSLs, design approaches and implementation techniques for DSLs, and the integration of DSLs with other software development approaches on a technical level. In this paper, we identify and describe the different activities that we conduct when engineering a DSL, and describe how these activities can be combined in order to define a tailored DSL engineering process. Our research results are based on the experiences we gained from multiple different DSL development projects and prototyping experiments. Copyright {\textcopyright} 2009 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spe.936},
	ISSN = {1097-024X},
	Keywords = {applied software engineering,domain-specific languages,language engineering,model-driven software development},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/spe.936}
}

@Article{Strickler20161232,
	Title = {{Deriving products for variability test of Feature Models with a hyper-heuristic approach}},
	Author = {Strickler, Andrei and Lima, Jackson A Prado and Vergilio, Silvia R and Pozo, Aurora T R},
	Journal = {Applied Soft Computing},
	Year = {2016},
	Pages = {1232--1242},
	Volume = {49},
	Abstract = {Abstract Deriving products from a Feature Model (FM) for testing Software Product Lines (SPLs) is a hard task. It is important to select a minimum number of products but, at the same time, to consider the coverage of testing criteria such as pairwise, among other factors. To solve such problems Multi-Objective Evolutionary Algorithms (MOEAs) have been successfully applied. However, to design a solution for this and other software engineering problems can be very difficult, because it is necessary to choose among different search operators and parameters. Hyper-heuristics can help in this task, and have raised interest in the Search-Based Software Engineering (SBSE) field. Considering the growing adoption of {\{}SPL{\}} in the industry and crescent demand for {\{}SPL{\}} testing approaches, this paper introduces a hyper-heuristic approach to automatically derive products to variability testing of SPLs. The approach works with {\{}MOEAs{\}} and two selection methods, random and based on FRR-MAB (Fitness Rate Rank based Multi-Armed Bandit). It was evaluated with real {\{}FMs{\}} and the results show that the proposed approach outperforms the traditional algorithms used in the literature, and that both selection methods present similar performance. },
	Doi = {https://doi.org/10.1016/j.asoc.2016.07.059},
	ISSN = {1568-4946},
	Keywords = {Hyper-heuristic,Software Product Line,Software testing},
	Url = {http://www.sciencedirect.com/science/article/pii/S1568494616303994}
}

@Article{Strode20121222,
	Title = {{Coordination in co-located agile software development projects}},
	Author = {Strode, Diane E and Huff, Sid L and Hope, Beverley and Link, Sebastian},
	Journal = {Journal of Systems and Software},
	Year = {2012},
	Number = {6},
	Pages = {1222--1238},
	Volume = {85},
	Abstract = {Agile software development provides a way to organise the complex task of multi-participant software development while accommodating constant project change. Agile software development is well accepted in the practitioner community but there is little understanding of how such projects achieve effective coordination, which is known to be critical in successful software projects. A theoretical model of coordination in the agile software development context is presented based on empirical data from three cases of co-located agile software development. Many practices in these projects act as coordination mechanisms, which together form a coordination strategy. Coordination strategy in this context has three components: synchronisation, structure, and boundary spanning. Coordination effectiveness has two components: implicit and explicit. The theoretical model of coordination in agile software development projects proposes that an agile coordination strategy increases coordination effectiveness. This model has application for practitioners who want to select appropriate practices from agile methods to ensure they achieve coordination coverage in their project. For the field of information systems development, this theory contributes to knowledge of coordination and coordination effectiveness in the context of agile software development. },
	Annote = {Special Issue: Agile Development},
	Doi = {https://doi.org/10.1016/j.jss.2012.02.017},
	ISSN = {0164-1212},
	Keywords = {Agile methods,Agile software development project,Coordination Theory,Coordination effectiveness,Coordination strategy,Extreme Programming,Scrum},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121212000465}
}

@Article{Sturm20141390,
	Title = {{Evaluating the productivity of a reference-based programming approach: A controlled experiment}},
	Author = {Sturm, Arnon and Kramer, Oded},
	Journal = {Information and Software Technology},
	Year = {2014},
	Number = {10},
	Pages = {1390--1402},
	Volume = {56},
	Abstract = {AbstractContext Domain engineering aims at facilitating software development in an efficient and economical way. One way to measure that is through productivity indicators, which refer to the ability of creating a quality software product in a limited period and with limited resources. Many approaches have been devised to increase productivity; however, these approaches seem to suffer from a tension between expressiveness on the one hand, and applicability (or the lack of it) in providing guidance for developers. Objective This paper evaluates the applicability and efficiency of adopting a domain engineering approach, called Application-based {\{}DOmain{\}} Modeling (ADOM), in the context of the programming task with Java, and thus termed ADOM-Java, for improving productivity in terms of code quality and development time. Method To achieve that objective we have qualitatively evaluate the approach using questionnaires and following a text analysis procedure. We also set a controlled experiment in which 50 undergraduate students performed a Java-based programming task using either ADOM-Java or Java alone. Results The qualitative evaluation reveal that the approach is easy to uses and provides valuable guidance. Nevertheless, it requires training. The outcomes of the experiment indicate that the approach is applicable and that the students that used ADOM-Java achieved better code quality, as well as better functionality and within less time than the students who used only Java. Conclusion The results of the experiments imply that by providing a code base equipped with reuse guidelines for programmers can increase programming productivity in terms of quality and development time. These guidelines may also enforce coding standards and architectural design. },
	Doi = {https://doi.org/10.1016/j.infsof.2014.05.003},
	ISSN = {0950-5849},
	Keywords = {Domain engineering,Productivity,Programming,Software quality,Software reusability},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584914001062}
}

@Article{Succi20031,
	Title = {{Practical assessment of the models for identification of defect-prone classes in object-oriented commercial systems using design metrics}},
	Author = {Succi, Giancarlo and Pedrycz, Witold and Stefanovic, Milorad and Miller, James},
	Journal = {Journal of Systems and Software},
	Year = {2003},
	Number = {1},
	Pages = {1--12},
	Volume = {65},
	Abstract = {The goal of this paper is to investigate and assess the ability of explanatory models based on design metrics to describe and predict defect counts in an object-oriented software system. Specifically, we empirically evaluate the influence of design decisions to defect behavior of the classes in two products from the commercial software domain. Information provided by these models can help in resource allocation and serve as a base for assessment and future improvements. We use innovative statistical methods to deal with the peculiarities of the software engineering data, such as non-normally distributed count data. To deal with overdispersed data and excess of zeroes in the dependent variable, we use negative binomial (NB) and zero-inflated {\{}NB{\}} regression in addition to Poisson regression. Furthermore, we form a framework for comparison of models' descriptive and predictive ability. Predictive capability of the models to identify most critical classes in the system early in the software development process can help in allocation of resources and foster software quality improvement. In addition to the correlation coefficients, we use additional statistics to assess a models' ability to explain high variability in the data and Pareto analysis to assess a models' ability to identify the most critical classes in the system. Results indicate that design aspects related to communication between classes and inheritance can be used as indicators of the most defect-prone classes, which require the majority of resources in development and testing phases. The zero-inflated negative binomial regression model, designed to explicitly model the occurrence of zero counts in the dataset, provides the best results for this purpose. },
	Doi = {https://doi.org/10.1016/S0164-1212(02)00024-9},
	ISSN = {0164-1212},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121202000249}
}

@Article{Sun2014134,
	Title = {{Cloud service selection: State-of-the-art and future research directions}},
	Author = {Sun, Le and Dong, Hai and Hussain, Farookh Khadeer and Hussain, Omar Khadeer and Chang, Elizabeth},
	Journal = {Journal of Network and Computer Applications},
	Year = {2014},
	Pages = {134--150},
	Volume = {45},
	Abstract = {Abstract Cloud technology connects a network of virtualized computers that are dynamically provisioned as computing resources, based on negotiated agreements between service providers and users. It delivers information technology resources in diverse forms of service, and the explosion of Cloud services on the Internet brings new challenges in Cloud service discovery and selection. To address these challenges, a range of studies has been carried out to develop advanced techniques that will assist service users to choose appropriate services. In this paper, we survey state-of-the-art Cloud service selection approaches, which are analyzed from the following five perspectives: decision-making techniques; data representation models; parameters and characteristics of Cloud services; contexts, purposes. After comparing and summarizing the reviewed approaches from these five perspectives, we identify the primary research issues in contemporary Cloud service selection. This survey is expected to bring benefits to both researchers and business agents. },
	Doi = {https://doi.org/10.1016/j.jnca.2014.07.019},
	ISSN = {1084-8045},
	Keywords = {Cloud computing,Cloud service selection,Decision-making},
	Url = {http://www.sciencedirect.com/science/article/pii/S108480451400160X}
}

@Article{SMR:SMR1606,
	Title = {{Automating the maintenance of nonfunctional system properties using demonstration-based model transformation}},
	Author = {Sun, Yu and Gray, Jeff and Delamare, Romain and Baudry, Benoit and White, Jules},
	Journal = {Journal of Software: Evolution and Process},
	Year = {2013},
	Number = {12},
	Pages = {1335--1356},
	Volume = {25},
	Abstract = {Domain-Specific Modeling Languages (DSMLs) are playing an increasingly significant role in software development. By raising the level of abstraction using notations that are representative of a specific domain, DSMLs allow the core essence of a problem to be separated from irrelevant accidental complexities, which are typically found at the implementation level in source code. In addition to modeling the functional aspects of a system, a number of nonfunctional properties (e.g., quality of service constraints and timing requirements) also need to be integrated into models in order to reach a complete specification of a system. This is particularly true for domains that have distributed real time and embedded needs. Given a base model with functional components, maintaining the nonfunctional properties that crosscut the base model has become an essential modeling task when using DSMLs.The task of maintaining nonfunctional properties in DSMLs is traditionally supported by manual model editing or by using model transformation languages. However, these approaches are challenging to use for those unfamiliar with the specific details of a modeling transformation language and the underlying metamodel of the domain, which presents a7 steep learning curve for many users. This paper presents a demonstration-based approach to automate the maintenance of nonfunctional properties in DSMLs. Instead of writing model transformation rules explicitly, users demonstrate how to apply the nonfunctional properties by directly editing the concrete model instances and simulating a single case of the maintenance process. By recording a user's operations, an inference engine analyzes the user's intention and generates generic model transformation patterns automatically, which can be refined by users and then reused to automate the same evolution and maintenance task in other models. Using this approach, users are able to automate the maintenance tasks without learning a complex model transformation language. In addition, because the demonstration is performed on model instances, users are isolated from the underlying abstract metamodel definitions. Our demonstration-based approach has been applied to several scenarios, such as auto scaling and model layout. The specific contribution in this paper is the application of the demonstration-based approach to capture crosscutting concerns representative of aspects at the modeling level. Several examples are presented across multiple modeling languages to demonstrate the benefits of our approach. Copyright {\textcopyright} 2013 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/smr.1606},
	ISSN = {2047-7481},
	Keywords = {domain-specific modeling language,model transformation by demonstration,nonfunctional property maintenance},
	Url = {http://dx.doi.org/10.1002/smr.1606}
}

@Article{SMR:SMR1806,
	Title = {{A model for assessing and re-assessing the value of software reuse}},
	Author = {Svahnberg, Mikael and Gorschek, Tony},
	Journal = {Journal of Software: Evolution and Process},
	Year = {2017},
	Number = {4},
	Pages = {n/a----n/a},
	Volume = {29},
	Abstract = {Background Software reuse is often seen as a cost avoidance rather than a gained value. This results in a rather one-sided debate where issues such a resource control, release schedule, quality, or reuse in more than one release are neglected.
		Aims We propose a reuse value assessment framework, intended to provide a more nuanced view of the value and costs associated with different reuse candidates.
		Method This framework is constructed based on findings from an interview study at a large software development company.
		Results The framework considers the functionality, compliance to standards, provided quality, and provided support of a reuse candidate, thus enabling an informed comparison between different reuse candidates. Furthermore, the framework provides means for tracking the value of the reused asset throughout subsequent releases.
		Conclusions The reuse value assessment framework is a tool to assist in the selection between different reuse candidates. The framework also provides a means to assess the current value of a reusable asset in a product, which can be used to indicate where maintenance efforts would increase the utilized potential of the reusable asset.
		},
	Doi = {10.1002/smr.1806},
	ISSN = {2047-7481},
	Keywords = {assessment,software reuse,value},
	Url = {http://dx.doi.org/10.1002/smr.1806}
}

@Article{SPE:SPE652,
	Title = {{A taxonomy of variability realization techniques}},
	Author = {Svahnberg, Mikael and van Gurp, Jilles and Bosch, Jan},
	Journal = {Software: Practice and Experience},
	Year = {2005},
	Number = {8},
	Pages = {705--754},
	Volume = {35},
	Abstract = {Development of software product families relies heavily on the use of variability to manage the differences between products by delaying design decisions to later stages of the development and usage of the constructed software systems. Implementation of variability is not a trivial task, and is governed by a number of factors. In this paper, we describe the factors that are relevant in determining how to implement variability, and present a taxonomy of variability realization techniques. Copyright {\textcopyright} 2005 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spe.652},
	ISSN = {1097-024X},
	Keywords = {development process,software architecture,software product families,variability,variability realization techniques},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/spe.652}
}

@Article{Svahnberg2002143,
	Title = {{Conditions and restrictions for product line generation migration}},
	Author = {Svahnberg, M and Mattsson, M},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2002},
	Pages = {143--154},
	Volume = {2290},
	Abstract = {In this paper we describe a case study of a company in the domain of automatic guided vehicles (AGVs) that is in the process of migrating from a previous generation of software product line, which has mainly been centered around hardware, into a new product line generation, which will be software-centered. We describe the issues motivating this transition, and the factors that complicate it. Moreover, we present a three stage process for migrating into a new software product line. This process is currently initiated in collaboration with the aforementioned company. {\textcopyright} Springer-Verlag Berlin Heidelberg 2002.},
	Annote = {cited By 2},
	Keywords = {Automatic guided vehicles; Computer software,Network architecture,Product-lines; Software Product Line; Three-stage},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-21644455855{\&}partnerID=40{\&}md5=94b9321031ab3b8a77cf09a3cc6a56ff}
}

@Article{Svendsen2010106,
	Title = {{Developing a software product line for train control: A case study of CVL}},
	Author = {Svendsen, A and Zhang, X and Lind-Tviberg, R and Fleurey, F and Haugen, {\O} and M{\o}ller-Pedersen, B and Olsen, G K},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2010},
	Pages = {106--120},
	Volume = {6287 LNCS},
	Abstract = {This paper presents a case study of creating a software product line for the train signaling domain. The Train Control Language (TCL) is a DSL which automates the production of source code for computers controlling train stations. By applying the Common Variability Language (CVL), which is a separate and generic language to define variability on base models, we form a software product line of stations. We discuss the process and experience of using CVL to automate the production of three real train stations. A brief discussion about the verification needed for the generated products is also included. {\textcopyright} 2010 Springer-Verlag Berlin Heidelberg.},
	Annote = {cited By 10},
	Doi = {10.1007/978-3-642-15579-6_8},
	Keywords = {Base models; Software Product Line; Source codes;,DSL; Network architecture; Signaling,Software design},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049365870{\&}doi=10.1007{\%}2F978-3-642-15579-6{\_}8{\&}partnerID=40{\&}md5=4fee3eb9c1d9378b4e41d2e9c202d08a}
}

@Article{Tuzun2015136,
	Title = {{Analyzing impact of experience curve on {\{}ROI{\}} in the software product line adoption process}},
	Author = {T{\"{u}}z{\"{u}}n, Eray and Tekinerdogan, Bedir},
	Journal = {Information and Software Technology},
	Year = {2015},
	Pages = {136--148},
	Volume = {59},
	Abstract = {AbstractContext Experience curve is a well-known concept in management and education science, which explains the phenomenon of increased worker efficiency with repetitive production of a good or service. Objective We aim to analyze the impact of the experience curve effect on the Return on Investment (ROI) in the software product line engineering (SPLE) process. Method We first present the results of a systematic literature review (SLR) to explicitly depict the studies that have considered the impact of experience curve effect on software development in general. Subsequently, based on the results of the SLR, the experience curve effect models in the literature, and the {\{}SPLE{\}} cost models, we define an approach for extending the cost models with the experience curve effect. Finally, we discuss the application of the refined cost models in a real industrial context. Results The {\{}SLR{\}} resulted in 15 primary studies which confirm the impact of experience curve effect on software development in general but the experience curve effect in the adoption of {\{}SPLE{\}} got less attention. The analytical discussion of the cost models and the application of the refined {\{}SPLE{\}} cost models in the industrial context showed a clear impact of the experience curve effect on the time-to-market, cost of development and {\{}ROI{\}} in the {\{}SPLE{\}} adoption process. Conclusions The proposed analysis with the newly defined cost models for {\{}SPLE{\}} adoption provides a more precise analysis tool for the management, and as such helps to support a better decision making. },
	Doi = {https://doi.org/10.1016/j.infsof.2014.09.008},
	ISSN = {0950-5849},
	Keywords = {Cost models,Experience curve,Learning curve,Productivity,Software product line engineering,Software reuse},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584914002079}
}

@Article{Tuzun201577,
	Title = {{Empirical evaluation of a decision support model for adopting software product line engineering}},
	Author = {T{\"{u}}z{\"{u}}n, E and Tekinerdogan, B and Kalender, M E and Bilgen, S},
	Journal = {Information and Software Technology},
	Year = {2015},
	Pages = {77--101},
	Volume = {60},
	Abstract = {Context The software product line engineering (SPLE) community has provided several different approaches for assessing the feasibility of SPLE adoption and selecting transition strategies. These approaches usually include many rules and guidelines which are very often implicit or scattered over different publications. Hence, for the practitioners it is not always easy to select and use these rules to support the decision making process. Even in case the rules are known, the lack of automated support for storing and executing the rules seriously impedes the decision making process. Objective We aim to evaluate the impact of a decision support system (DSS) on decision-making in SPLE adoption. In alignment with this goal, we provide a decision support model (DSM) and the corresponding DSS. Method First, we apply a systematic literature review (SLR) on the existing primary studies that discuss and present approaches for analyzing the feasibility of SPLE adoption and transition strategies. Second, based on the data extraction and synthesis activities of the SLR, the required questions and rules are derived and implemented in the DSS. Third, for validation of the approach we conduct multiple case studies. Results In the course of the SLR, 31 primary studies were identified from which we could construct 25 aspects, 39 questions and 312 rules. We have developed the DSS tool Transit-PL that embodies these elements. Conclusions The multiple case study validation showed that the adoption of the developed DSS tool is justified to support the decision making process in SPLE adoption. {\textcopyright}2015 Elsevier B.V. All rights reserved.},
	Annote = {cited By 2},
	Doi = {10.1016/j.infsof.2014.12.007},
	File = {:Users/mac/Downloads/Sciencedirect{\_}articles{\_}13Jun2017{\_}14-36-55.006/Empirical-evaluation-of-a-decision-support-model-for-adopting-software-product-line-engineering{\_}2015{\_}Information-and-Software-Technology.pdf:pdf},
	Keywords = {Artificial intelligence; Computer software; Decisi,Decision support models; Decision support system,Decision support systems},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921772938{\&}doi=10.1016{\%}2Fj.infsof.2014.12.007{\&}partnerID=40{\&}md5=8aa699041661142533209386bfcf15cd}
}

@Article{Tahir20132877,
	Title = {{A systematic review on the functional testing of semantic web services}},
	Author = {Tahir, Abbas and Tosi, Davide and Morasca, Sandro},
	Journal = {Journal of Systems and Software},
	Year = {2013},
	Number = {11},
	Pages = {2877--2889},
	Volume = {86},
	Abstract = {Abstract Semantic web services are gaining more attention as an important element of the emerging semantic web. Therefore, testing semantic web services is becoming a key concern as an essential quality assurance measure. The objective of this systematic literature review is to summarize the current state of the art of functional testing of semantic web services by providing answers to a set of research questions. The review follows a predefined procedure that involves automatically searching 5 well-known digital libraries. After applying the selection criteria to the results, a total of 34 studies were identified as relevant. Required information was extracted from the studies and summarized. Our systematic literature review identified some approaches available for deriving test cases from the specifications of semantic web services. However, many of the approaches are either not validated or the validation done lacks credibility. We believe that a substantial amount of work remains to be done to improve the current state of research in the area of testing semantic web services. },
	Doi = {https://doi.org/10.1016/j.jss.2013.06.064},
	ISSN = {0164-1212},
	Keywords = {Functional testing,Semantic web services,Systematic literature review,Testing approach},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121213001659}
}

@InProceedings{Tamayo:2011:ACX:1999320.1999337,
	Title = {{Analysing Complexity of XML Schemas in Geospatial Web Services}},
	Author = {Tamayo, Alain and Granell, Carlos and Huerta, Joaqu$\backslash$'$\backslash$in},
	Booktitle = {Proceedings of the 2Nd International Conference on Computing for Geospatial Research {\&} Applications},
	Year = {2011},
	Address = {New York, NY, USA},
	Pages = {17:1----17:9},
	Publisher = {ACM},
	Series = {COM.Geo '11},
	Doi = {10.1145/1999320.1999337},
	ISBN = {978-1-4503-0681-2},
	Keywords = {XML schema,complexity analysis,geospatial information,software metrics,web services},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/1999320.1999337}
}

@Article{Tan:2009:CDM:1571629.1571630,
	Title = {{Conceptual Data Model-based Software Size Estimation for Information Systems}},
	Author = {Tan, Hee Beng Kuan and Zhao, Yuan and Zhang, Hongyu},
	Journal = {ACM Trans. Softw. Eng. Methodol.},
	Year = {2009},
	Number = {2},
	Pages = {4:1----4:37},
	Volume = {19},
	Address = {New York, NY, USA},
	Doi = {10.1145/1571629.1571630},
	ISSN = {1049-331X},
	Keywords = {Software sizing,conceptual data model,line of code (LOC),multiple linear regression model},
	Publisher = {ACM},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/1571629.1571630}
}

@InProceedings{Tan:2014:SDC:2677758.2677773,
	Title = {{Software Development in the City Evolutions Project}},
	Author = {Tan, Lei and Bille, Ross and Lin, Yuqing and Chalup, Stephan and Tucker, Chris},
	Booktitle = {Proceedings of the 2014 Conference on Interactive Entertainment},
	Year = {2014},
	Address = {New York, NY, USA},
	Pages = {18:1----18:7},
	Publisher = {ACM},
	Series = {IE2014},
	Doi = {10.1145/2677758.2677773},
	ISBN = {978-1-4503-2790-9},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2677758.2677773}
}

@InProceedings{Tan:2015:OSC:2771783.2771808,
	Title = {{Optimizing Selection of Competing Features via Feedback-directed Evolutionary Algorithms}},
	Author = {Tan, Tian Huat and Xue, Yinxing and Chen, Manman and Sun, Jun and Liu, Yang and Dong, Jin Song},
	Booktitle = {Proceedings of the 2015 International Symposium on Software Testing and Analysis},
	Year = {2015},
	Address = {New York, NY, USA},
	Pages = {246--256},
	Publisher = {ACM},
	Series = {ISSTA 2015},
	Doi = {10.1145/2771783.2771808},
	ISBN = {978-1-4503-3620-8},
	Keywords = {SAT solvers,Software product line,evolutionary algorithms},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2771783.2771808}
}

@Article{Tang2010352,
	Title = {{A comparative study of architecture knowledge management tools}},
	Author = {Tang, Antony and Avgeriou, Paris and Jansen, Anton and Capilla, Rafael and Babar, Muhammad Ali},
	Journal = {Journal of Systems and Software},
	Year = {2010},
	Number = {3},
	Pages = {352--370},
	Volume = {83},
	Abstract = {Recent research suggests that architectural knowledge, such as design decisions, is important and should be recorded alongside the architecture description. Different approaches have emerged to support such architectural knowledge (AK) management activities. However, there are different notions of and emphasis on what and how architectural activities should be supported. This is reflected in the design and implementation of existing {\{}AK{\}} tools. To understand the current status of software architecture knowledge engineering and future research trends, this paper compares five architectural knowledge management tools and the support they provide in the architecture life-cycle. The comparison is based on an evaluation framework defined by a set of 10 criteria. The results of the comparison provide insights into the current focus of architectural knowledge management support, their advantages, deficiencies, and conformance to the current architectural description standard. Based on the outcome of this comparison a research agenda is proposed for future work on {\{}AK{\}} tools. },
	Doi = {https://doi.org/10.1016/j.jss.2009.08.032},
	ISSN = {0164-1212},
	Keywords = {Architectural design,Architectural knowledge management tool,Design rationale},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121209002295}
}

@InProceedings{Tang2017,
	Title = {{StiCProb: A novel feature mining approach using conditional probability}},
	Author = {Tang, Yutian and Leung, Hareton},
	Booktitle = {2017 IEEE 24th International Conference on Software Analysis, Evolution and Reengineering (SANER)},
	Year = {2017},
	Month = {feb},
	Pages = {45--55},
	Publisher = {IEEE},
	Doi = {10.1109/SANER.2017.7884608},
	ISBN = {978-1-5090-5501-2},
	Url = {http://ieeexplore.ieee.org/document/7884608/}
}

@Article{Tanhaei2016138,
	Title = {{Automating feature model refactoring: A Model transformation approach}},
	Author = {Tanhaei, Mohammad and Habibi, Jafar and Mirian-Hosseinabadi, Seyed-Hassan},
	Journal = {Information and Software Technology},
	Year = {2016},
	Pages = {138--157},
	Volume = {80},
	Abstract = {Abstract Context: Feature model is an appropriate and indispensable tool for modeling similarities and differences among products of the Software Product Line (SPL). It not only exposes the validity of the products' configurations in an {\{}SPL{\}} but also changes in the course of time to support new requirements of the SPL. Modifications made on the feature model in the course of time raise a number of issues. Useless enlargements of the feature model, the existence of dead features, and violated constraints in the feature model are some of the key problems that make its maintenance difficult. Objective: The initial approach to dealing with the above-mentioned problems and improving maintainability of the feature model is refactoring. Refactoring modifies software artifacts in a way that their externally visible behavior does not change. Method: We introduce a method for defining refactoring rules and executing them on the feature model. We use the {\{}ATL{\}} model transformation language to define the refactoring rules. Moreover, we provide an Alloy model to check the feature model and the safety of the refactorings that are performed on it. Results: In this research, we propose a safe framework for refactoring a feature model. This framework enables users to perform automatic and semi-automatic refactoring on the feature model. Conclusions: Automated tool support for refactoring is a key issue for adopting approaches such as utilizing feature models and integrating them into the software development process of companies. In this work, we define some of the important refactoring rules on the feature model and provide tools that enable users to add new rules using the {\{}ATL{\}} {\{}M2M{\}} language. Our framework assesses the correctness of the refactorings using the Alloy language. },
	Doi = {https://doi.org/10.1016/j.infsof.2016.08.011},
	ISSN = {0950-5849},
	Keywords = {Feature model refactoring,Model transformation {\&} refactoring},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584916301422}
}

@Article{Tanhaei2016951,
	Title = {{A Feature Model Based Framework for Refactoring Software Product Line Architecture}},
	Author = {Tanhaei, M and Habibi, J and Mirian-Hosseinabadi, S.-H.},
	Journal = {Journal of Computer Science and Technology},
	Year = {2016},
	Number = {5},
	Pages = {951--986},
	Volume = {31},
	Abstract = {Software product line (SPL) is an approach used to develop a range of software products with a high degree of similarity. In this approach, a feature model is usually used to keep track of similarities and differences. Over time, as modifications are made to the SPL, inconsistencies with the feature model could arise. The first approach to dealing with these inconsistencies is refactoring. Refactoring consists of small steps which, when accumulated, may lead to large-scale changes in the SPL, resulting in features being added to or eliminated from the SPL. In this paper, we propose a framework for refactoring SPLs, which helps keep SPLs consistent with the feature model. After some introductory remarks, we describe a formal model for representing the feature model. We express various refactoring patterns applicable to the feature model and the SPL formally, and then introduce an algorithm for finding them in the SPL. In the end, we use a real-world case study of an SPL to illustrate the applicability of the framework introduced in the paper. {\textcopyright} 2016, Springer Science+Business Media New York.},
	Annote = {cited By 2},
	Doi = {10.1007/s11390-016-1674-y},
	Keywords = {Computer software,Degree of similarity; Feature modeling; Keep trac,Software architecture},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984908557{\&}doi=10.1007{\%}2Fs11390-016-1674-y{\&}partnerID=40{\&}md5=bb87ce0a2218e447cdac76a5b15fa444}
}

@Article{Tarhan2014477,
	Title = {{Systematic analyses and comparison of development performance and product quality of Incremental Process and Agile Process}},
	Author = {Tarhan, Ayca and Yilmaz, Seda Gunes},
	Journal = {Information and Software Technology},
	Year = {2014},
	Number = {5},
	Pages = {477--494},
	Volume = {56},
	Abstract = {AbstractContext Although Agile software development models have been widely used as a base for the software project life-cycle since 1990s, the number of studies that follow a sound empirical method and quantitatively reveal the effect of using these models over Traditional models is scarce. Objective This article explains the empirical method of and the results from systematic analyses and comparison of development performance and product quality of Incremental Process and Agile Process adapted in two projects of a middle-size, telecommunication software development company. The Incremental Process is an adaption of the Waterfall Model whereas the newly introduced Agile Process is a combination of the Unified Software Development Process, Extreme Programming, and Scrum. Method The method followed to perform the analyses and comparison is benefited from the combined use of qualitative and quantitative methods. It utilizes; {\{}GQM{\}} Approach to set measurement objectives, {\{}CMMI{\}} as the reference model to map the activities of the software development processes, and a pre-defined assessment approach to verify consistency of process executions and evaluate measure characteristics prior to quantitative analysis. Results The results of the comparison showed that the Agile Process had performed better than the Incremental Process in terms of productivity (79{\%}), defect density (57{\%}), defect resolution effort ratio (26{\%}), Test Execution V{\&}V Effectiveness (21{\%}), and effort prediction capability (4{\%}). These results indicate that development performance and product quality achieved by following the Agile Process was superior to those achieved by following the Incremental Process in the projects compared. Conclusion The acts of measurement, analysis, and comparison enabled comprehensive review of the two development processes, and resulted in understanding their strengths and weaknesses. The comparison results constituted objective evidence for organization-wide deployment of the Agile Process in the company. },
	Annote = {Performance in Software Development},
	Doi = {https://doi.org/10.1016/j.infsof.2013.12.002},
	ISSN = {0950-5849},
	Keywords = {Agile development,Empirical method,Process performance,Qualitative analysis,Quantitative analysis,Software measurement},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584913002310}
}

@Article{Taulavuori2004535,
	Title = {{Component documentation—a key issue in software product lines}},
	Author = {Taulavuori, Anne and Niemel{\"{a}}, Eila and Kallio, P{\"{a}}ivi},
	Journal = {Information and Software Technology},
	Year = {2004},
	Number = {8},
	Pages = {535--546},
	Volume = {46},
	Abstract = {Product lines embody a strategic reuse of both intellectual effort and existing artefacts, such as software architectures and components. Third-party components are increasingly being used in product line based software engineering, in which case the integration is controlled by the product line architecture. However, the software integrators have difficulties in finding out the capabilities of components, because components are not documented in a standard way. Documentation is often the only way of assessing the applicability, credibility and quality of a third-party component. Our contribution is a standard documentation pattern for software components. The pattern provides guidelines and structure for component documentation and ensures the quality of documentation. The pattern has been validated by applying and analysing it in practice. },
	Doi = {https://doi.org/10.1016/j.infsof.2003.10.004},
	ISSN = {0950-5849},
	Keywords = {Component documentation,Software product line,Third-party component},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584903002131}
}

@Article{Tawhid2008490,
	Title = {{Integrating performance analysis in the model driven development of software product lines}},
	Author = {Tawhid, R and Petriu, D},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2008},
	Pages = {490--504},
	Volume = {5301 LNCS},
	Abstract = {The paper proposes to integrate performance analysis in the early phases of the model-driven development process for Software Product Lines (SPL). We start by adding generic performance annotations to the UML model representing the set of core reusable SPL assets. The annotations are generic and use the MARTE Profile recently adopted by OMG. A first model transformation realized in the Atlas Transformation Language (ATL), which is the focus of this paper, derives the UML model of a specific product with concrete MARTE performance annotations from the SPL model. A second transformation generates a Layered Queueing Network performance model for the given product by applying an existing transformation approach named PUMA, developed in previous work. The proposed technique is illustrated with an e-commerce case study that models the commonality and variability in both structural and behavioural SPL views. A product is derived and the performance of two design alternatives is compared. {\textcopyright} 2008 Springer-Verlag Berlin Heidelberg.},
	Annote = {cited By 20},
	Doi = {10.1007/978-3-540-87875-9_35},
	Keywords = {ATL; MARTE; Performance analysis; Software produc,Electronic commerce; Linguistics; Models; Network,Modal analysis},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-56649124490{\&}doi=10.1007{\%}2F978-3-540-87875-9{\_}35{\&}partnerID=40{\&}md5=509aa5e29e0be93164757d1736f3283a}
}

@Article{Teixeira20131038,
	Title = {{Safe composition of configuration knowledge-based software product lines}},
	Author = {Teixeira, Leopoldo and Borba, Paulo and Gheyi, Rohit},
	Journal = {Journal of Systems and Software},
	Year = {2013},
	Number = {4},
	Pages = {1038--1053},
	Volume = {86},
	Abstract = {Mistakes made when implementing or specifying the models of a Software Product Line (SPL) can result in ill-formed products — the safe composition problem. Such problem can hinder productivity and it might be hard to detect, since {\{}SPLs{\}} can have thousands of products. In this article, we propose a language independent approach for verifying safe composition of {\{}SPLs{\}} with dedicated Configuration Knowledge models. We translate feature model and Configuration Knowledge into propositional logic and use the Alloy Analyzer to perform the verification. To provide evidence for the generality of our approach, we instantiate this approach in different compositional settings. We deal with different kinds of assets such as use case scenarios and Eclipse {\{}RCP{\}} components. We analyze both the code and the requirements for a larger scale SPL, finding problems that affect thousands of products in minutes. Moreover, our evaluation suggests that the analysis time grows linearly with respect to the number of products in the analyzed SPLs. },
	Annote = {{\{}SI{\}} : Software Engineering in Brazil: Retrospective and Prospective Views},
	Doi = {https://doi.org/10.1016/j.jss.2012.11.006},
	ISSN = {0164-1212},
	Keywords = {Configuration Knowledge,Safe composition,Software Product Lines},
	Url = {http://www.sciencedirect.com/science/article/pii/S016412121200307X}
}

@Article{Tekinerdogan2008558,
	Title = {{Software architecture reliability analysis using failure scenarios}},
	Author = {Tekinerdogan, Bedir and Sozer, Hasan and Aksit, Mehmet},
	Journal = {Journal of Systems and Software},
	Year = {2008},
	Number = {4},
	Pages = {558--575},
	Volume = {81},
	Abstract = {With the increasing size and complexity of software in embedded systems, software has now become a primary threat for the reliability. Several mature conventional reliability engineering techniques exist in literature but traditionally these have primarily addressed failures in hardware components and usually assume the availability of a running system. Software architecture analysis methods aim to analyze the quality of software-intensive system early at the software architecture design level and before a system is implemented. We propose a Software Architecture Reliability Analysis Approach (SARAH) that benefits from mature reliability engineering techniques and scenario-based software architecture analysis to provide an early software reliability analysis at the architecture design level. {\{}SARAH{\}} defines the notion of failure scenario model that is based on the Failure Modes and Effects Analysis method (FMEA) in the reliability engineering domain. The failure scenario model is applied to represent so-called failure scenarios that are utilized to derive fault tree sets (FTS). Fault tree sets are utilized to provide a severity analysis for the overall software architecture and the individual architectural elements. Despite conventional reliability analysis techniques which prioritize failures based on criteria such as safety concerns, in {\{}SARAH{\}} failure scenarios are prioritized based on severity from the end-user perspective. {\{}SARAH{\}} results in a failure analysis report that can be utilized to identify architectural tactics for improving the reliability of the software architecture. The approach is illustrated using an industrial case for analyzing reliability of the software architecture of the next release of a Digital TV. },
	Annote = {Selected papers from the 10th Conference on Software Maintenance and Reengineering (CSMR 2006)},
	Doi = {https://doi.org/10.1016/j.jss.2007.10.029},
	ISSN = {0164-1212},
	Keywords = {FMEA,Fault trees,Reliability analysis,Scenario-based architectural evaluation},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121207003032}
}

@Conference{TerBeek201556,
	Title = {{Quantitative analysis of probabilistic models of software product lines with statistical model checking}},
	Author = {{Ter Beek}, M H and Legay, A and Lafuente, A L and Vandin, A},
	Booktitle = {Electronic Proceedings in Theoretical Computer Science, EPTCS},
	Year = {2015},
	Pages = {56--70},
	Volume = {182},
	Abstract = {We investigate the suitability of statistical model checking techniques for analysing quantitative properties of software product line models with probabilistic aspects. For this purpose, we enrich the feature-oriented language FLAN with action rates, which specify the likelihood of exhibiting particular behaviour or of installing features at a specific moment or in a specific order. The enriched language (called PFLAN) allows us to specify models of software product lines with probabilistic configurations and behaviour, e.g. by considering a PFLAN semantics based on discrete-time Markov chains. The Maude implementation of PFLAN is combined with the distributed statistical model checker MultiVeStA to perform quantitative analyses of a simple product line case study. The presented analyses include the likelihood of certain behaviour of interest (e.g. product malfunctioning) and the expected average cost of products. {\textcopyright} M.H. ter Beek, A. Legay, A. Lluch Lafuente {\&} A. Vandin.},
	Annote = {cited By 6},
	Doi = {10.4204/EPTCS.182.5},
	Keywords = {Chemical analysis; Computer software; Formal metho,Discrete time Markov chains; Feature-oriented; Pr,Model checking},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990844198{\&}doi=10.4204{\%}2FEPTCS.182.5{\&}partnerID=40{\&}md5=6fd048ee4b28bf001260b16fad310120}
}

@Article{Thorn2010411,
	Title = {{Current state and potential of variability management practices in software-intensive SMEs: Results from a regional industrial survey}},
	Author = {Th{\"{o}}rn, C},
	Journal = {Information and Software Technology},
	Year = {2010},
	Number = {4},
	Pages = {411--421},
	Volume = {52},
	Abstract = {Context: More and more, small and medium-sized enterprises (SMEs) are using software to augment the functionality of their products and offerings. Variability management of software is becoming an interesting topic for SMEs with expanding portfolios and increasingly complex product structures. While the use of software product lines to resolve high variability is well known in larger organizations, there is less known about the practices in SMEs. Objective: This paper presents results from a survey of software developing SMEs. The purpose of the paper is to provide a snapshot of the current awareness and practices of variability modeling in organizations that are developing software with the constraints present in SMEs. Method: A survey with questions regarding the variability practices was distributed to software developing organizations in a region of Sweden that has many SMEs. The response rate was 13{\%} and 25 responses are used in this analysis. Results: We find that, although there are SMEs that develop implicit software product lines and have relatively sophisticated variability structures for their solution space, the structures of the problem space and the product space have room for improvement. Conclusions: The answers in the survey indicate that SMEs are in situations where they can benefit from more structured variability management, but the awareness need to be raised. Even though the problem space similarity is high, there is little reuse and traceability activities performed. The existence of SMEs with qualified variability management and product line practices indicates that small organizations are capable to apply such practices. {\textcopyright} 2009 Elsevier B.V. All rights reserved.},
	Annote = {cited By 10},
	Doi = {10.1016/j.infsof.2009.10.009},
	Keywords = {Complex products; High variability; Industrial sur,Computer software,Societies and institutions; Surveys},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77049124496{\&}doi=10.1016{\%}2Fj.infsof.2009.10.009{\&}partnerID=40{\&}md5=c6c79551ab7aa0aaf04e6ead9774ed78}
}

@Article{Thum:2014:CSA:2620784.2580950,
	Title = {{A Classification and Survey of Analysis Strategies for Software Product Lines}},
	Author = {Th{\"{u}}m, Thomas and Apel, Sven and K{\"{a}}stner, Christian and Schaefer, Ina and Saake, Gunter},
	Journal = {ACM Comput. Surv.},
	Year = {2014},
	Number = {1},
	Pages = {6:1----6:45},
	Volume = {47},
	Address = {New York, NY, USA},
	Doi = {10.1145/2580950},
	ISSN = {0360-0300},
	Keywords = {Product-line analysis,model checking,program family,software analysis,software product line,static analysis,theorem proving,type checking},
	Publisher = {ACM},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2580950}
}

@Article{Thianniwet2016128,
	Title = {{Scaling up the fitness function for reverse engineering feature models}},
	Author = {Thianniwet, T and Cohen, M B},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2016},
	Pages = {128--142},
	Volume = {9962 LNCS},
	Abstract = {Recent research on software product line engineering has led to several search-based frameworks for reverse engineering feature models. The most common fitness function utilized maximizes the number of matched products with an oracle set of products. However, to calculate this fitness each product defined by the chromosome has to be enumerated using a SAT solver and this limits scalability to product lines with fewer than 30 features. In this paper we propose SATff, a fitness function that simulates validity by computing the difference between constraints in the chromosome and oracle. In an empirical study on 101 feature models comparing SATff with two existing fitness functions that use the enumeration technique we find that SATff shows a significant improvement over one, and no significant difference with the other one. We also find that SATff requires only 7{\%} of the runtime on average scaling to feature models with as many as 97 features. {\textcopyright} Springer International Publishing AG 2016.},
	Annote = {cited By 0},
	Doi = {10.1007/978-3-319-47106-8_9},
	Keywords = {Chromosomes; Computer software; Genetic algorithms,Empirical studies; Enumeration techniques; Featur,Software design},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989920884{\&}doi=10.1007{\%}2F978-3-319-47106-8{\_}9{\&}partnerID=40{\&}md5=17b955197bdc3d3d0c031db010197984}
}

@Article{SPIP:SPIP217,
	Title = {{Topological properties for characterizing well-formedness of process components}},
	Author = {Thu, Tran Dan and Nhi, Tran Hanh and {Bich Thuy}, Dong Thi and Coulette, Bernard and Cregut, Xavier},
	Journal = {Software Process: Improvement and Practice},
	Year = {2005},
	Number = {2},
	Pages = {217--247},
	Volume = {10},
	Abstract = {RHODES is an environment for modelling software processes, in which software processes are described by using a Process Modelling Language called PBOOL+. In this environment, a software process is built from PBOOL+ process components that can be reused to construct other processes. To identify well-formed process components, we study properties to be able to characterize topological structure of the components. These properties should relate intrinsically to cohesion of a component, and coupling between components. We will consider two classical properties that originated from graph theory, which are connection and transitive closure. These two properties are sometimes too strict to be applied, so we propose several weaker properties that are still useful for characterizing good components. The article aims to present these topological properties and their applications to reusable process components in context of the RHODES environment. Copyright {\textcopyright} 2005 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spip.217},
	ISSN = {1099-1670},
	Keywords = {closure,cohesion,connection,coupling,process component,process pattern},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/spip.217}
}

@Article{Thurimella20131831,
	Title = {{A mixed-method approach for the empirical evaluation of the issue-based variability modeling}},
	Author = {Thurimella, A K and Br{\"{u}}gge, B},
	Journal = {Journal of Systems and Software},
	Year = {2013},
	Number = {7},
	Pages = {1831--1849},
	Volume = {86},
	Abstract = {Background: Variability management is the fundamental part of software product line engineering, which deals with customization and reuse of artifacts for developing a family of systems. Rationale approaches structure decision-making by managing the tacit-knowledge behind decisions. This paper reports a quasi-experiment for evaluating a rationale enriched collaborative variability management methodology called issue-based variability modeling. Objective: We studied the interaction of stakeholders with issue-based modeling to evaluate its applicability in requirements engineering teams. Furthermore, we evaluated the reuse of rationale while instantiating and changing variability. Approach: We enriched a quasi-experimental design with a variety of methods found in case study research. A sample of 258 students was employed with data collection and analysis based on a mix of qualitative and quantitative methods. Our study was performed in two phases: the first phase focused on variability identification and instantiation, while the second phase included tasks on variability evolution. Results: We obtained strong empirical evidence on reuse patterns for rationale during instantiation and evolution of variability. The tabular representations used by rationale modeling are learnable and usable in teams of diverse backgrounds. {\textcopyright} 2013 Elsevier Inc. All rights reserved.},
	Annote = {cited By 7},
	Doi = {10.1016/j.jss.2013.01.038},
	Keywords = {Computer software,Empirical evaluations; Empirical Software Engineer,Requirements engineering; Software design},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879943433{\&}doi=10.1016{\%}2Fj.jss.2013.01.038{\&}partnerID=40{\&}md5=2fc1982c04931e26e6518b14ddf38d75}
}

@Article{Thurimella2012933,
	Title = {{Issue-based variability management}},
	Author = {Thurimella, A K and Bruegge, B},
	Journal = {Information and Software Technology},
	Year = {2012},
	Number = {9},
	Pages = {933--950},
	Volume = {54},
	Abstract = {Context: Variability management is a key activity in software product line engineering. This paper focuses on managing rationale information during the decision-making activities that arise during variability management. By decision-making we refer to systematic problem solving by considering and evaluating various alternatives. Rationale management is a branch of science that enables decision-making based on the argumentation of stakeholders while capturing the reasons and justifications behind these decisions. Objective: Decision-making should be supported to identify variability in domain engineering and to resolve variation points in application engineering. We capture the rationale behind variability management decisions. The captured rationale information is useful to evaluate future changes of variability models as well as to handle future instantiations of variation points. We claim that maintaining rationale will enhance the longevity of variability models. Furthermore, decisions should be performed using a formal communication between domain engineering and application engineering. Method: We initiate the novel area of issue-based variability management (IVM) by extending variability management with rationale management. The key contributions of this paper are: (i) an issue-based variability management methodology (IVMM), which combines questions, options and criteria (QOC) and a specific variability approach; (ii) a meta-model for IVMM and a process for variability management and (iii) a tool for the methodology, which was developed by extending an open source rationale management tool. Results: Rationale approaches (e.g. questions, options and criteria) guide distributed stakeholders when selecting choices for instantiating variation points. Similarly, rationale approaches also aid the elicitation of variability and the evaluation of changes. The rationale captured within the decision-making process can be reused to perform future decisions on variability. Conclusion: IVMM was evaluated comparatively based on an experimental survey, which provided evidence that IVMM is more effective than a variability modeling approach that does not use issues. {\textcopyright} 2012 Elsevier B.V. All rights reserved.},
	Annote = {cited By 17},
	Doi = {10.1016/j.infsof.2012.02.005},
	Keywords = {Application engineering; Decision making process;,Decision making,Production engineering; Requirements engineering;},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861953812{\&}doi=10.1016{\%}2Fj.infsof.2012.02.005{\&}partnerID=40{\&}md5=3f2ec22a5e8fc4126819f77a5b6012e2}
}

@Article{Tibermacine201637,
	Title = {{Software architecture constraint reuse-by-composition}},
	Author = {Tibermacine, Chouki and Sadou, Salah and That, Minh Tu Ton and Dony, Christophe},
	Journal = {Future Generation Computer Systems},
	Year = {2016},
	Pages = {37--53},
	Volume = {61},
	Abstract = {Abstract Architecture constraints are specifications which enable developers to formalize design rules that architectures should respect, like the topological conditions of a given architecture pattern or style. These constraints can serve as a documentation to better understand an existing architecture description, or can serve as invariants that can be checked after the application of an architecture change to see whether design rules still hold. Like any specifications, architecture constraints are frequently subject to reuse. Besides, these constraints are specified and checked during architecture design time, when component descriptions are specified or selected from repositories, then instantiated and connected together to define architecture descriptions. These two facts (being subject to reuse and instantiation/connection) make architecture constraints good candidates for component-based design within a unified environment. In this paper, we propose a component model for specifying architecture constraints. This model has been implemented as an extension to an {\{}ADL{\}} that we have developed, which is called CLACS. The obtained process advocates the idea of specifying architecture constraints using the same paradigm of component-based development as for architecture description. To evaluate the component model, we conducted an experiment with a catalog of constraints formalizing the topological conditions of architecture patterns. The results of this experiment showed that constraint specification is improved by this reuse-by-composition model. },
	Doi = {https://doi.org/10.1016/j.future.2016.02.006},
	ISSN = {0167-739X},
	Keywords = {Architecture constraint,Architecture description,OCL,Software component},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167739X1630019X}
}

@Article{SMR:SMR283,
	Title = {{On the business value and technical challenges of adopting Web services}},
	Author = {Tilley, S and Gerdes, J and Hamilton, T and Huang, S and M{\"{u}}ller, H and Smith, D and Wong, K},
	Journal = {Journal of Software Maintenance and Evolution: Research and Practice},
	Year = {2004},
	Number = {1-2},
	Pages = {31--50},
	Volume = {16},
	Abstract = {This paper provides a balanced perspective of the business value and technical challenges of adopting Web services. Technology adoption is a continual challenge for both tool developers and enterprise users. Web services are a prime example of an emerging technology that is fraught with adoption issues. Part of the problem is separating marketing hype from business reality. Web services are network-accessible interfaces to application functionality. They are built using Internet technologies such as XML and standard protocols such as SOAP. The adoption issues related to Web services are complex and multifaceted. For example, determining whether this technology is a fundamental advance, rather than something old under a new name, requires technical depth, business acumen, and considerable historical knowledge of past developments. A sample problem from the health care industry is used to illustrate some of the adoption issues. Copyright {\textcopyright} 2004 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/smr.283},
	ISSN = {1532-0618},
	Keywords = {SOAP,Web services,XML,adoption,business value,migration,technology transfer},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/smr.283}
}

@Article{Tiwari:2013:RRT:2439976.2439982,
	Title = {{Reuse: Reducing Test Effort}},
	Author = {Tiwari, Rajeev and Goel, Noopur},
	Journal = {SIGSOFT Softw. Eng. Notes},
	Year = {2013},
	Number = {2},
	Pages = {1--11},
	Volume = {38},
	Address = {New York, NY, USA},
	Doi = {10.1145/2439976.2439982},
	ISSN = {0163-5948},
	Keywords = {reusable test cases,reuse-oriented test approaches,software product lines,test effort},
	Publisher = {ACM},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2439976.2439982}
}

@Article{Tizzei2011121,
	Title = {{Components meet aspects: Assessing design stability of a software product line}},
	Author = {Tizzei, Leonardo P and Dias, Marcelo and Rubira, Cec{\'{i}}lia M F and Garcia, Alessandro and Lee, Jaejoon},
	Journal = {Information and Software Technology},
	Year = {2011},
	Number = {2},
	Pages = {121--136},
	Volume = {53},
	Abstract = {Context It is important for Product Line Architectures (PLA) to remain stable accommodating evolutionary changes of stakeholder's requirements. Otherwise, architectural modifications may have to be propagated to products of a product line, thereby increasing maintenance costs. A key challenge is that several features are likely to exert a crosscutting impact on the {\{}PLA{\}} decomposition, thereby making it more difficult to preserve its stability in the presence of changes. Some researchers claim that the use of aspects can ameliorate instabilities caused by changes in crosscutting features. Hence, it is important to understand which aspect-oriented (AO) and non-aspect-oriented techniques better cope with {\{}PLA{\}} stability through evolution. Objective This paper evaluates the positive and negative change impact of component and aspect based design on PLAs. The objective of the evaluation is to assess how aspects and components promote {\{}PLA{\}} stability in the presence of various types of evolutionary change. To support a broader analysis, we also evaluate the {\{}PLA{\}} stability of a hybrid approach (i.e. combined use of aspects and components) against the isolated use of component-based, OO, and {\{}AO{\}} approaches. Method An quantitative and qualitative analysis of {\{}PLA{\}} stability which involved four different implementations of a PLA: (i) an {\{}OO{\}} implementation, (ii) an {\{}AO{\}} implementation, (iii) a component-based implementation, and (iv) a hybrid implementation where both components and aspects are employed. Each implementation has eight releases and they are functionally equivalent. We used conventional metrics suites for change impact and modularity to measure the architecture stability evaluation of the 4 implementations. Results The combination of aspects and components promotes superior {\{}PLA{\}} resilience than the other {\{}PLAs{\}} in most of the circumstances. Conclusion It is concluded that the combination of aspects and components supports the design of high cohesive and loosely coupled PLAs. It also contributes to improve modularity by untangling feature implementation. },
	Doi = {https://doi.org/10.1016/j.infsof.2010.08.007},
	ISSN = {0950-5849},
	Keywords = {Aspect-Oriented Software Development,Component-based Development,Design stability,Product Line Architecture},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584910001564}
}

@Article{Tofan2014850,
	Title = {{Past and future of software architectural decisions – A systematic mapping study}},
	Author = {Tofan, Dan and Galster, Matthias and Avgeriou, Paris and Schuitema, Wes},
	Journal = {Information and Software Technology},
	Year = {2014},
	Number = {8},
	Pages = {850--872},
	Volume = {56},
	Abstract = {AbstractContext The software architecture of a system is the result of a set of architectural decisions. The topic of architectural decisions in software engineering has received significant attention in recent years. However, no systematic overview exists on the state of research on architectural decisions. Objective The goal of this study is to provide a systematic overview of the state of research on architectural decisions. Such an overview helps researchers reflect on previous research and plan future research. Furthermore, such an overview helps practitioners understand the state of research, and how research results can help practitioners in their architectural decision-making. Method We conducted a systematic mapping study, covering studies published between January 2002 and January 2012. We defined six research questions. We queried six reference databases and obtained an initial result set of 28,895 papers. We followed a search and filtering process that resulted in 144 relevant papers. Results After classifying the 144 relevant papers for each research question, we found that current research focuses on documenting architectural decisions. We found that only several studies describe architectural decisions from the industry. We identified potential future research topics: domain-specific architectural decisions (such as mobile), achieving specific quality attributes (such as reliability or scalability), uncertainty in decision-making, and group architectural decisions. Regarding empirical evaluations of the papers, around half of the papers use systematic empirical evaluation approaches (such as surveys, or case studies). Still, few papers on architectural decisions use experiments. Conclusion Our study confirms the increasing interest in the topic of architectural decisions. This study helps the community reflect on the past ten years of research on architectural decisions. Researchers are offered a number of promising future research directions, while practitioners learn what existing papers offer. },
	Doi = {https://doi.org/10.1016/j.infsof.2014.03.009},
	ISSN = {0950-5849},
	Keywords = {Architectural decisions,Software architecture,Systematic mapping study},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584914000706}
}

@InProceedings{Tolvanen:2011:MDM:2019136.2019195,
	Title = {{MetaEdit+: Domain-specific Modeling Environment for Product Lines}},
	Author = {Tolvanen, Juha-Pekka},
	Booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
	Year = {2011},
	Address = {New York, NY, USA},
	Pages = {51:1----51:2},
	Publisher = {ACM},
	Series = {SPLC '11},
	Doi = {10.1145/2019136.2019195},
	ISBN = {978-1-4503-0789-5},
	Keywords = {code generation,domain-specific language,domain-specific modeling,language workbench},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2019136.2019195}
}

@Article{TorrecillaSalinas201692,
	Title = {{Agile, Web Engineering and Capability Maturity Model Integration: A systematic literature review.}},
	Author = {Torrecilla-Salinas, C J and Sede{\~{n}}o, J and Escalona, M J and Mej{\'{i}}as, M},
	Journal = {Information and Software Technology},
	Year = {2016},
	Pages = {92--107},
	Volume = {71},
	Abstract = {AbstractContext Agile approaches are an alternative for organizations developing software, particularly for those who develop Web applications. Besides, {\{}CMMI{\}} (Capability Maturity Model Integration) models are well-established approaches focused on assessing the maturity of an organization that develops software. Web Engineering is the field of Software Engineering responsible for analyzing and studying the specific characteristics of the Web. The suitability of an Agile approach to help organizations reach a certain {\{}CMMI{\}} maturity level in Web environments will be very interesting, as they will be able to keep the ability to quickly react and adapt to changes as long as their development processes get mature. Objective This paper responds to whether it is feasible or not, for an organization developing Web systems, to achieve a certain maturity level of the CMMI-DEV model using Agile methods. Method The proposal is analyzed by means of a systematic literature review of the relevant approaches in the field, defining a characterization schema in order to compare them to introduce the current state-of-the-art. Results The results achieved after the systematic literature review are presented, analyzed and compared against the defined schema, extracting relevant conclusions for the different dimensions of the problem: compatibility, compliance, experience, maturity and Web. Conclusion It is concluded that although the definition of an Agile approach to meet the different {\{}CMMI{\}} maturity levels goals could be possible for an organization developing Web systems, there is still a lack of detailed studies and analysis on the field. },
	Doi = {https://doi.org/10.1016/j.infsof.2015.11.002},
	ISSN = {0950-5849},
	Keywords = {Agile,CMMI,Scrum,Software Engineering,Web Engineering},
	Url = {http://www.sciencedirect.com/science/article/pii/S095058491500186X}
}

@InProceedings{Trask,
	Title = {{Using Model-Driven Engineering to Complement Software Product Line Engineering in Developing Software Defined Radio Components and Applications}},
	Author = {Trask, B. and Roman, A. and Paniscotti, D. and Bhanot, V.},
	Booktitle = {10th International Software Product Line Conference (SPLC'06)},
	Pages = {192--202},
	Publisher = {IEEE},
	Doi = {10.1109/SPLINE.2006.1691591},
	ISBN = {0-7695-2599-7},
	Url = {http://ieeexplore.ieee.org/document/1691591/}
}

@Article{Trinidad2008883,
	Title = {{Automated error analysis for the agilization of feature modeling}},
	Author = {Trinidad, P and Benavides, D and Dur{\'{a}}n, A and Ruiz-Cort{\'{e}}s, A and Toro, M},
	Journal = {Journal of Systems and Software},
	Year = {2008},
	Number = {6},
	Pages = {883--896},
	Volume = {81},
	Abstract = {Software Product Lines (SPL) and agile methods share the common goal of rapidly developing high-quality software. Although they follow different approaches to achieve it, some synergies can be found between them by (i) applying agile techniques to SPL activities so SPL development becomes more agile; and (ii) tailoring agile methodologies to support the development of SPL. Both options require an intensive use of feature models, which are usually strongly affected by changes on requirements. Changing large-scale feature models as a consequence of changes on requirements is a well-known error-prone activity. Since one of the objectives of agile methods is a rapid response to changes in requirements, it is essential an automated error analysis support in order to make SPL development more agile and to produce error-free feature models. As a contribution to find the intended synergies, this article sets the basis to provide an automated support to feature model error analysis by means of a framework which is organized in three levels: a feature model level, where the problem of error treatment is described; a diagnosis level, where an abstract solution that relies on Reiter's theory of diagnosis is proposed; and an implementation level, where the abstract solution is implemented by using Constraint Satisfaction Problems (CSP). To show an application of our proposal, a real case study is presented where the Feature-Driven Development (FDD) methodology is adapted to develop an SPL. Current proposals on error analysis are also studied and a comparison among them and our proposal is provided. Lastly, the support of new kinds of errors and different implementation levels for the proposed framework are proposed as the focus of our future work. {\textcopyright} 2007 Elsevier Inc. All rights reserved.},
	Annote = {cited By 62},
	Doi = {10.1016/j.jss.2007.10.030},
	Keywords = {Agile manufacturing systems,Agile methods; Constraint programming; Feature mo,Automation; Constraint theory; Error analysis; Lar},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-42049110531{\&}doi=10.1016{\%}2Fj.jss.2007.10.030{\&}partnerID=40{\&}md5=37e7cc86948271c403360a1535f92809}
}

@InProceedings{Trujillo:2011:TVS:2019136.2019166,
	Title = {{Towards Variability Support for Security and Dependability Patterns: A Case Study}},
	Author = {Trujillo, Salvador and Alonso, I{\~{n}}aki and Hamid, Brahim and Gonzalez, David and Blanco, Manuel and Zhang, Huaxi (Yulin)},
	Booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
	Year = {2011},
	Address = {New York, NY, USA},
	Pages = {27:1----27:4},
	Publisher = {ACM},
	Series = {SPLC '11},
	Doi = {10.1145/2019136.2019166},
	ISBN = {978-1-4503-0789-5},
	Keywords = {dependability,embedded systems,model driven engineering,pattern,security,variability},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2019136.2019166}
}

@Article{Turner201425,
	Title = {{{\{}DRE{\}} system performance optimization with the {\{}SMACK{\}} cache efficiency metric}},
	Author = {Turner, Hamilton and Dougherty, Brian and White, Jules and Kegley, Russell and Preston, Jonathan and Schmidt, Douglas C and Gokhale, Aniruddha},
	Journal = {Journal of Systems and Software},
	Year = {2014},
	Pages = {25--43},
	Volume = {98},
	Abstract = {Abstract System performance improvements are critical for the resource-limited environment of multiple integrated applications executing inside a single distributed real-time and embedded (DRE) system, such as integrated avionics platform or vehtronics systems. While processor caches can effectively reduce execution time there are several factors, such as cache size, system data sharing, and task execution schedule, which make it hard to quantify, predict, and optimize the cache usage of a {\{}DRE{\}} system. This article presents SMACK, a novel heuristic for estimating the hardware cache usage of a {\{}DRE{\}} system, and describes a method of varying the runtime behavior of {\{}DRE{\}} system software without (1) requiring extensive safety recertification or (2) violating the real-time scheduling deadlines. By using {\{}SMACK{\}} as a maximization target, we were able to reduce integrated {\{}DRE{\}} system execution time by an average of 2.4{\%} and a maximum of 4.34{\%}. },
	Doi = {https://doi.org/10.1016/j.jss.2014.08.031},
	ISSN = {0164-1212},
	Keywords = {Cache,DRE,Deployment,Heuristic,Optimization},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121214001836}
}

@Article{Ubayashi20132331,
	Title = {{Context-dependent product line engineering with lightweight formal approaches}},
	Author = {Ubayashi, Naoyasu and Nakajima, Shin and Hirayama, Masayuki},
	Journal = {Science of Computer Programming},
	Year = {2013},
	Number = {12},
	Pages = {2331--2346},
	Volume = {78},
	Abstract = {This paper proposes a new style of product line engineering methods. It focuses on constructing embedded systems that take into account the contexts such as the external physical environments. In current product line development projects, Feature Analysis is mainly conducted from the viewpoint of system configurations: how hardware and software components are configured to constitute a system. In most cases, contexts are not considered explicitly. As a result, unexpected and unfavorable behavior might emerge in a system if a developer does not recognize any possible conflicting combinations between the system and contexts. To deal with this problem, this paper provides the notion of a context-dependent product line, which is composed of the system and context lines. The former is obtained by analyzing a family of systems. The latter is obtained by analyzing features of contexts associated to the systems. The system and context lines contain reusable core assets. The configuration of selected system components and contexts can be formally checked at the specification level. In this paper, we show a development process that includes the creation of both product line assets as well as context assets. },
	Annote = {Special Section on International Software Product Line Conference 2010 and Fundamentals of Software Engineering (selected papers of {\{}FSEN{\}} 2011)},
	Doi = {https://doi.org/10.1016/j.scico.2012.06.006},
	ISSN = {0167-6423},
	Keywords = {Context analysis,Formal methods,Product line engineering},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642312001177}
}

@Article{Ullah20102496,
	Title = {{Decision support for moving from a single product to a product portfolio in evolving software systems}},
	Author = {Ullah, Muhammad Irfan and Ruhe, G{\"{u}}nther and Garousi, Vahid},
	Journal = {Journal of Systems and Software},
	Year = {2010},
	Number = {12},
	Pages = {2496--2512},
	Volume = {83},
	Abstract = {Successful software systems continuously evolve to accommodate ever-changing needs of customers. Accommodating the feature requests of all the customers in a single product increases the risks and costs of software maintenance. A possible approach to mitigate these risks is to transition the evolving software system (ESS) from a single system to a portfolio of related product variants, each addressing a specific customers' segment. This evolution should be conducted such that the extent of modifications required in ESS's structure is reduced. The proposed method COPE+ uses preferences of customers on product features to generate multiple product portfolios each containing one product variant per segment of customers. Recommendations are given to the decision maker to update the product portfolios based on structural analysis of ESS. Product portfolios are compared with the {\{}ESS{\}} using statechart representations to identify the level of similarity in their behaviors. A proof of concept is presented by application to an open-source text editing system. Structural and behavioral analysis of candidate portfolios helped the decision maker to select one portfolio out of three candidates. },
	Annote = {{\{}TAIC{\}} {\{}PART{\}} 2009 - Testing: Academic {\&} Industrial Conference - Practice And Research Techniques},
	Doi = {https://doi.org/10.1016/j.jss.2010.07.049},
	ISSN = {0164-1212},
	Keywords = {Behavioral analysis,Decision support,Open-source systems,Software product evolution,Software product lines,Software product management},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121210002062}
}

@Article{Unphon20102211,
	Title = {{Software architecture awareness in long-term software product evolution}},
	Author = {Unphon, Hataichanok and Dittrich, Yvonne},
	Journal = {Journal of Systems and Software},
	Year = {2010},
	Number = {11},
	Pages = {2211--2226},
	Volume = {83},
	Abstract = {Software architecture has been established in software engineering for almost 40 years. When developing and evolving software products, architecture is expected to be even more relevant compared to contract development. However, the research results seem not to have influenced the development practice around software products very much. The architecture often only exists implicitly in discussions that accompany the development. Nonetheless many of the software products have been used for over 10, or even 20 years. How do development teams manage to accommodate changing needs and at the same time maintain the quality of the product? In order to answer this question, grounded theory study based on 15 semi-structured interviews was conducted in order to find out about the wide spectrum of architecture practices in software product developing organisations. Our results indicate that a chief architect or central developer acts as a ‘walking architecture' devising changes and discussing local designs while at the same time updating his own knowledge about problematic aspects that need to be addressed. Architecture documentation and representations might not be used, especially if they replace the feedback from on-going developments into the ‘architecturing' practices. Referring to results from Computer Supported Cooperative Work, we discuss how explicating the existing architecture needs to be complemented by social protocols to support the communication and knowledge sharing processes of the ‘walking architecture'. },
	Annote = {Interplay between Usability Evaluation and Software Development},
	Doi = {https://doi.org/10.1016/j.jss.2010.06.043},
	ISSN = {0164-1212},
	Keywords = {Architecture knowledge management,Cooperative and human aspects,Long-term evolution,Qualitative empirical studies,Software architecture,Software products},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121210001743}
}

@Article{SMR:SMR1637,
	Title = {{A conceptual framework for SPI evaluation}},
	Author = {Unterkalmsteiner, Michael and Gorschek, Tony and Islam, A K M Moinul and Cheng, Chow Kian and Permadi, Rahadian Bayu and Feldt, Robert},
	Journal = {Journal of Software: Evolution and Process},
	Year = {2014},
	Number = {2},
	Pages = {251--279},
	Volume = {26},
	Abstract = {Software Process Improvement (SPI) encompasses the analysis and modification of the processes within software development, aimed at improving key areas that contribute to the organizations' goals. The task of evaluating whether the selected improvement path meets these goals is challenging. On the basis of the results of a systematic literature review on SPI measurement and evaluation practices, we developed a framework (SPI Measurement and Evaluation Framework (SPI-MEF)) that supports the planning and implementation of SPI evaluations. SPI-MEF guides the practitioner in scoping the evaluation, determining measures, and performing the assessment. SPI-MEF does not assume a specific approach to process improvement and can be integrated in existing measurement programs, refocusing the assessment on evaluating the improvement initiative's outcome. Sixteen industry and academic experts evaluated the framework's usability and capability to support practitioners, providing additional insights that were integrated in the application guidelines of the framework. Copyright {\textcopyright} 2013 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/smr.1637},
	ISSN = {2047-7481},
	Keywords = {Software Process Improvement,software measurement,software process evaluation},
	Url = {http://dx.doi.org/10.1002/smr.1637}
}

@Article{Usman20171,
	Title = {{A product-line model-driven engineering approach for generating feature-based mobile applications}},
	Author = {Usman, M and Iqbal, M Z and Khan, M U},
	Journal = {Journal of Systems and Software},
	Year = {2017},
	Pages = {1--32},
	Volume = {123},
	Abstract = {A significant challenge faced by the mobile application industry is developing and maintaining multiple native variants of mobile applications to support different mobile operating systems, devices and varying application functional requirements. The current industrial practice is to develop and maintain these variants separately. Any potential change has to be applied across variants manually, which is neither efficient nor scalable. We consider the problem of supporting multiple platforms as a ‘software product-line engineering' problem. The paper proposes a novel application of product-line model-driven engineering to mobile application development and addresses the key challenges of feature-based native mobile application variants for multiple platforms. Specifically, we deal with three types of variations in mobile applications: variation due to operation systems and their versions, software and hardware capabilities of mobile devices, and functionalities offered by the mobile application. We develop a tool MOPPET that automates the proposed approach. Finally, the results of applying the approach on two industrial case studies show that the proposed approach is applicable to industrial mobile applications and have potential to significantly reduce the development effort and time. {\textcopyright} 2016 Elsevier Inc.},
	Annote = {cited By 0},
	Doi = {10.1016/j.jss.2016.09.049},
	Keywords = {Application programs,Computer software; Mobile computing; Mobile device,Feature modeling; Functional requirement; Industr},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991221546{\&}doi=10.1016{\%}2Fj.jss.2016.09.049{\&}partnerID=40{\&}md5=ed8bb39053d6229343132f57b665dd57}
}

@Article{STVR:STVR456,
	Title = {{A taxonomy of model-based testing approaches}},
	Author = {Utting, Mark and Pretschner, Alexander and Legeard, Bruno},
	Journal = {Software Testing, Verification and Reliability},
	Year = {2012},
	Number = {5},
	Pages = {297--312},
	Volume = {22},
	Abstract = {Model-based testing (MBT) relies on models of a system under test and/or its environment to derive test cases for the system. This paper discusses the process of MBT and defines a taxonomy that covers the key aspects of MBT approaches. It is intended to help with understanding the characteristics, similarities and differences of those approaches, and with classifying the approach used in a particular MBT tool. To illustrate the taxonomy, a description of how three different examples of MBT tools fit into the taxonomy is provided. Copyright {\textcopyright} 2011 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/stvr.456},
	ISSN = {1099-1689},
	Keywords = {model-based testing approaches,survey,taxonomy},
	Publisher = {John Wiley {\&} Sons, Ltd},
	Url = {http://dx.doi.org/10.1002/stvr.456}
}

@Article{Uzunov2015217,
	Title = {{A comprehensive pattern-oriented approach to engineering security methodologies}},
	Author = {Uzunov, Anton V and Falkner, Katrina and Fernandez, Eduardo B},
	Journal = {Information and Software Technology},
	Year = {2015},
	Pages = {217--247},
	Volume = {57},
	Abstract = {AbstractContext Developing secure software systems is an issue of ever-growing importance. Researchers have generally come to acknowledge that to develop such systems successfully, their security features must be incorporated in the context of a systematic approach: a security methodology. There are a number of such methodologies in the literature, but no single security methodology is adequate for every situation, requiring the construction of “fit-to-purpose? methodologies or the tailoring of existing methodologies to the project specifics at hand. While a large body of research exists addressing the same requirement for development methodologies – constituting the field of Method Engineering – there is nothing comparable for security methodologies as such; in fact, the topic has never been studied before in such a context. Objective In this paper we draw inspiration from a number of Method Engineering ideas and fill the latter gap by proposing a comprehensive approach to engineering security methodologies. Method Our approach is embodied in three interconnected parts: a framework of interrelated security process patterns; a security-specific meta-model; and a meta-methodology to guide engineers in using the latter artefacts in a step-wise fashion. A UML-inspired notation is used for representing all pattern-based methodology models during design and construction. The approach is illustrated and evaluated by tailoring an existing, real-life security methodology to a distributed-system-specific project situation. Results The paper proposes a novel pattern-oriented approach to modeling, constructing, tailoring and combining security methodologies, which is the very first and currently sole such approach in the literature. We illustrate and evaluate our approach in an academic setting, and perform a feature analysis to highlight benefits and deficiencies. Conclusion Using our proposal, developers, architects and researchers can analyze and engineer security methodologies in a structured, systematic fashion, taking into account all security methodology aspects. },
	Doi = {https://doi.org/10.1016/j.infsof.2014.09.001},
	ISSN = {0950-5849},
	Keywords = {Method engineering,Modeling,Process patterns,Secure software engineering,Security methodologies,Software security},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584914002006}
}

@Article{Uzunov2015112,
	Title = {{ASE: A comprehensive pattern-driven security methodology for distributed systems}},
	Author = {Uzunov, Anton V and Fernandez, Eduardo B and Falkner, Katrina},
	Journal = {Computer Standards {\&} Interfaces},
	Year = {2015},
	Pages = {112--137},
	Volume = {41},
	Abstract = {Abstract Incorporating security features is one of the most important and challenging tasks in designing distributed systems. Over the last decade, researchers and practitioners have come to recognize that the incorporation of security features should proceed by means of a structured, systematic approach, combining principles from both software and security engineering. Such systematic approaches, particularly those implying some sort of process aligned with the development life-cycle, are termed security methodologies. There are a number of security methodologies in the literature, of which the most flexible and, according to a recent survey, most satisfactory from an industry-adoption viewpoint are methodologies that encapsulate their security solutions in some fashion, especially via the use of security patterns. While the literature does present several mature pattern-driven security methodologies with either a general or a highly specific system applicability, there are currently no (pattern-driven) security methodologies specifically designed for general distributed systems. Going further, there are also currently no methodologies with mixed specific applicability, e.g. for both general and peer-to-peer distributed systems. In this paper we aim to fill these gaps by presenting a comprehensive pattern-driven security methodology – arrived at by applying a previously devised approach to engineering security methodologies – specifically designed for general distributed systems, which is also capable of taking into account the specifics of peer-to-peer systems as needed. Our methodology takes the principle of encapsulation several steps further, by employing patterns not only for the incorporation of security features (via security solution frames), but also for the modeling of threats, and even as part of its process. We illustrate and evaluate the presented methodology in detail via a realistic example – the development of a distributed system for file sharing and collaborative editing. In both the presentation of the methodology and example our focus is on the early life-cycle phases (analysis and design). },
	Doi = {https://doi.org/10.1016/j.csi.2015.02.011},
	ISSN = {0920-5489},
	Keywords = {Distributed systems security,Secure software engineering,Security methodologies,Security patterns,Security solution frames},
	Url = {http://www.sciencedirect.com/science/article/pii/S0920548915000276}
}

@Article{Vale20171,
	Title = {{Software product lines traceability: A systematic mapping study}},
	Author = {Vale, Tassio and de Almeida, Eduardo Santana and Alves, Vander and Kulesza, Uir{\'{a}} and Niu, Nan and de Lima, Ricardo},
	Journal = {Information and Software Technology},
	Year = {2017},
	Pages = {1--18},
	Volume = {84},
	Abstract = {Abstract Context: Traceability in Software Product Lines (SPL) is the ability to interrelate software engineering artifacts through required links to answer specific questions related to the families of products and underlying development processes. Despite the existence of studies to map out available evidence on traceability for single systems development, there is a lack of understanding on common strategies, activities, artifacts, and research gaps for {\{}SPL{\}} traceability. Objective: This paper analyzes 62 studies dating from 2001 to 2015 and discusses seven aspects of {\{}SPL{\}} traceability: main goals, strategies, application domains, research intensity, research challenges, rigor, and industrial relevance. In addition to the analysis, this paper also synthesizes the available evidence, identifies open issues and points out areas calling for further research. Method: To gather evidence, we defined a mapping study process adapted from existing guidelines. Driven by a set of research questions, this process comprises three major phases: planning, conducting, and documenting the review. Results: This work provides a structured understanding of {\{}SPL{\}} traceability, indicating areas for further research. The lack of evidence regarding the application of research methods indicates the need for more rigorous {\{}SPL{\}} traceability studies with better description of context, study design, and limitations. For practitioners, although most identified studies have low industrial relevance, a few of them have high relevance and thus could provide some decision making support for application of {\{}SPL{\}} traceability in practice. Conclusions: This work concludes that {\{}SPL{\}} traceability is maturing and pinpoints areas where further investigation should be performed. As future work, we intend to improve the comparison between traceability proposals for {\{}SPL{\}} and single-system development. },
	Doi = {https://doi.org/10.1016/j.infsof.2016.12.004},
	ISSN = {0950-5849},
	Keywords = {Software and systems traceability,Software product lines,Software reuse,Systematic mapping study},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584916304463}
}

@Article{Vale2016128,
	Title = {{Twenty-eight years of component-based software engineering}},
	Author = {Vale, Tassio and Crnkovic, Ivica and de Almeida, Eduardo Santana and {da Mota Silveira Neto}, Paulo Anselmo and Cavalcanti, Yguarat{\~{a}} Cerqueira and {de Lemos Meira}, Silvio Romero},
	Journal = {Journal of Systems and Software},
	Year = {2016},
	Pages = {128--148},
	Volume = {111},
	Abstract = {Abstract The idea of developing software components was envisioned more than forty years ago. In the past two decades, Component-Based Software Engineering (CBSE) has emerged as a distinguishable approach in software engineering, and it has attracted the attention of many researchers, which has led to many results being published in the research literature. There is a huge amount of knowledge encapsulated in conferences and journals targeting this area, but a systematic analysis of that knowledge is missing. For this reason, we aim to investigate the state-of-the-art of the {\{}CBSE{\}} area through a detailed literature review. To do this, 1231 studies dating from 1984 to 2012 were analyzed. Using the available evidence, this paper addresses five dimensions of CBSE: main objectives, research topics, application domains, research intensity and applied research methods. The main objectives found were to increase productivity, save costs and improve quality. The most addressed application domains are homogeneously divided between commercial-off-the-shelf (COTS), distributed and embedded systems. Intensity of research showed a considerable increase in the last fourteen years. In addition to the analysis, this paper also synthesizes the available evidence, identifies open issues and points out areas that call for further research. },
	Doi = {https://doi.org/10.1016/j.jss.2015.09.019},
	ISSN = {0164-1212},
	Keywords = {Component-based software development,Component-based software engineering,Software component,Systematic mapping study},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121215002095}
}

@Article{SMR:SMR574,
	Title = {{Supporting software architects to improve their software system's decomposition – lessons learned}},
	Author = {Vanya, Adam and Klusener, Steven and Premraj, Rahul and van Vliet, Hans},
	Journal = {Journal of Software: Evolution and Process},
	Year = {2013},
	Number = {3},
	Pages = {219--232},
	Volume = {25},
	Abstract = {The architect of a large, evolving system may wish to revise its decomposition from time to time; for instance, because the structure has deteriorated over time, certain components need to be outsourced to another site. One way to assess the current decomposition is to consider the past evolution, searching for components that often changed together. We iteratively devised and implemented a process for doing so at Philips Healthcare MRI. In this paper, we describe the lessons learned on how to effectively support architects to improve their system decomposition. Copyright {\textcopyright} 2011 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/smr.574},
	ISSN = {2047-7481},
	Keywords = {lessons learned,mining software repositories,software architecture,software evolution},
	Publisher = {John Wiley {\&} Sons, Ltd},
	Url = {http://dx.doi.org/10.1002/smr.574}
}

@Article{VarelaVaca20131948,
	Title = {{Towards the automatic and optimal selection of risk treatments for business processes using a constraint programming approach}},
	Author = {Varela-Vaca, Angel Jesus and Gasca, Rafael M},
	Journal = {Information and Software Technology},
	Year = {2013},
	Number = {11},
	Pages = {1948--1973},
	Volume = {55},
	Abstract = {AbstractContext The use of Business Process Management Systems (BPMS) has emerged in the {\{}IT{\}} arena for the automation of business processes. In the majority of cases, the issue of security is overlooked by default in these systems, and hence the potential cost and consequences of the materialization of threats could produce catastrophic loss for organizations. Therefore, the early selection of security controls that mitigate risks is a real and important necessity. Nevertheless, there exists an enormous range of {\{}IT{\}} security controls and their configuration is a human, manual, time-consuming and error-prone task. Furthermore, configurations are carried out separately from the organization perspective and involve many security stakeholders. This separation makes difficult to ensure the effectiveness of the configuration with regard to organizational requirements. Objective In this paper, we strive to provide security stakeholders with automated tools for the optimal selection of {\{}IT{\}} security configurations in accordance with a range of business process scenarios and organizational multi-criteria. Method An approach based on feature model analysis and constraint programming techniques is presented, which enable the automated analysis and selection of optimal security configurations. Results A catalogue of feature models is determined by analyzing typical {\{}IT{\}} security controls for {\{}BPMSs{\}} for the enforcement of the standard goals of security: integrity, confidentiality, availability, authorization, and authentication. These feature models have been implemented through constraint programs, and Constraint Programming techniques based on optimized and non-optimized searches are used to automate the selection and generation of configurations. In order to compare the results of the determination of configuration a comparative analysis is given. Conclusion In this paper, we present innovative tools based on feature models, Constraint Programming and multi-objective techniques that enable the agile, adaptable and automatic selection and generation of security configurations in accordance with the needs of the organization. },
	Doi = {https://doi.org/10.1016/j.infsof.2013.05.007},
	ISSN = {0950-5849},
	Keywords = {Business process,Business process management systems,Constraint programming,Feature model,Risk treatments,Security},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584913001286}
}

@Article{SEC:SEC819,
	Title = {{Integrating security mechanisms into embedded systems by domain-specific modelling}},
	Author = {Vasilevskaya, Maria and Gunawan, Linda Ariani and Nadjm-Tehrani, Simin and Herrmann, Peter},
	Journal = {Security and Communication Networks},
	Year = {2014},
	Number = {12},
	Pages = {2815--2832},
	Volume = {7},
	Abstract = {Embedded devices are crucial enablers of the Internet of Things and become increasingly common in our daily life. They store, manipulate and transmit sensitive information and, therefore, must be protected against security threats. Due to the security and also resource constraint concerns, designing secure networked embedded systems is a difficult task. Model-based development (MBD) is promoted to address complexity and ease the design of software intensive systems. We leverage MBD and domain-specific modelling to characterise common issues related to security and embedded systems that are specific to a given application domain. Security-specific knowledge relevant for a certain application domain is represented in the form of an adapted information security ontology. Further, the elements of the ontology are associated with security building blocks modelled with the MBD method SPACE. The selection of relevant security building blocks is based on (i) assets automatically elicited from the functional models, (ii) domain security knowledge captured by the security expert and (iii) the platform adopted by the embedded system engineer. A tool is developed to support the steps supporting this methodology and help to bridge between the security and embedded systems domains. We illustrate our approach with a case study from the smart metering domain. {\textcopyright} 2013 The Authors. Security and Communication Networks published by John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/sec.819},
	ISSN = {1939-0122},
	Keywords = {domain-specific modelling,embedded systems,model-based engineering,security engineering,security ontology,smart metering},
	Url = {http://dx.doi.org/10.1002/sec.819}
}

@Article{EXSY:EXSY533,
	Title = {{A case-based reasoning approach to derive object-oriented models from software architectures}},
	Author = {Vazquez, German L and D{\'{i}}az-Pace, J Andres and Campo, Marcelo R},
	Journal = {Expert Systems},
	Year = {2010},
	Number = {4},
	Pages = {267--290},
	Volume = {27},
	Abstract = {Abstract: Software architectures are very important to capture early design decisions and reason about quality attributes of a system. Unfortunately, there are mismatches between the quality attributes prescribed by the architecture and those realized by its object-oriented implementation. The mismatches decrease the ability to reason architecturally about the system. Developing an object-oriented materialization that conforms to the original architecture depends on both the application of the right patterns and the developer's expertise. Since the space of allowed materializations can be really large, tool support for assisting the developer in the exploration of alternative materializations is of great help. In previous research, we developed a prototype for generating quality-preserving implementations of software architectures, using pre-compiled knowledge about architectural styles and frameworks. In this paper, we present a more flexible approach, called SAME, which focuses on the architectural connectors as the pillars for the materialization process. The SAME design assistant applies a case-based reasoning (CBR) metaphor to deal with connector-related materialization experiences and quality attributes. The CBR engine is able to recall and adapt past experiences to solve new materialization problems; thus SAME can take advantage of developers' knowledge. Preliminary experiments have shown that this approach can improve the exploration of object-oriented solutions that are still faithful to the architectural prescriptions.},
	Doi = {10.1111/j.1468-0394.2010.00533.x},
	ISSN = {1468-0394},
	Keywords = {architecture and object-oriented design,automated design assistance,case-based reasoning,quality attributes},
	Publisher = {Blackwell Publishing Ltd},
	Url = {http://dx.doi.org/10.1111/j.1468-0394.2010.00533.x}
}

@Article{COIN:COIN415,
	Title = {{PERFORMANCE IMPROVEMENT USING ADAPTIVE LEARNING ITINERARIES}},
	Author = {Vazquez, Jose Manuel Marquez and Gonzalez-Abril, Luis and Morente, Francisco Velasco and Ramirez, Juan Antonio Ortega},
	Journal = {Computational Intelligence},
	Year = {2012},
	Number = {2},
	Pages = {234--260},
	Volume = {28},
	Abstract = {In this paper, Bayesian-Networks (BN) and Ant Colony Optimization (ACO) techniques are combined to find the best path through a graph representing all available itineraries to acquire a professional competence. The combination of these methods allows us to design a dynamic learning path, useful in a rapidly changing world. One of the most important advances in this work is that the amount of pheromones released is variable. This amount is calculated by taking into account the results acquired in the last completed course in relation to the minimum score required.By using ACO and BN, a fitness function, responsible of automatically selecting the next course in the learning graph, is defined. This is done by generating a path that maximizes the probability of each user's success in the course. Therefore, the path can change to improve learners' average performance, taking into account the pedagogical weight of each learning unit and the social behavior of the system. Furthermore, a discrete dynamical system is obtained and its stability is studied. How to wrap an existing Learning Management System is also described in this work. Finally, an experiment compares this approach with the old on-line learning system being used previously.},
	Doi = {10.1111/j.1467-8640.2012.00415.x},
	ISSN = {1467-8640},
	Keywords = {Ant Colony Optimization,Bayesian network,adaptivity,e-learning,features model},
	Publisher = {Blackwell Publishing Inc},
	Url = {http://dx.doi.org/10.1111/j.1467-8640.2012.00415.x}
}

@InProceedings{Veado:2016:TTD:2915970.2916014,
	Title = {{TDTool: Threshold Derivation Tool}},
	Author = {Veado, Lucas and Vale, Gustavo and Fernandes, Eduardo and Figueiredo, Eduardo},
	Booktitle = {Proceedings of the 20th International Conference on Evaluation and Assessment in Software Engineering},
	Year = {2016},
	Address = {New York, NY, USA},
	Pages = {24:1----24:5},
	Publisher = {ACM},
	Series = {EASE '16},
	Doi = {10.1145/2915970.2916014},
	ISBN = {978-1-4503-3691-8},
	Keywords = {metrics,software systems,thresholds},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2915970.2916014}
}

@Article{Veerman2006287,
	Title = {{Automated mass maintenance of a software portfolio}},
	Author = {Veerman, Niels},
	Journal = {Science of Computer Programming},
	Year = {2006},
	Number = {3},
	Pages = {287--317},
	Volume = {62},
	Abstract = {This is an experience report on automated mass maintenance of a large Cobol software portfolio. A company in the financial services and insurance industry upgraded their database system to a new version, affecting their entire software portfolio. The database system was accessed by the portfolio of 45 systems, totalling nearly 3000 programs and covering over 4 million lines of Cobol code. We upgraded the programs to the new database version using several automatic tools, and we performed an automated analysis supporting further manual modifications by the system experts. The automatic tools were built using a combination of lexical and syntactic technology, and they were deployed in a mass update factory to allow large-scale application to the software portfolio. The updated portfolio has been accepted and taken into production by the company, serving over 600 employees with the new database version. In this paper, we discuss the automated upgrade from problem statement to project costs. },
	Annote = {Special issue on Source code analysis and manipulation (SCAM 2005)},
	Doi = {https://doi.org/10.1016/j.scico.2006.04.006},
	ISSN = {0167-6423},
	Keywords = {Automated maintenance,Automatic transformations,Cobol,Mass maintenance,Mass modification,Mass update,Reengineering,Software maintenance,Software portfolio,Tool-supported maintenance},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642306000827}
}

@Article{Veerman2005129,
	Title = {{Towards lightweight checks for mass maintenance transformations}},
	Author = {Veerman, Niels},
	Journal = {Science of Computer Programming},
	Year = {2005},
	Number = {2},
	Pages = {129--163},
	Volume = {57},
	Abstract = {We propose a lightweight, practical approach to check mass maintenance transformations. We present checks for both transformation tools and transformed source code, and illustrate them using examples of real-world transformations. Our approach is not a fully fledged, formal one but provides circumstantial evidence for transformation correctness, and has been applied to the mass maintenance of industrial Cobol systems. },
	Doi = {https://doi.org/10.1016/j.scico.2005.01.001},
	ISSN = {0167-6423},
	Keywords = {Lightweight checks,Mass maintenance,Transformations},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642305000134}
}

@Article{SEC:SEC1537,
	Title = {{Modelling of Internet of Things units for estimating security-energy-performance relationships for quality of service and environment awareness}},
	Author = {Venckauskas, Algimantas and Stuikys, Vytautas and Damasevicius, Robertas and Jusas, Nerijus},
	Journal = {Security and Communication Networks},
	Year = {2016},
	Number = {16},
	Pages = {3324--3339},
	Volume = {9},
	Abstract = {Complexity of Internet of Things (IoT) applications and difficulty to predict their behaviour in wireless communications make modelling of IoT units an important research topic. The IoT unit is considered here as the two-node IoT model that supports bi-directional wireless communications. There are many approaches to model security and energy awareness; however, little is known about the synergistic effect of those factors at the application level under the influence of environmental factors such as noise. The paper introduces a modelling framework to model the security-energy-environment issues as main attributes to allow defining quality of service (QoS) for the IoT-based applications. Among others, the healthcare ones are regarded as predominant now. We model IoT units using the feature-based modelling methodology adopted from the software engineering domain. The result of modelling is a set of feature models with valid configurations that describe the energy-security-environment-performance relationships and possible constraints to support various IoT applications. Having feature models, we can select a configuration that is best suited for a given IoT application with respect to QoS requirements. As feature models represent abstract relationships of domain factors, we need additionally to provide experiments to determine concrete values of modelled features. We describe the experimental system to measure energy consumption under the influencing factors. Both models (abstract and concrete) allow reasoning about QoS of the IoT applications. Copyright {\textcopyright} 2016 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/sec.1537},
	ISSN = {1939-0122},
	Keywords = {Internet of Things,energy consumption,feature-based model,quality of service,security},
	Url = {http://dx.doi.org/10.1002/sec.1537}
}

@Article{Verhoef2007247,
	Title = {{Quantifying the effects of IT-governance rules}},
	Author = {Verhoef, C},
	Journal = {Science of Computer Programming},
	Year = {2007},
	Number = {2–3},
	Pages = {247--277},
	Volume = {67},
	Abstract = {Via quantitative analyses of large IT-portfolio databases, we detected unique data patterns pointing to certain IT-governance rules and styles, plus their sometimes nonintuitive and negative side-effects. We grouped the most important patterns in seven categories and highlighted them separately. These patterns relate to the five fundamental parameters for IT-governance: data, control, time, cost and functionality. We revealed patterns of overperfect and heterogeneous data signifying reporting anomalies or ambiguous IT-governance rules, respectively. We also detected patterns of overregulation and underregulation, portending bloated control or no IT-control at all, both with negative side-effects: productivity loss, and too costly IT-development. Uniform management on time, cost or functionality showed clear patterns in the time and cost case, and more diffuse combined patterns for functionality. For these in total seven types of patterns, it was possible to take corrective measures to reduce unwanted side-effects, and/or amplify the intended purpose of the underlying IT-governance rules. These modifications ranged from refinements and additions, to eradications of IT-governance rules. For each of the seven patterns we provided lessons learned and recommendations on how to recognize and remove unwanted effects. Some effects were dangerous, and addressing them led to significant risk reduction and cost savings. },
	Doi = {https://doi.org/10.1016/j.scico.2007.01.010},
	ISSN = {0167-6423},
	Keywords = {Heterogeneous data,IT-governance,IT-governance rules,IT-portfolio analysis,Managing on budget,Managing on functionality,Managing on time,Overperfect data,Overregulation,Quantitative IT-governance,Seasonality effects,Time compression,Time decompression,Underregulation},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642307000780}
}

@Article{Viana20133123,
	Title = {{Domain-Specific Modeling Languages to improve framework instantiation}},
	Author = {Viana, Matheus C and Penteado, Ros{\^{a}}ngela A D and do Prado, Ant{\^{o}}nio F},
	Journal = {Journal of Systems and Software},
	Year = {2013},
	Number = {12},
	Pages = {3123--3139},
	Volume = {86},
	Abstract = {Abstract Frameworks are reusable software composed of concrete and abstract classes that implement the functionality of a domain. Applications reuse frameworks to enhance quality and development efficiency. However, frameworks are hard to learn and reuse. Application developers must understand the complex class hierarchy of the framework to instantiate it properly. In this paper, we present an approach to build a Domain-Specific Modeling Language (DSML) of a framework and use it to facilitate framework reuse during application development. The {\{}DSML{\}} of a framework is built by identifying the features of this framework and the information required to instantiate them. Application generators transform models created with the {\{}DSML{\}} into application code, hiding framework complexities. In this paper, we illustrate the use of our approach in a framework for the domain of business resource transactions and a experiment that evaluated the efficiency obtained with our approach. },
	Doi = {https://doi.org/10.1016/j.jss.2013.07.030},
	ISSN = {0164-1212},
	Keywords = {Domain-Specific Modeling Language,Framework,Reuse},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121213001775}
}

@Article{Vierhauser201689,
	Title = {{Requirements monitoring frameworks: A systematic review}},
	Author = {Vierhauser, Michael and Rabiser, Rick and Gr{\"{u}}nbacher, Paul},
	Journal = {Information and Software Technology},
	Year = {2016},
	Pages = {89--109},
	Volume = {80},
	Abstract = {AbstractContext Software systems today often interoperate with each other, thus forming a system of systems (SoS). Due to the scale, complexity, and heterogeneity of SoS, determining compliance with their requirements is challenging, despite the range of existing monitoring approaches. The fragmented research landscape and the diversity of existing approaches, however, make it hard to understand and analyze existing research regarding its suitability for SoS. Objective The aims of this paper are thus to systematically identify, describe, and classify existing approaches for requirements-based monitoring of software systems at runtime. Specifically, we (i) analyze the characteristics and application areas of monitoring approaches proposed in different domains, we (ii) systematically identify frameworks supporting requirements monitoring, and finally (iii) analyze their support for requirements monitoring in SoS. Method We performed a systematic literature review (SLR) to identify existing monitoring approaches and to classify their key characteristics and application areas. Based on this analysis we selected requirements monitoring frameworks, following a definition by Robinson, and analyzed them regarding their support for requirements monitoring in SoS. Results We identified 330 publications, which we used to produce a comprehensive overview of the landscape of requirements monitoring approaches. We analyzed these publications regarding their support for Robinson's requirements monitoring layers, resulting in 37 identified frameworks. We investigated how well these frameworks support requirements monitoring in SoS. Conclusions We conclude that most existing approaches are restricted to certain kinds of checks, particular types of events and data, and mostly also limited to one particular architectural style and technology. This lack of flexibility makes their application in an SoS context difficult. Also, systematic and automated variability management is still missing. Regarding their evaluation, many existing frameworks focus on measuring the performance overhead, while only few frameworks have been assessed in cases studies with real-world systems. },
	Doi = {https://doi.org/10.1016/j.infsof.2016.08.005},
	ISSN = {0950-5849},
	Keywords = {Requirements monitoring,Systematic literature review,Systems of systems},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584916301288}
}

@Article{Vierhauser2016123,
	Title = {{ReMinds : A flexible runtime monitoring framework for systems of systems}},
	Author = {Vierhauser, Michael and Rabiser, Rick and Gr{\"{u}}nbacher, Paul and Seyerlehner, Klaus and Wallner, Stefan and Zeisel, Helmut},
	Journal = {Journal of Systems and Software},
	Year = {2016},
	Pages = {123--136},
	Volume = {112},
	Abstract = {Abstract Many software-intensive systems today can be characterized as systems of systems (SoS) comprising complex, interrelated, and heterogeneous systems. The behavior of SoS often only emerges at runtime due to complex interactions between the involved systems and their environment. It is thus necessary to determine unexpected behavior by monitoring SoS at runtime, i.e., collecting and analyzing events and data at different layers and levels of granularity. Existing monitoring approaches are often limited to individual systems, particular architectural styles, or technologies. In this paper we thus derive challenges for monitoring SoS based on an industrial case. We present a flexible framework adaptable to different system architectures and technologies. We discuss its capabilities for instrumenting systems, collecting and persisting events and data, checking constraints on events and data, and visualizing the systems' behavior to users. We demonstrate the framework's flexibility by tailoring and applying it to an industrial SoS and assessing its performance and scalability. Our results show that the framework is flexible and scalable for monitoring an industrial SoS with realistic event loads. },
	Doi = {https://doi.org/10.1016/j.jss.2015.07.008},
	ISSN = {0164-1212},
	Keywords = {Framework,Runtime monitoring,System-of-systems architectures},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121215001478}
}

@Article{Vilela201768,
	Title = {{Integration between requirements engineering and safety analysis: A systematic literature review}},
	Author = {Vilela, J{\'{e}}ssyka and Castro, Jaelson and Martins, Luiz Eduardo G and Gorschek, Tony},
	Journal = {Journal of Systems and Software},
	Year = {2017},
	Pages = {68--92},
	Volume = {125},
	Abstract = {Abstract Context: Safety-Critical Systems (SCS) require more sophisticated requirements engineering (RE) approaches as inadequate, incomplete or misunderstood requirements have been recognized as a major cause in many accidents and safety-related catastrophes. Objective: In order to cope with the complexity of specifying {\{}SCS{\}} by RE, we investigate the approaches proposed to improve the communication or integration between {\{}RE{\}} and safety engineering in {\{}SCS{\}} development. We analyze the activities that should be performed by {\{}RE{\}} during safety analysis, the hazard/safety techniques it could use, the relationships between safety information that it should specify, the tools to support safety analysis as well as integration benefits between these areas. Method: We use a Systematic Literature Review (SLR) as the basis for our work. Results: We developed four taxonomies to help {\{}RE{\}} during specification of {\{}SCS{\}} that classify: techniques used in (1) hazard analysis; (2) safety analysis; (3) safety-related information and (4) a detailed set of information regarding hazards specification. Conclusions: This paper is a step towards developing a body of knowledge in safety concerns necessary to {\{}RE{\}} in the specification of {\{}SCS{\}} that is derived from a large-scale SLR. We believe the results will benefit both researchers and practitioners. },
	Doi = {https://doi.org/10.1016/j.jss.2016.11.031},
	ISSN = {0164-1212},
	Keywords = {Communication,Integration,Requirements engineering,Safety analysis,Safety-critical systems,Systematic literature review},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121216302333}
}

@Article{Villela2010113,
	Title = {{Evaluation of a method for proactively managing the evolving scope of a software product line}},
	Author = {Villela, K and D{\"{o}}rr, J and John, I},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2010},
	Pages = {113--127},
	Volume = {6182 LNCS},
	Abstract = {[Context and motivation] PLEvo-Scoping is a method intended to help Product Line (PL) scoping teams anticipate emergent features and distinguish unstable from stable features, with the aim of preparing their PL for likely future adaptation needs. [Question/problem]This paper describes a quasi-experiment performed to characterize PLEvo-Scoping in terms of adequacy and feasibility. [Principal ideas/results] This quasi-experiment was performed by two scoping teams in charge of scoping the same PL, where one scoping team applied first an existing PL scoping approach and then PLEvo-Scoping, while the other scoping team interweaved activities from both. The two approaches achieved similar results: The method could be applied in just one day, and it was considered adequate and feasible. [Contribution] Ideas on how to improve the method and its tool support have been obtained, and similar results are expected from other professionals facing the problem of evolution-centered PL scoping. However, further empirical studies should be performed. {\textcopyright} 2010 Springer-Verlag.},
	Annote = {cited By 5},
	Doi = {10.1007/978-3-642-14192-8_13},
	Keywords = {Biology; Computer software selection and evaluati,Empirical studies; Product Line Scoping; Product-l,Experiments},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955447396{\&}doi=10.1007{\%}2F978-3-642-14192-8{\_}13{\&}partnerID=40{\&}md5=8190e353b8258f66db2decb3c020113b}
}

@Article{VogelHeuser201554,
	Title = {{Evolution of software in automated production systems: Challenges and research directions}},
	Author = {Vogel-Heuser, Birgit and Fay, Alexander and Schaefer, Ina and Tichy, Matthias},
	Journal = {Journal of Systems and Software},
	Year = {2015},
	Pages = {54--84},
	Volume = {110},
	Abstract = {Abstract Coping with evolution in automated production systems implies a cross-disciplinary challenge along the system's life-cycle for variant-rich systems of high complexity. The authors from computer science and automation provide an interdisciplinary survey on challenges and state of the art in evolution of automated production systems. Selected challenges are illustrated on the case of a simple pick and place unit. In the first part of the paper, we discuss the development process of automated production systems as well as the different type of evolutions during the system's life-cycle on the case of a pick and place unit. In the second part, we survey the challenges associated with evolution in the different development phases and a couple of cross-cutting areas and review existing approaches addressing the challenges. We close with summarizing future research directions to address the challenges of evolution in automated production systems. },
	Doi = {https://doi.org/10.1016/j.jss.2015.08.026},
	ISSN = {0164-1212},
	Keywords = {Automated production systems,Automation,Evolution,Software engineering},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121215001818}
}

@Article{VogelHeuser201735,
	Title = {{Modularity and architecture of PLC-based software for automated production Systems: An analysis in industrial companies}},
	Author = {Vogel-Heuser, Birgit and Fischer, Juliane and Feldmann, Stefan and Ulewicz, Sebastian and R{\"{o}}sch, Susanne},
	Journal = {Journal of Systems and Software},
	Year = {2017},
	Pages = {35--62},
	Volume = {131},
	Abstract = {Abstract Adaptive and flexible production systems require modular and reusable software especially considering their long-term life cycle of up to 50 years. SWMAT4aPS, an approach to measure Software Maturity for automated Production Systems is introduced. The approach identifies weaknesses and strengths of various companies' solutions for modularity of software in the design of automated Production Systems (aPS). At first, a self-assessed questionnaire is used to evaluate a large number of companies concerning their software maturity. Secondly, we analyze {\{}PLC{\}} code, architectural levels, workflows and abilities to configure code automatically out of engineering information in four selected companies. In this paper, the questionnaire results from 16 German world-leading companies in machine and plant manufacturing and four case studies validating the results from the detailed analyses are introduced to prove the applicability of the approach and give a survey of the state of the art in industry. },
	Doi = {https://doi.org/10.1016/j.jss.2017.05.051},
	ISSN = {0164-1212},
	Keywords = {Automated production systems,Control software,Factory automation,Maturity,Modularity,Programmable logic controller},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121217300985}
}

@Article{Walker20132467,
	Title = {{Automatic optimisation of system architectures using EAST-ADL}},
	Author = {Walker, Martin and Reiser, Mark-Oliver and Tucci-Piergiovanni, Sara and Papadopoulos, Yiannis and L{\"{o}}nn, Henrik and Mraidha, Chokri and Parker, David and Chen, DeJiu and Servat, David},
	Journal = {Journal of Systems and Software},
	Year = {2013},
	Number = {10},
	Pages = {2467--2487},
	Volume = {86},
	Abstract = {Abstract There are many challenges which face designers of complex system architectures, particularly safety–critical or real-time systems. The introduction of Architecture Description Languages (ADLs) has helped to meet these challenges by consolidating information about a system and providing a platform for modelling and analysis capabilities. However, managing this wealth of information can still be problematic, and evaluation of potential design decisions is still often performed manually. Automatic architectural optimisation can be used to assist this decision process, enabling designers to rapidly explore many different options and evaluate them according to specific criteria. In this paper, we present a multi-objective optimisation approach based on EAST-ADL, an {\{}ADL{\}} in the automotive domain, with the goal of combining the advantages of {\{}ADLs{\}} and architectural optimisation. The approach is designed to be extensible and leverages the capabilities of EAST-ADL to provide support for evaluation according to different factors, including dependability, timing/performance, and cost. The technique is applied to an illustrative example system featuring both hardware and software perspectives, demonstrating the potential benefits of this concept to the design of embedded system architectures. },
	Doi = {https://doi.org/10.1016/j.jss.2013.04.001},
	ISSN = {0164-1212},
	Keywords = {Architectural description languages,Dependability Analysis,Multi-objective optimisation,Timing Analysis},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121213000885}
}

@Article{Wallin2012686,
	Title = {{Problems and their mitigation in system and software architecting}},
	Author = {Wallin, Peter and Larsson, Stig and Fr{\"{o}}berg, Joakim and Axelsson, Jakob},
	Journal = {Information and Software Technology},
	Year = {2012},
	Number = {7},
	Pages = {686--700},
	Volume = {54},
	Abstract = {Context Today, software and embedded systems act as enablers for developing new functionality in traditional industries such as the automotive, process automation, and manufacturing automation domains. This differs from 25–30 years ago when these systems where based on electronics and electro-mechanical solutions. The architecture of the embedded system and of the software is important to ensure the qualities of these applications. However, the effort of designing and evolving the architecture is in practice often neglected during system development, whilst development efforts are centered on implementing new functionality. Objective We present problems and success factors that are central to the architectural development of software intensive systems in the domain of automotive and automation products as judged by practitioners. Method The method consisted of three steps. First, we used semi-structured interviews to collect data in an exploratory manner. As a second step, a survey based on problems extracted from the interview data was used to investigate the occurrence of these problems at a wider range of organizations. In order to identify and suggest how to mitigate the problems that were considered important, we finally performed root cause analysis workshops, and from these a number of success factors were elicited. Results A total of 21 problems have been identified based on the interview data, and these are related to the technical, organizational, project, and agreement processes. Based on the survey results, the following four problems were selected for a root cause analysis: (1) there is a lack of process for architecture development, (2) there is a lack of method or model to evaluate the business value when choosing the architecture, (3) there is a lack of clear long-term architectural strategy, and (4) processes and methods are less valued than knowledge and competence of individuals. Conclusion In conclusion, the following identified success factors are crucial components to be successful in developing software intensive systems: (1) define an architectural strategy, (2) implement a process for architectural work, (3) ensure authority for architects, (4) clarify the business impact of the architecture, and (5) optimize on the project portfolio level instead of optimizing each project. },
	Doi = {https://doi.org/10.1016/j.infsof.2012.01.004},
	ISSN = {0950-5849},
	Keywords = {Embedded systems,Experience from practice,Success factors,System and software architecture},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584912000158}
}

@Article{SPE:SPE1128,
	Title = {{Policy-driven customization of cross-organizational features in distributed service systems}},
	Author = {Walraven, Stefan and Lagaisse, Bert and Truyen, Eddy and Joosen, Wouter},
	Journal = {Software: Practice and Experience},
	Year = {2013},
	Number = {10},
	Pages = {1145--1163},
	Volume = {43},
	Abstract = {In a cross-organizational context, software services are provided and consumed by different organizations. Ensuring that the non-functional requirements of all the involved organizations are satisfied is hard to achieve in such a distributed and heterogeneous environment: the implementation of features, for example, security, is scattered across the services of multiple organizations. In this paper, we present a coordination architecture for flexible and policy-driven composition of cross-organizational features in distributed service systems. The underlying approach of this architecture is to specify the features and their composition at a higher level that abstracts the internal implementation mechanisms of the organizations involved. By means of feature composition policies, the organizations specify at a fine-grained level which features are required and when they have to apply. Driven by these policies, our coordination middleware dynamically integrates the appropriate features throughout the cross-organizational service composition in a consistent and efficient way. We have validated our architecture in a proof of concept showing limited performance overhead. Copyright {\textcopyright} 2011 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spe.1128},
	ISSN = {1097-024X},
	Keywords = {cross-organizational,dynamic composition,feature-oriented,service engineering},
	Url = {http://dx.doi.org/10.1002/spe.1128}
}

@Article{Walraven201448,
	Title = {{Efficient customization of multi-tenant Software-as-a-Service applications with service lines}},
	Author = {Walraven, Stefan and Landuyt, Dimitri Van and Truyen, Eddy and Handekyn, Koen and Joosen, Wouter},
	Journal = {Journal of Systems and Software},
	Year = {2014},
	Pages = {48--62},
	Volume = {91},
	Abstract = {Abstract Application-level multi-tenancy is an architectural approach for Software-as-a-Service (SaaS) applications which enables high operational cost efficiency by sharing one application instance among multiple customer organizations (the so-called tenants). However, the focus on increased resource sharing typically results in a one-size-fits-all approach. In principle, the shared application instance satisfies only the requirements common to all tenants, without supporting potentially different and varying requirements of these tenants. As a consequence, multi-tenant SaaS applications are inherently limited in terms of flexibility and variability. This paper presents an integrated service engineering method, called service line engineering, that supports co-existing tenant-specific configurations and that facilitates the development and management of customizable, multi-tenant SaaS applications, without compromising scalability. Specifically, the method spans the design, implementation, configuration, composition, operations and maintenance of a SaaS application that bundles all variations that are based on a common core. We validate this work by illustrating the benefits of our method in the development of a real-world SaaS offering for document processing. We explicitly show that the effort to configure and compose an application variant for each individual tenant is significantly reduced, though at the expense of a higher initial development effort. },
	Doi = {https://doi.org/10.1016/j.jss.2014.01.021},
	ISSN = {0164-1212},
	Keywords = {Multi-tenancy,SaaS,Variability},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121214000326}
}

@Article{Walther20162,
	Title = {{On-the-fly construction of provably correct service compositions – templates and proofs}},
	Author = {Walther, Sven and Wehrheim, Heike},
	Journal = {Science of Computer Programming},
	Year = {2016},
	Pages = {2--23},
	Volume = {127},
	Abstract = {Abstract Today, service compositions often need to be assembled or changed on-the-fly, which leaves only little time for quality assurance. Moreover, quality assurance is complicated by service providers only giving information on their services in terms of domain specific concepts with only limited semantic meaning. In this paper, we propose a method for constructing service compositions based on pre-verified templates. Templates, given as workflow descriptions, are typed over a (domain-independent) template ontology defining concepts and predicates. Their meaning is defined by an abstract semantics, leaving the specific meaning of ontology concepts open, however, only up to given ontology rules. Templates are proven correct using a Hoare-style proof calculus, extended by a specific rule for service calls. Construction of service compositions amounts to instantiation of templates with domain-specific services. Correctness of an instantiation can then simply be checked by verifying that the domain ontology (a) adheres to the rules of the template ontology, and (b) fulfills the constraints of the employed template. },
	Annote = {Special issue of the 11th International Symposium on Formal Aspects of Component Software},
	Doi = {https://doi.org/10.1016/j.scico.2016.04.002},
	ISSN = {0167-6423},
	Keywords = {Correctness by construction,Hoare-calculus,Service compositions,Templates,Verification},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167642316300028}
}

@Article{IIS2:IIS203149,
	Title = {{3.3.1 A Generalized Systems Engineering Reuse Framework and Its Cost Estimating Relationship}},
	Author = {Wang, Gan and Roedler, Garry J and Pena, Mauricio and Valerdi, Ricardo},
	Journal = {INCOSE International Symposium},
	Year = {2014},
	Number = {1},
	Pages = {274--297},
	Volume = {24},
	Abstract = {This paper describes a Generalized Reuse Framework for systems development that consists of two interrelated and interacting processes – Development with Reuse (DWR) and Development for Reuse (DFR) – and a parametric cost estimating relationship defined for the framework in an extended form of COSYSMO, a systems engineering cost estimating model. It also discusses the Delphi process undertook to derive the model coefficients or the quantitative weights for the defined reuse categories. The framework, along with the quantitative estimating model, can be directly applied, in more ways than one, to planning, evaluating and managing reuse for system development efforts.},
	Doi = {10.1002/j.2334-5837.2014.tb03149.x},
	ISSN = {2334-5837},
	Url = {http://dx.doi.org/10.1002/j.2334-5837.2014.tb03149.x}
}

@Article{Wang2007117,
	Title = {{Verifying feature models using {\{}OWL{\}}}},
	Author = {Wang, Hai H and Li, Yuan Fang and Sun, Jing and Zhang, Hongyu and Pan, Jeff},
	Journal = {Web Semantics: Science, Services and Agents on the World Wide Web},
	Year = {2007},
	Number = {2},
	Pages = {117--129},
	Volume = {5},
	Abstract = {Feature models are widely used in domain engineering to capture common and variant features among systems in a particular domain. However, the lack of a formal semantics and reasoning support of feature models has hindered the development of this area. Industrial experiences also show that methods and tools that can support feature model analysis are badly appreciated. Such reasoning tool should be fully automated and efficient. At the same time, the reasoning tool should scale up well since it may need to handle hundreds or even thousands of features a that modern software systems may have. This paper presents an approach to modeling and verifying feature diagrams using Semantic Web {\{}OWL{\}} ontologies. We use {\{}OWL{\}} {\{}DL{\}} ontologies to precisely capture the inter-relationships among the features in a feature diagram. {\{}OWL{\}} reasoning engines such as FaCT++ are deployed to check for the inconsistencies of feature configurations fully automatically. Furthermore, a general {\{}OWL{\}} debugger has been developed to tackle the disadvantage of lacking debugging aids for the current {\{}OWL{\}} reasoner and to complement our verification approach. We also developed a {\{}CASE{\}} tool to facilitate visual development, interchange and reasoning of feature diagrams in the Semantic Web environment. },
	Annote = {Software Engineering and the Semantic Web},
	Doi = {https://doi.org/10.1016/j.websem.2006.11.006},
	ISSN = {1570-8268},
	Keywords = {Feature modeling,OWL,Ontologies,Semantic Web},
	Url = {http://www.sciencedirect.com/science/article/pii/S1570826807000042}
}

@InProceedings{Wang:2009:SMS:1566445.1566509,
	Title = {{Security Metrics for Software Systems}},
	Author = {Wang, Ju An and Wang, Hao and Guo, Minzhe and Xia, Min},
	Booktitle = {Proceedings of the 47th Annual Southeast Regional Conference},
	Year = {2009},
	Address = {New York, NY, USA},
	Pages = {47:1----47:6},
	Publisher = {ACM},
	Series = {ACM-SE 47},
	Doi = {10.1145/1566445.1566509},
	ISBN = {978-1-60558-421-8},
	Keywords = {security metrics,software quality,software security,software vulnerabilities},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/1566445.1566509}
}

@Article{SMR:SMR1593,
	Title = {{How developers perform feature location tasks: a human-centric and process-oriented exploratory study}},
	Author = {Wang, Jinshui and Peng, Xin and Xing, Zhenchang and Zhao, Wenyun},
	Journal = {Journal of Software: Evolution and Process},
	Year = {2013},
	Number = {11},
	Pages = {1193--1224},
	Volume = {25},
	Abstract = {Developers often have to locate the parts of source code that contribute to a specific feature during software maintenance tasks. This activity, referred to as feature location in software engineering, is a human-intensive and knowledge-intensive process. Researchers have investigated (semi-)automatic analysis-based techniques to assist developers in such feature location activities. However, little work has been carried out on better understanding how developers perform feature location tasks. In this paper, we report an exploratory study of feature location process, consisting of three experiments in which developers were given unfamiliar systems and asked to complete six feature location tasks. Our study suggests that feature location process can be understood hierarchically at three levels of granularity: phase, pattern, and action. Furthermore, our statistical analysis shows that these feature location phases, patterns, and actions can be effectively imparted to junior developers and consequently improve their performance on feature location tasks. Our qualitative observations and interviews also suggest that external factors, for example, human factors, task properties, and in-process feedbacks, affect the choices and usage of different feature location patterns and actions. Our results open up new opportunities to feature location research, which could lead to better tool support and more rigorous feature location process. Copyright {\textcopyright} 2013 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/smr.1593},
	ISSN = {2047-7481},
	Keywords = {cognitive process,conceptual framework,feature location,human study},
	Url = {http://dx.doi.org/10.1002/smr.1593}
}

@Article{PMIC:PMIC8003,
	Title = {{Open source libraries and frameworks for biological data visualisation: A guide for developers}},
	Author = {Wang, Rui and Perez-Riverol, Yasset and Hermjakob, Henning and Vizca{\'{i}}no, Juan Antonio},
	Journal = {PROTEOMICS},
	Year = {2015},
	Number = {8},
	Pages = {1356--1374},
	Volume = {15},
	Abstract = {Recent advances in high-throughput experimental techniques have led to an exponential increase in both the size and the complexity of the data sets commonly studied in biology. Data visualisation is increasingly used as the key to unlock this data, going from hypothesis generation to model evaluation and tool implementation. It is becoming more and more the heart of bioinformatics workflows, enabling scientists to reason and communicate more effectively. In parallel, there has been a corresponding trend towards the development of related software, which has triggered the maturation of different visualisation libraries and frameworks. For bioinformaticians, scientific programmers and software developers, the main challenge is to pick out the most fitting one(s) to create clear, meaningful and integrated data visualisation for their particular use cases. In this review, we introduce a collection of open source or free to use libraries and frameworks for creating data visualisation, covering the generation of a wide variety of charts and graphs. We will focus on software written in Java, JavaScript or Python. We truly believe this software offers the potential to turn tedious data into exciting visual stories.},
	Doi = {10.1002/pmic.201400377},
	ISSN = {1615-9861},
	Keywords = {Bioinformatics,Chart,Hierarchy,Network,Software library},
	Url = {http://dx.doi.org/10.1002/pmic.201400377}
}

@Article{Wang2015370,
	Title = {{Cost-effective test suite minimization in product lines using search techniques}},
	Author = {Wang, Shuai and Ali, Shaukat and Gotlieb, Arnaud},
	Journal = {Journal of Systems and Software},
	Year = {2015},
	Pages = {370--391},
	Volume = {103},
	Abstract = {Abstract Cost-effective testing of a product in a product line requires obtaining a set of relevant test cases from the entire test suite via test selection and minimization techniques. In this paper, we particularly focus on test minimization for product lines, which identifies and eliminates redundant test cases from test suites in order to reduce the total number of test cases to execute, thereby improving the efficiency of testing. However, such minimization may result in the minimized test suite with low test coverage, low fault revealing capability, low priority test cases, and require more time than the allowed testing budget (e.g., time) as compared to the original test suite. To deal with the above issues, we formulated the minimization problem as a search problem and defined a fitness function considering various optimization objectives based on the above issues. To assess the performance of our fitness function, we conducted an extensive empirical evaluation by investigating the fitness function with three weight-based Genetic Algorithms (GAs) and seven multi-objective search algorithms using an industrial case study and 500 artificial problems inspired from the industrial case study. The results show that Random-Weighted Genetic Algorithm (RWGA) significantly outperforms the other algorithms since {\{}RWGA{\}} can balance all the objectives together by dynamically updating weights during each generation. Based on the results of our empirical evaluation, we also implemented a tool called {\{}TEst{\}} Minimization using Search Algorithms (TEMSA) to support test minimization using various search algorithms in the context of product lines. },
	Doi = {https://doi.org/10.1016/j.jss.2014.08.024},
	ISSN = {0164-1212},
	Keywords = {Product line,Search algorithm,Test suite minimization},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121214001757}
}

@Article{ISJ:ISJ393,
	Title = {{Assimilation of agile practices in use}},
	Author = {Wang, Xiaofeng and Conboy, Kieran and Pikkarainen, Minna},
	Journal = {Information Systems Journal},
	Year = {2012},
	Number = {6},
	Pages = {435--455},
	Volume = {22},
	Abstract = {Agile method use in information systems development (ISD) has grown dramatically in recent years. The emergence of these alternative approaches was very much industry-led at the outset, and while agile method research is growing, the vast majority of these studies are descriptive and often lack a strong theoretical and conceptual base. Insights from innovation adoption research can provide a new perspective on analysing agile method use. This paper is based on an exploratory study of the application of the innovation assimilation stages to understand the use of agile practices, focusing in particular on the later stages of assimilation, namely acceptance, routinisation and infusion. Four case studies were conducted, and based on the case study findings, the concepts of acceptance, routinisation and infusion were adapted and applied to agile software development. These adapted concepts were used to glean interesting insights into agile practice use. For example, it was shown that the period of use of agile practices does not have a proportional effect on their assimilation depths. We also reflected on the sequential assumption underlying the assimilation stages, showing that adopting teams do not always move through the assimilation stages in a linear manner.},
	Doi = {10.1111/j.1365-2575.2011.00393.x},
	ISSN = {1365-2575},
	Keywords = {acceptance,agile method,agile practice assimilation stages,assimilation stages,infusion,routinisation},
	Publisher = {Blackwell Publishing Ltd},
	Url = {http://dx.doi.org/10.1111/j.1365-2575.2011.00393.x}
}

@Article{Wang201341,
	Title = {{A PLA-based privacy-enhancing user modeling framework and its evaluation}},
	Author = {Wang, Y and Kobsa, A},
	Journal = {User Modeling and User-Adapted Interaction},
	Year = {2013},
	Number = {1},
	Pages = {41--82},
	Volume = {23},
	Abstract = {Reconciling personalization with privacy has been a continuing interest in user modeling research. This aim has computational, legal and behavioral/attitudinal ramifications. We present a dynamic privacy-enhancing user modeling framework that supports compliance with users' individual privacy preferences and with the privacy laws and regulations that apply to each user. The framework is based on a software product line architecture. It dynamically selects personalization methods during runtime that meet the current privacy constraints. Since dynamic architectural reconfiguration is typically resource-intensive, we conducted a performance evaluation with four implementations of our system that vary two factors. The results demonstrate that at least one implementation of our approach is technically feasible with comparatively modest additional resources, even for websites with the highest traffic today. To gauge user reactions to privacy controls that our framework enables, we also conducted a controlled experiment that allowed one group of users to specify privacy preferences and view the resulting effects on employed personalization methods. We found that users in this treatment group utilized this feature, deemed it useful, and had fewer privacy concerns as measured by higher disclosure of their personal data. {\textcopyright} 2012 Springer Science+Business Media B.V.},
	Annote = {cited By 3},
	Doi = {10.1007/s11257-011-9114-8},
	Keywords = {Compliance; Disclosure behavior; Performance evalu,Data privacy,Experiments; Laws and legislation; Mathematical m},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872381970{\&}doi=10.1007{\%}2Fs11257-011-9114-8{\&}partnerID=40{\&}md5=1a3a96d04f816a8a746cd93b932acbf8}
}

@Article{Weber2011467,
	Title = {{Refactoring large process model repositories}},
	Author = {Weber, Barbara and Reichert, Manfred and Mendling, Jan and Reijers, Hajo A},
	Journal = {Computers in Industry},
	Year = {2011},
	Number = {5},
	Pages = {467--486},
	Volume = {62},
	Abstract = {With the increasing adoption of process-aware information systems, large process model repositories have emerged. Typically, the models in such repositories are re-aligned to real-world events and demands through adaptation on a day-to-day basis. This bears the risk of introducing model redundancies and of unnecessarily increasing model complexity. If no continuous investment is made in keeping process models simple, changes will become more difficult and error-prone over time. Although refactoring techniques are widely used in software engineering to address similar problems, so far, no comparable state-of-the-art has evolved in the business process management domain. Process designers either have to refactor process models by hand or are simply unable to apply respective techniques at all. This paper proposes a catalogue of process model “smells? for identifying refactoring opportunities. In addition, it introduces a set of behavior-preserving techniques for refactoring large process repositories. The proposed refactorings enable process designers to effectively deal with model complexity by making process models better understandable and easier to maintain. The refactorings have been evaluated using large process repositories from the healthcare and automotive domain. To demonstrate the feasibility of the refactoring techniques, a proof-of-concept prototype has been implemented. },
	Doi = {https://doi.org/10.1016/j.compind.2010.12.012},
	ISSN = {0166-3615},
	Keywords = {Process model quality,Process model refactoring,Process model smell,Process-aware information system},
	Url = {http://www.sciencedirect.com/science/article/pii/S0166361510001843}
}

@Article{Weidlich20121885,
	Title = {{Propagating changes between aligned process models}},
	Author = {Weidlich, Matthias and Mendling, Jan and Weske, Mathias},
	Journal = {Journal of Systems and Software},
	Year = {2012},
	Number = {8},
	Pages = {1885--1898},
	Volume = {85},
	Abstract = {There is a wide variety of drivers for business process modelling initiatives, reaching from organisational redesign to the development of information systems. Consequently, a common business process is often captured in multiple models that overlap in content due to serving different purposes. Business process management aims at flexible adaptation to changing business needs. Hence, changes of business processes occur frequently and have to be incorporated in the respective process models. Once a process model is changed, related process models have to be updated accordingly, despite the fact that those process models may only be loosely coupled. In this article, we introduce an approach that supports change propagation between related process models. Given a change in one process model, we leverage the behavioural abstraction of behavioural profiles for corresponding activities in order to determine a change region in another model. Our approach is able to cope with changes in pairs of models that are not related by hierarchical refinement and show behavioural inconsistencies. We evaluate the applicability of our approach with two real-world process model collections. To this end, we either deduce change operations from different model revisions or rely on synthetic change operations. },
	Doi = {https://doi.org/10.1016/j.jss.2012.02.044},
	ISSN = {0164-1212},
	Keywords = {Behavioural analysis,Change propagation,Model synchronisation,Process model alignment},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121212000672}
}

@Article{Wendler20121317,
	Title = {{The maturity of maturity model research: A systematic mapping study}},
	Author = {Wendler, Roy},
	Journal = {Information and Software Technology},
	Year = {2012},
	Number = {12},
	Pages = {1317--1339},
	Volume = {54},
	Abstract = {Context Maturity models offer organizations a simple but effective possibility to measure the quality of their processes. Emerged out of software engineering, the application fields have widened and maturity model research is becoming more important. During the last two decades the publication amount steadily rose as well. Until today, no studies have been available summarizing the activities and results of the field of maturity model research. Objective The objective of this paper is to structure and analyze the available literature of the field of maturity model research to identify the state-of-the-art research as well as research gaps. Method A systematic mapping study was conducted. It included relevant publications of journals and {\{}IS{\}} conferences. Mapping studies are a suitable method for structuring a broad research field concerning research questions about contents, methods, and trends in the available publications. Results The mapping of 237 articles showed that current maturity model research is applicable to more than 20 domains, heavily dominated by software development and software engineering. The study revealed that most publications deal with the development of maturity models and empirical studies. Theoretical reflective publications are scarce. Furthermore, the relation between conceptual and design-oriented maturity model development was analyzed, indicating that there is still a gap in evaluating and validating developed maturity models. Finally, a comprehensive research framework was derived from the study results and implications for further research are given. Conclusion The mapping study delivers the first systematic summary of maturity model research. The categorization of available publications helps researchers gain an overview of the state-of-the-art research and current research gaps. The proposed research framework supports researchers categorizing their own projects. In addition, practitioners planning to use a maturity model may use the study as starting point to identify which maturity models are suitable for their domain and where limitations exist. },
	Annote = {Special Section on Software Reliability and Security},
	Doi = {https://doi.org/10.1016/j.infsof.2012.07.007},
	ISSN = {0950-5849},
	Keywords = {Design-oriented research,Maturity models,Software management,Systematic mapping study},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584912001334}
}

@Article{White20101094,
	Title = {{Automated diagnosis of feature model configurations}},
	Author = {White, J and Benavides, D and Schmidt, D C and Trinidad, P and Dougherty, B and Ruiz-Cortes, A},
	Journal = {Journal of Systems and Software},
	Year = {2010},
	Number = {7},
	Pages = {1094--1107},
	Volume = {83},
	Abstract = {Software product-lines (SPLs) are software platforms that can be readily reconfigured for different project requirements. A key part of an {\{}SPL{\}} is a model that captures the rules for reconfiguring the software. {\{}SPLs{\}} commonly use feature models to capture {\{}SPL{\}} configuration rules. Each {\{}SPL{\}} configuration is represented as a selection of features from the feature model. Invalid {\{}SPL{\}} configurations can be created due to feature conflicts introduced via staged or parallel configuration or changes to the constraints in a feature model. When invalid configurations are created, a method is needed to automate the diagnosis of the errors and repair the feature selections. This paper provides two contributions to research on automated configuration of SPLs. First, it shows how configurations and feature models can be transformed into constraint satisfaction problems to automatically diagnose errors and repair invalid feature selections. Second, it presents empirical results from diagnosing configuration errors in feature models ranging in size from 100 to 5,000 features. The results of our experiments show that our CSP-based diagnostic technique can scale up to models with thousands of features. },
	Annote = {{\{}SPLC{\}} 2008},
	Doi = {https://doi.org/10.1016/j.jss.2010.02.017},
	ISSN = {0164-1212},
	Keywords = {Configuration,Constraint satisfaction,Diagnosis,Optimization,Software product-lines},
	Url = {http://www.sciencedirect.com/science/article/pii/S016412121000049X}
}

@Article{White2014119,
	Title = {{Evolving feature model configurations in software product lines}},
	Author = {White, J and Galindo, J A and Saxena, T and Dougherty, B and Benavides, D and Schmidt, D C},
	Journal = {Journal of Systems and Software},
	Year = {2014},
	Number = {1},
	Pages = {119--136},
	Volume = {87},
	Abstract = {The increasing complexity and cost of software-intensive systems has led developers to seek ways of reusing software components across development projects. One approach to increasing software reusability is to develop a software product-line (SPL), which is a software architecture that can be reconfigured and reused across projects. Rather than developing software from scratch for a new project, a new configuration of the SPL is produced. It is hard, however, to find a configuration of an SPL that meets an arbitrary requirement set and does not violate any configuration constraints in the SPL. Existing research has focused on techniques that produce a configuration of an SPL in a single step. Budgetary constraints or other restrictions, however, may require multi-step configuration processes. For example, an aircraft manufacturer may want to produce a series of configurations of a plane over a span of years without exceeding a yearly budget to add features. This paper provides three contributions to the study of multi-step configuration for SPLs. First, we present a formal model of multi-step SPL configuration and map this model to constraint satisfaction problems (CSPs). Second, we show how solutions to these SPL configuration problems can be automatically derived with a constraint solver by mapping them to CSPs. Moreover, we show how feature model changes can be mapped to our approach in a multi-step scenario by using feature model drift. Third, we present empirical results demonstrating that our CSP-based reasoning technique can scale to SPL models with hundreds of features and multiple configuration steps. {\textcopyright} 2013 Elsevier Inc.},
	Annote = {cited By 11},
	Doi = {10.1016/j.jss.2013.10.010},
	Keywords = {Aircraft manufacturers; Budgetary constraints; Con,Budget control; Computer software reusability; So,Constraint satisfaction problems},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888645293{\&}doi=10.1016{\%}2Fj.jss.2013.10.010{\&}partnerID=40{\&}md5=8ba32b418ee9714829c54449d3e18c41}
}

@Article{SPE:SPE641,
	Title = {{Classifying product families using platform coverage and variation mechanisms}},
	Author = {Wijnstra, Jan Gerben},
	Journal = {Software: Practice and Experience},
	Year = {2005},
	Number = {5},
	Pages = {413--444},
	Volume = {35},
	Doi = {10.1002/spe.641},
	ISSN = {1097-024X},
	Keywords = {architecture,coverage,introduction strategy,platform,product family,variation mechanisms},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/spe.641}
}

@Article{Wijnstra2004111,
	Title = {{Evolving a product family in a changing context}},
	Author = {Wijnstra, J G},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2004},
	Pages = {111--128},
	Volume = {3014},
	Abstract = {The notion of software product families is becoming more and more popular, both in research and in industry. There is no single best product family approach that is suitable for all, since each product family has its unique context. Such a context comprises elements such as scope, organization, and business strategy. As these elements can change over time, the product family approach may have to evolve with them. In this paper we describe our ideas for a method to assess the variability approach of an existing product family, and to improve that approach to match the changing context. This is illustrated in a case study from the medical imaging domain. This product family in question started out with only a few family members, but over time, the growth in the number of different applications and new application domains have put higher variability demands on the family. These changes also require an evolution in the product family approach. We will describe the current product family approach and the changing requirements on this approach. We also performed a partially automated analysis of the variation to give us a good overview of the way variation is currently handled in the system. Based on that, a direction for evolving the product family approach is proposed. {\textcopyright} Springer-Verlag Berlin Heidelberg 2004.},
	Annote = {cited By 5},
	Keywords = {Architecture; Artificial intelligence; Computer sc,Automated analysis; Business strategy; Evolution;,Medical imaging},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646178917{\&}partnerID=40{\&}md5=e3cfbc922130b5306062cd710b4b2adb}
}

@Article{SPIP:SPIP223,
	Title = {{An evaluation of CMMI process areas for small- to medium-sized software development organisations}},
	Author = {Wilkie, F G and McFall, D and McCaffery, F},
	Journal = {Software Process: Improvement and Practice},
	Year = {2005},
	Number = {2},
	Pages = {189--201},
	Volume = {10},
	Abstract = {In this article, we describe the results of CMMI software process appraisal work with six small- to medium-sized software development companies. Our analysis of six CMMI process areas appraised within each of these organisations is presented. Commonly practiced or not practiced elements of the model are identified, leading to the notion of perceived value associated with each specific CMMI practice. A finer-grained framework, which encompasses the notion of perceived value within specific practices, is presented. We argue that such a framework provides incentive to small- to medium-sized enterprises starting process improvement programmes. Copyright {\textcopyright} 2005 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/spip.223},
	ISSN = {1099-1670},
	Keywords = {CMMI,SME,light-weight appraisal,software engineering practices},
	Publisher = {John Wiley {\&} Sons, Ltd.},
	Url = {http://dx.doi.org/10.1002/spip.223}
}

@Article{Wille2016547,
	Title = {{Identifying variability in object-oriented code using model-based code mining}},
	Author = {Wille, D and Tiede, M and Schulze, S and Seidl, C and Schaefer, I},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2016},
	Pages = {547--562},
	Volume = {9953 LNCS},
	Abstract = {A large set of object-oriented programming (OOP) languages exists to realize software for different purposes. Companies often create variants of their existing software by copying and modifying them to changed requirements. While these so-called clone-and-own approaches allow to save money in short-term, they expose the company to severe risks regarding long-term evolution and product quality. The main reason is the high manual maintenance effort which is needed due to the unknown relations between variants. In this paper, we introduce a modelbased approach to identify variability information for OOP code, allowing companies to better understand and manage variability between their variants. This information allows to improve maintenance of the variants and to transition from single variant development to more elaborate reuse strategies such as software product lines. We demonstrate the applicability of our approach by means of a case study analyzing variants generated from an existing software product line and comparing our findings to the managed reuse strategy. {\textcopyright} Springer International Publishing AG 2016.},
	Annote = {cited By 0},
	Doi = {10.1007/978-3-319-47169-3_43},
	Keywords = {Codes (symbols); Computer software; Computer softw,Maintenance efforts; Model based approach; Model-,Object oriented programming},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84993995738{\&}doi=10.1007{\%}2F978-3-319-47169-3{\_}43{\&}partnerID=40{\&}md5=8c065ef5e07b363942ab4c1f1e3a46cc}
}

@Article{Williams201031,
	Title = {{Characterizing software architecture changes: A systematic review}},
	Author = {Williams, Byron J and Carver, Jeffrey C},
	Journal = {Information and Software Technology},
	Year = {2010},
	Number = {1},
	Pages = {31--51},
	Volume = {52},
	Abstract = {With today's ever increasing demands on software, software developers must produce software that can be changed without the risk of degrading the software architecture. One way to address software changes is to characterize their causes and effects. A software change characterization mechanism allows developers to characterize the effects of a change using different criteria, e.g. the cause of the change, the type of change that needs to be made, and the part of the system where the change must take place. This information then can be used to illustrate the potential impact of the change. This paper presents a systematic literature review of software architecture change characteristics. The results of this systematic review were used to create the Software Architecture Change Characterization Scheme (SACCS). This report addresses key areas involved in making changes to software architecture. SACCS's purpose is to identify the characteristics of a software change that will have an impact on the high-level software architecture. },
	Doi = {https://doi.org/10.1016/j.infsof.2009.07.002},
	ISSN = {0950-5849},
	Keywords = {Change characterization,Software architecture,Software changes,Software evolution,Software maintenance,Systematic review},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584909001207}
}

@Article{Wnuk2013921,
	Title = {{Obsolete software requirements}},
	Author = {Wnuk, Krzysztof and Gorschek, Tony and Zahda, Showayb},
	Journal = {Information and Software Technology},
	Year = {2013},
	Number = {6},
	Pages = {921--940},
	Volume = {55},
	Abstract = {AbstractContext Coping with rapid requirements change is crucial for staying competitive in the software business. Frequently changing customer needs and fierce competition are typical drivers of rapid requirements evolution resulting in requirements obsolescence even before project completion. Objective Although the obsolete requirements phenomenon and the implications of not addressing them are known, there is a lack of empirical research dedicated to understanding the nature of obsolete software requirements and their role in requirements management. Method In this paper, we report results from an empirical investigation with 219 respondents aimed at investigating the phenomenon of obsolete software requirements. Results Our results contain, but are not limited to, defining the phenomenon of obsolete software requirements, investigating how they are handled in industry today and their potential impact. Conclusion We conclude that obsolete software requirements constitute a significant challenge for companies developing software intensive products, in particular in large projects, and that companies rarely have processes for handling obsolete software requirements. Further, our results call for future research in creating automated methods for obsolete software requirements identification and management, methods that could enable efficient obsolete software requirements management in large projects. },
	Doi = {https://doi.org/10.1016/j.infsof.2012.12.001},
	ISSN = {0950-5849},
	Keywords = {Change impact analysis,Empirical study,Market driven requirements engineering,Obsolete requirements,Requirements management,Survey},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584912002364}
}

@Article{Wnuk2015647,
	Title = {{Exploring factors affecting decision outcome and lead time in large-scale requirements engineering}},
	Author = {Wnuk, K and Kabbedijk, J and Brinkkemper, S and Regnell, B and Callele, D},
	Journal = {Journal of Software: Evolution and Process},
	Year = {2015},
	Number = {9},
	Pages = {647--673},
	Volume = {27},
	Abstract = {Optimizing decision lead time and outcome is important for successful product management. This work identifies decision lead time and outcome factors in large-scale requirements engineering. Our investigation brings supporting evidence that complex changes have longer lead time and that important customers more likely get what they request. The results provide input into the discussion of whether a large company should focus on only a few of its large customers and disregard its significantly larger group of small customers. Lead time, defined as the duration between the moment a request was filed and the moment the decision was made, is an important aspect of decision making in market-driven requirements engineering. Minimizing lead time allows software companies to focus their resources on the most profitable functionality and enables them to remain competitive within the quickly changing software market. Achieving and sustaining low decision lead time and the resulting high decision efficiency require a better understanding of factors that may affect both decision lead time and outcome. In order to identify possible factors, we conducted an exploratory two-stage case study that combines the statistical analysis of seven possible relationships among decision characteristics at a large company with a survey of industry participants. Our results show that the number of products affected by a decision increases the time needed to make a decision. Practitioners should take this aspect into consideration when planning for efficient decision making and possibly reducing the complexity of decisions. Our results also show that when a change request originates from an important customer, the request is more often accepted. The results provide input into the discussion of whether a large company should focus on only a few of its large customers and disregard its significantly larger group of small customers. The results provide valuable insights for researchers, who can use them to plan research of decision-making processes and methods, and for practitioners, who can use them to optimize their decision-making processes. In future work, we plan to investigate other decision characteristics, such as the number of stakeholders involved in the discussion about the potential change or the number of dependencies between software components. {\textcopyright} 2015 John Wiley {\&} Sons, Ltd.},
	Annote = {cited By 0},
	Doi = {10.1002/smr.1721},
	Keywords = {Commerce; Requirements engineering; Sales; Surveys,Decision making,Decision making process; Decision outcome; Large-},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941654310{\&}doi=10.1002{\%}2Fsmr.1721{\&}partnerID=40{\&}md5=499f3f223a5e1a9a599d1570af30de82}
}

@InProceedings{Wnuk:2016:SMS:2915970.2915985,
	Title = {{A Systematic Mapping Study on Requirements Scoping}},
	Author = {Wnuk, Krzysztof and Kollu, Ravichandra Kumar},
	Booktitle = {Proceedings of the 20th International Conference on Evaluation and Assessment in Software Engineering},
	Year = {2016},
	Address = {New York, NY, USA},
	Pages = {32:1----32:11},
	Publisher = {ACM},
	Series = {EASE '16},
	Doi = {10.1145/2915970.2915985},
	ISBN = {978-1-4503-3691-8},
	Keywords = {requirements scoping,snowballing,systematic mapping study},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2915970.2915985}
}

@Article{Wnuk20141493,
	Title = {{Bridges and barriers to hardware-dependent software ecosystem participation – A case study}},
	Author = {Wnuk, Krzysztof and Runeson, Per and Lantz, Matilda and Weijden, Oskar},
	Journal = {Information and Software Technology},
	Year = {2014},
	Number = {11},
	Pages = {1493--1507},
	Volume = {56},
	Abstract = {abstractBackground Software ecosystems emerged as means for several actors to jointly provide more value to the market than any of them can do on its own. Recently, software ecosystems are more often used to support the development of hardware-dependent solutions. Objectives This work aims at studying barriers and bridges to participation in an ecosystem with substantial hardware dependencies. Method We conducted an interview-based case study of an ecosystem around Axis' network video surveillance systems, interviewing 10 internal experts and 8 external representatives of 6 companies, complemented by document studies at Axis. Results Major bridges to the ecosystem include end customer demands, open and transparent communication and relationship, as well as internal and external standardizations. Barriers include the two-tier business model, entry barriers and execution performance issues. Approximately half of the identified bridges and barriers could be considered hardware-dependent ecosystems specific. Conclusion Our results suggest that ecosystem leaders should share their sales channels with the ecosystem participants and focus on good communication and relationships as the dominant factors for the ecosystem participation. Moreover, we report that internal and external standardization can play a dual role, not only ease the development but also enable additional sales channels and new opportunities for the ecosystem participants. At the same time, the business model selected by the ecosystem leaders and performance, are identified as the main barriers to ecosystem participation. We believe that the business model barrier may be much more important for similar hardware-dependent software ecosystems. },
	Annote = {Special issue on Software Ecosystems},
	Doi = {https://doi.org/10.1016/j.infsof.2014.05.015},
	ISSN = {0950-5849},
	Keywords = {Business strategy,Empirical study,Hardware-dependent software ecosystem,Software ecosystems},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584914001323}
}

@Article{Wohlin20132594,
	Title = {{On the reliability of mapping studies in software engineering}},
	Author = {Wohlin, C and Runeson, P and {Da Mota Silveira Neto}, P A and Engstr{\"{o}}m, E and {Do Carmo Machado}, I and {De Almeida}, E S},
	Journal = {Journal of Systems and Software},
	Year = {2013},
	Number = {10},
	Pages = {2594--2610},
	Volume = {86},
	Abstract = {Background Systematic literature reviews and systematic mapping studies are becoming increasingly common in software engineering, and hence it becomes even more important to better understand the reliability of such studies. Objective This paper presents a study of two systematic mapping studies to evaluate the reliability of mapping studies and point out some challenges related to this type of study in software engineering. Method The research is based on an in-depth case study of two published mapping studies on software product line testing. Results We found that despite the fact that the two studies are addressing the same topic, there are quite a number of differences when it comes to papers included and in terms of classification of the papers included in the two mapping studies. Conclusions From this we conclude that although mapping studies are important, their reliability cannot simply be taken for granted. Based on the findings we also provide four conjectures that further research has to address to make secondary studies (systematic mapping studies and systematic literature reviews) even more valuable to both researchers and practitioners. {\textcopyright} 2013 Elsevier Inc.},
	Annote = {cited By 31},
	Doi = {10.1016/j.jss.2013.04.076},
	Keywords = {Mapping,Mapping studies; Software Product Line; Software p,Reliability; Research; Software design; Software},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882695716{\&}doi=10.1016{\%}2Fj.jss.2013.04.076{\&}partnerID=40{\&}md5=097cdd6ea83b702f2c96adbfb2e4edda}
}

@Article{Wong2012567,
	Title = {{The ABS tool suite: Modelling, executing and analysing distributed adaptable object-oriented systems}},
	Author = {Wong, P Y H and Albert, E and Muschevici, R and Proen{\c{c}}a, J and Sch{\"{a}}fer, J and Schlatte, R},
	Journal = {International Journal on Software Tools for Technology Transfer},
	Year = {2012},
	Number = {5},
	Pages = {567--588},
	Volume = {14},
	Abstract = {Modern software systems must support a high degree of variability to accommodate a wide range of requirements and operating conditions. This paper introduces the Abstract Behavioural Specification (ABS) language and tool suite, a comprehensive platform for developing and analysing highly adaptable distributed concurrent software systems. The ABS language has a hybrid functional and object- oriented core, and comes with extensions that support the development of systems that are adaptable to diversified requirements, yet capable to maintain a high level of trustworthiness. Using ABS, system variability is consistently traceable from the level of requirements engineering down to object behaviour. This facilitates temporal evolution, as changes to the required set of features of a system are automatically reflected by functional adaptation of the system's behaviour. The analysis capabilities of ABS stretch from debugging, observing and simulating to resource analysis of ABS models and help ensure that a system will remain dependable throughout its evolutionary lifetime. We report on the experience of using the ABS language and the ABS tool suite in an industrial case study. {\textcopyright} 2012 Springer-Verlag.},
	Annote = {cited By 17},
	Doi = {10.1007/s10009-012-0250-1},
	Keywords = {Computer software; Industrial applications,Concurrency; Feature modelling; Formal modelling;,Program debugging},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866277516{\&}doi=10.1007{\%}2Fs10009-012-0250-1{\&}partnerID=40{\&}md5=b359c6bba10bf37ad3cd65f572643aba}
}

@Article{Wong201249,
	Title = {{Modelling adaptable distributed object oriented systems using the HATS approach: A fredhopper case study}},
	Author = {Wong, P Y H and Diakov, N and Schaefer, I},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2012},
	Pages = {49--66},
	Volume = {7421 LNCS},
	Abstract = {The HATS project aims at developing a model-centric engineering methodology for the design, implementation and verification of distributed, concurrent and highly configurable systems. Such systems also have high demands on their dependability and trustworthiness. The HATS approach is centered around the Abstract Behavioural Specification modelling language (ABS) and its accompanying tools suite. The HATS approach allows the precise specification and analysis of the abstract behaviour of distributed software systems and their variability. The HATS project measures its success by applying its framework not only to toy examples, but to real industrial scenarios. In this paper, we evaluate the HATS approach for modelling an industrial scale case study provided by the eCommerce company Fredhopper. In this case study we consider Fredhopper Access Server (FAS). We model the commonality and variability of FAS's replication system using the ABS language and provide an evaluation based on our experience. {\textcopyright} 2012 Springer-Verlag Berlin Heidelberg.},
	Annote = {cited By 3},
	Doi = {10.1007/978-3-642-31762-0_5},
	Keywords = {Commonality and variability; Configurable systems;,Industrial applications; Research; Specifications,Verification},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864860105{\&}doi=10.1007{\%}2F978-3-642-31762-0{\_}5{\&}partnerID=40{\&}md5=eb2461c92499b0e064e3a2d26fba502b}
}

@Article{Wu2011135,
	Title = {{Architecture evolution in software product line: An industrial case study}},
	Author = {Wu, Y and Peng, X and Zhao, W},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2011},
	Pages = {135--150},
	Volume = {6727 LNCS},
	Abstract = {A software product line (SPL) usually involves a shared set of core assets and a series of application products. To ensure consistency, the evolution of the core assets and all the application products should be coordinated and synchronized under a unified evolution process. Therefore, SPL evolution often involves cross-product propagation and synchronization besides application derivation based on core assets, presenting quite different characteristic from the evolution of individual software products. As software architectures, including the product line architecture (PLA) and application architectures, play a central role in SPL engineering and evolution, architecture-based evolution analysis is a natural way for analyzing and managing SPL evolution. In this paper, we explore common practices of architecture evolution and the rationale behind in industrial SPL development. To this end, we conduct a case study with Wingsoft examination system product line (WES-PL), an industrial product line with an evolution history of eight years and more than 10 application products. In the case study, we reviewed the evolution history of WES-PL architecture and analyzed several typical evolution cases. Based on the historical analysis, we identify some special problems in industrial SPL practice from the aspect of architecture evolution and summarize some useful experiences about SPL evolution decisions to complement classical SPL methodology. On the other hand, we also propose some possible improvements for the evolution management in WES-PL. {\textcopyright} 2011 Springer-Verlag.},
	Annote = {cited By 3},
	Doi = {10.1007/978-3-642-21347-2_11},
	Keywords = {Application architecture; Core asset; Evolution an,Computer software reusability; Industrial applica,Software architecture},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959631849{\&}doi=10.1007{\%}2F978-3-642-21347-2{\_}11{\&}partnerID=40{\&}md5=a23b7140cc27a369f02eb0788da985cc}
}

@Article{Wu2011119,
	Title = {{Recovering object-oriented framework for software product line reengineering}},
	Author = {Wu, Y and Yang, Y and Peng, X and Qiu, C and Zhao, W},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Year = {2011},
	Pages = {119--134},
	Volume = {6727 LNCS},
	Abstract = {A large number of software product lines (SPL) in practice are not constructed from scratch, but reengineered from legacy variant products. In order to transfer legacy products to SPL core assets, reverse variability analysis should be involved to find commonality and differences among variant artifacts. In this paper we concentrate on the recovery of SPL framework which can be represented by an object-oriented design model with variation points. We propose a semi-automatic SPL framework recovery approach with the assumption that involved legacy products have similar designs and implementations. In this approach, we adopt a bottom-up process based on clone detection and context analysis to identify corresponding mappings among design elements in different products. Then we use a top-down process from class level to method level with some heuristic rules to determine the commonality/variability classification and the variability type for each design element. In order to evaluate the effectiveness of our approach, we conduct a case study on an industrial product line and present comprehensive analysis and discussions on the results. {\textcopyright} 2011 Springer-Verlag.},
	Annote = {cited By 3},
	Doi = {10.1007/978-3-642-21347-2_10},
	Keywords = {Class level; Clone detection; Comprehensive analys,Cloning; Design; Heuristic methods; Network archi,Computer software reusability},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959663971{\&}doi=10.1007{\%}2F978-3-642-21347-2{\_}10{\&}partnerID=40{\&}md5=f1f9e1ce1c8ebf696009f7651a2c0e00}
}

@Article{Wu2011811,
	Title = {{An optimization model for reuse scenario selection considering reliability and cost in software product line development}},
	Author = {Wu, Z and Tang, J and Kwong, C K and Chan, C Y},
	Journal = {International Journal of Information Technology and Decision Making},
	Year = {2011},
	Number = {5},
	Pages = {811--841},
	Volume = {10},
	Abstract = {In this paper, a model that assists developers to evaluate and compare alternative reuse scenarios in software product line (SPL) development systematically in proposed. The model can identify basic activities (abstracted as operations) and precisely relate cost and reliability with each basic operation. A typical reuse mode is described from the perspectives of application and domain engineering. According to this scheme, six reuse modes are identified, and alternative industry reuse scenarios can be derived from these modes. A bi-objective 0-1 integer programming model is developed to help decision makers select reuse scenarios when they develop a SPL to minimize cost and maximize reliability while satisfying system requirements to a certain degree. This model is called the cost and reliability optimization under constraint satisfaction (CROS). To design the model efficiently, a three-phase algorithm for finding all efficient solutions is developed, where the first two phases can obtain an efficient solution, and the last phase can generate a nonsupported efficient solution. Two practical methods are presented to facilitate decision making on selecting from the entire range of efficient solutions in light of the decision-maker's preference for mancomputer interaction. An application of the CROS model in a mail server system development is presented as a case study. {\textcopyright} 2011 World Scientific Publishing Company.},
	Annote = {cited By 5},
	Doi = {10.1142/S0219622011004580},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052562807{\&}doi=10.1142{\%}2FS0219622011004580{\&}partnerID=40{\&}md5=a0262f562a9e74e50aa90712dde54c2d}
}

@Article{Wulf-Hadash2013354,
	Title = {{Constructing domain knowledge through cross product line analysis}},
	Author = {Wulf-Hadash, O and Reinhartz-Berger, I},
	Journal = {Lecture Notes in Business Information Processing},
	Year = {2013},
	Pages = {354--369},
	Volume = {147 LNBIP},
	Abstract = {Nowadays many companies develop and maintain families of systems, termed product lines (PL), rather than individual systems. Furthermore, due to increase in market competition and the dynamic nature of companies' emergence, several PLs may exist under the same roof. These PLs may be independently developed taking into consideration different sets of products and requirements. Thus the developed artifacts potentially have a different and partial view of the domain. Moreover, future development and maintenance of the different PLs may require consolidating the various artifacts into a single coherent one. In this work, we present a method for constructing domain knowledge through cross PL analysis. This method uses similarity metrics, text clustering, and mining techniques in order to create domain models and recommend on improvements to the existing PLs artifacts. Preliminary results reveal that the method outcomes reflect human perception of the examined domain. {\textcopyright} 2013 Springer-Verlag.},
	Annote = {cited By 2},
	Doi = {10.1007/978-3-642-38484-4_25},
	Keywords = {Competition; Systems analysis; Systems engineering,Domain analysis; Empirical evaluations; Feature c,Industry},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879855069{\&}doi=10.1007{\%}2F978-3-642-38484-4{\_}25{\&}partnerID=40{\&}md5=dd7ff26179d87acb7e982ace6933682d}
}

@Article{Xue20161215,
	Title = {{IBED: Combining IBEA and DE for optimal feature selection in software product line engineering}},
	Author = {Xue, Y and Zhong, J and Tan, T H and Liu, Y and Cai, W and Chen, M and Sun, J},
	Journal = {Applied Soft Computing Journal},
	Year = {2016},
	Pages = {1215--1231},
	Volume = {49},
	Abstract = {Software configuration, which aims to customize the software for different users (e.g., Linux kernel configuration), is an important and complicated task. In software product line engineering (SPLE), feature oriented domain analysis is adopted and feature model is used to guide the configuration of new product variants. In SPLE, product configuration is an optimal feature selection problem, which needs to find a set of features that have no conflicts and meanwhile achieve multiple design objectives (e.g., minimizing cost and maximizing the number of features). In previous studies, several multi-objective evolutionary algorithms (MOEAs) were used for the optimal feature selection problem and indicator-based evolutionary algorithm (IBEA) was proven to be the best MOEA for this problem. However, IBEA still suffers from the issues of correctness and diversity of found solutions. In this paper, we propose a dual-population evolutionary algorithm, named IBED, to achieve both correctness and diversity of solutions. In IBED, two populations are individually evolved with two different types of evolutionary operators, i.e., IBEA operators and differential evolution (DE) operators. Furthermore, we propose two enhancement techniques for existing MOEAs, namely the feedback-directed mechanism to fast find the correct solutions (e.g., solutions that satisfy the feature model constraints) and the preprocessing method to reduce the search space. Our empirical results have shown that IBED with the enhancement techniques can outperform several state-of-the-art MOEAs on most case studies in terms of correctness and diversity of found solutions. {\textcopyright} 2016 Elsevier B.V.},
	Annote = {cited By 0},
	Doi = {10.1016/j.asoc.2016.07.040},
	Keywords = {Computer operating systems; Computer software; Com,Differential Evolution; Differential evolutionary,Evolutionary algorithms},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997496779{\&}doi=10.1016{\%}2Fj.asoc.2016.07.040{\&}partnerID=40{\&}md5=3b3bec8813e78da624b567c95244b62e}
}

@Article{Yague2016184,
	Title = {{An exploratory study in communication in Agile Global Software Development}},
	Author = {Yag{\"{u}}e, Agustin and Garbajosa, Juan and D{\'{i}}az, Jessica and Gonz{\'{a}}lez, Eloy},
	Journal = {Computer Standards {\&} Interfaces},
	Year = {2016},
	Pages = {184--197},
	Volume = {48},
	Abstract = {Abstract Global software development (GSD) is gaining ever more relevance. Although communication is key in the exchange of information between team members, multi-site software development has introduced additional obstacles (different time-zones and cultures, {\{}IT{\}} infrastructure, etc.) and delays into the act of communication, which is already problematic. Communication is even more critical in the case of Agile Global Software Development (AGSD) in which communication plays a primary role. This paper reports an exploratory study of the effects of tools supporting communication in AGSD. More precisely, this paper analyses the perception of team members about communication infrastructures in AGSD. The research question to which this study responds concerns how development teams perceive the communication infrastructure while developing products using agile methodologies. Most previous studies have dealt with communication support from a highly technological media tool perspective. In this research work, instead, observations were obtained from three perspectives: communication among team members, communication of the status of the development process, and communication of the status of the progress of the product under development. It has been possible to show that team members perceive advantages to using media tools that make them feel in practice that teams are co-located, such as smartboards supported by efficient video-tools, and combining media tools with centralized repository tools, with information from the process development and product characteristics, that allow distributed teams to effectively share information about the status of the project/process/product during the development process in order to overcome some of the still existing problems in communication in AGSD. },
	Annote = {Special Issue on Information System in Distributed Environment},
	Doi = {https://doi.org/10.1016/j.csi.2016.06.002},
	ISSN = {0920-5489},
	Keywords = {Agile,Exploratory research,Global Distributed Software Development,Infrastructure,Tools and technologies},
	Url = {http://www.sciencedirect.com/science/article/pii/S0920548916300381}
}

@Article{CPE:CPE3367,
	Title = {{The SPD approach to deploy service-based applications in the cloud}},
	Author = {Yangui, Sami and Tata, Samir},
	Journal = {Concurrency and Computation: Practice and Experience},
	Year = {2015},
	Number = {15},
	Pages = {3943--3960},
	Volume = {27},
	Abstract = {Cloud computing is becoming more and more an essential component in the cyberspace system. In this context, Cloud providers offer a multitude of resources and services that constitute a heterogeneous ecosystem and a new economic model. However, several issues remain to be addressed so that the Cloud becomes an efficient part of the cyber-physical system. In this paper, we propose to address a platform as-a-service (PaaS) drawback that we noted as part of our work on service-based applications deployment in Cloud platforms. To deal with that, we propose a new approach that we called SPD approach to provision appropriate platform resources and ensure hosting and deployment of applications based on Service Component Architecture or business process orchestration (Business Process Execution Language) specifications in Cloud platforms. The SPD approach stands for the three main steps of the process that we have defined, that is, (1) slice the application deployables into a set of elementary services, (2) package them into our already developed service micro-containers, and then (3) deploy the obtained service micro-containers in a target PaaS provider. Our approach is a PaaS-independent approach that allows developers to deploy service-based applications on any PaaS regardless of its capabilities. To illustrate our approach and to experiment its performances, we provide two realistic use cases scenarios, that is, a Service Component Architecture-based application and a Business Process Execution Language-based process deployment in Cloud Foundry PaaS. Copyright {\textcopyright} 2014 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/cpe.3367},
	ISSN = {1532-0634},
	Keywords = {BPEL,SCA,cloud resource provisioning,service deployment,service micro-container},
	Publisher = {John Wiley {\&} Sons, Ltd},
	Url = {http://dx.doi.org/10.1002/cpe.3367}
}

@InProceedings{Yokomori:2011:MEA:1960275.1960301,
	Title = {{Measuring the Effects of Aspect-oriented Refactoring on Component Relationships: Two Case Studies}},
	Author = {Yokomori, Reishi and Siy, Harvey and Yoshida, Norihiro and Noro, Masami and Inoue, Katsuro},
	Booktitle = {Proceedings of the Tenth International Conference on Aspect-oriented Software Development},
	Year = {2011},
	Address = {New York, NY, USA},
	Pages = {215--226},
	Publisher = {ACM},
	Series = {AOSD '11},
	Doi = {10.1145/1960275.1960301},
	ISBN = {978-1-4503-0605-8},
	Keywords = {aspect-oriented programming,code clone analysis,coupling,refactoring,use-relation analysis},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/1960275.1960301}
}

@Article{Yongsiriwit2016168,
	Title = {{A semantic framework for configurable business process as a service in the cloud}},
	Author = {Yongsiriwit, Karn and Assy, Nour and Gaaloul, Walid},
	Journal = {Journal of Network and Computer Applications},
	Year = {2016},
	Pages = {168--184},
	Volume = {59},
	Abstract = {Abstract With the advent of Cloud Computing, new opportunities for Business Process Outsourcing services have emerged. Business Process as a Service (BPaaS), a new cloud service model, has recently gained a great importance for outsourcing cloud-based business processes constructed for multi-tenancy. In such a multi-tenant environment, using configurable business process models enables the sharing of a reference process among different tenants that can be customized according to specific needs. With a large choice of configurable process modeling languages, different providers may deliver configurable processes with common functionalities but different representations which makes the process discovery and configuration a manual tedious task. This in turn creates cloud silos and vendors lock-in with non-reusable configurable {\{}BPaaS{\}} models. Therefore, with the aim of enabling the interoperability between multiple {\{}BPaaS{\}} providers, we propose in this paper a semantic framework for {\{}BPaaS{\}} configurable models. Taking advantage of Semantic Web technologies and data mining techniques, our framework allows for (1) an ontology-based high level abstract representation of {\{}BPaaS{\}} configurable models enriched with configuration guidelines and (2) an automated approach for extracting the configuration guidelines from existing process repositories. To show the feasibility and effectiveness of our approach, we extend Signavio with our semantic framework and conduct experiments on a dataset from {\{}SAP{\}} reference model. },
	Doi = {https://doi.org/10.1016/j.jnca.2015.07.007},
	ISSN = {1084-8045},
	Keywords = {BPaaS,Business Process as a Service,Cloud Computing,Configurable process model,Green {\{}IT{\}},Semantic technology},
	Url = {http://www.sciencedirect.com/science/article/pii/S1084804515001708}
}

@Article{Yu2015533,
	Title = {{Model-driven development of adaptive web service processes with aspects and rules}},
	Author = {Yu, Jian and Sheng, Quan Z and Swee, Joshua K Y and Han, Jun and Liu, Chengfei and Noor, Talal H},
	Journal = {Journal of Computer and System Sciences},
	Year = {2015},
	Number = {3},
	Pages = {533--552},
	Volume = {81},
	Abstract = {Abstract Modern software systems are frequently required to be adaptive in order to cope with constant changes. Unfortunately, service-oriented systems built with WS-BPEL are still too rigid. In this paper, we propose a novel model-driven approach to supporting the development of dynamically adaptive WS-BPEL based systems. We model the system functionality with two distinct but highly correlated parts: a stable part called the base model describing the flow logic aspect and a volatile part called the variable model describing the decision logic aspect. We develop an aspect-oriented method to weave the base model and the variable model together so that runtime changes can be applied to the variable model without affecting the base model. A model-driven platform has been implemented to support the development of adaptive WS-BPEL processes. In-lab experiments show that our approach has low performance overhead. A real-life case study also validates the applicability of our approach. },
	Annote = {Special Issue on selected papers from the 4th International Conference on Ambient Systems, Networks and Technologies (ANT 2013)},
	Doi = {https://doi.org/10.1016/j.jcss.2014.11.008},
	ISSN = {0022-0000},
	Keywords = {Adaptive systems,Aspect-oriented methodology,Design tools and techniques,Model-driven development,Web services},
	Url = {http://www.sciencedirect.com/science/article/pii/S0022000014001494}
}

@Article{GarousiYusifoğlu2015123,
	Title = {{Software test-code engineering: A systematic mapping}},
	Author = {Yusifoğlu, Vahid Garousi and Amannejad, Yasaman and Can, Aysu Betin},
	Journal = {Information and Software Technology},
	Year = {2015},
	Pages = {123--147},
	Volume = {58},
	Abstract = {AbstractContext As a result of automated software testing, large amounts of software test code (script) are usually developed by software teams. Automated test scripts provide many benefits, such as repeatable, predictable, and efficient test executions. However, just like any software development activity, development of test scripts is tedious and error prone. We refer, in this study, to all activities that should be conducted during the entire lifecycle of test-code as Software Test-Code Engineering (STCE). Objective As the {\{}STCE{\}} research area has matured and the number of related studies has increased, it is important to systematically categorize the current state-of-the-art and to provide an overview of the trends in this field. Such summarized and categorized results provide many benefits to the broader community. For example, they are valuable resources for new researchers (e.g., PhD students) aiming to conduct additional secondary studies. Method In this work, we systematically classify the body of knowledge related to {\{}STCE{\}} through a systematic mapping (SM) study. As part of this study, we pose a set of research questions, define selection and exclusion criteria, and systematically develop and refine a systematic map. Results Our study pool includes a set of 60 studies published in the area of {\{}STCE{\}} between 1999 and 2012. Our mapping data is available through an online publicly-accessible repository. We derive the trends for various aspects of STCE. Among our results are the following: (1) There is an acceptable mix of papers with respect to different contribution facets in the field of {\{}STCE{\}} and the top two leading facets are tool (68{\%}) and method (65{\%}). The studies that presented new processes, however, had a low rate (3{\%}), which denotes the need for more process-related studies in this area. (2) Results of investigation about research facet of studies and comparing our result to other {\{}SM{\}} studies shows that, similar to other fields in software engineering, {\{}STCE{\}} is moving towards more rigorous validation approaches. (3) A good mixture of {\{}STCE{\}} activities has been presented in the primary studies. Among them, the two leading activities are quality assessment and co-maintenance of test-code with production code. The highest growth rate for co-maintenance activities in recent years shows the importance and challenges involved in this activity. (4) There are two main categories of quality assessment activity: detection of test smells and oracle assertion adequacy. (5) {\{}JUnit{\}} is the leading test framework which has been used in about 50{\%} of the studies. (6) There is a good mixture of {\{}SUT{\}} types used in the studies: academic experimental systems (or simple code examples), real open-source and commercial systems. (7) Among 41 tools that are proposed for STCE, less than half of the tools (45{\%}) were available for download. It is good to have this percentile of tools to be available, although not perfect, since the availability of tools can lead to higher impact on research community and industry. Conclusion We discuss the emerging trends in STCE, and discuss the implications for researchers and practitioners in this area. The results of our systematic mapping can help researchers to obtain an overview of existing {\{}STCE{\}} approaches and spot areas in the field that require more attention from the research community. },
	Doi = {https://doi.org/10.1016/j.infsof.2014.06.009},
	ISSN = {0950-5849},
	Keywords = {Development of test code,Quality assessment of test code,Software test-code engineering,Study repository,Survey,Systematic mapping},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584914001487}
}

@Article{Zamli201657,
	Title = {{A Tabu Search hyper-heuristic strategy for t-way test suite generation}},
	Author = {Zamli, Kamal Z and Alkazemi, Basem Y and Kendall, Graham},
	Journal = {Applied Soft Computing},
	Year = {2016},
	Pages = {57--74},
	Volume = {44},
	Abstract = {Abstract This paper proposes a novel hybrid t-way test generation strategy (where t indicates interaction strength), called High Level Hyper-Heuristic (HHH). {\{}HHH{\}} adopts Tabu Search as its high level meta-heuristic and leverages on the strength of four low level meta-heuristics, comprising of Teaching Learning based Optimization, Global Neighborhood Algorithm, Particle Swarm Optimization, and Cuckoo Search Algorithm. {\{}HHH{\}} is able to capitalize on the strengths and limit the deficiencies of each individual algorithm in a collective and synergistic manner. Unlike existing hyper-heuristics, {\{}HHH{\}} relies on three defined operators, based on improvement, intensification and diversification, to adaptively select the most suitable meta-heuristic at any particular time. Our results are promising as {\{}HHH{\}} manages to outperform existing t-way strategies on many of the benchmarks. },
	Doi = {https://doi.org/10.1016/j.asoc.2016.03.021},
	ISSN = {1568-4946},
	Keywords = {Cuckoo Search Algorithm,Global Neighborhood Algorithm,Hyper-heuristic,Particle Swarm Optimization,Software testing,Teaching Learning based Optimization,t-way Testing},
	Url = {http://www.sciencedirect.com/science/article/pii/S1568494616301302}
}

@Article{Zanardini2016173,
	Title = {{Resource–usage–aware configuration in software product lines}},
	Author = {Zanardini, Damiano and Albert, Elvira and Villela, Karina},
	Journal = {Journal of Logical and Algebraic Methods in Programming},
	Year = {2016},
	Number = {1, Part 2},
	Pages = {173--199},
	Volume = {85},
	Abstract = {Abstract Deriving concrete products from a product-line infrastructure requires resolving the variability captured in the product line, based on the company market strategy or requirements from specific customers. Selecting the most appropriate set of features for a product is a complex task, especially if quality requirements have to be considered. Resource–usage–aware configuration aims at providing awareness of resource–usage properties of artifacts throughout the configuration process. This article envisages several strategies for resource–usage–aware configuration which feature different performance and efficiency trade-offs. The common idea in all strategies is the use of resource–usage estimates obtained by an off-the-shelf static resource–usage analyzer as a heuristic for choosing among different candidate configurations. We report on a prototype implementation of the most practical strategies for resource–usage–aware configuration and apply it on an industrial case study. },
	Annote = {Formal Methods for Software Product Line Engineering},
	Doi = {https://doi.org/10.1016/j.jlamp.2015.08.003},
	ISSN = {2352-2208},
	Url = {http://www.sciencedirect.com/science/article/pii/S2352220815000814}
}

@Article{Zdravkovic201571,
	Title = {{Capturing consumer preferences as requirements for software product lines}},
	Author = {Zdravkovic, J and Svee, E.-O. and Giannoulis, C},
	Journal = {Requirements Engineering},
	Year = {2015},
	Number = {1},
	Pages = {71--90},
	Volume = {20},
	Abstract = {Delivering great consumer experiences in competitive market conditions requires software vendors to move away from traditional modes of thinking to an outside-in perspective, one that shifts their business to becoming consumer-centric. Requirements engineers operating in these conditions thus need new means to both capture real preferences of consumers and then relate them to requirements for software customized in different ways to fit anyone. Additionally, because system development models require inputs that are more concrete than abstract, the indistinct values of consumers need to be classified and formalized. To address this challenge, this study aims to establish a conceptual link between preferences of consumers and system requirements, using software product line (SPL) as a means for systematically accommodating the variations within the preferences. The novelty of this study is a conceptual model of consumer preference, which integrates generic value frameworks from both psychology and marketing, and a method for its transformation to requirements for SPL using a goal-oriented RE framework as the mediator. The presented artifacts are grounded in an empirical study related to the development of a system for online education. {\textcopyright} 2013, The Author(s).},
	Annote = {cited By 4},
	Doi = {10.1007/s00766-013-0187-2},
	Keywords = {Commerce; Computer software; Distance education; R,Consumer value; Features; Goal modeling; Requirem,Software design},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924223166{\&}doi=10.1007{\%}2Fs00766-013-0187-2{\&}partnerID=40{\&}md5=427f356ae79e2831846fe0a519b4f852}
}

@Article{Zdun200656,
	Title = {{Tailorable language for behavioral composition and configuration of software components}},
	Author = {Zdun, Uwe},
	Journal = {Computer Languages, Systems {\&} Structures},
	Year = {2006},
	Number = {1},
	Pages = {56--82},
	Volume = {32},
	Abstract = {Many software systems suffer from missing support for behavioral (runtime) composition and configuration of software components. The concern “behavioral composition and configuration? is not treated as a first-class entity, but instead it is hard-coded in different programming styles, leading to tangled composition and configuration code that is hard to understand and maintain. We propose to embed a dynamic language with a tailorable object and class concept into the host language in which the components are written, and use the tailorable language for behavioral composition and configuration tasks. Using this approach we can separate the concerns “behavioral composition and configuration? from the rest of the software system, leading to a more reusable, understandable, and maintainable composition and configuration of software components. },
	Doi = {https://doi.org/10.1016/j.cl.2005.04.001},
	ISSN = {1477-8424},
	Keywords = {Component composition,Component configuration,Software components,Tailorable language},
	Url = {http://www.sciencedirect.com/science/article/pii/S1477842405000205}
}

@InProceedings{Zeller:2011:FFW:2020390.2020395,
	Title = {{Failure is a Four-letter Word: A Parody in Empirical Research}},
	Author = {Zeller, Andreas and Zimmermann, Thomas and Bird, Christian},
	Booktitle = {Proceedings of the 7th International Conference on Predictive Models in Software Engineering},
	Year = {2011},
	Address = {New York, NY, USA},
	Pages = {5:1----5:7},
	Publisher = {ACM},
	Series = {Promise '11},
	Doi = {10.1145/2020390.2020395},
	ISBN = {978-1-4503-0709-3},
	Keywords = {empirical research,parody},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2020390.2020395}
}

@Article{Zhang2014365,
	Title = {{Quality attribute modeling and quality aware product configuration in software product lines}},
	Author = {Zhang, G and Ye, H and Lin, Y},
	Journal = {Software Quality Journal},
	Year = {2014},
	Number = {3},
	Pages = {365--401},
	Volume = {22},
	Abstract = {In software product line engineering, the customers mostly concentrate on the functionalities of the target product during product configuration. The quality attributes of a target product, such as security and performance, are often assessed until the final product is generated. However, it might be very costly to fix the problem if it is found that the generated product cannot satisfy the customers' quality requirements. Although the quality of a generated product will be affected by all the life cycles of product development, feature-based product configuration is the first stage where the estimation or prediction of the quality attributes should be considered. As we know, the key issue of predicting the quality attributes for a product configured from feature models is to measure the interdependencies between functional features and quality attributes. The current existing approaches have several limitations on this issue, such as requiring real products for the measurement or involving domain experts' efforts. To overcome these limitations, we propose a systematic approach of modeling quality attributes in feature models based on domain experts' judgments using the analytic hierarchical process (AHP) and conducting quality aware product configuration based on the captured quality knowledge. Domain experts' judgments are adapted to avoid generating the real products for quality evaluation, and AHP is used to reduce domain experts' efforts involved in the judgments. A prototype tool is developed to implement the concepts of the proposed approach, and a formal evaluation is carried out based on a large-scale case study. {\textcopyright} 2013, Springer Science+Business Media New York.},
	Annote = {cited By 2},
	Doi = {10.1007/s11219-013-9197-z},
	Url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875049200{\&}doi=10.1007{\%}2Fs11219-013-9197-z{\&}partnerID=40{\&}md5=ad822ef7f6b285eb153f76658a503708}
}

@InProceedings{Zhang:2011:UKS:2019136.2019172,
	Title = {{Using Knowledge-based Systems to Manage Quality Attributes in Software Product Lines}},
	Author = {Zhang, Guoheng and Ye, Huilin and Lin, Yuqing},
	Booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
	Year = {2011},
	Address = {New York, NY, USA},
	Pages = {32:1----32:7},
	Publisher = {ACM},
	Series = {SPLC '11},
	Doi = {10.1145/2019136.2019172},
	ISBN = {978-1-4503-0789-5},
	Keywords = {feature model,non-functional requirements,product configuration,quality attributes,software product line},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/2019136.2019172}
}

@Article{SMR:SMR516,
	Title = {{Toward trustworthy software process models: an exploratory study on transformable process modeling}},
	Author = {Zhang, He and Kitchenham, Barbara and Jeffery, Ross},
	Journal = {Journal of Software: Evolution and Process},
	Year = {2012},
	Number = {7},
	Pages = {741--763},
	Volume = {24},
	Abstract = {Software process modeling and simulation have become effective tools for support of software process management and improvement over the past two decades. They have recently been integrated into the Trustworthy Process Management Framework (TPMF) as the infrastructural components to facilitate the delivery of trustworthy software products. This paper proposes the concept of Trustworthy Software Process Models as inputs to TPMF and introduces transformable process modeling for supporting effective and productive development of trustworthy process models. Furthermore, this paper undertakes an exploratory study on process model transformation by investigating and comparing process modeling semantics between quantitative (e.g., System Dynamics, SD) and qualitative forms of modeling and simulation. By following the model transformation scheme, a quantitative continuous (SD) software evolution process model is successfully transformed into its qualitative form for simulation. The results present the different capabilities and performance between these two modeling paradigms, as well as the possible benefits and interesting perspectives of transformable process modeling. Copyright {\textcopyright} 2010 John Wiley {\&} Sons, Ltd.},
	Doi = {10.1002/smr.516},
	ISSN = {2047-7481},
	Keywords = {qualitative modeling and simulation,software evolution,software process modeling and simulation,system dynamics,transformable process modeling,trustworthy process models},
	Publisher = {John Wiley {\&} Sons, Ltd},
	Url = {http://dx.doi.org/10.1002/smr.516}
}

@InProceedings{Zhang:2010:VLD:1868328.1868350,
	Title = {{On the Value of Learning from Defect Dense Components for Software Defect Prediction}},
	Author = {Zhang, Hongyu and Nelson, Adam and Menzies, Tim},
	Booktitle = {Proceedings of the 6th International Conference on Predictive Models in Software Engineering},
	Year = {2010},
	Address = {New York, NY, USA},
	Pages = {14:1----14:9},
	Publisher = {ACM},
	Series = {PROMISE '10},
	Doi = {10.1145/1868328.1868350},
	ISBN = {978-1-4503-0404-7},
	Keywords = {ceiling effect,defect dense components,defect prediction,sampling},
	Url = {http://0-doi.acm.org.fama.us.es/10.1145/1868328.1868350}
}

@Article{Zhang2016,
	Title = {{STPSO: Optimal configuration for cloud environments}},
	Author = {Zhang, Hongxia and Wang, Fei and Zhang, Yang and Xu, Jiuyun},
	Journal = {China Communications},
	Year = {2016},
	Month = {oct},
	Number = {10},
	Pages = {198--208},
	Volume = {13},
	Doi = {10.1109/CC.2016.7733044},
	ISSN = {1673-5447},
	Url = {http://ieeexplore.ieee.org/document/7733044/}
}

@Article{Zhang2010723,
	Title = {{A classification and comparison of model checking software architecture techniques}},
	Author = {Zhang, Pengcheng and Muccini, Henry and Li, Bixin},
	Journal = {Journal of Systems and Software},
	Year = {2010},
	Number = {5},
	Pages = {723--744},
	Volume = {83},
	Abstract = {Software architecture specifications are used for many different purposes, such as documenting architectural decisions, predicting architectural qualities before the system is implemented, and guiding the design and coding process. In these contexts, assessing the architectural model as early as possible becomes a relevant challenge. Various analysis techniques have been proposed for testing, model checking, and evaluating performance based on architectural models. Among them, model checking is an exhaustive and automatic verification technique, used to verify whether an architectural specification conforms to expected properties. While model checking is being extensively applied to software architectures, little work has been done to comprehensively enumerate and classify these different techniques. The goal of this paper is to investigate the state-of-the-art in model checking software architectures. For this purpose, we first define the main activities in a model checking software architecture process. Then, we define a classification and comparison framework and compare model checking software architecture techniques according to it. },
	Doi = {https://doi.org/10.1016/j.jss.2009.11.709},
	ISSN = {0164-1212},
	Keywords = {Model checking,Software architecture},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121209003070}
}

@Article{Zhang2010198,
	Title = {{Enhancing intelligence and dependability of a product line enabled pervasive middleware}},
	Author = {Zhang, Weishan and Hansen, Klaus Marius and Kunz, Thomas},
	Journal = {Pervasive and Mobile Computing},
	Year = {2010},
	Number = {2},
	Pages = {198--217},
	Volume = {6},
	Abstract = {To provide good support for user-centered application scenarios in pervasive computing environments, pervasive middleware must react to context changes and prepare services accordingly. At the same time, pervasive middleware should provide extended dependability via self-management capabilities, to conduct self-diagnosis of possible malfunctions using the current runtime context, and self-configuration and self-adaptation when there are service mismatches. In this article, we present an approach to combine the power of {\{}BDI{\}} practical reasoning and OWL/SWRL ontologies theoretical reasoning in order to improve the intelligence of pervasive middleware, supported by a set of Self-Management Pervasive Service (SeMaPS) ontologies featuring dynamic context, complex context, and self-management rules modeling. In this approach, belief sets are enriched with the results of OWL/SWRL theoretical reasoning to derive beliefs that cannot be obtained directly or explicitly. This is demonstrated with agents negotiating sports appointments. To cope with self-management, the corresponding monitoring, configuration, adaptation and diagnosis rules are developed based on {\{}OWL{\}} and {\{}SWRL{\}} utilizing SeMaPS ontologies. Evaluations show this combined reasoning approach can perform well, and that Semantic Web-based self-management is promising for pervasive computing environments. },
	Annote = {Context Modelling, Reasoning and Management},
	Doi = {https://doi.org/10.1016/j.pmcj.2009.07.002},
	ISSN = {1574-1192},
	Keywords = {Middleware,OWL (Web Ontology Language),SWRL (Semantic Web Rule Language),Self-diagnosis,Self-management,XVCL (XML-based Variant Configuration Language),{\{}BDI{\}} (Belief-Desire-Intention) agents},
	Url = {http://www.sciencedirect.com/science/article/pii/S1574119209000637}
}

@Article{Zhao20081272,
	Title = {{A pattern language for designing e-business architecture}},
	Author = {Zhao, Liping and Macaulay, Linda and Adams, Jonathan and Verschueren, Paul},
	Journal = {Journal of Systems and Software},
	Year = {2008},
	Number = {8},
	Pages = {1272--1287},
	Volume = {81},
	Abstract = {The pattern language for e-business provides a holistic support for developing software architectures for the e-business domain. The pattern language contains four related pattern categories: Business Patterns, Integration Patterns, Application Patterns, and Runtime Patterns. These pattern categories organise an e-business architecture into three layers—business interaction, application infrastructure and middleware infrastructure—and provide reusable design solutions to these layers in a top–down decomposition fashion. Business and Integration Patterns partition the business interaction layer into a set of subsystems; Application Patterns provide a high-level application infrastructure for these subsystems and separate business abstractions from their software solutions; Runtime Patterns then define a middleware infrastructure for the subsystems and shield design solutions from their implementations. The paper describes, demonstrates and evaluates this pattern language. },
	Doi = {https://doi.org/10.1016/j.jss.2007.11.717},
	ISSN = {0164-1212},
	Keywords = {Architectural design,Pattern,Pattern languages,Software architecture,e-Business architecture},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121207003123}
}

@Article{Zhao2015370,
	Title = {{Toward SLA-constrained service composition: An approach based on a fuzzy linguistic preference model and an evolutionary algorithm}},
	Author = {Zhao, Xin and Shen, Liwei and Peng, Xin and Zhao, Wenyun},
	Journal = {Information Sciences},
	Year = {2015},
	Pages = {370--396},
	Volume = {316},
	Abstract = {Abstract In a market-oriented service computing environment, both back-end {\{}SLA{\}} (service level agreement) offers and front-end {\{}SLA{\}} requirements should be considered when performing service composition. In this paper, we address the optimization problem of SLA-constrained service composition and focus on the following issues: the difficulties related to preference definition and to weight assignment, the limitation of linear utility functions in identifying preferred skyline solutions, and the efficiency and scalability requirements of the optimization algorithm. We present a systematic approach based on a fuzzy preference model and on evolutionary algorithms. Specifically, we first model this multi-objective optimization problem using the weighted Tchebycheff distance rather than a linear utility function. We then present a fuzzy preference model for preference representation and weight assignment. In the model, a set of fuzzy linguistic preference terms and their properties are introduced for establishing consistent preference order of multiple QoS dimensions, and a weighting procedure is proposed to transform the preference into numeric weights. Finally, we present two evolutionary algorithms, i.e., single{\_}EA and hybrid{\_}EA, that implement different optimization objectives and that can be used in different {\{}SLA{\}} management scenarios for service composition. We conduct a set of experimental studies to evaluate the effectiveness of the proposed algorithms in determining the optimal solutions, and to evaluate their efficiency and scalability for different problem scales. },
	Annote = {Nature-Inspired Algorithms for Large Scale Global Optimization},
	Doi = {https://doi.org/10.1016/j.ins.2014.11.016},
	ISSN = {0020-0255},
	Keywords = {Evolutionary algorithm,Linguistic preference,Multi-objective optimization,SLA,Service composition,Weighted Tchebycheff distance},
	Url = {http://www.sciencedirect.com/science/article/pii/S002002551401086X}
}